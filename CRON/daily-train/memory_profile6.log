2018-04-29 11:06:32,338 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 11:06:32,342 - memory_profile6_log - INFO - date_generated: 
2018-04-29 11:06:32,342 - memory_profile6_log - INFO -  
2018-04-29 11:06:32,342 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 28, 11, 6, 32, 339000)]
2018-04-29 11:06:32,342 - memory_profile6_log - INFO - 

2018-04-29 11:06:32,342 - memory_profile6_log - INFO - using current date: 2018-04-29
2018-04-29 11:06:32,344 - memory_profile6_log - INFO - using start date: 2018-04-28 00:00:00
2018-04-29 11:06:32,344 - memory_profile6_log - INFO - using end date: 2018-04-28
2018-04-29 11:06:32,476 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 11:06:32,480 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-28
2018-04-29 11:06:36,414 - memory_profile6_log - INFO - size of df: 24.0 Byte
2018-04-29 11:06:36,415 - memory_profile6_log - INFO - 2018-04-28 data is empty!
2018-04-29 11:06:36,417 - memory_profile6_log - INFO - tframe for date: 2018-04-28 is empty
2018-04-29 11:06:36,418 - memory_profile6_log - INFO - len datalist: 0
2018-04-29 11:06:36,420 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 11:06:36,421 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:06:36,421 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:06:36,421 - memory_profile6_log - INFO - ================================================

2018-04-29 11:06:36,423 - memory_profile6_log - INFO -    289     87.0 MiB     87.0 MiB   @profile

2018-04-29 11:06:36,424 - memory_profile6_log - INFO -    290                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 11:06:36,424 - memory_profile6_log - INFO -    291     87.0 MiB      0.0 MiB       bq_client = client

2018-04-29 11:06:36,426 - memory_profile6_log - INFO -    292     87.0 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:06:36,427 - memory_profile6_log - INFO -    293                             

2018-04-29 11:06:36,427 - memory_profile6_log - INFO -    294     87.0 MiB      0.0 MiB       datalist = []

2018-04-29 11:06:36,428 - memory_profile6_log - INFO -    295     87.0 MiB      0.0 MiB       datalist_hist = []

2018-04-29 11:06:36,430 - memory_profile6_log - INFO -    296                             

2018-04-29 11:06:36,431 - memory_profile6_log - INFO -    297     87.0 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 11:06:36,433 - memory_profile6_log - INFO -    298     90.6 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 11:06:36,434 - memory_profile6_log - INFO -    299     90.6 MiB      3.6 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 11:06:36,436 - memory_profile6_log - INFO -    300     90.6 MiB      0.0 MiB           if tframe is not None:

2018-04-29 11:06:36,437 - memory_profile6_log - INFO -    301                                         if not tframe.empty:

2018-04-29 11:06:36,437 - memory_profile6_log - INFO -    302                                             X_split = np.array_split(tframe, 5)

2018-04-29 11:06:36,438 - memory_profile6_log - INFO -    303                                             logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 11:06:36,440 - memory_profile6_log - INFO -    304                                             logger.info("Appending history data...")

2018-04-29 11:06:36,446 - memory_profile6_log - INFO -    305                                             for ix in range(len(X_split)):

2018-04-29 11:06:36,448 - memory_profile6_log - INFO -    306                                                 # ~ loading history

2018-04-29 11:06:36,450 - memory_profile6_log - INFO -    307                                                 """

2018-04-29 11:06:36,451 - memory_profile6_log - INFO -    308                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 11:06:36,453 - memory_profile6_log - INFO -    309                                                 """

2018-04-29 11:06:36,459 - memory_profile6_log - INFO -    310                                                 logger.info("processing batch-%d", ix)

2018-04-29 11:06:36,460 - memory_profile6_log - INFO -    311                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 11:06:36,463 - memory_profile6_log - INFO -    312                                                 logger.info("creating list history data...")

2018-04-29 11:06:36,464 - memory_profile6_log - INFO -    313                                                 lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 11:06:36,464 - memory_profile6_log - INFO -    314                             

2018-04-29 11:06:36,467 - memory_profile6_log - INFO -    315                                                 logger.info("call history data...")

2018-04-29 11:06:36,469 - memory_profile6_log - INFO -    316                                                 h_frame = mh.loadDSHistory(lhistory)

2018-04-29 11:06:36,470 - memory_profile6_log - INFO -    317                             

2018-04-29 11:06:36,471 - memory_profile6_log - INFO -    318                                                 # me = os.getpid()

2018-04-29 11:06:36,473 - memory_profile6_log - INFO -    319                                                 # kill_proc_tree(me)

2018-04-29 11:06:36,474 - memory_profile6_log - INFO -    320                             

2018-04-29 11:06:36,480 - memory_profile6_log - INFO -    321                                                 logger.info("done collecting history data, appending now...")

2018-04-29 11:06:36,480 - memory_profile6_log - INFO -    322                                                 for m in h_frame:

2018-04-29 11:06:36,483 - memory_profile6_log - INFO -    323                                                     if m is not None:

2018-04-29 11:06:36,483 - memory_profile6_log - INFO -    324                                                         if len(m) > 0:

2018-04-29 11:06:36,484 - memory_profile6_log - INFO -    325                                                             datalist_hist.append(pd.DataFrame(m))

2018-04-29 11:06:36,486 - memory_profile6_log - INFO -    326                                                 del h_frame

2018-04-29 11:06:36,486 - memory_profile6_log - INFO -    327                                                 del lhistory

2018-04-29 11:06:36,490 - memory_profile6_log - INFO -    328                             

2018-04-29 11:06:36,490 - memory_profile6_log - INFO -    329                                             logger.info("Appending training data...")

2018-04-29 11:06:36,492 - memory_profile6_log - INFO -    330                                             datalist.append(tframe)

2018-04-29 11:06:36,493 - memory_profile6_log - INFO -    331                                     else: 

2018-04-29 11:06:36,493 - memory_profile6_log - INFO -    332     90.6 MiB      0.0 MiB               logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 11:06:36,493 - memory_profile6_log - INFO -    333     90.6 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 11:06:36,494 - memory_profile6_log - INFO -    334     90.6 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 11:06:36,496 - memory_profile6_log - INFO -    335                             

2018-04-29 11:06:36,497 - memory_profile6_log - INFO -    336     90.6 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 11:06:36,500 - memory_profile6_log - INFO - 


2018-04-29 11:10:03,549 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 11:10:03,552 - memory_profile6_log - INFO - date_generated: 
2018-04-29 11:10:03,552 - memory_profile6_log - INFO -  
2018-04-29 11:10:03,552 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 28, 11, 10, 3, 549000)]
2018-04-29 11:10:03,552 - memory_profile6_log - INFO - 

2018-04-29 11:10:03,552 - memory_profile6_log - INFO - using current date: 2018-04-29
2018-04-29 11:10:03,552 - memory_profile6_log - INFO - using start date: 2018-04-28 00:00:00
2018-04-29 11:10:03,553 - memory_profile6_log - INFO - using end date: 2018-04-28
2018-04-29 11:10:03,683 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 11:10:03,687 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-28
2018-04-29 11:10:07,240 - memory_profile6_log - INFO - size of df: 24.0 Byte
2018-04-29 11:10:07,240 - memory_profile6_log - INFO - 2018-04-28 data is empty!
2018-04-29 11:10:07,243 - memory_profile6_log - INFO - tframe for date: 2018-04-28 is empty
2018-04-29 11:10:07,244 - memory_profile6_log - INFO - len datalist: 0
2018-04-29 11:10:07,246 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 11:10:07,247 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:10:07,249 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:10:07,249 - memory_profile6_log - INFO - ================================================

2018-04-29 11:10:07,252 - memory_profile6_log - INFO -    289     87.1 MiB     87.1 MiB   @profile

2018-04-29 11:10:07,253 - memory_profile6_log - INFO -    290                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 11:10:07,253 - memory_profile6_log - INFO -    291     87.1 MiB      0.0 MiB       bq_client = client

2018-04-29 11:10:07,255 - memory_profile6_log - INFO -    292     87.1 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:10:07,256 - memory_profile6_log - INFO -    293                             

2018-04-29 11:10:07,257 - memory_profile6_log - INFO -    294     87.1 MiB      0.0 MiB       datalist = []

2018-04-29 11:10:07,259 - memory_profile6_log - INFO -    295     87.1 MiB      0.0 MiB       datalist_hist = []

2018-04-29 11:10:07,259 - memory_profile6_log - INFO -    296                             

2018-04-29 11:10:07,262 - memory_profile6_log - INFO -    297     87.1 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 11:10:07,263 - memory_profile6_log - INFO -    298     90.6 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 11:10:07,266 - memory_profile6_log - INFO -    299     90.6 MiB      3.5 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 11:10:07,266 - memory_profile6_log - INFO -    300     90.6 MiB      0.0 MiB           if tframe is not None:

2018-04-29 11:10:07,266 - memory_profile6_log - INFO -    301                                         if not tframe.empty:

2018-04-29 11:10:07,267 - memory_profile6_log - INFO -    302                                             X_split = np.array_split(tframe, 5)

2018-04-29 11:10:07,269 - memory_profile6_log - INFO -    303                                             logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 11:10:07,269 - memory_profile6_log - INFO -    304                                             logger.info("Appending history data...")

2018-04-29 11:10:07,273 - memory_profile6_log - INFO -    305                                             for ix in range(len(X_split)):

2018-04-29 11:10:07,273 - memory_profile6_log - INFO -    306                                                 # ~ loading history

2018-04-29 11:10:07,275 - memory_profile6_log - INFO -    307                                                 """

2018-04-29 11:10:07,276 - memory_profile6_log - INFO -    308                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 11:10:07,278 - memory_profile6_log - INFO -    309                                                 """

2018-04-29 11:10:07,279 - memory_profile6_log - INFO -    310                                                 logger.info("processing batch-%d", ix)

2018-04-29 11:10:07,282 - memory_profile6_log - INFO -    311                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 11:10:07,286 - memory_profile6_log - INFO -    312                                                 logger.info("creating list history data...")

2018-04-29 11:10:07,286 - memory_profile6_log - INFO -    313                                                 lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 11:10:07,288 - memory_profile6_log - INFO -    314                             

2018-04-29 11:10:07,289 - memory_profile6_log - INFO -    315                                                 logger.info("call history data...")

2018-04-29 11:10:07,289 - memory_profile6_log - INFO -    316                                                 h_frame = mh.loadDSHistory(lhistory)

2018-04-29 11:10:07,290 - memory_profile6_log - INFO -    317                             

2018-04-29 11:10:07,290 - memory_profile6_log - INFO -    318                                                 # me = os.getpid()

2018-04-29 11:10:07,292 - memory_profile6_log - INFO -    319                                                 # kill_proc_tree(me)

2018-04-29 11:10:07,292 - memory_profile6_log - INFO -    320                             

2018-04-29 11:10:07,298 - memory_profile6_log - INFO -    321                                                 logger.info("done collecting history data, appending now...")

2018-04-29 11:10:07,299 - memory_profile6_log - INFO -    322                                                 for m in h_frame:

2018-04-29 11:10:07,299 - memory_profile6_log - INFO -    323                                                     if m is not None:

2018-04-29 11:10:07,301 - memory_profile6_log - INFO -    324                                                         if len(m) > 0:

2018-04-29 11:10:07,302 - memory_profile6_log - INFO -    325                                                             datalist_hist.append(pd.DataFrame(m))

2018-04-29 11:10:07,302 - memory_profile6_log - INFO -    326                                                 del h_frame

2018-04-29 11:10:07,303 - memory_profile6_log - INFO -    327                                                 del lhistory

2018-04-29 11:10:07,305 - memory_profile6_log - INFO -    328                             

2018-04-29 11:10:07,306 - memory_profile6_log - INFO -    329                                             logger.info("Appending training data...")

2018-04-29 11:10:07,308 - memory_profile6_log - INFO -    330                                             datalist.append(tframe)

2018-04-29 11:10:07,309 - memory_profile6_log - INFO -    331                                     else: 

2018-04-29 11:10:07,309 - memory_profile6_log - INFO -    332     90.6 MiB      0.0 MiB               logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 11:10:07,311 - memory_profile6_log - INFO -    333     90.6 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 11:10:07,311 - memory_profile6_log - INFO -    334     90.6 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 11:10:07,312 - memory_profile6_log - INFO -    335                             

2018-04-29 11:10:07,312 - memory_profile6_log - INFO -    336     90.6 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 11:10:07,313 - memory_profile6_log - INFO - 


2018-04-29 11:11:47,276 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 11:11:47,279 - memory_profile6_log - INFO - date_generated: 
2018-04-29 11:11:47,279 - memory_profile6_log - INFO -  
2018-04-29 11:11:47,279 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 28, 11, 11, 47, 277000)]
2018-04-29 11:11:47,279 - memory_profile6_log - INFO - 

2018-04-29 11:11:47,279 - memory_profile6_log - INFO - using current date: 2018-04-29
2018-04-29 11:11:47,280 - memory_profile6_log - INFO - using start date: 2018-04-28 00:00:00
2018-04-29 11:11:47,280 - memory_profile6_log - INFO - using end date: 2018-04-28
2018-04-29 11:11:47,424 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 11:11:47,427 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-28
2018-04-29 11:11:51,010 - memory_profile6_log - INFO - size of df: 24.0 Byte
2018-04-29 11:11:51,012 - memory_profile6_log - INFO - 2018-04-28 data is empty!
2018-04-29 11:11:51,013 - memory_profile6_log - INFO - tframe for date: 2018-04-28 is empty
2018-04-29 11:11:51,015 - memory_profile6_log - INFO - len datalist: 0
2018-04-29 11:11:51,016 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 11:11:51,017 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:11:51,019 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:11:51,019 - memory_profile6_log - INFO - ================================================

2018-04-29 11:11:51,023 - memory_profile6_log - INFO -    289     86.8 MiB     86.8 MiB   @profile

2018-04-29 11:11:51,025 - memory_profile6_log - INFO -    290                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 11:11:51,026 - memory_profile6_log - INFO -    291     86.8 MiB      0.0 MiB       bq_client = client

2018-04-29 11:11:51,026 - memory_profile6_log - INFO -    292     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:11:51,029 - memory_profile6_log - INFO -    293                             

2018-04-29 11:11:51,029 - memory_profile6_log - INFO -    294     86.8 MiB      0.0 MiB       datalist = []

2018-04-29 11:11:51,032 - memory_profile6_log - INFO -    295     86.8 MiB      0.0 MiB       datalist_hist = []

2018-04-29 11:11:51,033 - memory_profile6_log - INFO -    296                             

2018-04-29 11:11:51,035 - memory_profile6_log - INFO -    297     86.8 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 11:11:51,036 - memory_profile6_log - INFO -    298     90.5 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 11:11:51,036 - memory_profile6_log - INFO -    299     90.5 MiB      3.6 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 11:11:51,038 - memory_profile6_log - INFO -    300     90.5 MiB      0.0 MiB           if tframe is not None:

2018-04-29 11:11:51,038 - memory_profile6_log - INFO -    301                                         if not tframe.empty:

2018-04-29 11:11:51,039 - memory_profile6_log - INFO -    302                                             X_split = np.array_split(tframe, 5)

2018-04-29 11:11:51,039 - memory_profile6_log - INFO -    303                                             logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 11:11:51,042 - memory_profile6_log - INFO -    304                                             logger.info("Appending history data...")

2018-04-29 11:11:51,043 - memory_profile6_log - INFO -    305                                             for ix in range(len(X_split)):

2018-04-29 11:11:51,045 - memory_profile6_log - INFO -    306                                                 # ~ loading history

2018-04-29 11:11:51,046 - memory_profile6_log - INFO -    307                                                 """

2018-04-29 11:11:51,046 - memory_profile6_log - INFO -    308                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 11:11:51,048 - memory_profile6_log - INFO -    309                                                 """

2018-04-29 11:11:51,049 - memory_profile6_log - INFO -    310                                                 logger.info("processing batch-%d", ix)

2018-04-29 11:11:51,049 - memory_profile6_log - INFO -    311                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 11:11:51,051 - memory_profile6_log - INFO -    312                                                 logger.info("creating list history data...")

2018-04-29 11:11:51,052 - memory_profile6_log - INFO -    313                                                 lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 11:11:51,055 - memory_profile6_log - INFO -    314                             

2018-04-29 11:11:51,055 - memory_profile6_log - INFO -    315                                                 logger.info("call history data...")

2018-04-29 11:11:51,059 - memory_profile6_log - INFO -    316                                                 h_frame = mh.loadDSHistory(lhistory)

2018-04-29 11:11:51,059 - memory_profile6_log - INFO -    317                             

2018-04-29 11:11:51,061 - memory_profile6_log - INFO -    318                                                 # me = os.getpid()

2018-04-29 11:11:51,061 - memory_profile6_log - INFO -    319                                                 # kill_proc_tree(me)

2018-04-29 11:11:51,062 - memory_profile6_log - INFO -    320                             

2018-04-29 11:11:51,065 - memory_profile6_log - INFO -    321                                                 logger.info("done collecting history data, appending now...")

2018-04-29 11:11:51,065 - memory_profile6_log - INFO -    322                                                 for m in h_frame:

2018-04-29 11:11:51,068 - memory_profile6_log - INFO -    323                                                     if m is not None:

2018-04-29 11:11:51,069 - memory_profile6_log - INFO -    324                                                         if len(m) > 0:

2018-04-29 11:11:51,071 - memory_profile6_log - INFO -    325                                                             datalist_hist.append(pd.DataFrame(m))

2018-04-29 11:11:51,072 - memory_profile6_log - INFO -    326                                                 del h_frame

2018-04-29 11:11:51,075 - memory_profile6_log - INFO -    327                                                 del lhistory

2018-04-29 11:11:51,076 - memory_profile6_log - INFO -    328                             

2018-04-29 11:11:51,078 - memory_profile6_log - INFO -    329                                             logger.info("Appending training data...")

2018-04-29 11:11:51,078 - memory_profile6_log - INFO -    330                                             datalist.append(tframe)

2018-04-29 11:11:51,079 - memory_profile6_log - INFO -    331                                     else: 

2018-04-29 11:11:51,081 - memory_profile6_log - INFO -    332     90.5 MiB      0.0 MiB               logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 11:11:51,082 - memory_profile6_log - INFO -    333     90.5 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 11:11:51,082 - memory_profile6_log - INFO -    334     90.5 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 11:11:51,084 - memory_profile6_log - INFO -    335                             

2018-04-29 11:11:51,085 - memory_profile6_log - INFO -    336     90.5 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 11:11:51,086 - memory_profile6_log - INFO - 


2018-04-29 11:12:06,967 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 11:12:06,970 - memory_profile6_log - INFO - date_generated: 
2018-04-29 11:12:06,971 - memory_profile6_log - INFO -  
2018-04-29 11:12:06,971 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 28, 11, 12, 6, 968000)]
2018-04-29 11:12:06,973 - memory_profile6_log - INFO - 

2018-04-29 11:12:06,973 - memory_profile6_log - INFO - using current date: 2018-04-29
2018-04-29 11:12:06,973 - memory_profile6_log - INFO - using start date: 2018-04-28 00:00:00
2018-04-29 11:12:06,974 - memory_profile6_log - INFO - using end date: 2018-04-28
2018-04-29 11:12:07,118 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 11:12:07,124 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-28
2018-04-29 11:12:09,585 - memory_profile6_log - INFO - size of df: 24.0 Byte
2018-04-29 11:12:09,586 - memory_profile6_log - INFO - 2018-04-28 data is empty!
2018-04-29 11:12:09,588 - memory_profile6_log - INFO - tframe for date: 2018-04-28 is empty
2018-04-29 11:12:09,589 - memory_profile6_log - INFO - len datalist: 0
2018-04-29 11:12:09,592 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 11:12:09,592 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:12:09,594 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:12:09,594 - memory_profile6_log - INFO - ================================================

2018-04-29 11:12:09,596 - memory_profile6_log - INFO -    289     86.8 MiB     86.8 MiB   @profile

2018-04-29 11:12:09,598 - memory_profile6_log - INFO -    290                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 11:12:09,598 - memory_profile6_log - INFO -    291     86.8 MiB      0.0 MiB       bq_client = client

2018-04-29 11:12:09,599 - memory_profile6_log - INFO -    292     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:12:09,601 - memory_profile6_log - INFO -    293                             

2018-04-29 11:12:09,601 - memory_profile6_log - INFO -    294     86.8 MiB      0.0 MiB       datalist = []

2018-04-29 11:12:09,602 - memory_profile6_log - INFO -    295     86.8 MiB      0.0 MiB       datalist_hist = []

2018-04-29 11:12:09,604 - memory_profile6_log - INFO -    296                             

2018-04-29 11:12:09,605 - memory_profile6_log - INFO -    297     86.8 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 11:12:09,608 - memory_profile6_log - INFO -    298     90.5 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 11:12:09,608 - memory_profile6_log - INFO -    299     90.5 MiB      3.6 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 11:12:09,609 - memory_profile6_log - INFO -    300     90.5 MiB      0.0 MiB           if tframe is not None:

2018-04-29 11:12:09,611 - memory_profile6_log - INFO -    301                                         if not tframe.empty:

2018-04-29 11:12:09,611 - memory_profile6_log - INFO -    302                                             X_split = np.array_split(tframe, 5)

2018-04-29 11:12:09,612 - memory_profile6_log - INFO -    303                                             logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 11:12:09,614 - memory_profile6_log - INFO -    304                                             logger.info("Appending history data...")

2018-04-29 11:12:09,615 - memory_profile6_log - INFO -    305                                             for ix in range(len(X_split)):

2018-04-29 11:12:09,615 - memory_profile6_log - INFO -    306                                                 # ~ loading history

2018-04-29 11:12:09,618 - memory_profile6_log - INFO -    307                                                 """

2018-04-29 11:12:09,618 - memory_profile6_log - INFO -    308                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 11:12:09,619 - memory_profile6_log - INFO -    309                                                 """

2018-04-29 11:12:09,621 - memory_profile6_log - INFO -    310                                                 logger.info("processing batch-%d", ix)

2018-04-29 11:12:09,621 - memory_profile6_log - INFO -    311                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 11:12:09,622 - memory_profile6_log - INFO -    312                                                 logger.info("creating list history data...")

2018-04-29 11:12:09,624 - memory_profile6_log - INFO -    313                                                 lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 11:12:09,625 - memory_profile6_log - INFO -    314                             

2018-04-29 11:12:09,625 - memory_profile6_log - INFO -    315                                                 logger.info("call history data...")

2018-04-29 11:12:09,627 - memory_profile6_log - INFO -    316                                                 h_frame = mh.loadDSHistory(lhistory)

2018-04-29 11:12:09,630 - memory_profile6_log - INFO -    317                             

2018-04-29 11:12:09,631 - memory_profile6_log - INFO -    318                                                 # me = os.getpid()

2018-04-29 11:12:09,631 - memory_profile6_log - INFO -    319                                                 # kill_proc_tree(me)

2018-04-29 11:12:09,632 - memory_profile6_log - INFO -    320                             

2018-04-29 11:12:09,632 - memory_profile6_log - INFO -    321                                                 logger.info("done collecting history data, appending now...")

2018-04-29 11:12:09,634 - memory_profile6_log - INFO -    322                                                 for m in h_frame:

2018-04-29 11:12:09,634 - memory_profile6_log - INFO -    323                                                     if m is not None:

2018-04-29 11:12:09,635 - memory_profile6_log - INFO -    324                                                         if len(m) > 0:

2018-04-29 11:12:09,637 - memory_profile6_log - INFO -    325                                                             datalist_hist.append(pd.DataFrame(m))

2018-04-29 11:12:09,640 - memory_profile6_log - INFO -    326                                                 del h_frame

2018-04-29 11:12:09,641 - memory_profile6_log - INFO -    327                                                 del lhistory

2018-04-29 11:12:09,641 - memory_profile6_log - INFO -    328                             

2018-04-29 11:12:09,642 - memory_profile6_log - INFO -    329                                             logger.info("Appending training data...")

2018-04-29 11:12:09,644 - memory_profile6_log - INFO -    330                                             datalist.append(tframe)

2018-04-29 11:12:09,644 - memory_profile6_log - INFO -    331                                     else: 

2018-04-29 11:12:09,645 - memory_profile6_log - INFO -    332     90.5 MiB      0.0 MiB               logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 11:12:09,648 - memory_profile6_log - INFO -    333     90.5 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 11:12:09,648 - memory_profile6_log - INFO -    334     90.5 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 11:12:09,651 - memory_profile6_log - INFO -    335                             

2018-04-29 11:12:09,653 - memory_profile6_log - INFO -    336     90.5 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 11:12:09,654 - memory_profile6_log - INFO - 


2018-04-29 11:12:09,654 - memory_profile6_log - INFO - Training cannot be empty..
2018-04-29 11:12:09,655 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:12:09,655 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:12:09,657 - memory_profile6_log - INFO - ================================================

2018-04-29 11:12:09,657 - memory_profile6_log - INFO -    338     86.7 MiB     86.7 MiB   @profile

2018-04-29 11:12:09,657 - memory_profile6_log - INFO -    339                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 11:12:09,658 - memory_profile6_log - INFO -    340     86.8 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 11:12:09,658 - memory_profile6_log - INFO -    341     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:12:09,663 - memory_profile6_log - INFO -    342                             

2018-04-29 11:12:09,663 - memory_profile6_log - INFO -    343                                 # ~~~ Begin collecting data ~~~

2018-04-29 11:12:09,664 - memory_profile6_log - INFO -    344     86.8 MiB      0.0 MiB       t0 = time.time()

2018-04-29 11:12:09,664 - memory_profile6_log - INFO -    345     90.5 MiB      3.6 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 11:12:09,664 - memory_profile6_log - INFO -    346     90.5 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 11:12:09,664 - memory_profile6_log - INFO -    347     90.5 MiB      0.0 MiB           logger.info("Training cannot be empty..")

2018-04-29 11:12:09,665 - memory_profile6_log - INFO -    348     90.5 MiB      0.0 MiB           return

2018-04-29 11:12:09,665 - memory_profile6_log - INFO -    349                                 big_frame_hist = pd.concat(datalist_hist)

2018-04-29 11:12:09,667 - memory_profile6_log - INFO -    350                                 print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 11:12:09,667 - memory_profile6_log - INFO -    351                                 logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 11:12:09,667 - memory_profile6_log - INFO -    352                             

2018-04-29 11:12:09,667 - memory_profile6_log - INFO -    353                                 big_frame = pd.concat(datalist)

2018-04-29 11:12:09,668 - memory_profile6_log - INFO -    354                                 logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 11:12:09,668 - memory_profile6_log - INFO -    355                                 del datalist

2018-04-29 11:12:09,668 - memory_profile6_log - INFO -    356                             

2018-04-29 11:12:09,670 - memory_profile6_log - INFO -    357                                 big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 11:12:09,670 - memory_profile6_log - INFO -    358                             

2018-04-29 11:12:09,674 - memory_profile6_log - INFO -    359                                 # ~ get current news interest ~

2018-04-29 11:12:09,674 - memory_profile6_log - INFO -    360                                 if not cd:

2018-04-29 11:12:09,676 - memory_profile6_log - INFO -    361                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 11:12:09,676 - memory_profile6_log - INFO -    362                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 11:12:09,677 - memory_profile6_log - INFO -    363                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 11:12:09,677 - memory_profile6_log - INFO -    364                                 else:

2018-04-29 11:12:09,677 - memory_profile6_log - INFO -    365                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 11:12:09,677 - memory_profile6_log - INFO -    366                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 11:12:09,678 - memory_profile6_log - INFO -    367                             

2018-04-29 11:12:09,680 - memory_profile6_log - INFO -    368                                     # safe handling of query parameter

2018-04-29 11:12:09,680 - memory_profile6_log - INFO -    369                                     query_params = [

2018-04-29 11:12:09,680 - memory_profile6_log - INFO -    370                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 11:12:09,681 - memory_profile6_log - INFO -    371                                     ]

2018-04-29 11:12:09,681 - memory_profile6_log - INFO -    372                             

2018-04-29 11:12:09,686 - memory_profile6_log - INFO -    373                                     job_config.query_parameters = query_params

2018-04-29 11:12:09,686 - memory_profile6_log - INFO -    374                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 11:12:09,687 - memory_profile6_log - INFO -    375                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 11:12:09,687 - memory_profile6_log - INFO -    376                             

2018-04-29 11:12:09,687 - memory_profile6_log - INFO -    377                                 current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 11:12:09,688 - memory_profile6_log - INFO -    378                                 current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 11:12:09,688 - memory_profile6_log - INFO -    379                                                                        format='%Y-%m-%d', errors='coerce')

2018-04-29 11:12:09,690 - memory_profile6_log - INFO -    380                                 logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 11:12:09,690 - memory_profile6_log - INFO -    381                                 train_time = time.time() - t0

2018-04-29 11:12:09,690 - memory_profile6_log - INFO -    382                                 logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 11:12:09,691 - memory_profile6_log - INFO -    383                             

2018-04-29 11:12:09,694 - memory_profile6_log - INFO -    384                                 return big_frame, current_frame, big_frame_hist

2018-04-29 11:12:09,694 - memory_profile6_log - INFO - 


2018-04-29 11:12:52,010 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 11:12:52,013 - memory_profile6_log - INFO - date_generated: 
2018-04-29 11:12:52,013 - memory_profile6_log - INFO -  
2018-04-29 11:12:52,015 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 28, 11, 12, 52, 12000)]
2018-04-29 11:12:52,016 - memory_profile6_log - INFO - 

2018-04-29 11:12:52,016 - memory_profile6_log - INFO - using current date: 2018-04-29
2018-04-29 11:12:52,016 - memory_profile6_log - INFO - using start date: 2018-04-28 00:00:00
2018-04-29 11:12:52,017 - memory_profile6_log - INFO - using end date: 2018-04-28
2018-04-29 11:12:52,150 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 11:12:52,154 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-28
2018-04-29 11:12:54,575 - memory_profile6_log - INFO - size of df: 24.0 Byte
2018-04-29 11:12:54,576 - memory_profile6_log - INFO - 2018-04-28 data is empty!
2018-04-29 11:12:54,578 - memory_profile6_log - INFO - tframe for date: 2018-04-28 is empty
2018-04-29 11:12:54,579 - memory_profile6_log - INFO - len datalist: 0
2018-04-29 11:12:54,581 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 11:12:54,582 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:12:54,582 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:12:54,584 - memory_profile6_log - INFO - ================================================

2018-04-29 11:12:54,585 - memory_profile6_log - INFO -    289     86.6 MiB     86.6 MiB   @profile

2018-04-29 11:12:54,586 - memory_profile6_log - INFO -    290                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 11:12:54,588 - memory_profile6_log - INFO -    291     86.6 MiB      0.0 MiB       bq_client = client

2018-04-29 11:12:54,588 - memory_profile6_log - INFO -    292     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:12:54,589 - memory_profile6_log - INFO -    293                             

2018-04-29 11:12:54,591 - memory_profile6_log - INFO -    294     86.6 MiB      0.0 MiB       datalist = []

2018-04-29 11:12:54,592 - memory_profile6_log - INFO -    295     86.6 MiB      0.0 MiB       datalist_hist = []

2018-04-29 11:12:54,592 - memory_profile6_log - INFO -    296                             

2018-04-29 11:12:54,592 - memory_profile6_log - INFO -    297     86.6 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 11:12:54,594 - memory_profile6_log - INFO -    298     90.2 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 11:12:54,594 - memory_profile6_log - INFO -    299     90.2 MiB      3.6 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 11:12:54,598 - memory_profile6_log - INFO -    300     90.2 MiB      0.0 MiB           if tframe is not None:

2018-04-29 11:12:54,598 - memory_profile6_log - INFO -    301                                         if not tframe.empty:

2018-04-29 11:12:54,598 - memory_profile6_log - INFO -    302                                             X_split = np.array_split(tframe, 5)

2018-04-29 11:12:54,599 - memory_profile6_log - INFO -    303                                             logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 11:12:54,601 - memory_profile6_log - INFO -    304                                             logger.info("Appending history data...")

2018-04-29 11:12:54,601 - memory_profile6_log - INFO -    305                                             for ix in range(len(X_split)):

2018-04-29 11:12:54,601 - memory_profile6_log - INFO -    306                                                 # ~ loading history

2018-04-29 11:12:54,602 - memory_profile6_log - INFO -    307                                                 """

2018-04-29 11:12:54,602 - memory_profile6_log - INFO -    308                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 11:12:54,604 - memory_profile6_log - INFO -    309                                                 """

2018-04-29 11:12:54,605 - memory_profile6_log - INFO -    310                                                 logger.info("processing batch-%d", ix)

2018-04-29 11:12:54,608 - memory_profile6_log - INFO -    311                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 11:12:54,608 - memory_profile6_log - INFO -    312                                                 logger.info("creating list history data...")

2018-04-29 11:12:54,609 - memory_profile6_log - INFO -    313                                                 lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 11:12:54,611 - memory_profile6_log - INFO -    314                             

2018-04-29 11:12:54,611 - memory_profile6_log - INFO -    315                                                 logger.info("call history data...")

2018-04-29 11:12:54,612 - memory_profile6_log - INFO -    316                                                 h_frame = mh.loadDSHistory(lhistory)

2018-04-29 11:12:54,614 - memory_profile6_log - INFO -    317                             

2018-04-29 11:12:54,615 - memory_profile6_log - INFO -    318                                                 # me = os.getpid()

2018-04-29 11:12:54,615 - memory_profile6_log - INFO -    319                                                 # kill_proc_tree(me)

2018-04-29 11:12:54,615 - memory_profile6_log - INFO -    320                             

2018-04-29 11:12:54,618 - memory_profile6_log - INFO -    321                                                 logger.info("done collecting history data, appending now...")

2018-04-29 11:12:54,619 - memory_profile6_log - INFO -    322                                                 for m in h_frame:

2018-04-29 11:12:54,619 - memory_profile6_log - INFO -    323                                                     if m is not None:

2018-04-29 11:12:54,621 - memory_profile6_log - INFO -    324                                                         if len(m) > 0:

2018-04-29 11:12:54,621 - memory_profile6_log - INFO -    325                                                             datalist_hist.append(pd.DataFrame(m))

2018-04-29 11:12:54,622 - memory_profile6_log - INFO -    326                                                 del h_frame

2018-04-29 11:12:54,624 - memory_profile6_log - INFO -    327                                                 del lhistory

2018-04-29 11:12:54,624 - memory_profile6_log - INFO -    328                             

2018-04-29 11:12:54,625 - memory_profile6_log - INFO -    329                                             logger.info("Appending training data...")

2018-04-29 11:12:54,625 - memory_profile6_log - INFO -    330                                             datalist.append(tframe)

2018-04-29 11:12:54,627 - memory_profile6_log - INFO -    331                                     else: 

2018-04-29 11:12:54,630 - memory_profile6_log - INFO -    332     90.2 MiB      0.0 MiB               logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 11:12:54,631 - memory_profile6_log - INFO -    333     90.2 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 11:12:54,631 - memory_profile6_log - INFO -    334     90.2 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 11:12:54,632 - memory_profile6_log - INFO -    335                             

2018-04-29 11:12:54,634 - memory_profile6_log - INFO -    336     90.2 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 11:12:54,634 - memory_profile6_log - INFO - 


2018-04-29 11:14:36,135 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 11:14:36,138 - memory_profile6_log - INFO - date_generated: 
2018-04-29 11:14:36,138 - memory_profile6_log - INFO -  
2018-04-29 11:14:36,138 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 28, 11, 14, 36, 136000)]
2018-04-29 11:14:36,138 - memory_profile6_log - INFO - 

2018-04-29 11:14:36,140 - memory_profile6_log - INFO - using current date: 2018-04-29
2018-04-29 11:14:36,140 - memory_profile6_log - INFO - using start date: 2018-04-28 00:00:00
2018-04-29 11:14:36,140 - memory_profile6_log - INFO - using end date: 2018-04-28
2018-04-29 11:14:36,273 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 11:14:36,276 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-28
2018-04-29 11:14:38,703 - memory_profile6_log - INFO - size of df: 24.0 Byte
2018-04-29 11:14:38,704 - memory_profile6_log - INFO - 2018-04-28 data is empty!
2018-04-29 11:14:38,707 - memory_profile6_log - INFO - tframe for date: 2018-04-28 is empty
2018-04-29 11:14:38,707 - memory_profile6_log - INFO - len datalist: 0
2018-04-29 11:14:38,709 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 11:14:38,710 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:14:38,710 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:14:38,710 - memory_profile6_log - INFO - ================================================

2018-04-29 11:14:38,711 - memory_profile6_log - INFO -    289     87.1 MiB     87.1 MiB   @profile

2018-04-29 11:14:38,713 - memory_profile6_log - INFO -    290                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 11:14:38,714 - memory_profile6_log - INFO -    291     87.1 MiB      0.0 MiB       bq_client = client

2018-04-29 11:14:38,716 - memory_profile6_log - INFO -    292     87.1 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:14:38,717 - memory_profile6_log - INFO -    293                             

2018-04-29 11:14:38,717 - memory_profile6_log - INFO -    294     87.1 MiB      0.0 MiB       datalist = []

2018-04-29 11:14:38,717 - memory_profile6_log - INFO -    295     87.1 MiB      0.0 MiB       datalist_hist = []

2018-04-29 11:14:38,719 - memory_profile6_log - INFO -    296                             

2018-04-29 11:14:38,720 - memory_profile6_log - INFO -    297     87.1 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 11:14:38,720 - memory_profile6_log - INFO -    298     90.6 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 11:14:38,720 - memory_profile6_log - INFO -    299     90.6 MiB      3.6 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 11:14:38,721 - memory_profile6_log - INFO -    300     90.6 MiB      0.0 MiB           if tframe is not None:

2018-04-29 11:14:38,723 - memory_profile6_log - INFO -    301                                         if not tframe.empty:

2018-04-29 11:14:38,723 - memory_profile6_log - INFO -    302                                             X_split = np.array_split(tframe, 5)

2018-04-29 11:14:38,724 - memory_profile6_log - INFO -    303                                             logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 11:14:38,726 - memory_profile6_log - INFO -    304                                             logger.info("Appending history data...")

2018-04-29 11:14:38,726 - memory_profile6_log - INFO -    305                                             for ix in range(len(X_split)):

2018-04-29 11:14:38,727 - memory_profile6_log - INFO -    306                                                 # ~ loading history

2018-04-29 11:14:38,729 - memory_profile6_log - INFO -    307                                                 """

2018-04-29 11:14:38,730 - memory_profile6_log - INFO -    308                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 11:14:38,730 - memory_profile6_log - INFO -    309                                                 """

2018-04-29 11:14:38,732 - memory_profile6_log - INFO -    310                                                 logger.info("processing batch-%d", ix)

2018-04-29 11:14:38,734 - memory_profile6_log - INFO -    311                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 11:14:38,736 - memory_profile6_log - INFO -    312                                                 logger.info("creating list history data...")

2018-04-29 11:14:38,736 - memory_profile6_log - INFO -    313                                                 lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 11:14:38,737 - memory_profile6_log - INFO -    314                             

2018-04-29 11:14:38,740 - memory_profile6_log - INFO -    315                                                 logger.info("call history data...")

2018-04-29 11:14:38,740 - memory_profile6_log - INFO -    316                                                 h_frame = mh.loadDSHistory(lhistory)

2018-04-29 11:14:38,742 - memory_profile6_log - INFO -    317                             

2018-04-29 11:14:38,743 - memory_profile6_log - INFO -    318                                                 # me = os.getpid()

2018-04-29 11:14:38,744 - memory_profile6_log - INFO -    319                                                 # kill_proc_tree(me)

2018-04-29 11:14:38,746 - memory_profile6_log - INFO -    320                             

2018-04-29 11:14:38,746 - memory_profile6_log - INFO -    321                                                 logger.info("done collecting history data, appending now...")

2018-04-29 11:14:38,749 - memory_profile6_log - INFO -    322                                                 for m in h_frame:

2018-04-29 11:14:38,750 - memory_profile6_log - INFO -    323                                                     if m is not None:

2018-04-29 11:14:38,752 - memory_profile6_log - INFO -    324                                                         if len(m) > 0:

2018-04-29 11:14:38,753 - memory_profile6_log - INFO -    325                                                             datalist_hist.append(pd.DataFrame(m))

2018-04-29 11:14:38,753 - memory_profile6_log - INFO -    326                                                 del h_frame

2018-04-29 11:14:38,756 - memory_profile6_log - INFO -    327                                                 del lhistory

2018-04-29 11:14:38,756 - memory_profile6_log - INFO -    328                             

2018-04-29 11:14:38,759 - memory_profile6_log - INFO -    329                                             logger.info("Appending training data...")

2018-04-29 11:14:38,759 - memory_profile6_log - INFO -    330                                             datalist.append(tframe)

2018-04-29 11:14:38,760 - memory_profile6_log - INFO -    331                                     else: 

2018-04-29 11:14:38,762 - memory_profile6_log - INFO -    332     90.6 MiB      0.0 MiB               logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 11:14:38,763 - memory_profile6_log - INFO -    333     90.6 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 11:14:38,765 - memory_profile6_log - INFO -    334     90.6 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 11:14:38,766 - memory_profile6_log - INFO -    335                             

2018-04-29 11:14:38,767 - memory_profile6_log - INFO -    336     90.6 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 11:14:38,769 - memory_profile6_log - INFO - 


2018-04-29 11:14:38,769 - memory_profile6_log - INFO - []
2018-04-29 11:14:38,770 - memory_profile6_log - INFO - 

2018-04-29 11:14:53,122 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 11:14:53,125 - memory_profile6_log - INFO - date_generated: 
2018-04-29 11:14:53,127 - memory_profile6_log - INFO -  
2018-04-29 11:14:53,127 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 28, 11, 14, 53, 124000)]
2018-04-29 11:14:53,128 - memory_profile6_log - INFO - 

2018-04-29 11:14:53,128 - memory_profile6_log - INFO - using current date: 2018-04-29
2018-04-29 11:14:53,130 - memory_profile6_log - INFO - using start date: 2018-04-28 00:00:00
2018-04-29 11:14:53,130 - memory_profile6_log - INFO - using end date: 2018-04-28
2018-04-29 11:14:53,259 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 11:14:53,263 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-28
2018-04-29 11:14:56,737 - memory_profile6_log - INFO - size of df: 24.0 Byte
2018-04-29 11:14:56,740 - memory_profile6_log - INFO - 2018-04-28 data is empty!
2018-04-29 11:14:56,742 - memory_profile6_log - INFO - tframe for date: 2018-04-28 is empty
2018-04-29 11:14:56,743 - memory_profile6_log - INFO - len datalist: 0
2018-04-29 11:14:56,743 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 11:14:56,746 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:14:56,746 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:14:56,749 - memory_profile6_log - INFO - ================================================

2018-04-29 11:14:56,750 - memory_profile6_log - INFO -    289     87.1 MiB     87.1 MiB   @profile

2018-04-29 11:14:56,752 - memory_profile6_log - INFO -    290                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 11:14:56,753 - memory_profile6_log - INFO -    291     87.1 MiB      0.0 MiB       bq_client = client

2018-04-29 11:14:56,753 - memory_profile6_log - INFO -    292     87.1 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:14:56,755 - memory_profile6_log - INFO -    293                             

2018-04-29 11:14:56,756 - memory_profile6_log - INFO -    294     87.1 MiB      0.0 MiB       datalist = []

2018-04-29 11:14:56,756 - memory_profile6_log - INFO -    295     87.1 MiB      0.0 MiB       datalist_hist = []

2018-04-29 11:14:56,756 - memory_profile6_log - INFO -    296                             

2018-04-29 11:14:56,759 - memory_profile6_log - INFO -    297     87.1 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 11:14:56,760 - memory_profile6_log - INFO -    298     90.8 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 11:14:56,762 - memory_profile6_log - INFO -    299     90.8 MiB      3.7 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 11:14:56,763 - memory_profile6_log - INFO -    300     90.8 MiB      0.0 MiB           if tframe is not None:

2018-04-29 11:14:56,763 - memory_profile6_log - INFO -    301                                         if not tframe.empty:

2018-04-29 11:14:56,763 - memory_profile6_log - INFO -    302                                             X_split = np.array_split(tframe, 5)

2018-04-29 11:14:56,765 - memory_profile6_log - INFO -    303                                             logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 11:14:56,766 - memory_profile6_log - INFO -    304                                             logger.info("Appending history data...")

2018-04-29 11:14:56,766 - memory_profile6_log - INFO -    305                                             for ix in range(len(X_split)):

2018-04-29 11:14:56,766 - memory_profile6_log - INFO -    306                                                 # ~ loading history

2018-04-29 11:14:56,767 - memory_profile6_log - INFO -    307                                                 """

2018-04-29 11:14:56,770 - memory_profile6_log - INFO -    308                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 11:14:56,772 - memory_profile6_log - INFO -    309                                                 """

2018-04-29 11:14:56,776 - memory_profile6_log - INFO -    310                                                 logger.info("processing batch-%d", ix)

2018-04-29 11:14:56,776 - memory_profile6_log - INFO -    311                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 11:14:56,778 - memory_profile6_log - INFO -    312                                                 logger.info("creating list history data...")

2018-04-29 11:14:56,779 - memory_profile6_log - INFO -    313                                                 lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 11:14:56,779 - memory_profile6_log - INFO -    314                             

2018-04-29 11:14:56,782 - memory_profile6_log - INFO -    315                                                 logger.info("call history data...")

2018-04-29 11:14:56,783 - memory_profile6_log - INFO -    316                                                 h_frame = mh.loadDSHistory(lhistory)

2018-04-29 11:14:56,785 - memory_profile6_log - INFO -    317                             

2018-04-29 11:14:56,785 - memory_profile6_log - INFO -    318                                                 # me = os.getpid()

2018-04-29 11:14:56,786 - memory_profile6_log - INFO -    319                                                 # kill_proc_tree(me)

2018-04-29 11:14:56,786 - memory_profile6_log - INFO -    320                             

2018-04-29 11:14:56,788 - memory_profile6_log - INFO -    321                                                 logger.info("done collecting history data, appending now...")

2018-04-29 11:14:56,788 - memory_profile6_log - INFO -    322                                                 for m in h_frame:

2018-04-29 11:14:56,789 - memory_profile6_log - INFO -    323                                                     if m is not None:

2018-04-29 11:14:56,789 - memory_profile6_log - INFO -    324                                                         if len(m) > 0:

2018-04-29 11:14:56,793 - memory_profile6_log - INFO -    325                                                             datalist_hist.append(pd.DataFrame(m))

2018-04-29 11:14:56,793 - memory_profile6_log - INFO -    326                                                 del h_frame

2018-04-29 11:14:56,795 - memory_profile6_log - INFO -    327                                                 del lhistory

2018-04-29 11:14:56,796 - memory_profile6_log - INFO -    328                             

2018-04-29 11:14:56,796 - memory_profile6_log - INFO -    329                                             logger.info("Appending training data...")

2018-04-29 11:14:56,798 - memory_profile6_log - INFO -    330                                             datalist.append(tframe)

2018-04-29 11:14:56,799 - memory_profile6_log - INFO -    331                                     else: 

2018-04-29 11:14:56,799 - memory_profile6_log - INFO -    332     90.8 MiB      0.0 MiB               logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 11:14:56,801 - memory_profile6_log - INFO -    333     90.8 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 11:14:56,802 - memory_profile6_log - INFO -    334     90.8 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 11:14:56,803 - memory_profile6_log - INFO -    335                             

2018-04-29 11:14:56,805 - memory_profile6_log - INFO -    336     90.8 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 11:14:56,805 - memory_profile6_log - INFO - 


2018-04-29 11:14:56,806 - memory_profile6_log - INFO - 0
2018-04-29 11:14:56,808 - memory_profile6_log - INFO - 

2018-04-29 11:15:23,556 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 11:15:23,559 - memory_profile6_log - INFO - date_generated: 
2018-04-29 11:15:23,561 - memory_profile6_log - INFO -  
2018-04-29 11:15:23,562 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 28, 11, 15, 23, 558000)]
2018-04-29 11:15:23,562 - memory_profile6_log - INFO - 

2018-04-29 11:15:23,562 - memory_profile6_log - INFO - using current date: 2018-04-29
2018-04-29 11:15:23,563 - memory_profile6_log - INFO - using start date: 2018-04-28 00:00:00
2018-04-29 11:15:23,565 - memory_profile6_log - INFO - using end date: 2018-04-28
2018-04-29 11:15:23,697 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 11:15:23,701 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-28
2018-04-29 11:15:27,134 - memory_profile6_log - INFO - size of df: 24.0 Byte
2018-04-29 11:15:27,135 - memory_profile6_log - INFO - 2018-04-28 data is empty!
2018-04-29 11:15:27,137 - memory_profile6_log - INFO - tframe for date: 2018-04-28 is empty
2018-04-29 11:15:27,138 - memory_profile6_log - INFO - len datalist: 0
2018-04-29 11:15:27,140 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 11:15:27,141 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:15:27,141 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:15:27,144 - memory_profile6_log - INFO - ================================================

2018-04-29 11:15:27,145 - memory_profile6_log - INFO -    289     87.0 MiB     87.0 MiB   @profile

2018-04-29 11:15:27,147 - memory_profile6_log - INFO -    290                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 11:15:27,148 - memory_profile6_log - INFO -    291     87.0 MiB      0.0 MiB       bq_client = client

2018-04-29 11:15:27,148 - memory_profile6_log - INFO -    292     87.0 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:15:27,150 - memory_profile6_log - INFO -    293                             

2018-04-29 11:15:27,151 - memory_profile6_log - INFO -    294     87.0 MiB      0.0 MiB       datalist = []

2018-04-29 11:15:27,151 - memory_profile6_log - INFO -    295     87.0 MiB      0.0 MiB       datalist_hist = []

2018-04-29 11:15:27,153 - memory_profile6_log - INFO -    296                             

2018-04-29 11:15:27,154 - memory_profile6_log - INFO -    297     87.0 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 11:15:27,155 - memory_profile6_log - INFO -    298     90.6 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 11:15:27,157 - memory_profile6_log - INFO -    299     90.6 MiB      3.6 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 11:15:27,157 - memory_profile6_log - INFO -    300     90.6 MiB      0.0 MiB           if tframe is not None:

2018-04-29 11:15:27,160 - memory_profile6_log - INFO -    301                                         if not tframe.empty:

2018-04-29 11:15:27,161 - memory_profile6_log - INFO -    302                                             X_split = np.array_split(tframe, 5)

2018-04-29 11:15:27,161 - memory_profile6_log - INFO -    303                                             logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 11:15:27,163 - memory_profile6_log - INFO -    304                                             logger.info("Appending history data...")

2018-04-29 11:15:27,164 - memory_profile6_log - INFO -    305                                             for ix in range(len(X_split)):

2018-04-29 11:15:27,167 - memory_profile6_log - INFO -    306                                                 # ~ loading history

2018-04-29 11:15:27,167 - memory_profile6_log - INFO -    307                                                 """

2018-04-29 11:15:27,168 - memory_profile6_log - INFO -    308                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 11:15:27,170 - memory_profile6_log - INFO -    309                                                 """

2018-04-29 11:15:27,171 - memory_profile6_log - INFO -    310                                                 logger.info("processing batch-%d", ix)

2018-04-29 11:15:27,171 - memory_profile6_log - INFO -    311                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 11:15:27,173 - memory_profile6_log - INFO -    312                                                 logger.info("creating list history data...")

2018-04-29 11:15:27,174 - memory_profile6_log - INFO -    313                                                 lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 11:15:27,177 - memory_profile6_log - INFO -    314                             

2018-04-29 11:15:27,177 - memory_profile6_log - INFO -    315                                                 logger.info("call history data...")

2018-04-29 11:15:27,178 - memory_profile6_log - INFO -    316                                                 h_frame = mh.loadDSHistory(lhistory)

2018-04-29 11:15:27,180 - memory_profile6_log - INFO -    317                             

2018-04-29 11:15:27,181 - memory_profile6_log - INFO -    318                                                 # me = os.getpid()

2018-04-29 11:15:27,183 - memory_profile6_log - INFO -    319                                                 # kill_proc_tree(me)

2018-04-29 11:15:27,183 - memory_profile6_log - INFO -    320                             

2018-04-29 11:15:27,184 - memory_profile6_log - INFO -    321                                                 logger.info("done collecting history data, appending now...")

2018-04-29 11:15:27,187 - memory_profile6_log - INFO -    322                                                 for m in h_frame:

2018-04-29 11:15:27,187 - memory_profile6_log - INFO -    323                                                     if m is not None:

2018-04-29 11:15:27,190 - memory_profile6_log - INFO -    324                                                         if len(m) > 0:

2018-04-29 11:15:27,190 - memory_profile6_log - INFO -    325                                                             datalist_hist.append(pd.DataFrame(m))

2018-04-29 11:15:27,190 - memory_profile6_log - INFO -    326                                                 del h_frame

2018-04-29 11:15:27,191 - memory_profile6_log - INFO -    327                                                 del lhistory

2018-04-29 11:15:27,193 - memory_profile6_log - INFO -    328                             

2018-04-29 11:15:27,194 - memory_profile6_log - INFO -    329                                             logger.info("Appending training data...")

2018-04-29 11:15:27,194 - memory_profile6_log - INFO -    330                                             datalist.append(tframe)

2018-04-29 11:15:27,197 - memory_profile6_log - INFO -    331                                     else: 

2018-04-29 11:15:27,197 - memory_profile6_log - INFO -    332     90.6 MiB      0.0 MiB               logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 11:15:27,200 - memory_profile6_log - INFO -    333     90.6 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 11:15:27,200 - memory_profile6_log - INFO -    334     90.6 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 11:15:27,201 - memory_profile6_log - INFO -    335                             

2018-04-29 11:15:27,203 - memory_profile6_log - INFO -    336     90.6 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 11:15:27,203 - memory_profile6_log - INFO - 


2018-04-29 11:15:27,204 - memory_profile6_log - INFO - Training cannot be empty..
2018-04-29 11:15:27,206 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:15:27,207 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:15:27,207 - memory_profile6_log - INFO - ================================================

2018-04-29 11:15:27,210 - memory_profile6_log - INFO -    338     86.8 MiB     86.8 MiB   @profile

2018-04-29 11:15:27,210 - memory_profile6_log - INFO -    339                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 11:15:27,211 - memory_profile6_log - INFO -    340     87.0 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 11:15:27,213 - memory_profile6_log - INFO -    341     87.0 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:15:27,213 - memory_profile6_log - INFO -    342                             

2018-04-29 11:15:27,213 - memory_profile6_log - INFO -    343                                 # ~~~ Begin collecting data ~~~

2018-04-29 11:15:27,213 - memory_profile6_log - INFO -    344     87.0 MiB      0.0 MiB       t0 = time.time()

2018-04-29 11:15:27,214 - memory_profile6_log - INFO -    345     90.6 MiB      3.7 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 11:15:27,214 - memory_profile6_log - INFO -    346     90.6 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 11:15:27,216 - memory_profile6_log - INFO -    347     90.6 MiB      0.0 MiB           logger.info("Training cannot be empty..")

2018-04-29 11:15:27,216 - memory_profile6_log - INFO -    348     90.6 MiB      0.0 MiB           return False

2018-04-29 11:15:27,220 - memory_profile6_log - INFO -    349                                 big_frame_hist = pd.concat(datalist_hist)

2018-04-29 11:15:27,220 - memory_profile6_log - INFO -    350                                 print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 11:15:27,221 - memory_profile6_log - INFO -    351                                 logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 11:15:27,223 - memory_profile6_log - INFO -    352                             

2018-04-29 11:15:27,223 - memory_profile6_log - INFO -    353                                 big_frame = pd.concat(datalist)

2018-04-29 11:15:27,223 - memory_profile6_log - INFO -    354                                 logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 11:15:27,223 - memory_profile6_log - INFO -    355                                 del datalist

2018-04-29 11:15:27,224 - memory_profile6_log - INFO -    356                             

2018-04-29 11:15:27,224 - memory_profile6_log - INFO -    357                                 big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 11:15:27,224 - memory_profile6_log - INFO -    358                             

2018-04-29 11:15:27,226 - memory_profile6_log - INFO -    359                                 # ~ get current news interest ~

2018-04-29 11:15:27,226 - memory_profile6_log - INFO -    360                                 if not cd:

2018-04-29 11:15:27,226 - memory_profile6_log - INFO -    361                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 11:15:27,226 - memory_profile6_log - INFO -    362                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 11:15:27,226 - memory_profile6_log - INFO -    363                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 11:15:27,232 - memory_profile6_log - INFO -    364                                 else:

2018-04-29 11:15:27,232 - memory_profile6_log - INFO -    365                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 11:15:27,233 - memory_profile6_log - INFO -    366                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 11:15:27,233 - memory_profile6_log - INFO -    367                             

2018-04-29 11:15:27,234 - memory_profile6_log - INFO -    368                                     # safe handling of query parameter

2018-04-29 11:15:27,234 - memory_profile6_log - INFO -    369                                     query_params = [

2018-04-29 11:15:27,236 - memory_profile6_log - INFO -    370                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 11:15:27,237 - memory_profile6_log - INFO -    371                                     ]

2018-04-29 11:15:27,237 - memory_profile6_log - INFO -    372                             

2018-04-29 11:15:27,239 - memory_profile6_log - INFO -    373                                     job_config.query_parameters = query_params

2018-04-29 11:15:27,239 - memory_profile6_log - INFO -    374                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 11:15:27,239 - memory_profile6_log - INFO -    375                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 11:15:27,240 - memory_profile6_log - INFO -    376                             

2018-04-29 11:15:27,240 - memory_profile6_log - INFO -    377                                 current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 11:15:27,240 - memory_profile6_log - INFO -    378                                 current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 11:15:27,243 - memory_profile6_log - INFO -    379                                                                        format='%Y-%m-%d', errors='coerce')

2018-04-29 11:15:27,244 - memory_profile6_log - INFO -    380                                 logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 11:15:27,246 - memory_profile6_log - INFO -    381                                 train_time = time.time() - t0

2018-04-29 11:15:27,246 - memory_profile6_log - INFO -    382                                 logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 11:15:27,247 - memory_profile6_log - INFO -    383                             

2018-04-29 11:15:27,247 - memory_profile6_log - INFO -    384                                 return big_frame, current_frame, big_frame_hist

2018-04-29 11:15:27,249 - memory_profile6_log - INFO - 


2018-04-29 11:17:06,348 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 11:17:06,351 - memory_profile6_log - INFO - date_generated: 
2018-04-29 11:17:06,351 - memory_profile6_log - INFO -  
2018-04-29 11:17:06,352 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 28, 11, 17, 6, 350000)]
2018-04-29 11:17:06,352 - memory_profile6_log - INFO - 

2018-04-29 11:17:06,354 - memory_profile6_log - INFO - using current date: 2018-04-29
2018-04-29 11:17:06,354 - memory_profile6_log - INFO - using start date: 2018-04-28 00:00:00
2018-04-29 11:17:06,355 - memory_profile6_log - INFO - using end date: 2018-04-28
2018-04-29 11:17:06,516 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 11:17:06,519 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-28
2018-04-29 11:17:08,996 - memory_profile6_log - INFO - size of df: 24.0 Byte
2018-04-29 11:17:08,999 - memory_profile6_log - INFO - 2018-04-28 data is empty!
2018-04-29 11:17:09,000 - memory_profile6_log - INFO - tframe for date: 2018-04-28 is empty
2018-04-29 11:17:09,000 - memory_profile6_log - INFO - len datalist: 0
2018-04-29 11:17:09,002 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 11:17:09,003 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:17:09,003 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:17:09,005 - memory_profile6_log - INFO - ================================================

2018-04-29 11:17:09,006 - memory_profile6_log - INFO -    289     87.0 MiB     87.0 MiB   @profile

2018-04-29 11:17:09,007 - memory_profile6_log - INFO -    290                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 11:17:09,009 - memory_profile6_log - INFO -    291     87.0 MiB      0.0 MiB       bq_client = client

2018-04-29 11:17:09,009 - memory_profile6_log - INFO -    292     87.0 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:17:09,009 - memory_profile6_log - INFO -    293                             

2018-04-29 11:17:09,010 - memory_profile6_log - INFO -    294     87.0 MiB      0.0 MiB       datalist = []

2018-04-29 11:17:09,010 - memory_profile6_log - INFO -    295     87.0 MiB      0.0 MiB       datalist_hist = []

2018-04-29 11:17:09,012 - memory_profile6_log - INFO -    296                             

2018-04-29 11:17:09,013 - memory_profile6_log - INFO -    297     87.0 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 11:17:09,013 - memory_profile6_log - INFO -    298     90.7 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 11:17:09,015 - memory_profile6_log - INFO -    299     90.7 MiB      3.7 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 11:17:09,016 - memory_profile6_log - INFO -    300     90.7 MiB      0.0 MiB           if tframe is not None:

2018-04-29 11:17:09,017 - memory_profile6_log - INFO -    301                                         if not tframe.empty:

2018-04-29 11:17:09,017 - memory_profile6_log - INFO -    302                                             X_split = np.array_split(tframe, 5)

2018-04-29 11:17:09,019 - memory_profile6_log - INFO -    303                                             logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 11:17:09,019 - memory_profile6_log - INFO -    304                                             logger.info("Appending history data...")

2018-04-29 11:17:09,020 - memory_profile6_log - INFO -    305                                             for ix in range(len(X_split)):

2018-04-29 11:17:09,023 - memory_profile6_log - INFO -    306                                                 # ~ loading history

2018-04-29 11:17:09,025 - memory_profile6_log - INFO -    307                                                 """

2018-04-29 11:17:09,025 - memory_profile6_log - INFO -    308                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 11:17:09,028 - memory_profile6_log - INFO -    309                                                 """

2018-04-29 11:17:09,029 - memory_profile6_log - INFO -    310                                                 logger.info("processing batch-%d", ix)

2018-04-29 11:17:09,029 - memory_profile6_log - INFO -    311                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 11:17:09,032 - memory_profile6_log - INFO -    312                                                 logger.info("creating list history data...")

2018-04-29 11:17:09,032 - memory_profile6_log - INFO -    313                                                 lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 11:17:09,033 - memory_profile6_log - INFO -    314                             

2018-04-29 11:17:09,033 - memory_profile6_log - INFO -    315                                                 logger.info("call history data...")

2018-04-29 11:17:09,035 - memory_profile6_log - INFO -    316                                                 h_frame = mh.loadDSHistory(lhistory)

2018-04-29 11:17:09,036 - memory_profile6_log - INFO -    317                             

2018-04-29 11:17:09,039 - memory_profile6_log - INFO -    318                                                 # me = os.getpid()

2018-04-29 11:17:09,039 - memory_profile6_log - INFO -    319                                                 # kill_proc_tree(me)

2018-04-29 11:17:09,042 - memory_profile6_log - INFO -    320                             

2018-04-29 11:17:09,042 - memory_profile6_log - INFO -    321                                                 logger.info("done collecting history data, appending now...")

2018-04-29 11:17:09,042 - memory_profile6_log - INFO -    322                                                 for m in h_frame:

2018-04-29 11:17:09,043 - memory_profile6_log - INFO -    323                                                     if m is not None:

2018-04-29 11:17:09,045 - memory_profile6_log - INFO -    324                                                         if len(m) > 0:

2018-04-29 11:17:09,046 - memory_profile6_log - INFO -    325                                                             datalist_hist.append(pd.DataFrame(m))

2018-04-29 11:17:09,046 - memory_profile6_log - INFO -    326                                                 del h_frame

2018-04-29 11:17:09,049 - memory_profile6_log - INFO -    327                                                 del lhistory

2018-04-29 11:17:09,049 - memory_profile6_log - INFO -    328                             

2018-04-29 11:17:09,052 - memory_profile6_log - INFO -    329                                             logger.info("Appending training data...")

2018-04-29 11:17:09,052 - memory_profile6_log - INFO -    330                                             datalist.append(tframe)

2018-04-29 11:17:09,053 - memory_profile6_log - INFO -    331                                     else: 

2018-04-29 11:17:09,055 - memory_profile6_log - INFO -    332     90.7 MiB      0.0 MiB               logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 11:17:09,055 - memory_profile6_log - INFO -    333     90.7 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 11:17:09,056 - memory_profile6_log - INFO -    334     90.7 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 11:17:09,058 - memory_profile6_log - INFO -    335                             

2018-04-29 11:17:09,059 - memory_profile6_log - INFO -    336     90.7 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 11:17:09,061 - memory_profile6_log - INFO - 


2018-04-29 11:17:09,062 - memory_profile6_log - INFO - Training cannot be empty..
2018-04-29 11:17:09,062 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:17:09,062 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:17:09,062 - memory_profile6_log - INFO - ================================================

2018-04-29 11:17:09,063 - memory_profile6_log - INFO -    338     86.8 MiB     86.8 MiB   @profile

2018-04-29 11:17:09,063 - memory_profile6_log - INFO -    339                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 11:17:09,065 - memory_profile6_log - INFO -    340     87.0 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 11:17:09,065 - memory_profile6_log - INFO -    341     87.0 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:17:09,065 - memory_profile6_log - INFO -    342                             

2018-04-29 11:17:09,065 - memory_profile6_log - INFO -    343                                 # ~~~ Begin collecting data ~~~

2018-04-29 11:17:09,066 - memory_profile6_log - INFO -    344     87.0 MiB      0.0 MiB       t0 = time.time()

2018-04-29 11:17:09,066 - memory_profile6_log - INFO -    345     90.7 MiB      3.7 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 11:17:09,066 - memory_profile6_log - INFO -    346     90.7 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 11:17:09,071 - memory_profile6_log - INFO -    347     90.7 MiB      0.0 MiB           logger.info("Training cannot be empty..")

2018-04-29 11:17:09,072 - memory_profile6_log - INFO -    348     90.7 MiB      0.0 MiB           return False

2018-04-29 11:17:09,073 - memory_profile6_log - INFO -    349                                 big_frame_hist = pd.concat(datalist_hist)

2018-04-29 11:17:09,075 - memory_profile6_log - INFO -    350                                 print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 11:17:09,075 - memory_profile6_log - INFO -    351                                 logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 11:17:09,075 - memory_profile6_log - INFO -    352                             

2018-04-29 11:17:09,075 - memory_profile6_log - INFO -    353                                 big_frame = pd.concat(datalist)

2018-04-29 11:17:09,076 - memory_profile6_log - INFO -    354                                 logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 11:17:09,076 - memory_profile6_log - INFO -    355                                 del datalist

2018-04-29 11:17:09,078 - memory_profile6_log - INFO -    356                             

2018-04-29 11:17:09,084 - memory_profile6_log - INFO -    357                                 big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 11:17:09,085 - memory_profile6_log - INFO -    358                             

2018-04-29 11:17:09,085 - memory_profile6_log - INFO -    359                                 # ~ get current news interest ~

2018-04-29 11:17:09,085 - memory_profile6_log - INFO -    360                                 if not cd:

2018-04-29 11:17:09,086 - memory_profile6_log - INFO -    361                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 11:17:09,088 - memory_profile6_log - INFO -    362                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 11:17:09,088 - memory_profile6_log - INFO -    363                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 11:17:09,089 - memory_profile6_log - INFO -    364                                 else:

2018-04-29 11:17:09,092 - memory_profile6_log - INFO -    365                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 11:17:09,092 - memory_profile6_log - INFO -    366                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 11:17:09,095 - memory_profile6_log - INFO -    367                             

2018-04-29 11:17:09,095 - memory_profile6_log - INFO -    368                                     # safe handling of query parameter

2018-04-29 11:17:09,095 - memory_profile6_log - INFO -    369                                     query_params = [

2018-04-29 11:17:09,096 - memory_profile6_log - INFO -    370                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 11:17:09,096 - memory_profile6_log - INFO -    371                                     ]

2018-04-29 11:17:09,101 - memory_profile6_log - INFO -    372                             

2018-04-29 11:17:09,104 - memory_profile6_log - INFO -    373                                     job_config.query_parameters = query_params

2018-04-29 11:17:09,105 - memory_profile6_log - INFO -    374                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 11:17:09,107 - memory_profile6_log - INFO -    375                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 11:17:09,108 - memory_profile6_log - INFO -    376                             

2018-04-29 11:17:09,108 - memory_profile6_log - INFO -    377                                 current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 11:17:09,109 - memory_profile6_log - INFO -    378                                 current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 11:17:09,109 - memory_profile6_log - INFO -    379                                                                        format='%Y-%m-%d', errors='coerce')

2018-04-29 11:17:09,111 - memory_profile6_log - INFO -    380                                 logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 11:17:09,111 - memory_profile6_log - INFO -    381                                 train_time = time.time() - t0

2018-04-29 11:17:09,115 - memory_profile6_log - INFO -    382                                 logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 11:17:09,115 - memory_profile6_log - INFO -    383                             

2018-04-29 11:17:09,117 - memory_profile6_log - INFO -    384                                 return big_frame, current_frame, big_frame_hist

2018-04-29 11:17:09,118 - memory_profile6_log - INFO - 


2018-04-29 11:17:09,118 - memory_profile6_log - INFO - Train return False, please cek your data availability
2018-04-29 11:17:48,953 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 11:17:48,956 - memory_profile6_log - INFO - date_generated: 
2018-04-29 11:17:48,956 - memory_profile6_log - INFO -  
2018-04-29 11:17:48,957 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 28, 11, 17, 48, 954000)]
2018-04-29 11:17:48,957 - memory_profile6_log - INFO - 

2018-04-29 11:17:48,957 - memory_profile6_log - INFO - using current date: 2018-04-29
2018-04-29 11:17:48,957 - memory_profile6_log - INFO - using start date: 2018-04-28 00:00:00
2018-04-29 11:17:48,957 - memory_profile6_log - INFO - using end date: 2018-04-28
2018-04-29 11:17:49,085 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 11:17:49,088 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-28
2018-04-29 11:17:52,569 - memory_profile6_log - INFO - size of df: 24.0 Byte
2018-04-29 11:17:52,569 - memory_profile6_log - INFO - 2018-04-28 data is empty!
2018-04-29 11:17:52,572 - memory_profile6_log - INFO - tframe for date: 2018-04-28 is empty
2018-04-29 11:17:52,572 - memory_profile6_log - INFO - len datalist: 0
2018-04-29 11:17:52,573 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 11:17:52,575 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:17:52,575 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:17:52,576 - memory_profile6_log - INFO - ================================================

2018-04-29 11:17:52,578 - memory_profile6_log - INFO -    289     87.2 MiB     87.2 MiB   @profile

2018-04-29 11:17:52,578 - memory_profile6_log - INFO -    290                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 11:17:52,579 - memory_profile6_log - INFO -    291     87.2 MiB      0.0 MiB       bq_client = client

2018-04-29 11:17:52,581 - memory_profile6_log - INFO -    292     87.2 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:17:52,582 - memory_profile6_log - INFO -    293                             

2018-04-29 11:17:52,582 - memory_profile6_log - INFO -    294     87.2 MiB      0.0 MiB       datalist = []

2018-04-29 11:17:52,582 - memory_profile6_log - INFO -    295     87.2 MiB      0.0 MiB       datalist_hist = []

2018-04-29 11:17:52,584 - memory_profile6_log - INFO -    296                             

2018-04-29 11:17:52,585 - memory_profile6_log - INFO -    297     87.2 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 11:17:52,585 - memory_profile6_log - INFO -    298     90.9 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 11:17:52,585 - memory_profile6_log - INFO -    299     90.9 MiB      3.7 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 11:17:52,586 - memory_profile6_log - INFO -    300     90.9 MiB      0.0 MiB           if tframe is not None:

2018-04-29 11:17:52,588 - memory_profile6_log - INFO -    301                                         if not tframe.empty:

2018-04-29 11:17:52,589 - memory_profile6_log - INFO -    302                                             X_split = np.array_split(tframe, 5)

2018-04-29 11:17:52,591 - memory_profile6_log - INFO -    303                                             logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 11:17:52,592 - memory_profile6_log - INFO -    304                                             logger.info("Appending history data...")

2018-04-29 11:17:52,592 - memory_profile6_log - INFO -    305                                             for ix in range(len(X_split)):

2018-04-29 11:17:52,592 - memory_profile6_log - INFO -    306                                                 # ~ loading history

2018-04-29 11:17:52,594 - memory_profile6_log - INFO -    307                                                 """

2018-04-29 11:17:52,595 - memory_profile6_log - INFO -    308                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 11:17:52,595 - memory_profile6_log - INFO -    309                                                 """

2018-04-29 11:17:52,596 - memory_profile6_log - INFO -    310                                                 logger.info("processing batch-%d", ix)

2018-04-29 11:17:52,596 - memory_profile6_log - INFO -    311                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 11:17:52,601 - memory_profile6_log - INFO -    312                                                 logger.info("creating list history data...")

2018-04-29 11:17:52,601 - memory_profile6_log - INFO -    313                                                 lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 11:17:52,602 - memory_profile6_log - INFO -    314                             

2018-04-29 11:17:52,604 - memory_profile6_log - INFO -    315                                                 logger.info("call history data...")

2018-04-29 11:17:52,605 - memory_profile6_log - INFO -    316                                                 h_frame = mh.loadDSHistory(lhistory)

2018-04-29 11:17:52,605 - memory_profile6_log - INFO -    317                             

2018-04-29 11:17:52,607 - memory_profile6_log - INFO -    318                                                 # me = os.getpid()

2018-04-29 11:17:52,607 - memory_profile6_log - INFO -    319                                                 # kill_proc_tree(me)

2018-04-29 11:17:52,608 - memory_profile6_log - INFO -    320                             

2018-04-29 11:17:52,611 - memory_profile6_log - INFO -    321                                                 logger.info("done collecting history data, appending now...")

2018-04-29 11:17:52,611 - memory_profile6_log - INFO -    322                                                 for m in h_frame:

2018-04-29 11:17:52,612 - memory_profile6_log - INFO -    323                                                     if m is not None:

2018-04-29 11:17:52,614 - memory_profile6_log - INFO -    324                                                         if len(m) > 0:

2018-04-29 11:17:52,615 - memory_profile6_log - INFO -    325                                                             datalist_hist.append(pd.DataFrame(m))

2018-04-29 11:17:52,617 - memory_profile6_log - INFO -    326                                                 del h_frame

2018-04-29 11:17:52,617 - memory_profile6_log - INFO -    327                                                 del lhistory

2018-04-29 11:17:52,618 - memory_profile6_log - INFO -    328                             

2018-04-29 11:17:52,618 - memory_profile6_log - INFO -    329                                             logger.info("Appending training data...")

2018-04-29 11:17:52,621 - memory_profile6_log - INFO -    330                                             datalist.append(tframe)

2018-04-29 11:17:52,622 - memory_profile6_log - INFO -    331                                     else: 

2018-04-29 11:17:52,625 - memory_profile6_log - INFO -    332     90.9 MiB      0.0 MiB               logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 11:17:52,628 - memory_profile6_log - INFO -    333     90.9 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 11:17:52,628 - memory_profile6_log - INFO -    334     90.9 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 11:17:52,628 - memory_profile6_log - INFO -    335                             

2018-04-29 11:17:52,631 - memory_profile6_log - INFO -    336     90.9 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 11:17:52,632 - memory_profile6_log - INFO - 


2018-04-29 11:17:52,634 - memory_profile6_log - INFO - Training cannot be empty..
2018-04-29 11:17:52,635 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:17:52,635 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:17:52,637 - memory_profile6_log - INFO - ================================================

2018-04-29 11:17:52,637 - memory_profile6_log - INFO -    338     87.1 MiB     87.1 MiB   @profile

2018-04-29 11:17:52,637 - memory_profile6_log - INFO -    339                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 11:17:52,638 - memory_profile6_log - INFO -    340     87.2 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 11:17:52,638 - memory_profile6_log - INFO -    341     87.2 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:17:52,638 - memory_profile6_log - INFO -    342                             

2018-04-29 11:17:52,638 - memory_profile6_log - INFO -    343                                 # ~~~ Begin collecting data ~~~

2018-04-29 11:17:52,642 - memory_profile6_log - INFO -    344     87.2 MiB      0.0 MiB       t0 = time.time()

2018-04-29 11:17:52,642 - memory_profile6_log - INFO -    345     90.9 MiB      3.7 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 11:17:52,644 - memory_profile6_log - INFO -    346     90.9 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 11:17:52,644 - memory_profile6_log - INFO -    347     90.9 MiB      0.0 MiB           logger.info("Training cannot be empty..")

2018-04-29 11:17:52,645 - memory_profile6_log - INFO -    348     90.9 MiB      0.0 MiB           return False

2018-04-29 11:17:52,645 - memory_profile6_log - INFO -    349                                 big_frame_hist = pd.concat(datalist_hist)

2018-04-29 11:17:52,647 - memory_profile6_log - INFO -    350                                 print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 11:17:52,648 - memory_profile6_log - INFO -    351                                 logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 11:17:52,648 - memory_profile6_log - INFO -    352                             

2018-04-29 11:17:52,650 - memory_profile6_log - INFO -    353                                 big_frame = pd.concat(datalist)

2018-04-29 11:17:52,651 - memory_profile6_log - INFO -    354                                 logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 11:17:52,653 - memory_profile6_log - INFO -    355                                 del datalist

2018-04-29 11:17:52,654 - memory_profile6_log - INFO -    356                             

2018-04-29 11:17:52,655 - memory_profile6_log - INFO -    357                                 big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 11:17:52,655 - memory_profile6_log - INFO -    358                             

2018-04-29 11:17:52,657 - memory_profile6_log - INFO -    359                                 # ~ get current news interest ~

2018-04-29 11:17:52,658 - memory_profile6_log - INFO -    360                                 if not cd:

2018-04-29 11:17:52,660 - memory_profile6_log - INFO -    361                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 11:17:52,660 - memory_profile6_log - INFO -    362                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 11:17:52,661 - memory_profile6_log - INFO -    363                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 11:17:52,661 - memory_profile6_log - INFO -    364                                 else:

2018-04-29 11:17:52,661 - memory_profile6_log - INFO -    365                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 11:17:52,664 - memory_profile6_log - INFO -    366                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 11:17:52,667 - memory_profile6_log - INFO -    367                             

2018-04-29 11:17:52,668 - memory_profile6_log - INFO -    368                                     # safe handling of query parameter

2018-04-29 11:17:52,668 - memory_profile6_log - INFO -    369                                     query_params = [

2018-04-29 11:17:52,670 - memory_profile6_log - INFO -    370                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 11:17:52,670 - memory_profile6_log - INFO -    371                                     ]

2018-04-29 11:17:52,671 - memory_profile6_log - INFO -    372                             

2018-04-29 11:17:52,671 - memory_profile6_log - INFO -    373                                     job_config.query_parameters = query_params

2018-04-29 11:17:52,671 - memory_profile6_log - INFO -    374                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 11:17:52,671 - memory_profile6_log - INFO -    375                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 11:17:52,674 - memory_profile6_log - INFO -    376                             

2018-04-29 11:17:52,676 - memory_profile6_log - INFO -    377                                 current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 11:17:52,677 - memory_profile6_log - INFO -    378                                 current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 11:17:52,677 - memory_profile6_log - INFO -    379                                                                        format='%Y-%m-%d', errors='coerce')

2018-04-29 11:17:52,678 - memory_profile6_log - INFO -    380                                 logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 11:17:52,680 - memory_profile6_log - INFO -    381                                 train_time = time.time() - t0

2018-04-29 11:17:52,680 - memory_profile6_log - INFO -    382                                 logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 11:17:52,681 - memory_profile6_log - INFO -    383                             

2018-04-29 11:17:52,681 - memory_profile6_log - INFO -    384                                 return big_frame, current_frame, big_frame_hist

2018-04-29 11:17:52,681 - memory_profile6_log - INFO - 


2018-04-29 11:17:52,683 - memory_profile6_log - INFO - Train return False, please cek your data availability
2018-04-29 11:18:11,003 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 11:18:11,006 - memory_profile6_log - INFO - date_generated: 
2018-04-29 11:18:11,006 - memory_profile6_log - INFO -  
2018-04-29 11:18:11,007 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 11:18:11,009 - memory_profile6_log - INFO - 

2018-04-29 11:18:11,009 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 11:18:11,009 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 11:18:11,009 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 11:18:11,138 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 11:18:11,142 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 11:19:24,322 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 11:19:24,323 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 11:19:24,368 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 11:19:24,369 - memory_profile6_log - INFO - Appending history data...
2018-04-29 11:19:24,371 - memory_profile6_log - INFO - processing batch-0
2018-04-29 11:19:24,372 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:19:24,431 - memory_profile6_log - INFO - call history data...
2018-04-29 11:20:17,217 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:20:17,864 - memory_profile6_log - INFO - processing batch-1
2018-04-29 11:20:17,865 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:20:17,872 - memory_profile6_log - INFO - call history data...
2018-04-29 11:21:02,270 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:21:03,061 - memory_profile6_log - INFO - processing batch-2
2018-04-29 11:21:03,062 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:21:03,069 - memory_profile6_log - INFO - call history data...
2018-04-29 11:21:43,440 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:21:44,213 - memory_profile6_log - INFO - processing batch-3
2018-04-29 11:21:44,214 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:21:44,226 - memory_profile6_log - INFO - call history data...
2018-04-29 11:22:24,788 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:22:25,665 - memory_profile6_log - INFO - processing batch-4
2018-04-29 11:22:25,667 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:22:25,674 - memory_profile6_log - INFO - call history data...
2018-04-29 11:23:05,878 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:23:06,538 - memory_profile6_log - INFO - Appending training data...
2018-04-29 11:23:06,539 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 11:23:06,540 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 11:23:06,542 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:23:06,543 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:23:06,545 - memory_profile6_log - INFO - ================================================

2018-04-29 11:23:06,546 - memory_profile6_log - INFO -    289     86.9 MiB     86.9 MiB   @profile

2018-04-29 11:23:06,546 - memory_profile6_log - INFO -    290                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 11:23:06,548 - memory_profile6_log - INFO -    291     86.9 MiB      0.0 MiB       bq_client = client

2018-04-29 11:23:06,549 - memory_profile6_log - INFO -    292     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:23:06,551 - memory_profile6_log - INFO -    293                             

2018-04-29 11:23:06,555 - memory_profile6_log - INFO -    294     86.9 MiB      0.0 MiB       datalist = []

2018-04-29 11:23:06,555 - memory_profile6_log - INFO -    295     86.9 MiB      0.0 MiB       datalist_hist = []

2018-04-29 11:23:06,556 - memory_profile6_log - INFO -    296                             

2018-04-29 11:23:06,558 - memory_profile6_log - INFO -    297     86.9 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 11:23:06,559 - memory_profile6_log - INFO -    298    350.3 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 11:23:06,561 - memory_profile6_log - INFO -    299    339.0 MiB    252.1 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 11:23:06,562 - memory_profile6_log - INFO -    300    339.0 MiB      0.0 MiB           if tframe is not None:

2018-04-29 11:23:06,562 - memory_profile6_log - INFO -    301    339.0 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 11:23:06,563 - memory_profile6_log - INFO -    302    346.9 MiB      7.9 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 11:23:06,565 - memory_profile6_log - INFO -    303    346.9 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 11:23:06,565 - memory_profile6_log - INFO -    304    346.9 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 11:23:06,565 - memory_profile6_log - INFO -    305    350.3 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 11:23:06,566 - memory_profile6_log - INFO -    306                                                 # ~ loading history

2018-04-29 11:23:06,568 - memory_profile6_log - INFO -    307                                                 """

2018-04-29 11:23:06,571 - memory_profile6_log - INFO -    308                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 11:23:06,572 - memory_profile6_log - INFO -    309                                                 """

2018-04-29 11:23:06,572 - memory_profile6_log - INFO -    310    350.1 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 11:23:06,573 - memory_profile6_log - INFO -    311                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 11:23:06,575 - memory_profile6_log - INFO -    312    350.1 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 11:23:06,575 - memory_profile6_log - INFO -    313    350.1 MiB      0.5 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 11:23:06,576 - memory_profile6_log - INFO -    314                             

2018-04-29 11:23:06,578 - memory_profile6_log - INFO -    315    350.1 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 11:23:06,581 - memory_profile6_log - INFO -    316    350.3 MiB      2.2 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 11:23:06,581 - memory_profile6_log - INFO -    317                             

2018-04-29 11:23:06,582 - memory_profile6_log - INFO -    318                                                 # me = os.getpid()

2018-04-29 11:23:06,582 - memory_profile6_log - INFO -    319                                                 # kill_proc_tree(me)

2018-04-29 11:23:06,585 - memory_profile6_log - INFO -    320                             

2018-04-29 11:23:06,585 - memory_profile6_log - INFO -    321    350.3 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 11:23:06,586 - memory_profile6_log - INFO -    322    350.3 MiB     -0.1 MiB                       for m in h_frame:

2018-04-29 11:23:06,588 - memory_profile6_log - INFO -    323    350.3 MiB     -0.1 MiB                           if m is not None:

2018-04-29 11:23:06,591 - memory_profile6_log - INFO -    324    350.3 MiB     -0.1 MiB                               if len(m) > 0:

2018-04-29 11:23:06,592 - memory_profile6_log - INFO -    325    350.3 MiB      0.6 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 11:23:06,592 - memory_profile6_log - INFO -    326    350.3 MiB     -0.0 MiB                       del h_frame

2018-04-29 11:23:06,592 - memory_profile6_log - INFO -    327    350.3 MiB      0.0 MiB                       del lhistory

2018-04-29 11:23:06,595 - memory_profile6_log - INFO -    328                             

2018-04-29 11:23:06,595 - memory_profile6_log - INFO -    329    350.3 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 11:23:06,596 - memory_profile6_log - INFO -    330    350.3 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 11:23:06,596 - memory_profile6_log - INFO -    331                                     else: 

2018-04-29 11:23:06,598 - memory_profile6_log - INFO -    332                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 11:23:06,601 - memory_profile6_log - INFO -    333    350.3 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 11:23:06,601 - memory_profile6_log - INFO -    334    350.3 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 11:23:06,605 - memory_profile6_log - INFO -    335                             

2018-04-29 11:23:06,605 - memory_profile6_log - INFO -    336    350.3 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 11:23:06,607 - memory_profile6_log - INFO - 


2018-04-29 11:23:07,664 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 11:23:07,749 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0   0.001543         655.760344        45             665.760344  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
1   0.001543        1595.092728        37            1605.092728  10960288  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
2   0.001543         100.714046       293             110.714046  10960288  1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...
3   0.000023      109703.065469       779          109713.065469  10959239  161129f951435-0d660a18dbcd87-173a7640-100200-1...
4   0.001543        1241.341593       356            1251.341593  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
5   0.001543        1455.707850       478            1465.707850  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
6   0.001543         151.693334       366             161.693334  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
0   0.001543         259.277950       205             269.277950  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
1   0.001543         133.525862       221             143.525862  10960288  161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...
2   0.001543          87.305371       169              97.305371  10960288  161e219cd891c8-0e882943705df4-15290f5f-38400-1...
3   0.001543         156.963912        94             166.963912  10960288  16169731ce618-00742c5e307396-5b4d6a36-38400-16...
4   0.001543         136.906453       441             146.906453  10960288  1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...
5   0.001543         125.571130       235             135.571130  10960288  1626fd160d9114-055d801ae6b39d-66265f05-38400-1...
0   0.001543          51.589538       143              61.589538  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
1   0.001543          52.695028       280              62.695028  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
2   0.001543         175.650092        42             185.650092  10960288  16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...
3   0.001543         150.557222        49             160.557222  10960288  1612d88e55ab8-09af9658f30a46-65117b23-38400-16...
4   0.001543          63.281471       193              73.281471  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
5   0.001543         139.194413       106             149.194413  10960288  16149f0a1611-00a034775-63446032-38400-16149f0a...
6   0.001543          88.645278       187              98.645278  10960288  16132f1045f30b-0613b72b33445e-5d640103-38400-1...
2018-04-29 11:23:07,750 - memory_profile6_log - INFO - 

2018-04-29 11:23:07,759 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 11:23:07,828 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 11:23:07,842 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 11:23:58,000 - memory_profile6_log - INFO - size of df: 58.30 MB
2018-04-29 11:23:58,002 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-29 11:23:58,071 - memory_profile6_log - INFO - size of current_frame: 58.30 MB
2018-04-29 11:23:58,072 - memory_profile6_log - INFO - loading time of: 484420 total genuine-current interest data ~ take 346.956s
2018-04-29 11:23:58,078 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:23:58,078 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:23:58,079 - memory_profile6_log - INFO - ================================================

2018-04-29 11:23:58,082 - memory_profile6_log - INFO -    338     86.8 MiB     86.8 MiB   @profile

2018-04-29 11:23:58,082 - memory_profile6_log - INFO -    339                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 11:23:58,085 - memory_profile6_log - INFO -    340     86.9 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 11:23:58,085 - memory_profile6_log - INFO -    341     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:23:58,088 - memory_profile6_log - INFO -    342                             

2018-04-29 11:23:58,088 - memory_profile6_log - INFO -    343                                 # ~~~ Begin collecting data ~~~

2018-04-29 11:23:58,088 - memory_profile6_log - INFO -    344     86.9 MiB      0.0 MiB       t0 = time.time()

2018-04-29 11:23:58,088 - memory_profile6_log - INFO -    345    350.3 MiB    263.4 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 11:23:58,089 - memory_profile6_log - INFO -    346    350.3 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 11:23:58,089 - memory_profile6_log - INFO -    347                                     logger.info("Training cannot be empty..")

2018-04-29 11:23:58,091 - memory_profile6_log - INFO -    348                                     return False

2018-04-29 11:23:58,092 - memory_profile6_log - INFO -    349    350.9 MiB      0.7 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 11:23:58,096 - memory_profile6_log - INFO -    350    351.0 MiB      0.1 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 11:23:58,096 - memory_profile6_log - INFO -    351    351.0 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 11:23:58,098 - memory_profile6_log - INFO -    352                             

2018-04-29 11:23:58,098 - memory_profile6_log - INFO -    353    356.8 MiB      5.8 MiB       big_frame = pd.concat(datalist)

2018-04-29 11:23:58,098 - memory_profile6_log - INFO -    354    356.8 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 11:23:58,099 - memory_profile6_log - INFO -    355    351.0 MiB     -5.8 MiB       del datalist

2018-04-29 11:23:58,099 - memory_profile6_log - INFO -    356                             

2018-04-29 11:23:58,099 - memory_profile6_log - INFO -    357    351.0 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 11:23:58,101 - memory_profile6_log - INFO -    358                             

2018-04-29 11:23:58,101 - memory_profile6_log - INFO -    359                                 # ~ get current news interest ~

2018-04-29 11:23:58,105 - memory_profile6_log - INFO -    360    351.0 MiB      0.0 MiB       if not cd:

2018-04-29 11:23:58,105 - memory_profile6_log - INFO -    361                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 11:23:58,108 - memory_profile6_log - INFO -    362                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 11:23:58,108 - memory_profile6_log - INFO -    363                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 11:23:58,108 - memory_profile6_log - INFO -    364                                 else:

2018-04-29 11:23:58,108 - memory_profile6_log - INFO -    365    351.0 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 11:23:58,109 - memory_profile6_log - INFO -    366    351.0 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 11:23:58,111 - memory_profile6_log - INFO -    367                             

2018-04-29 11:23:58,115 - memory_profile6_log - INFO -    368                                     # safe handling of query parameter

2018-04-29 11:23:58,115 - memory_profile6_log - INFO -    369                                     query_params = [

2018-04-29 11:23:58,117 - memory_profile6_log - INFO -    370    351.0 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 11:23:58,118 - memory_profile6_log - INFO -    371                                     ]

2018-04-29 11:23:58,118 - memory_profile6_log - INFO -    372                             

2018-04-29 11:23:58,121 - memory_profile6_log - INFO -    373    351.0 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-29 11:23:58,121 - memory_profile6_log - INFO -    374    420.6 MiB     69.6 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 11:23:58,121 - memory_profile6_log - INFO -    375    420.6 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 11:23:58,124 - memory_profile6_log - INFO -    376                             

2018-04-29 11:23:58,125 - memory_profile6_log - INFO -    377    420.7 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 11:23:58,125 - memory_profile6_log - INFO -    378    420.7 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 11:23:58,127 - memory_profile6_log - INFO -    379    420.7 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 11:23:58,127 - memory_profile6_log - INFO -    380    420.7 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 11:23:58,130 - memory_profile6_log - INFO -    381    420.7 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 11:23:58,130 - memory_profile6_log - INFO -    382    420.7 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 11:23:58,131 - memory_profile6_log - INFO -    383                             

2018-04-29 11:23:58,131 - memory_profile6_log - INFO -    384    420.7 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 11:23:58,131 - memory_profile6_log - INFO - 


2018-04-29 11:23:58,134 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 11:23:58,165 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 11:23:58,167 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-29 11:23:58,168 - memory_profile6_log - INFO - apply on: 5000 total history data(D(t))
2018-04-29 11:23:59,267 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 11:23:59,269 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 11:24:40,630 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-29 11:24:40,657 - memory_profile6_log - INFO - Total train time: 42.492s
2018-04-29 11:24:40,658 - memory_profile6_log - INFO - memory left before cleaning: 70.600 percent memory...
2018-04-29 11:24:40,660 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-29 11:24:40,661 - memory_profile6_log - INFO - deleting df_dut...
2018-04-29 11:24:40,663 - memory_profile6_log - INFO - deleting df_dt...
2018-04-29 11:24:40,664 - memory_profile6_log - INFO - deleting df_input...
2018-04-29 11:24:40,671 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-29 11:24:40,673 - memory_profile6_log - INFO - deleting df_current...
2018-04-29 11:24:40,674 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-29 11:24:40,684 - memory_profile6_log - INFO - deleting model_fit...
2018-04-29 11:24:40,687 - memory_profile6_log - INFO - deleting result...
2018-04-29 11:24:40,706 - memory_profile6_log - INFO - deleting model_transform...
2018-04-29 11:24:40,707 - memory_profile6_log - INFO - memory left after cleaning: 70.400 percent memory...
2018-04-29 11:24:40,709 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-29 11:24:40,710 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-29 11:25:09,720 - memory_profile6_log - INFO - Saving fitted_models as history...
2018-04-29 11:37:04,145 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 11:37:04,148 - memory_profile6_log - INFO - date_generated: 
2018-04-29 11:37:04,148 - memory_profile6_log - INFO -  
2018-04-29 11:37:04,148 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 11:37:04,148 - memory_profile6_log - INFO - 

2018-04-29 11:37:04,148 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 11:37:04,150 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 11:37:04,150 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 11:37:04,272 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 11:37:04,275 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 11:38:14,252 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 11:38:14,253 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 11:38:14,295 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 11:38:14,296 - memory_profile6_log - INFO - Appending history data...
2018-04-29 11:38:14,298 - memory_profile6_log - INFO - processing batch-0
2018-04-29 11:38:14,299 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:38:14,341 - memory_profile6_log - INFO - call history data...
2018-04-29 11:38:59,778 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:39:00,421 - memory_profile6_log - INFO - processing batch-1
2018-04-29 11:39:00,423 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:39:00,430 - memory_profile6_log - INFO - call history data...
2018-04-29 11:39:46,427 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:39:47,104 - memory_profile6_log - INFO - processing batch-2
2018-04-29 11:39:47,105 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:39:47,115 - memory_profile6_log - INFO - call history data...
2018-04-29 11:40:35,519 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:40:36,200 - memory_profile6_log - INFO - processing batch-3
2018-04-29 11:40:36,200 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:40:36,209 - memory_profile6_log - INFO - call history data...
2018-04-29 11:41:18,819 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:41:19,486 - memory_profile6_log - INFO - processing batch-4
2018-04-29 11:41:19,486 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:41:19,496 - memory_profile6_log - INFO - call history data...
2018-04-29 11:42:01,890 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:42:02,546 - memory_profile6_log - INFO - Appending training data...
2018-04-29 11:42:02,546 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 11:42:02,548 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 11:42:02,551 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:42:02,552 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:42:02,552 - memory_profile6_log - INFO - ================================================

2018-04-29 11:42:02,553 - memory_profile6_log - INFO -    289     86.8 MiB     86.8 MiB   @profile

2018-04-29 11:42:02,555 - memory_profile6_log - INFO -    290                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 11:42:02,556 - memory_profile6_log - INFO -    291     86.8 MiB      0.0 MiB       bq_client = client

2018-04-29 11:42:02,559 - memory_profile6_log - INFO -    292     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:42:02,559 - memory_profile6_log - INFO -    293                             

2018-04-29 11:42:02,561 - memory_profile6_log - INFO -    294     86.8 MiB      0.0 MiB       datalist = []

2018-04-29 11:42:02,562 - memory_profile6_log - INFO -    295     86.8 MiB      0.0 MiB       datalist_hist = []

2018-04-29 11:42:02,562 - memory_profile6_log - INFO -    296                             

2018-04-29 11:42:02,563 - memory_profile6_log - INFO -    297     86.8 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 11:42:02,565 - memory_profile6_log - INFO -    298    350.5 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 11:42:02,565 - memory_profile6_log - INFO -    299    339.4 MiB    252.6 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 11:42:02,569 - memory_profile6_log - INFO -    300    339.4 MiB      0.0 MiB           if tframe is not None:

2018-04-29 11:42:02,571 - memory_profile6_log - INFO -    301    339.4 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 11:42:02,572 - memory_profile6_log - INFO -    302    347.3 MiB      7.9 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 11:42:02,572 - memory_profile6_log - INFO -    303    347.3 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 11:42:02,573 - memory_profile6_log - INFO -    304    347.3 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 11:42:02,575 - memory_profile6_log - INFO -    305    350.5 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 11:42:02,575 - memory_profile6_log - INFO -    306                                                 # ~ loading history

2018-04-29 11:42:02,578 - memory_profile6_log - INFO -    307                                                 """

2018-04-29 11:42:02,578 - memory_profile6_log - INFO -    308                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 11:42:02,579 - memory_profile6_log - INFO -    309                                                 """

2018-04-29 11:42:02,581 - memory_profile6_log - INFO -    310    350.4 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 11:42:02,582 - memory_profile6_log - INFO -    311                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 11:42:02,584 - memory_profile6_log - INFO -    312    350.4 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 11:42:02,585 - memory_profile6_log - INFO -    313    350.4 MiB      0.7 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 11:42:02,588 - memory_profile6_log - INFO -    314                             

2018-04-29 11:42:02,588 - memory_profile6_log - INFO -    315    350.4 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 11:42:02,589 - memory_profile6_log - INFO -    316    350.5 MiB      1.9 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 11:42:02,591 - memory_profile6_log - INFO -    317                             

2018-04-29 11:42:02,592 - memory_profile6_log - INFO -    318                                                 # me = os.getpid()

2018-04-29 11:42:02,594 - memory_profile6_log - INFO -    319                                                 # kill_proc_tree(me)

2018-04-29 11:42:02,595 - memory_profile6_log - INFO -    320                             

2018-04-29 11:42:02,595 - memory_profile6_log - INFO -    321    350.5 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 11:42:02,598 - memory_profile6_log - INFO -    322    350.5 MiB      0.0 MiB                       for m in h_frame:

2018-04-29 11:42:02,598 - memory_profile6_log - INFO -    323    350.5 MiB      0.0 MiB                           if m is not None:

2018-04-29 11:42:02,599 - memory_profile6_log - INFO -    324    350.5 MiB      0.0 MiB                               if len(m) > 0:

2018-04-29 11:42:02,601 - memory_profile6_log - INFO -    325    350.5 MiB      0.5 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 11:42:02,601 - memory_profile6_log - INFO -    326    350.5 MiB      0.0 MiB                       del h_frame

2018-04-29 11:42:02,601 - memory_profile6_log - INFO -    327    350.5 MiB      0.0 MiB                       del lhistory

2018-04-29 11:42:02,602 - memory_profile6_log - INFO -    328                             

2018-04-29 11:42:02,604 - memory_profile6_log - INFO -    329    350.5 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 11:42:02,605 - memory_profile6_log - INFO -    330    350.5 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 11:42:02,608 - memory_profile6_log - INFO -    331                                     else: 

2018-04-29 11:42:02,608 - memory_profile6_log - INFO -    332                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 11:42:02,611 - memory_profile6_log - INFO -    333    350.5 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 11:42:02,611 - memory_profile6_log - INFO -    334    350.5 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 11:42:02,612 - memory_profile6_log - INFO -    335                             

2018-04-29 11:42:02,614 - memory_profile6_log - INFO -    336    350.5 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 11:42:02,615 - memory_profile6_log - INFO - 


2018-04-29 11:42:03,648 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 11:42:03,721 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0   0.001543        1455.707850       478            1465.707850  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
1   0.001543         655.760344        45             665.760344  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
2   0.001543         100.714046       293             110.714046  10960288  1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...
3   0.001543        1241.341593       356            1251.341593  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
4   0.001543        1595.092728        37            1605.092728  10960288  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
5   0.000023      109703.065469       779          109713.065469  10959239  161129f951435-0d660a18dbcd87-173a7640-100200-1...
6   0.001543         151.693334       366             161.693334  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
0   0.001543         133.525862       221             143.525862  10960288  161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...
1   0.001543         259.277950       205             269.277950  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
2   0.001543         156.963912        94             166.963912  10960288  16169731ce618-00742c5e307396-5b4d6a36-38400-16...
3   0.001543         125.571130       235             135.571130  10960288  1626fd160d9114-055d801ae6b39d-66265f05-38400-1...
4   0.001543         136.906453       441             146.906453  10960288  1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...
5   0.001543          87.305371       169              97.305371  10960288  161e219cd891c8-0e882943705df4-15290f5f-38400-1...
0   0.001543         175.650092        42             185.650092  10960288  16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...
1   0.001543         139.194413       106             149.194413  10960288  16149f0a1611-00a034775-63446032-38400-16149f0a...
2   0.001543          88.645278       187              98.645278  10960288  16132f1045f30b-0613b72b33445e-5d640103-38400-1...
3   0.001543          52.695028       280              62.695028  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
4   0.001543          63.281471       193              73.281471  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
5   0.001543         150.557222        49             160.557222  10960288  1612d88e55ab8-09af9658f30a46-65117b23-38400-16...
6   0.001543          51.589538       143              61.589538  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
2018-04-29 11:42:03,723 - memory_profile6_log - INFO - 

2018-04-29 11:42:03,733 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 11:42:03,812 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 11:42:03,825 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 11:42:55,512 - memory_profile6_log - INFO - size of df: 58.30 MB
2018-04-29 11:42:55,513 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-29 11:42:55,573 - memory_profile6_log - INFO - size of current_frame: 58.30 MB
2018-04-29 11:42:55,575 - memory_profile6_log - INFO - loading time of: 484420 total genuine-current interest data ~ take 351.325s
2018-04-29 11:42:55,581 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:42:55,582 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:42:55,582 - memory_profile6_log - INFO - ================================================

2018-04-29 11:42:55,585 - memory_profile6_log - INFO -    338     86.7 MiB     86.7 MiB   @profile

2018-04-29 11:42:55,585 - memory_profile6_log - INFO -    339                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 11:42:55,588 - memory_profile6_log - INFO -    340     86.8 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 11:42:55,588 - memory_profile6_log - INFO -    341     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:42:55,588 - memory_profile6_log - INFO -    342                             

2018-04-29 11:42:55,589 - memory_profile6_log - INFO -    343                                 # ~~~ Begin collecting data ~~~

2018-04-29 11:42:55,589 - memory_profile6_log - INFO -    344     86.8 MiB      0.0 MiB       t0 = time.time()

2018-04-29 11:42:55,591 - memory_profile6_log - INFO -    345    350.5 MiB    263.7 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 11:42:55,591 - memory_profile6_log - INFO -    346    350.5 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 11:42:55,592 - memory_profile6_log - INFO -    347                                     logger.info("Training cannot be empty..")

2018-04-29 11:42:55,595 - memory_profile6_log - INFO -    348                                     return False

2018-04-29 11:42:55,595 - memory_profile6_log - INFO -    349    351.1 MiB      0.6 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 11:42:55,598 - memory_profile6_log - INFO -    350    351.3 MiB      0.1 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 11:42:55,598 - memory_profile6_log - INFO -    351    351.3 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 11:42:55,598 - memory_profile6_log - INFO -    352                             

2018-04-29 11:42:55,599 - memory_profile6_log - INFO -    353    357.1 MiB      5.8 MiB       big_frame = pd.concat(datalist)

2018-04-29 11:42:55,601 - memory_profile6_log - INFO -    354    357.1 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 11:42:55,601 - memory_profile6_log - INFO -    355    351.3 MiB     -5.8 MiB       del datalist

2018-04-29 11:42:55,602 - memory_profile6_log - INFO -    356                             

2018-04-29 11:42:55,602 - memory_profile6_log - INFO -    357    351.3 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 11:42:55,602 - memory_profile6_log - INFO -    358                             

2018-04-29 11:42:55,605 - memory_profile6_log - INFO -    359                                 # ~ get current news interest ~

2018-04-29 11:42:55,605 - memory_profile6_log - INFO -    360    351.3 MiB      0.0 MiB       if not cd:

2018-04-29 11:42:55,608 - memory_profile6_log - INFO -    361                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 11:42:55,608 - memory_profile6_log - INFO -    362                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 11:42:55,609 - memory_profile6_log - INFO -    363                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 11:42:55,611 - memory_profile6_log - INFO -    364                                 else:

2018-04-29 11:42:55,611 - memory_profile6_log - INFO -    365    351.3 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 11:42:55,612 - memory_profile6_log - INFO -    366    351.3 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 11:42:55,612 - memory_profile6_log - INFO -    367                             

2018-04-29 11:42:55,614 - memory_profile6_log - INFO -    368                                     # safe handling of query parameter

2018-04-29 11:42:55,614 - memory_profile6_log - INFO -    369                                     query_params = [

2018-04-29 11:42:55,617 - memory_profile6_log - INFO -    370    351.3 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 11:42:55,618 - memory_profile6_log - INFO -    371                                     ]

2018-04-29 11:42:55,619 - memory_profile6_log - INFO -    372                             

2018-04-29 11:42:55,621 - memory_profile6_log - INFO -    373    351.3 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-29 11:42:55,621 - memory_profile6_log - INFO -    374    419.0 MiB     67.8 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 11:42:55,622 - memory_profile6_log - INFO -    375    419.0 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 11:42:55,622 - memory_profile6_log - INFO -    376                             

2018-04-29 11:42:55,624 - memory_profile6_log - INFO -    377    419.1 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 11:42:55,625 - memory_profile6_log - INFO -    378    419.1 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 11:42:55,625 - memory_profile6_log - INFO -    379    419.1 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 11:42:55,625 - memory_profile6_log - INFO -    380    419.1 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 11:42:55,630 - memory_profile6_log - INFO -    381    419.1 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 11:42:55,631 - memory_profile6_log - INFO -    382    419.1 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 11:42:55,631 - memory_profile6_log - INFO -    383                             

2018-04-29 11:42:55,632 - memory_profile6_log - INFO -    384    419.1 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 11:42:55,634 - memory_profile6_log - INFO - 


2018-04-29 11:42:55,637 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 11:42:55,667 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 11:42:55,670 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-29 11:42:55,671 - memory_profile6_log - INFO - apply on: 5000 total history data(D(t))
2018-04-29 11:42:56,755 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 11:42:56,756 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 11:43:38,575 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-29 11:43:38,604 - memory_profile6_log - INFO - Total train time: 42.936s
2018-04-29 11:43:38,605 - memory_profile6_log - INFO - memory left before cleaning: 69.400 percent memory...
2018-04-29 11:43:38,605 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-29 11:43:38,607 - memory_profile6_log - INFO - deleting df_dut...
2018-04-29 11:43:38,608 - memory_profile6_log - INFO - deleting df_dt...
2018-04-29 11:43:38,609 - memory_profile6_log - INFO - deleting df_input...
2018-04-29 11:43:38,618 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-29 11:43:38,618 - memory_profile6_log - INFO - deleting df_current...
2018-04-29 11:43:38,619 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-29 11:43:38,631 - memory_profile6_log - INFO - deleting model_fit...
2018-04-29 11:43:38,632 - memory_profile6_log - INFO - deleting result...
2018-04-29 11:43:38,654 - memory_profile6_log - INFO - deleting model_transform...
2018-04-29 11:43:38,655 - memory_profile6_log - INFO - memory left after cleaning: 69.200 percent memory...
2018-04-29 11:43:38,657 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-29 11:43:38,657 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-29 11:44:08,559 - memory_profile6_log - INFO - Saving fitted_models as history...
2018-04-29 11:44:39,819 - memory_profile6_log - INFO - deleting BR...
2018-04-29 11:44:39,822 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-04-29 11:44:39,832 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:44:39,834 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:44:39,834 - memory_profile6_log - INFO - ================================================

2018-04-29 11:44:39,838 - memory_profile6_log - INFO -    113    419.1 MiB    419.1 MiB   @profile

2018-04-29 11:44:39,839 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-29 11:44:39,841 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-29 11:44:39,841 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-29 11:44:39,842 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-29 11:44:39,842 - memory_profile6_log - INFO -    118                                      saveto="datastore"):

2018-04-29 11:44:39,842 - memory_profile6_log - INFO -    119                                 """

2018-04-29 11:44:39,844 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-29 11:44:39,845 - memory_profile6_log - INFO -    121                                 """

2018-04-29 11:44:39,845 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-29 11:44:39,845 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-29 11:44:39,846 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-29 11:44:39,848 - memory_profile6_log - INFO -    125    419.1 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-29 11:44:39,851 - memory_profile6_log - INFO -    126    427.1 MiB      8.0 MiB       df_dut = df_input.copy(deep=True)

2018-04-29 11:44:39,854 - memory_profile6_log - INFO -    127    427.1 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 11:44:39,855 - memory_profile6_log - INFO -    128                             

2018-04-29 11:44:39,855 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-29 11:44:39,855 - memory_profile6_log - INFO -    130    434.3 MiB      7.3 MiB       df_dt = df_current.copy(deep=True)

2018-04-29 11:44:39,857 - memory_profile6_log - INFO -    131    434.3 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 11:44:39,858 - memory_profile6_log - INFO -    132                             

2018-04-29 11:44:39,858 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-29 11:44:39,861 - memory_profile6_log - INFO -    134    434.3 MiB      0.0 MiB       t0 = time.time()

2018-04-29 11:44:39,862 - memory_profile6_log - INFO -    135    434.3 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-29 11:44:39,865 - memory_profile6_log - INFO -    136    434.3 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-29 11:44:39,865 - memory_profile6_log - INFO -    137    434.3 MiB      0.0 MiB       logger.info("apply on: %d total history data(D(t))", len(df_hist))

2018-04-29 11:44:39,865 - memory_profile6_log - INFO -    138                             

2018-04-29 11:44:39,865 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-29 11:44:39,867 - memory_profile6_log - INFO -    140    434.3 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-29 11:44:39,867 - memory_profile6_log - INFO -    141                             

2018-04-29 11:44:39,868 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-29 11:44:39,868 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-29 11:44:39,871 - memory_profile6_log - INFO -    144    437.1 MiB      2.8 MiB       NB = BR.processX(df_dut)

2018-04-29 11:44:39,872 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-29 11:44:39,875 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-29 11:44:39,875 - memory_profile6_log - INFO -    147    447.0 MiB      9.9 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-29 11:44:39,875 - memory_profile6_log - INFO -    148                                 """

2018-04-29 11:44:39,877 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-29 11:44:39,878 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-29 11:44:39,878 - memory_profile6_log - INFO -    151                                 """

2018-04-29 11:44:39,880 - memory_profile6_log - INFO -    152    447.0 MiB      0.0 MiB       fitby_sigmant = True

2018-04-29 11:44:39,880 - memory_profile6_log - INFO -    153    447.0 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-29 11:44:39,884 - memory_profile6_log - INFO -    154    447.0 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-29 11:44:39,884 - memory_profile6_log - INFO -    155    456.7 MiB      9.7 MiB                            'is_general']]

2018-04-29 11:44:39,888 - memory_profile6_log - INFO -    156    456.7 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-29 11:44:39,888 - memory_profile6_log - INFO -    157    456.7 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-29 11:44:39,888 - memory_profile6_log - INFO -    158    483.0 MiB     26.3 MiB                          verbose=False)

2018-04-29 11:44:39,890 - memory_profile6_log - INFO -    159    483.0 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-29 11:44:39,891 - memory_profile6_log - INFO -    160    483.0 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-29 11:44:39,891 - memory_profile6_log - INFO -    161                             

2018-04-29 11:44:39,894 - memory_profile6_log - INFO -    162                                 # ~~ and Transform ~~

2018-04-29 11:44:39,894 - memory_profile6_log - INFO -    163                                 #   handling current news interest == current date

2018-04-29 11:44:39,897 - memory_profile6_log - INFO -    164    483.0 MiB      0.0 MiB       if df_dt.empty:

2018-04-29 11:44:39,897 - memory_profile6_log - INFO -    165                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-29 11:44:39,898 - memory_profile6_log - INFO -    166                                     return None

2018-04-29 11:44:39,898 - memory_profile6_log - INFO -    167    483.9 MiB      0.9 MiB       NB = BR.processX(df_dt)

2018-04-29 11:44:39,898 - memory_profile6_log - INFO -    168    494.5 MiB     10.6 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-29 11:44:39,900 - memory_profile6_log - INFO -    169                             

2018-04-29 11:44:39,900 - memory_profile6_log - INFO -    170    494.5 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-29 11:44:39,901 - memory_profile6_log - INFO -    171    493.6 MiB     -0.9 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-29 11:44:39,901 - memory_profile6_log - INFO -    172    493.6 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-29 11:44:39,901 - memory_profile6_log - INFO -    173    493.6 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-29 11:44:39,904 - memory_profile6_log - INFO -    174    512.5 MiB     18.9 MiB                                                     verbose=False)

2018-04-29 11:44:39,905 - memory_profile6_log - INFO -    175                                 # ~~~ filter is general and specific topic ~~~

2018-04-29 11:44:39,907 - memory_profile6_log - INFO -    176                                 # the idea is just we need to rerank every topic according

2018-04-29 11:44:39,907 - memory_profile6_log - INFO -    177                                 # user_id and and is_general by p0_posterior

2018-04-29 11:44:39,908 - memory_profile6_log - INFO -    178    512.5 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-29 11:44:39,910 - memory_profile6_log - INFO -    179    514.4 MiB      1.9 MiB                                     'is_general']].groupby(['topic_id',

2018-04-29 11:44:39,911 - memory_profile6_log - INFO -    180    512.5 MiB     -1.9 MiB                                                             'is_general']

2018-04-29 11:44:39,911 - memory_profile6_log - INFO -    181                                                                                      ).size().to_frame().reset_index()

2018-04-29 11:44:39,911 - memory_profile6_log - INFO -    182                             

2018-04-29 11:44:39,913 - memory_profile6_log - INFO -    183                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-29 11:44:39,914 - memory_profile6_log - INFO -    184                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-29 11:44:39,915 - memory_profile6_log - INFO -    185    512.5 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-29 11:44:39,917 - memory_profile6_log - INFO -    186                             

2018-04-29 11:44:39,917 - memory_profile6_log - INFO -    187                                 # ~ start by provide rank for each topic type ~

2018-04-29 11:44:39,918 - memory_profile6_log - INFO -    188    536.4 MiB     23.9 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-29 11:44:39,920 - memory_profile6_log - INFO -    189    539.9 MiB      3.5 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-29 11:44:39,921 - memory_profile6_log - INFO -    190                             

2018-04-29 11:44:39,921 - memory_profile6_log - INFO -    191                                 # ~ set threshold to filter output

2018-04-29 11:44:39,923 - memory_profile6_log - INFO -    192    539.9 MiB      0.0 MiB       if threshold > 0:

2018-04-29 11:44:39,923 - memory_profile6_log - INFO -    193    539.9 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-29 11:44:39,924 - memory_profile6_log - INFO -    194    539.9 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-29 11:44:39,927 - memory_profile6_log - INFO -    195    538.8 MiB     -1.1 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-29 11:44:39,928 - memory_profile6_log - INFO -    196                             

2018-04-29 11:44:39,930 - memory_profile6_log - INFO -    197    538.8 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 11:44:39,930 - memory_profile6_log - INFO -    198    538.8 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-29 11:44:39,930 - memory_profile6_log - INFO -    199                             

2018-04-29 11:44:39,931 - memory_profile6_log - INFO -    200    538.8 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 11:44:39,931 - memory_profile6_log - INFO -    201                             

2018-04-29 11:44:39,933 - memory_profile6_log - INFO -    202    538.8 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-29 11:44:39,933 - memory_profile6_log - INFO -    203    538.8 MiB      0.0 MiB       del df_dut

2018-04-29 11:44:39,934 - memory_profile6_log - INFO -    204    538.8 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-29 11:44:39,934 - memory_profile6_log - INFO -    205    538.8 MiB      0.0 MiB       del df_dt

2018-04-29 11:44:39,934 - memory_profile6_log - INFO -    206    538.8 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-29 11:44:39,934 - memory_profile6_log - INFO -    207    538.8 MiB      0.0 MiB       del df_input

2018-04-29 11:44:39,936 - memory_profile6_log - INFO -    208    538.8 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-29 11:44:39,940 - memory_profile6_log - INFO -    209    530.0 MiB     -8.8 MiB       del df_input_X

2018-04-29 11:44:39,940 - memory_profile6_log - INFO -    210    530.0 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-29 11:44:39,941 - memory_profile6_log - INFO -    211    530.0 MiB      0.0 MiB       del df_current

2018-04-29 11:44:39,943 - memory_profile6_log - INFO -    212    530.0 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-29 11:44:39,944 - memory_profile6_log - INFO -    213    530.0 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-29 11:44:39,944 - memory_profile6_log - INFO -    214    530.0 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-29 11:44:39,944 - memory_profile6_log - INFO -    215    506.8 MiB    -23.3 MiB       del model_fit

2018-04-29 11:44:39,946 - memory_profile6_log - INFO -    216    506.8 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-29 11:44:39,946 - memory_profile6_log - INFO -    217    506.8 MiB      0.0 MiB       del result

2018-04-29 11:44:39,947 - memory_profile6_log - INFO -    218    506.8 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-29 11:44:39,947 - memory_profile6_log - INFO -    219                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 11:44:39,947 - memory_profile6_log - INFO -    220                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 11:44:39,947 - memory_profile6_log - INFO -    221    506.8 MiB      0.0 MiB       if savetrain:

2018-04-29 11:44:39,951 - memory_profile6_log - INFO -    222    512.2 MiB      5.4 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-29 11:44:39,953 - memory_profile6_log - INFO -    223    512.2 MiB      0.0 MiB           del model_transform

2018-04-29 11:44:39,953 - memory_profile6_log - INFO -    224    512.2 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-29 11:44:39,954 - memory_profile6_log - INFO -    225    512.2 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 11:44:39,954 - memory_profile6_log - INFO -    226                             

2018-04-29 11:44:39,956 - memory_profile6_log - INFO -    227    512.2 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-29 11:44:39,956 - memory_profile6_log - INFO -    228                                     # ~ Place your code to save the training model here ~

2018-04-29 11:44:39,957 - memory_profile6_log - INFO -    229    512.2 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-29 11:44:39,957 - memory_profile6_log - INFO -    230    512.2 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-29 11:44:39,957 - memory_profile6_log - INFO -    231    512.2 MiB      0.0 MiB               if multproc:

2018-04-29 11:44:39,957 - memory_profile6_log - INFO -    232    489.3 MiB    -22.9 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-29 11:44:39,959 - memory_profile6_log - INFO -    233                             

2018-04-29 11:44:39,960 - memory_profile6_log - INFO -    234    489.3 MiB      0.0 MiB                   logger.info("Saving fitted_models as history...")

2018-04-29 11:44:39,963 - memory_profile6_log - INFO -    235    489.3 MiB      0.0 MiB                   save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 11:44:39,963 - memory_profile6_log - INFO -    236    503.2 MiB     13.9 MiB                   fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-29 11:44:39,964 - memory_profile6_log - INFO -    237    505.1 MiB      2.0 MiB                   mh.saveDataStorePutMulti(fitted_models_sigmant, kinds='topic_recomendation_history')

2018-04-29 11:44:39,966 - memory_profile6_log - INFO -    238                             

2018-04-29 11:44:39,966 - memory_profile6_log - INFO -    239    505.1 MiB      0.0 MiB                   del BR

2018-04-29 11:44:39,967 - memory_profile6_log - INFO -    240    505.1 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-29 11:44:39,967 - memory_profile6_log - INFO -    241                             

2018-04-29 11:44:39,967 - memory_profile6_log - INFO -    242    505.1 MiB      0.0 MiB                   del save_sigma_nt

2018-04-29 11:44:39,967 - memory_profile6_log - INFO -    243    505.1 MiB      0.0 MiB                   logger.info("deleting save_sigma_nt...")

2018-04-29 11:44:39,967 - memory_profile6_log - INFO -    244                                         else:

2018-04-29 11:44:39,969 - memory_profile6_log - INFO -    245                                             mh.saveDatastore(model_transformsv)

2018-04-29 11:44:39,970 - memory_profile6_log - INFO -    246                                             

2018-04-29 11:44:39,970 - memory_profile6_log - INFO -    247                                     elif str(saveto).lower() == "elastic":

2018-04-29 11:44:39,970 - memory_profile6_log - INFO -    248                                         logger.info("Using ElasticSearch as storage...")

2018-04-29 11:44:39,970 - memory_profile6_log - INFO -    249                                         mh.saveElasticS(model_transformsv)

2018-04-29 11:44:39,971 - memory_profile6_log - INFO -    250                             

2018-04-29 11:44:39,974 - memory_profile6_log - INFO -    251                                     # need save sigma_nt for daily train

2018-04-29 11:44:39,976 - memory_profile6_log - INFO -    252                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-29 11:44:39,976 - memory_profile6_log - INFO -    253                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-29 11:44:39,977 - memory_profile6_log - INFO -    254    505.1 MiB      0.0 MiB           if start_date and end_date:

2018-04-29 11:44:39,979 - memory_profile6_log - INFO -    255                                         if not fitby_sigmant:

2018-04-29 11:44:39,979 - memory_profile6_log - INFO -    256                                             logging.info("Saving sigma Nt...")

2018-04-29 11:44:39,980 - memory_profile6_log - INFO -    257                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 11:44:39,980 - memory_profile6_log - INFO -    258                                             save_sigma_nt['start_date'] = start_date

2018-04-29 11:44:39,980 - memory_profile6_log - INFO -    259                                             save_sigma_nt['end_date'] = end_date

2018-04-29 11:44:39,982 - memory_profile6_log - INFO -    260                                             print save_sigma_nt.head(5)

2018-04-29 11:44:39,983 - memory_profile6_log - INFO -    261                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-29 11:44:39,986 - memory_profile6_log - INFO -    262                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-29 11:44:39,986 - memory_profile6_log - INFO -    263    505.1 MiB      0.0 MiB       return

2018-04-29 11:44:39,987 - memory_profile6_log - INFO - 


2018-04-29 11:44:39,989 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-29 11:52:41,150 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 11:52:41,153 - memory_profile6_log - INFO - date_generated: 
2018-04-29 11:52:41,154 - memory_profile6_log - INFO -  
2018-04-29 11:52:41,154 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 11:52:41,154 - memory_profile6_log - INFO - 

2018-04-29 11:52:41,154 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 11:52:41,155 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 11:52:41,155 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 11:52:41,278 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 11:52:41,282 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 11:53:52,644 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 11:53:52,645 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 11:53:52,686 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 11:53:52,687 - memory_profile6_log - INFO - Appending history data...
2018-04-29 11:53:52,687 - memory_profile6_log - INFO - processing batch-0
2018-04-29 11:53:52,688 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:53:52,729 - memory_profile6_log - INFO - call history data...
2018-04-29 11:54:36,417 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:54:37,142 - memory_profile6_log - INFO - processing batch-1
2018-04-29 11:54:37,144 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:54:37,154 - memory_profile6_log - INFO - call history data...
2018-04-29 11:55:20,265 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:55:20,982 - memory_profile6_log - INFO - processing batch-2
2018-04-29 11:55:20,983 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:55:20,992 - memory_profile6_log - INFO - call history data...
2018-04-29 11:56:07,220 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:56:07,885 - memory_profile6_log - INFO - processing batch-3
2018-04-29 11:56:07,887 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:56:07,894 - memory_profile6_log - INFO - call history data...
2018-04-29 11:56:53,726 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:56:54,492 - memory_profile6_log - INFO - processing batch-4
2018-04-29 11:56:54,493 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:56:54,505 - memory_profile6_log - INFO - call history data...
2018-04-29 11:57:22,335 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 11:57:22,336 - memory_profile6_log - INFO - date_generated: 
2018-04-29 11:57:22,338 - memory_profile6_log - INFO -  
2018-04-29 11:57:22,338 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 11:57:22,338 - memory_profile6_log - INFO - 

2018-04-29 11:57:22,338 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 11:57:22,338 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 11:57:22,338 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 11:57:22,463 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 11:57:22,467 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 11:58:33,766 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 11:58:33,767 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 11:58:33,809 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 11:58:33,812 - memory_profile6_log - INFO - Appending history data...
2018-04-29 11:58:33,812 - memory_profile6_log - INFO - processing batch-0
2018-04-29 11:58:33,813 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:58:33,857 - memory_profile6_log - INFO - call history data...
2018-04-29 11:59:18,463 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:59:19,131 - memory_profile6_log - INFO - processing batch-1
2018-04-29 11:59:19,131 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:59:19,140 - memory_profile6_log - INFO - call history data...
2018-04-29 11:59:59,285 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:59:59,960 - memory_profile6_log - INFO - processing batch-2
2018-04-29 11:59:59,960 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:59:59,969 - memory_profile6_log - INFO - call history data...
2018-04-29 12:00:39,341 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 12:00:40,026 - memory_profile6_log - INFO - processing batch-3
2018-04-29 12:00:40,026 - memory_profile6_log - INFO - creating list history data...
2018-04-29 12:00:40,033 - memory_profile6_log - INFO - call history data...
2018-04-29 12:01:21,158 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 12:01:21,967 - memory_profile6_log - INFO - processing batch-4
2018-04-29 12:01:21,969 - memory_profile6_log - INFO - creating list history data...
2018-04-29 12:01:21,976 - memory_profile6_log - INFO - call history data...
2018-04-29 12:02:03,395 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 12:02:04,095 - memory_profile6_log - INFO - Appending training data...
2018-04-29 12:02:04,095 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 12:02:04,098 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 12:02:04,099 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 12:02:04,101 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 12:02:04,101 - memory_profile6_log - INFO - ================================================

2018-04-29 12:02:04,102 - memory_profile6_log - INFO -    295     86.8 MiB     86.8 MiB   @profile

2018-04-29 12:02:04,104 - memory_profile6_log - INFO -    296                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 12:02:04,107 - memory_profile6_log - INFO -    297     86.8 MiB      0.0 MiB       bq_client = client

2018-04-29 12:02:04,108 - memory_profile6_log - INFO -    298     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 12:02:04,109 - memory_profile6_log - INFO -    299                             

2018-04-29 12:02:04,111 - memory_profile6_log - INFO -    300     86.8 MiB      0.0 MiB       datalist = []

2018-04-29 12:02:04,111 - memory_profile6_log - INFO -    301     86.8 MiB      0.0 MiB       datalist_hist = []

2018-04-29 12:02:04,112 - memory_profile6_log - INFO -    302                             

2018-04-29 12:02:04,114 - memory_profile6_log - INFO -    303     86.8 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 12:02:04,115 - memory_profile6_log - INFO -    304    350.4 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 12:02:04,118 - memory_profile6_log - INFO -    305    340.3 MiB    253.5 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 12:02:04,118 - memory_profile6_log - INFO -    306    340.3 MiB      0.0 MiB           if tframe is not None:

2018-04-29 12:02:04,119 - memory_profile6_log - INFO -    307    340.3 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 12:02:04,121 - memory_profile6_log - INFO -    308    347.1 MiB      6.8 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 12:02:04,122 - memory_profile6_log - INFO -    309    347.1 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 12:02:04,124 - memory_profile6_log - INFO -    310    347.1 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 12:02:04,125 - memory_profile6_log - INFO -    311    350.4 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 12:02:04,128 - memory_profile6_log - INFO -    312                                                 # ~ loading history

2018-04-29 12:02:04,128 - memory_profile6_log - INFO -    313                                                 """

2018-04-29 12:02:04,130 - memory_profile6_log - INFO -    314                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 12:02:04,131 - memory_profile6_log - INFO -    315                                                 """

2018-04-29 12:02:04,132 - memory_profile6_log - INFO -    316    350.3 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 12:02:04,134 - memory_profile6_log - INFO -    317                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 12:02:04,134 - memory_profile6_log - INFO -    318    350.3 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 12:02:04,135 - memory_profile6_log - INFO -    319    350.3 MiB      0.8 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 12:02:04,135 - memory_profile6_log - INFO -    320                             

2018-04-29 12:02:04,138 - memory_profile6_log - INFO -    321    350.3 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 12:02:04,140 - memory_profile6_log - INFO -    322    350.4 MiB      1.8 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 12:02:04,141 - memory_profile6_log - INFO -    323                             

2018-04-29 12:02:04,141 - memory_profile6_log - INFO -    324                                                 # me = os.getpid()

2018-04-29 12:02:04,142 - memory_profile6_log - INFO -    325                                                 # kill_proc_tree(me)

2018-04-29 12:02:04,144 - memory_profile6_log - INFO -    326                             

2018-04-29 12:02:04,144 - memory_profile6_log - INFO -    327    350.4 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 12:02:04,145 - memory_profile6_log - INFO -    328    350.4 MiB     -0.1 MiB                       for m in h_frame:

2018-04-29 12:02:04,145 - memory_profile6_log - INFO -    329    350.4 MiB     -0.1 MiB                           if m is not None:

2018-04-29 12:02:04,148 - memory_profile6_log - INFO -    330    350.4 MiB     -0.1 MiB                               if len(m) > 0:

2018-04-29 12:02:04,150 - memory_profile6_log - INFO -    331    350.4 MiB      0.6 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 12:02:04,151 - memory_profile6_log - INFO -    332    350.4 MiB     -0.0 MiB                       del h_frame

2018-04-29 12:02:04,151 - memory_profile6_log - INFO -    333    350.4 MiB      0.0 MiB                       del lhistory

2018-04-29 12:02:04,153 - memory_profile6_log - INFO -    334                             

2018-04-29 12:02:04,154 - memory_profile6_log - INFO -    335    350.4 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 12:02:04,154 - memory_profile6_log - INFO -    336    350.4 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 12:02:04,155 - memory_profile6_log - INFO -    337                                     else: 

2018-04-29 12:02:04,157 - memory_profile6_log - INFO -    338                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 12:02:04,160 - memory_profile6_log - INFO -    339    350.4 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 12:02:04,161 - memory_profile6_log - INFO -    340    350.4 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 12:02:04,161 - memory_profile6_log - INFO -    341                             

2018-04-29 12:02:04,161 - memory_profile6_log - INFO -    342    350.4 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 12:02:04,163 - memory_profile6_log - INFO - 


2018-04-29 12:02:05,316 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 12:02:05,390 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0   0.000646        1844.325967        16            1854.325967  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
1   0.000646        1844.325967        32            1854.325967  10960288  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
2        NaN      333823.000000       256          333833.000000  10959239  161129f951435-0d660a18dbcd87-173a7640-100200-1...
3   0.000646        1844.325967        32            1854.325967  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
4   0.000646        1844.325967        32            1854.325967  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
5   0.000646        1844.325967        16            1854.325967  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
6   0.000646        1844.325967        16            1854.325967  10960288  1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...
0   0.000646        1844.325967        16            1854.325967  10960288  1626fd160d9114-055d801ae6b39d-66265f05-38400-1...
1   0.000646        1844.325967         8            1854.325967  10960288  16169731ce618-00742c5e307396-5b4d6a36-38400-16...
2   0.000646        1844.325967         8            1854.325967  10960288  1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...
3   0.000646        1844.325967         8            1854.325967  10960288  161e219cd891c8-0e882943705df4-15290f5f-38400-1...
4   0.000646        1844.325967        16            1854.325967  10960288  161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...
5   0.000646        1844.325967        16            1854.325967  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
0   0.000646        1844.325967         4            1854.325967  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
1   0.000646        1844.325967         4            1854.325967  10960288  16132f1045f30b-0613b72b33445e-5d640103-38400-1...
2   0.000646        1844.325967         8            1854.325967  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
3   0.000646        1844.325967         4            1854.325967  10960288  16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...
4   0.000646        1844.325967         4            1854.325967  10960288  1612d88e55ab8-09af9658f30a46-65117b23-38400-16...
5   0.000646        1844.325967         8            1854.325967  10960288  16149f0a1611-00a034775-63446032-38400-16149f0a...
6   0.000646        1844.325967         4            1854.325967  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
2018-04-29 12:02:05,391 - memory_profile6_log - INFO - 

2018-04-29 12:02:05,400 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 12:02:05,469 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 12:02:05,480 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 12:02:58,546 - memory_profile6_log - INFO - size of df: 58.30 MB
2018-04-29 12:02:58,548 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-29 12:02:58,609 - memory_profile6_log - INFO - size of current_frame: 58.30 MB
2018-04-29 12:02:58,611 - memory_profile6_log - INFO - loading time of: 484420 total genuine-current interest data ~ take 336.168s
2018-04-29 12:02:58,617 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 12:02:58,618 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 12:02:58,618 - memory_profile6_log - INFO - ================================================

2018-04-29 12:02:58,618 - memory_profile6_log - INFO -    344     86.7 MiB     86.7 MiB   @profile

2018-04-29 12:02:58,621 - memory_profile6_log - INFO -    345                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 12:02:58,621 - memory_profile6_log - INFO -    346     86.8 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 12:02:58,624 - memory_profile6_log - INFO -    347     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 12:02:58,625 - memory_profile6_log - INFO -    348                             

2018-04-29 12:02:58,625 - memory_profile6_log - INFO -    349                                 # ~~~ Begin collecting data ~~~

2018-04-29 12:02:58,625 - memory_profile6_log - INFO -    350     86.8 MiB      0.0 MiB       t0 = time.time()

2018-04-29 12:02:58,627 - memory_profile6_log - INFO -    351    350.5 MiB    263.6 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 12:02:58,627 - memory_profile6_log - INFO -    352    350.5 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 12:02:58,628 - memory_profile6_log - INFO -    353                                     logger.info("Training cannot be empty..")

2018-04-29 12:02:58,628 - memory_profile6_log - INFO -    354                                     return False

2018-04-29 12:02:58,628 - memory_profile6_log - INFO -    355    351.3 MiB      0.9 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 12:02:58,631 - memory_profile6_log - INFO -    356    351.5 MiB      0.1 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 12:02:58,631 - memory_profile6_log - INFO -    357    351.5 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 12:02:58,634 - memory_profile6_log - INFO -    358                             

2018-04-29 12:02:58,634 - memory_profile6_log - INFO -    359    357.3 MiB      5.8 MiB       big_frame = pd.concat(datalist)

2018-04-29 12:02:58,635 - memory_profile6_log - INFO -    360    357.3 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 12:02:58,635 - memory_profile6_log - INFO -    361    351.5 MiB     -5.8 MiB       del datalist

2018-04-29 12:02:58,637 - memory_profile6_log - INFO -    362                             

2018-04-29 12:02:58,638 - memory_profile6_log - INFO -    363    351.5 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 12:02:58,638 - memory_profile6_log - INFO -    364                             

2018-04-29 12:02:58,641 - memory_profile6_log - INFO -    365                                 # ~ get current news interest ~

2018-04-29 12:02:58,642 - memory_profile6_log - INFO -    366    351.5 MiB      0.0 MiB       if not cd:

2018-04-29 12:02:58,644 - memory_profile6_log - INFO -    367                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 12:02:58,644 - memory_profile6_log - INFO -    368                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 12:02:58,645 - memory_profile6_log - INFO -    369                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 12:02:58,647 - memory_profile6_log - INFO -    370                                 else:

2018-04-29 12:02:58,648 - memory_profile6_log - INFO -    371    351.5 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 12:02:58,648 - memory_profile6_log - INFO -    372    351.5 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 12:02:58,650 - memory_profile6_log - INFO -    373                             

2018-04-29 12:02:58,653 - memory_profile6_log - INFO -    374                                     # safe handling of query parameter

2018-04-29 12:02:58,653 - memory_profile6_log - INFO -    375                                     query_params = [

2018-04-29 12:02:58,654 - memory_profile6_log - INFO -    376    351.5 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 12:02:58,655 - memory_profile6_log - INFO -    377                                     ]

2018-04-29 12:02:58,657 - memory_profile6_log - INFO -    378                             

2018-04-29 12:02:58,657 - memory_profile6_log - INFO -    379    351.5 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-29 12:02:58,657 - memory_profile6_log - INFO -    380    420.2 MiB     68.7 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 12:02:58,658 - memory_profile6_log - INFO -    381    420.2 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 12:02:58,658 - memory_profile6_log - INFO -    382                             

2018-04-29 12:02:58,660 - memory_profile6_log - INFO -    383    420.2 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 12:02:58,660 - memory_profile6_log - INFO -    384    420.2 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 12:02:58,663 - memory_profile6_log - INFO -    385    420.2 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 12:02:58,663 - memory_profile6_log - INFO -    386    420.2 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 12:02:58,665 - memory_profile6_log - INFO -    387    420.2 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 12:02:58,665 - memory_profile6_log - INFO -    388    420.2 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 12:02:58,667 - memory_profile6_log - INFO -    389                             

2018-04-29 12:02:58,667 - memory_profile6_log - INFO -    390    420.2 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 12:02:58,667 - memory_profile6_log - INFO - 


2018-04-29 12:02:58,671 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 12:02:58,703 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 12:02:58,704 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-29 12:02:58,706 - memory_profile6_log - INFO - apply on: 5000 total history data(D(t))
2018-04-29 12:02:59,792 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 12:02:59,793 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 12:03:43,881 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-29 12:03:43,913 - memory_profile6_log - INFO - Total train time: 45.209s
2018-04-29 12:03:43,914 - memory_profile6_log - INFO - memory left before cleaning: 67.000 percent memory...
2018-04-29 12:03:43,914 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-29 12:03:43,915 - memory_profile6_log - INFO - deleting df_dut...
2018-04-29 12:03:43,917 - memory_profile6_log - INFO - deleting df_dt...
2018-04-29 12:03:43,917 - memory_profile6_log - INFO - deleting df_input...
2018-04-29 12:03:43,926 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-29 12:03:43,927 - memory_profile6_log - INFO - deleting df_current...
2018-04-29 12:03:43,928 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-29 12:03:43,940 - memory_profile6_log - INFO - deleting model_fit...
2018-04-29 12:03:43,943 - memory_profile6_log - INFO - deleting result...
2018-04-29 12:03:43,963 - memory_profile6_log - INFO - deleting model_transform...
2018-04-29 12:03:43,963 - memory_profile6_log - INFO - memory left after cleaning: 66.800 percent memory...
2018-04-29 12:03:43,964 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-29 12:03:43,967 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-29 12:03:44,148 - memory_profile6_log - INFO - Saving fitted_models as history...
2018-04-29 12:03:44,253 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 10
2018-04-29 12:03:44,255 - memory_profile6_log - INFO - processing batch-0
2018-04-29 12:03:44,444 - memory_profile6_log - INFO - processing batch-1
2018-04-29 12:03:44,628 - memory_profile6_log - INFO - processing batch-2
2018-04-29 12:03:44,819 - memory_profile6_log - INFO - processing batch-3
2018-04-29 12:03:45,035 - memory_profile6_log - INFO - processing batch-4
2018-04-29 12:03:45,230 - memory_profile6_log - INFO - processing batch-5
2018-04-29 12:03:45,446 - memory_profile6_log - INFO - processing batch-6
2018-04-29 12:03:45,651 - memory_profile6_log - INFO - processing batch-7
2018-04-29 12:03:45,835 - memory_profile6_log - INFO - processing batch-8
2018-04-29 12:03:46,030 - memory_profile6_log - INFO - processing batch-9
2018-04-29 12:03:46,250 - memory_profile6_log - INFO - deleting BR...
2018-04-29 12:03:46,252 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-04-29 12:03:46,263 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 12:03:46,263 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 12:03:46,263 - memory_profile6_log - INFO - ================================================

2018-04-29 12:03:46,263 - memory_profile6_log - INFO -    113    420.2 MiB    420.2 MiB   @profile

2018-04-29 12:03:46,266 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-29 12:03:46,267 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-29 12:03:46,269 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-29 12:03:46,269 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-29 12:03:46,269 - memory_profile6_log - INFO -    118                                      saveto="datastore"):

2018-04-29 12:03:46,270 - memory_profile6_log - INFO -    119                                 """

2018-04-29 12:03:46,270 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-29 12:03:46,272 - memory_profile6_log - INFO -    121                                 """

2018-04-29 12:03:46,272 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-29 12:03:46,273 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-29 12:03:46,273 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-29 12:03:46,273 - memory_profile6_log - INFO -    125    420.2 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-29 12:03:46,279 - memory_profile6_log - INFO -    126    428.2 MiB      8.0 MiB       df_dut = df_input.copy(deep=True)

2018-04-29 12:03:46,279 - memory_profile6_log - INFO -    127    428.2 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 12:03:46,283 - memory_profile6_log - INFO -    128                             

2018-04-29 12:03:46,285 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-29 12:03:46,289 - memory_profile6_log - INFO -    130    435.5 MiB      7.3 MiB       df_dt = df_current.copy(deep=True)

2018-04-29 12:03:46,289 - memory_profile6_log - INFO -    131    435.5 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 12:03:46,293 - memory_profile6_log - INFO -    132                             

2018-04-29 12:03:46,295 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-29 12:03:46,296 - memory_profile6_log - INFO -    134    435.5 MiB      0.0 MiB       t0 = time.time()

2018-04-29 12:03:46,296 - memory_profile6_log - INFO -    135    435.5 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-29 12:03:46,296 - memory_profile6_log - INFO -    136    435.5 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-29 12:03:46,296 - memory_profile6_log - INFO -    137    435.5 MiB      0.0 MiB       logger.info("apply on: %d total history data(D(t))", len(df_hist))

2018-04-29 12:03:46,299 - memory_profile6_log - INFO -    138                             

2018-04-29 12:03:46,302 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-29 12:03:46,302 - memory_profile6_log - INFO -    140    435.5 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-29 12:03:46,303 - memory_profile6_log - INFO -    141                             

2018-04-29 12:03:46,305 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-29 12:03:46,305 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-29 12:03:46,305 - memory_profile6_log - INFO -    144    437.9 MiB      2.4 MiB       NB = BR.processX(df_dut)

2018-04-29 12:03:46,305 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-29 12:03:46,305 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-29 12:03:46,306 - memory_profile6_log - INFO -    147    447.8 MiB     10.0 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-29 12:03:46,306 - memory_profile6_log - INFO -    148                                 """

2018-04-29 12:03:46,308 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-29 12:03:46,311 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-29 12:03:46,311 - memory_profile6_log - INFO -    151                                 """

2018-04-29 12:03:46,312 - memory_profile6_log - INFO -    152    447.8 MiB      0.0 MiB       fitby_sigmant = True

2018-04-29 12:03:46,312 - memory_profile6_log - INFO -    153    447.8 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-29 12:03:46,312 - memory_profile6_log - INFO -    154    447.8 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-29 12:03:46,313 - memory_profile6_log - INFO -    155    457.5 MiB      9.7 MiB                            'is_general']]

2018-04-29 12:03:46,313 - memory_profile6_log - INFO -    156    457.5 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-29 12:03:46,313 - memory_profile6_log - INFO -    157    457.5 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-29 12:03:46,315 - memory_profile6_log - INFO -    158    483.9 MiB     26.4 MiB                          verbose=False)

2018-04-29 12:03:46,315 - memory_profile6_log - INFO -    159    483.9 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-29 12:03:46,315 - memory_profile6_log - INFO -    160    483.9 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-29 12:03:46,315 - memory_profile6_log - INFO -    161                             

2018-04-29 12:03:46,316 - memory_profile6_log - INFO -    162                                 # ~~ and Transform ~~

2018-04-29 12:03:46,316 - memory_profile6_log - INFO -    163                                 #   handling current news interest == current date

2018-04-29 12:03:46,321 - memory_profile6_log - INFO -    164    483.9 MiB      0.0 MiB       if df_dt.empty:

2018-04-29 12:03:46,322 - memory_profile6_log - INFO -    165                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-29 12:03:46,322 - memory_profile6_log - INFO -    166                                     return None

2018-04-29 12:03:46,322 - memory_profile6_log - INFO -    167    484.9 MiB      1.1 MiB       NB = BR.processX(df_dt)

2018-04-29 12:03:46,323 - memory_profile6_log - INFO -    168    495.5 MiB     10.6 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-29 12:03:46,323 - memory_profile6_log - INFO -    169                             

2018-04-29 12:03:46,323 - memory_profile6_log - INFO -    170    495.5 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-29 12:03:46,325 - memory_profile6_log - INFO -    171    494.6 MiB     -0.9 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-29 12:03:46,325 - memory_profile6_log - INFO -    172    494.6 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-29 12:03:46,326 - memory_profile6_log - INFO -    173    494.6 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-29 12:03:46,326 - memory_profile6_log - INFO -    174    514.6 MiB     19.9 MiB                                                     verbose=False)

2018-04-29 12:03:46,328 - memory_profile6_log - INFO -    175                                 # ~~~ filter is general and specific topic ~~~

2018-04-29 12:03:46,328 - memory_profile6_log - INFO -    176                                 # the idea is just we need to rerank every topic according

2018-04-29 12:03:46,328 - memory_profile6_log - INFO -    177                                 # user_id and and is_general by p0_posterior

2018-04-29 12:03:46,328 - memory_profile6_log - INFO -    178    514.6 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-29 12:03:46,328 - memory_profile6_log - INFO -    179    516.5 MiB      1.9 MiB                                     'is_general']].groupby(['topic_id',

2018-04-29 12:03:46,334 - memory_profile6_log - INFO -    180    514.6 MiB     -1.9 MiB                                                             'is_general']

2018-04-29 12:03:46,335 - memory_profile6_log - INFO -    181                                                                                      ).size().to_frame().reset_index()

2018-04-29 12:03:46,335 - memory_profile6_log - INFO -    182                             

2018-04-29 12:03:46,335 - memory_profile6_log - INFO -    183                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-29 12:03:46,336 - memory_profile6_log - INFO -    184                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-29 12:03:46,336 - memory_profile6_log - INFO -    185    514.6 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-29 12:03:46,338 - memory_profile6_log - INFO -    186                             

2018-04-29 12:03:46,338 - memory_profile6_log - INFO -    187                                 # ~ start by provide rank for each topic type ~

2018-04-29 12:03:46,339 - memory_profile6_log - INFO -    188    538.6 MiB     24.0 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-29 12:03:46,339 - memory_profile6_log - INFO -    189    542.6 MiB      4.0 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-29 12:03:46,339 - memory_profile6_log - INFO -    190                             

2018-04-29 12:03:46,342 - memory_profile6_log - INFO -    191                                 # ~ set threshold to filter output

2018-04-29 12:03:46,346 - memory_profile6_log - INFO -    192    542.6 MiB      0.0 MiB       if threshold > 0:

2018-04-29 12:03:46,346 - memory_profile6_log - INFO -    193    542.6 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-29 12:03:46,346 - memory_profile6_log - INFO -    194    542.6 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-29 12:03:46,348 - memory_profile6_log - INFO -    195    541.5 MiB     -1.1 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-29 12:03:46,348 - memory_profile6_log - INFO -    196                             

2018-04-29 12:03:46,348 - memory_profile6_log - INFO -    197    541.5 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 12:03:46,349 - memory_profile6_log - INFO -    198    541.5 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-29 12:03:46,351 - memory_profile6_log - INFO -    199                             

2018-04-29 12:03:46,351 - memory_profile6_log - INFO -    200    541.5 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 12:03:46,355 - memory_profile6_log - INFO -    201                             

2018-04-29 12:03:46,355 - memory_profile6_log - INFO -    202    541.5 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-29 12:03:46,357 - memory_profile6_log - INFO -    203    541.5 MiB      0.0 MiB       del df_dut

2018-04-29 12:03:46,357 - memory_profile6_log - INFO -    204    541.5 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-29 12:03:46,357 - memory_profile6_log - INFO -    205    541.5 MiB      0.0 MiB       del df_dt

2018-04-29 12:03:46,358 - memory_profile6_log - INFO -    206    541.5 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-29 12:03:46,358 - memory_profile6_log - INFO -    207    541.5 MiB      0.0 MiB       del df_input

2018-04-29 12:03:46,358 - memory_profile6_log - INFO -    208    541.5 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-29 12:03:46,359 - memory_profile6_log - INFO -    209    532.7 MiB     -8.8 MiB       del df_input_X

2018-04-29 12:03:46,359 - memory_profile6_log - INFO -    210    532.7 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-29 12:03:46,361 - memory_profile6_log - INFO -    211    532.7 MiB      0.0 MiB       del df_current

2018-04-29 12:03:46,361 - memory_profile6_log - INFO -    212    532.7 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-29 12:03:46,361 - memory_profile6_log - INFO -    213    532.7 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-29 12:03:46,361 - memory_profile6_log - INFO -    214    532.7 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-29 12:03:46,361 - memory_profile6_log - INFO -    215    509.4 MiB    -23.3 MiB       del model_fit

2018-04-29 12:03:46,365 - memory_profile6_log - INFO -    216    509.4 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-29 12:03:46,371 - memory_profile6_log - INFO -    217    509.4 MiB      0.0 MiB       del result

2018-04-29 12:03:46,371 - memory_profile6_log - INFO -    218    509.4 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-29 12:03:46,371 - memory_profile6_log - INFO -    219                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 12:03:46,372 - memory_profile6_log - INFO -    220                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 12:03:46,375 - memory_profile6_log - INFO -    221    509.4 MiB      0.0 MiB       if savetrain:

2018-04-29 12:03:46,377 - memory_profile6_log - INFO -    222    514.9 MiB      5.4 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-29 12:03:46,378 - memory_profile6_log - INFO -    223    514.9 MiB      0.0 MiB           del model_transform

2018-04-29 12:03:46,378 - memory_profile6_log - INFO -    224    514.9 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-29 12:03:46,378 - memory_profile6_log - INFO -    225    514.9 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 12:03:46,380 - memory_profile6_log - INFO -    226                             

2018-04-29 12:03:46,381 - memory_profile6_log - INFO -    227    514.9 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-29 12:03:46,381 - memory_profile6_log - INFO -    228                                     # ~ Place your code to save the training model here ~

2018-04-29 12:03:46,381 - memory_profile6_log - INFO -    229    514.9 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-29 12:03:46,382 - memory_profile6_log - INFO -    230    514.9 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-29 12:03:46,382 - memory_profile6_log - INFO -    231    514.9 MiB      0.0 MiB               if multproc:

2018-04-29 12:03:46,385 - memory_profile6_log - INFO -    232    492.8 MiB    -22.1 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-29 12:03:46,387 - memory_profile6_log - INFO -    233                             

2018-04-29 12:03:46,388 - memory_profile6_log - INFO -    234    492.8 MiB      0.0 MiB                   logger.info("Saving fitted_models as history...")

2018-04-29 12:03:46,390 - memory_profile6_log - INFO -    235    492.8 MiB      0.0 MiB                   save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 12:03:46,390 - memory_profile6_log - INFO -    236    506.4 MiB     13.7 MiB                   fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-29 12:03:46,391 - memory_profile6_log - INFO -    237                             

2018-04-29 12:03:46,392 - memory_profile6_log - INFO -    238    511.9 MiB      5.5 MiB                   X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-29 12:03:46,394 - memory_profile6_log - INFO -    239    511.9 MiB      0.0 MiB                   logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-29 12:03:46,394 - memory_profile6_log - INFO -    240    513.4 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 12:03:46,397 - memory_profile6_log - INFO -    241    513.4 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 12:03:46,398 - memory_profile6_log - INFO -    242    513.4 MiB      1.5 MiB                       mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-29 12:03:46,398 - memory_profile6_log - INFO -    243                             

2018-04-29 12:03:46,400 - memory_profile6_log - INFO -    244    512.3 MiB     -1.2 MiB                   del X_split

2018-04-29 12:03:46,401 - memory_profile6_log - INFO -    245    512.3 MiB      0.0 MiB                   del BR

2018-04-29 12:03:46,401 - memory_profile6_log - INFO -    246    512.3 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-29 12:03:46,403 - memory_profile6_log - INFO -    247                             

2018-04-29 12:03:46,403 - memory_profile6_log - INFO -    248    512.3 MiB      0.0 MiB                   del save_sigma_nt

2018-04-29 12:03:46,404 - memory_profile6_log - INFO -    249    512.3 MiB      0.0 MiB                   logger.info("deleting save_sigma_nt...")

2018-04-29 12:03:46,404 - memory_profile6_log - INFO -    250                                         else:

2018-04-29 12:03:46,404 - memory_profile6_log - INFO -    251                                             mh.saveDatastore(model_transformsv)

2018-04-29 12:03:46,407 - memory_profile6_log - INFO -    252                                             

2018-04-29 12:03:46,408 - memory_profile6_log - INFO -    253                                     elif str(saveto).lower() == "elastic":

2018-04-29 12:03:46,411 - memory_profile6_log - INFO -    254                                         logger.info("Using ElasticSearch as storage...")

2018-04-29 12:03:46,411 - memory_profile6_log - INFO -    255                                         mh.saveElasticS(model_transformsv)

2018-04-29 12:03:46,413 - memory_profile6_log - INFO -    256                             

2018-04-29 12:03:46,413 - memory_profile6_log - INFO -    257                                     # need save sigma_nt for daily train

2018-04-29 12:03:46,414 - memory_profile6_log - INFO -    258                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-29 12:03:46,414 - memory_profile6_log - INFO -    259                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-29 12:03:46,414 - memory_profile6_log - INFO -    260    512.3 MiB      0.0 MiB           if start_date and end_date:

2018-04-29 12:03:46,415 - memory_profile6_log - INFO -    261                                         if not fitby_sigmant:

2018-04-29 12:03:46,418 - memory_profile6_log - INFO -    262                                             logging.info("Saving sigma Nt...")

2018-04-29 12:03:46,418 - memory_profile6_log - INFO -    263                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 12:03:46,421 - memory_profile6_log - INFO -    264                                             save_sigma_nt['start_date'] = start_date

2018-04-29 12:03:46,421 - memory_profile6_log - INFO -    265                                             save_sigma_nt['end_date'] = end_date

2018-04-29 12:03:46,421 - memory_profile6_log - INFO -    266                                             print save_sigma_nt.head(5)

2018-04-29 12:03:46,421 - memory_profile6_log - INFO -    267                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-29 12:03:46,423 - memory_profile6_log - INFO -    268                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-29 12:03:46,423 - memory_profile6_log - INFO -    269    512.3 MiB      0.0 MiB       return

2018-04-29 12:03:46,423 - memory_profile6_log - INFO - 


2018-04-29 12:03:46,424 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-29 12:23:00,022 - memory_profile6_log - INFO - Generating date range with N: 6
2018-04-29 12:23:00,025 - memory_profile6_log - INFO - date_generated: 
2018-04-29 12:23:00,025 - memory_profile6_log - INFO -  
2018-04-29 12:23:00,025 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 12:23:00,025 - memory_profile6_log - INFO - 

2018-04-29 12:23:00,026 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 12:23:00,026 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 12:23:00,026 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 12:23:00,151 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 12:23:00,154 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 12:24:56,684 - memory_profile6_log - INFO - Generating date range with N: 6
2018-04-29 12:24:56,687 - memory_profile6_log - INFO - date_generated: 
2018-04-29 12:24:56,687 - memory_profile6_log - INFO -  
2018-04-29 12:24:56,688 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0), datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 12:24:56,688 - memory_profile6_log - INFO - 

2018-04-29 12:24:56,688 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 12:24:56,688 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 12:24:56,690 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 12:24:56,821 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 12:24:56,823 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 12:27:09,190 - memory_profile6_log - INFO - Generating date range with N: 6
2018-04-29 12:27:09,194 - memory_profile6_log - INFO - date_generated: 
2018-04-29 12:27:09,194 - memory_profile6_log - INFO -  
2018-04-29 12:27:09,194 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 12:27:09,194 - memory_profile6_log - INFO - 

2018-04-29 12:27:09,194 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 12:27:09,194 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 12:27:09,196 - memory_profile6_log - INFO - using end date: 2018-04-08
2018-04-29 12:27:09,319 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 12:27:09,322 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 12:27:47,746 - memory_profile6_log - INFO - Generating date range with N: 6
2018-04-29 12:27:47,749 - memory_profile6_log - INFO - date_generated: 
2018-04-29 12:27:47,750 - memory_profile6_log - INFO -  
2018-04-29 12:27:47,750 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 8, 0, 0), datetime.datetime(2018, 4, 9, 0, 0), datetime.datetime(2018, 4, 10, 0, 0), datetime.datetime(2018, 4, 11, 0, 0), datetime.datetime(2018, 4, 12, 0, 0), datetime.datetime(2018, 4, 13, 0, 0), datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 12:27:47,750 - memory_profile6_log - INFO - 

2018-04-29 12:27:47,750 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 12:27:47,750 - memory_profile6_log - INFO - using start date: 2018-04-08 00:00:00
2018-04-29 12:27:47,750 - memory_profile6_log - INFO - using end date: 2018-04-08
2018-04-29 12:27:47,878 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 12:27:47,882 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-08
2018-04-29 12:28:28,931 - memory_profile6_log - INFO - Generating date range with N: 6
2018-04-29 12:28:28,934 - memory_profile6_log - INFO - date_generated: 
2018-04-29 12:28:28,934 - memory_profile6_log - INFO -  
2018-04-29 12:28:28,936 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 8, 0, 0), datetime.datetime(2018, 4, 9, 0, 0), datetime.datetime(2018, 4, 10, 0, 0), datetime.datetime(2018, 4, 11, 0, 0), datetime.datetime(2018, 4, 12, 0, 0), datetime.datetime(2018, 4, 13, 0, 0), datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 12:28:28,937 - memory_profile6_log - INFO - 

2018-04-29 12:28:28,937 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 12:28:28,937 - memory_profile6_log - INFO - using start date: 2018-04-08 00:00:00
2018-04-29 12:28:28,937 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 12:28:29,069 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 12:28:29,072 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-08
2018-04-29 12:30:09,130 - memory_profile6_log - INFO - size of df: 87.23 MB
2018-04-29 12:30:09,134 - memory_profile6_log - INFO - getting total: 344288 training data(genuine interest) for date: 2018-04-08
2018-04-29 12:30:09,217 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 12:30:09,217 - memory_profile6_log - INFO - Appending history data...
2018-04-29 12:30:09,219 - memory_profile6_log - INFO - processing batch-0
2018-04-29 12:30:09,220 - memory_profile6_log - INFO - creating list history data...
2018-04-29 12:30:09,309 - memory_profile6_log - INFO - call history data...
2018-04-29 12:45:21,756 - memory_profile6_log - INFO - Generating date range with N: 6
2018-04-29 12:45:21,759 - memory_profile6_log - INFO - date_generated: 
2018-04-29 12:45:21,759 - memory_profile6_log - INFO -  
2018-04-29 12:45:21,759 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 8, 0, 0), datetime.datetime(2018, 4, 9, 0, 0), datetime.datetime(2018, 4, 10, 0, 0), datetime.datetime(2018, 4, 11, 0, 0), datetime.datetime(2018, 4, 12, 0, 0), datetime.datetime(2018, 4, 13, 0, 0), datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 12:45:21,760 - memory_profile6_log - INFO - 

2018-04-29 12:45:21,760 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 12:45:21,762 - memory_profile6_log - INFO - using start date: 2018-04-08 00:00:00
2018-04-29 12:45:21,762 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 12:45:21,881 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 12:45:21,884 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-08
2018-04-29 12:46:55,066 - memory_profile6_log - INFO - size of df: 87.23 MB
2018-04-29 12:46:55,069 - memory_profile6_log - INFO - getting total: 344288 training data(genuine interest) for date: 2018-04-08
2018-04-29 12:46:55,127 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 12:46:55,128 - memory_profile6_log - INFO - Appending history data...
2018-04-29 12:46:55,128 - memory_profile6_log - INFO - processing batch-0
2018-04-29 12:46:55,130 - memory_profile6_log - INFO - creating list history data...
2018-04-29 12:46:55,217 - memory_profile6_log - INFO - call history data...
2018-04-29 12:48:29,744 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 12:48:31,190 - memory_profile6_log - INFO - processing batch-1
2018-04-29 12:48:31,190 - memory_profile6_log - INFO - creating list history data...
2018-04-29 12:48:31,253 - memory_profile6_log - INFO - call history data...
2018-04-29 12:50:03,388 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 12:50:05,000 - memory_profile6_log - INFO - processing batch-2
2018-04-29 12:50:05,006 - memory_profile6_log - INFO - creating list history data...
2018-04-29 12:50:05,075 - memory_profile6_log - INFO - call history data...
2018-04-29 12:51:41,326 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 12:51:42,759 - memory_profile6_log - INFO - processing batch-3
2018-04-29 12:51:42,760 - memory_profile6_log - INFO - creating list history data...
2018-04-29 12:51:42,826 - memory_profile6_log - INFO - call history data...
2018-04-29 12:53:16,128 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 12:53:17,562 - memory_profile6_log - INFO - processing batch-4
2018-04-29 12:53:17,563 - memory_profile6_log - INFO - creating list history data...
2018-04-29 12:53:17,624 - memory_profile6_log - INFO - call history data...
2018-04-29 12:54:51,486 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 12:54:52,956 - memory_profile6_log - INFO - Appending training data...
2018-04-29 12:54:52,960 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-09
2018-04-29 12:56:39,528 - memory_profile6_log - INFO - size of df: 102.25 MB
2018-04-29 12:56:39,529 - memory_profile6_log - INFO - getting total: 403532 training data(genuine interest) for date: 2018-04-09
2018-04-29 12:56:39,601 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 12:56:39,602 - memory_profile6_log - INFO - Appending history data...
2018-04-29 12:56:39,605 - memory_profile6_log - INFO - processing batch-0
2018-04-29 12:56:39,607 - memory_profile6_log - INFO - creating list history data...
2018-04-29 12:56:39,703 - memory_profile6_log - INFO - call history data...
2018-04-29 12:58:18,256 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 12:58:19,812 - memory_profile6_log - INFO - processing batch-1
2018-04-29 12:58:19,815 - memory_profile6_log - INFO - creating list history data...
2018-04-29 12:58:19,904 - memory_profile6_log - INFO - call history data...
2018-04-29 12:59:56,663 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 12:59:58,253 - memory_profile6_log - INFO - processing batch-2
2018-04-29 12:59:58,255 - memory_profile6_log - INFO - creating list history data...
2018-04-29 12:59:58,342 - memory_profile6_log - INFO - call history data...
2018-04-29 13:01:37,622 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:01:39,217 - memory_profile6_log - INFO - processing batch-3
2018-04-29 13:01:39,219 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:01:39,299 - memory_profile6_log - INFO - call history data...
2018-04-29 13:03:17,069 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:03:18,674 - memory_profile6_log - INFO - processing batch-4
2018-04-29 13:03:18,677 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:03:18,746 - memory_profile6_log - INFO - call history data...
2018-04-29 13:04:57,709 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:04:59,322 - memory_profile6_log - INFO - Appending training data...
2018-04-29 13:04:59,326 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-10
2018-04-29 13:06:53,644 - memory_profile6_log - INFO - size of df: 108.51 MB
2018-04-29 13:06:53,645 - memory_profile6_log - INFO - getting total: 427635 training data(genuine interest) for date: 2018-04-10
2018-04-29 13:06:53,726 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 13:06:53,726 - memory_profile6_log - INFO - Appending history data...
2018-04-29 13:06:53,729 - memory_profile6_log - INFO - processing batch-0
2018-04-29 13:06:53,730 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:06:53,812 - memory_profile6_log - INFO - call history data...
2018-04-29 13:08:37,424 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:08:39,076 - memory_profile6_log - INFO - processing batch-1
2018-04-29 13:08:39,078 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:08:39,170 - memory_profile6_log - INFO - call history data...
2018-04-29 13:11:01,941 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:11:01,944 - memory_profile6_log - INFO - date_generated: 
2018-04-29 13:11:01,944 - memory_profile6_log - INFO -  
2018-04-29 13:11:01,944 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 8, 0, 0), datetime.datetime(2018, 4, 9, 0, 0)]
2018-04-29 13:11:01,944 - memory_profile6_log - INFO - 

2018-04-29 13:11:01,946 - memory_profile6_log - INFO - using current date: 2018-04-29 13:11:01.944000
2018-04-29 13:11:01,946 - memory_profile6_log - INFO - using start date: 2018-04-08 00:00:00
2018-04-29 13:11:01,946 - memory_profile6_log - INFO - using end date: 2018-04-09
2018-04-29 13:11:02,071 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 13:11:02,073 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-08
2018-04-29 13:11:42,326 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:11:42,329 - memory_profile6_log - INFO - date_generated: 
2018-04-29 13:11:42,329 - memory_profile6_log - INFO -  
2018-04-29 13:11:42,329 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 9, 0, 0)]
2018-04-29 13:11:42,329 - memory_profile6_log - INFO - 

2018-04-29 13:11:42,329 - memory_profile6_log - INFO - using current date: 2018-04-29 13:11:42.330000
2018-04-29 13:11:42,331 - memory_profile6_log - INFO - using start date: 2018-04-09 00:00:00
2018-04-29 13:11:42,331 - memory_profile6_log - INFO - using end date: 2018-04-09
2018-04-29 13:11:42,456 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 13:11:42,460 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-09
2018-04-29 13:13:29,351 - memory_profile6_log - INFO - size of df: 102.25 MB
2018-04-29 13:13:29,351 - memory_profile6_log - INFO - getting total: 403532 training data(genuine interest) for date: 2018-04-09
2018-04-29 13:13:29,407 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 13:13:29,408 - memory_profile6_log - INFO - Appending history data...
2018-04-29 13:13:29,410 - memory_profile6_log - INFO - processing batch-0
2018-04-29 13:13:29,411 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:13:29,507 - memory_profile6_log - INFO - call history data...
2018-04-29 13:15:27,878 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:15:29,519 - memory_profile6_log - INFO - processing batch-1
2018-04-29 13:15:29,520 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:15:29,591 - memory_profile6_log - INFO - call history data...
2018-04-29 13:17:11,180 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:17:12,960 - memory_profile6_log - INFO - processing batch-2
2018-04-29 13:17:12,963 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:17:13,046 - memory_profile6_log - INFO - call history data...
2018-04-29 13:18:53,845 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:18:55,714 - memory_profile6_log - INFO - processing batch-3
2018-04-29 13:18:55,717 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:18:55,799 - memory_profile6_log - INFO - call history data...
2018-04-29 13:20:58,088 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:21:00,032 - memory_profile6_log - INFO - processing batch-4
2018-04-29 13:21:00,045 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:21:00,125 - memory_profile6_log - INFO - call history data...
2018-04-29 13:22:37,661 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:22:39,673 - memory_profile6_log - INFO - Appending training data...
2018-04-29 13:22:39,674 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 13:22:39,677 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 13:22:39,694 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 13:22:39,694 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 13:22:39,696 - memory_profile6_log - INFO - ================================================

2018-04-29 13:22:39,697 - memory_profile6_log - INFO -    295     86.7 MiB     86.7 MiB   @profile

2018-04-29 13:22:39,698 - memory_profile6_log - INFO -    296                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 13:22:39,700 - memory_profile6_log - INFO -    297     86.7 MiB      0.0 MiB       bq_client = client

2018-04-29 13:22:39,701 - memory_profile6_log - INFO -    298     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 13:22:39,703 - memory_profile6_log - INFO -    299                             

2018-04-29 13:22:39,706 - memory_profile6_log - INFO -    300     86.7 MiB      0.0 MiB       datalist = []

2018-04-29 13:22:39,707 - memory_profile6_log - INFO -    301     86.7 MiB      0.0 MiB       datalist_hist = []

2018-04-29 13:22:39,709 - memory_profile6_log - INFO -    302                             

2018-04-29 13:22:39,710 - memory_profile6_log - INFO -    303     86.7 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 13:22:39,710 - memory_profile6_log - INFO -    304    654.5 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 13:22:39,713 - memory_profile6_log - INFO -    305    393.1 MiB    306.4 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 13:22:39,713 - memory_profile6_log - INFO -    306    393.1 MiB      0.0 MiB           if tframe is not None:

2018-04-29 13:22:39,716 - memory_profile6_log - INFO -    307    393.1 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 13:22:39,717 - memory_profile6_log - INFO -    308    405.7 MiB     12.5 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 13:22:39,719 - memory_profile6_log - INFO -    309    405.7 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 13:22:39,720 - memory_profile6_log - INFO -    310    405.7 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 13:22:39,720 - memory_profile6_log - INFO -    311    654.5 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 13:22:39,721 - memory_profile6_log - INFO -    312                                                 # ~ loading history

2018-04-29 13:22:39,723 - memory_profile6_log - INFO -    313                                                 """

2018-04-29 13:22:39,723 - memory_profile6_log - INFO -    314                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 13:22:39,726 - memory_profile6_log - INFO -    315                                                 """

2018-04-29 13:22:39,729 - memory_profile6_log - INFO -    316    629.5 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 13:22:39,730 - memory_profile6_log - INFO -    317                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 13:22:39,730 - memory_profile6_log - INFO -    318    629.5 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 13:22:39,732 - memory_profile6_log - INFO -    319                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 13:22:39,733 - memory_profile6_log - INFO -    320    630.2 MiB      5.4 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 13:22:39,733 - memory_profile6_log - INFO -    321                             

2018-04-29 13:22:39,736 - memory_profile6_log - INFO -    322    630.2 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 13:22:39,737 - memory_profile6_log - INFO -    323    704.4 MiB    527.6 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 13:22:39,739 - memory_profile6_log - INFO -    324                             

2018-04-29 13:22:39,740 - memory_profile6_log - INFO -    325                                                 # me = os.getpid()

2018-04-29 13:22:39,740 - memory_profile6_log - INFO -    326                                                 # kill_proc_tree(me)

2018-04-29 13:22:39,743 - memory_profile6_log - INFO -    327                             

2018-04-29 13:22:39,743 - memory_profile6_log - INFO -    328    704.4 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 13:22:39,744 - memory_profile6_log - INFO -    329    706.4 MiB     -0.7 MiB                       for m in h_frame:

2018-04-29 13:22:39,747 - memory_profile6_log - INFO -    330    706.4 MiB     -0.7 MiB                           if m is not None:

2018-04-29 13:22:39,749 - memory_profile6_log - INFO -    331    706.4 MiB     -0.7 MiB                               if len(m) > 0:

2018-04-29 13:22:39,750 - memory_profile6_log - INFO -    332    706.4 MiB      8.9 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 13:22:39,750 - memory_profile6_log - INFO -    333    654.5 MiB   -293.6 MiB                       del h_frame

2018-04-29 13:22:39,752 - memory_profile6_log - INFO -    334    654.5 MiB      0.0 MiB                       del lhistory

2018-04-29 13:22:39,753 - memory_profile6_log - INFO -    335                             

2018-04-29 13:22:39,753 - memory_profile6_log - INFO -    336    654.5 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 13:22:39,755 - memory_profile6_log - INFO -    337    654.5 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 13:22:39,756 - memory_profile6_log - INFO -    338                                     else: 

2018-04-29 13:22:39,757 - memory_profile6_log - INFO -    339                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 13:22:39,759 - memory_profile6_log - INFO -    340    654.5 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 13:22:39,759 - memory_profile6_log - INFO -    341    654.5 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 13:22:39,762 - memory_profile6_log - INFO -    342                             

2018-04-29 13:22:39,762 - memory_profile6_log - INFO -    343    654.5 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 13:22:39,763 - memory_profile6_log - INFO - 


2018-04-29 13:22:41,141 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 13:22:41,220 - memory_profile6_log - INFO -     p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0    0.001543        1239.605475       485            1249.605475  10960288  1628aad345040e-0b2b0baa7aec7f8-4c322073-e1000-...
1    0.004470          72.776171       168              82.776171  22291119  1613bc4a7ac73-0af9eb163e2375-7d65040b-38400-16...
2    0.001543         384.718661       181             394.718661  10960288  1618a2dbbb9f-0da12bda3-133f7e20-38400-1618a2db...
3    0.000088        2407.461735        14            2417.461735  11911567  1613a83cda155-0c2edb2c45dedd-310a717d-38400-16...
4    0.001543         289.338273       228             299.338273  10960288  1622e1ec6ff15e-0b6bf723a0fb21-8030802-38400-16...
5    0.000088        2637.740683        69            2647.740683  11911567  1616b2c0b4c2d2-0db3e1132c1133-547f0e1a-4a640-1...
6    0.001543         155.294553       413             165.294553  10960288  1612d25f2726b-07eb9362fc5d22-482e0a73-38400-16...
7    0.000088        6740.892857        25            6750.892857  11911567  1618d8380e24a-0af4a6412042c5-1e46677b-38400-16...
8    0.000088        2712.798345        82            2722.798345  11911567  161476433b2a5-0855276a0c9606-167c2c34-38400-16...
9    0.001543        1083.999727        71            1093.999727  10960288  16298ec6d04561-0d34c035b60989-51693374-140000-...
10   0.000088         869.792627       341             879.792627  11911567  1628b5101bd170-0aaa1a9ed98fb4-2f233868-4df28-1...
11   0.001543         238.595157      1129             248.595157  10960288  1610cb3a8c1348-0c7cec1541e7ac-4323461-100200-1...
12   0.001543         144.669136       114             154.669136  10960288  16130c0d6563b8-091947d71b8641-76313118-3d10d-1...
13   0.001543         509.021036        72             519.021036  10960288  1611c4ae4c2239-0fd14eaf0808ba-5768397b-100200-...
14   0.004470         167.484887        73             177.484887  22291119  16137d86cfc93-0a4e58c10a7965-39626377-38400-16...
15   0.000088        3435.617685      2635            3445.617685  11911567  16116a35abf7b-02ebd04ab0ab75-4323461-100200-16...
16   0.001543        1837.258279       275            1847.258279  10960288  1627fb69adcf-0580e12c76435f-72203a15-38400-162...
17   0.000088        6740.892857        15            6750.892857  11911567  162294766a319c-0d16b05bb32617-18741b1f-38400-1...
18   0.001543        1610.180397        59            1620.180397  10960288  161f36169fce02-03becb8d4c95b1-b353461-15f900-1...
19   0.000088         802.487245        42             812.487245  11911567  161452195ee549-0bb068a0c-4f2c7b27-3d10d-161452...
2018-04-29 13:22:41,223 - memory_profile6_log - INFO - 

2018-04-29 13:22:41,365 - memory_profile6_log - INFO - size of big_frame_hist: 111.10 MB
2018-04-29 13:22:41,484 - memory_profile6_log - INFO - size of big_frame: 102.25 MB
2018-04-29 13:22:41,503 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-10
2018-04-29 13:31:32,665 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:31:32,668 - memory_profile6_log - INFO - date_generated: 
2018-04-29 13:31:32,668 - memory_profile6_log - INFO -  
2018-04-29 13:31:32,670 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 28, 13, 31, 32, 669000)]
2018-04-29 13:31:32,671 - memory_profile6_log - INFO - 

2018-04-29 13:31:32,671 - memory_profile6_log - INFO - using current date: 2018-04-29 13:31:32.669000
2018-04-29 13:31:32,671 - memory_profile6_log - INFO - using start date: 2018-04-28 00:00:00
2018-04-29 13:31:32,673 - memory_profile6_log - INFO - using end date: 2018-04-28
2018-04-29 13:31:32,803 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 13:31:32,809 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-28
2018-04-29 13:31:35,388 - memory_profile6_log - INFO - size of df: 24.0 Byte
2018-04-29 13:31:35,390 - memory_profile6_log - INFO - 2018-04-28 data is empty!
2018-04-29 13:31:35,391 - memory_profile6_log - INFO - tframe for date: 2018-04-28 is empty
2018-04-29 13:31:35,392 - memory_profile6_log - INFO - len datalist: 0
2018-04-29 13:31:35,394 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 13:31:35,397 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 13:31:35,397 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 13:31:35,398 - memory_profile6_log - INFO - ================================================

2018-04-29 13:31:35,401 - memory_profile6_log - INFO -    295     86.7 MiB     86.7 MiB   @profile

2018-04-29 13:31:35,401 - memory_profile6_log - INFO -    296                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 13:31:35,403 - memory_profile6_log - INFO -    297     86.7 MiB      0.0 MiB       bq_client = client

2018-04-29 13:31:35,404 - memory_profile6_log - INFO -    298     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 13:31:35,404 - memory_profile6_log - INFO -    299                             

2018-04-29 13:31:35,405 - memory_profile6_log - INFO -    300     86.7 MiB      0.0 MiB       datalist = []

2018-04-29 13:31:35,407 - memory_profile6_log - INFO -    301     86.7 MiB      0.0 MiB       datalist_hist = []

2018-04-29 13:31:35,407 - memory_profile6_log - INFO -    302                             

2018-04-29 13:31:35,411 - memory_profile6_log - INFO -    303     86.7 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 13:31:35,413 - memory_profile6_log - INFO -    304     90.2 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 13:31:35,414 - memory_profile6_log - INFO -    305     90.2 MiB      3.5 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 13:31:35,414 - memory_profile6_log - INFO -    306     90.2 MiB      0.0 MiB           if tframe is not None:

2018-04-29 13:31:35,415 - memory_profile6_log - INFO -    307                                         if not tframe.empty:

2018-04-29 13:31:35,417 - memory_profile6_log - INFO -    308                                             X_split = np.array_split(tframe, 5)

2018-04-29 13:31:35,417 - memory_profile6_log - INFO -    309                                             logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 13:31:35,417 - memory_profile6_log - INFO -    310                                             logger.info("Appending history data...")

2018-04-29 13:31:35,421 - memory_profile6_log - INFO -    311                                             for ix in range(len(X_split)):

2018-04-29 13:31:35,421 - memory_profile6_log - INFO -    312                                                 # ~ loading history

2018-04-29 13:31:35,423 - memory_profile6_log - INFO -    313                                                 """

2018-04-29 13:31:35,424 - memory_profile6_log - INFO -    314                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 13:31:35,424 - memory_profile6_log - INFO -    315                                                 """

2018-04-29 13:31:35,426 - memory_profile6_log - INFO -    316                                                 logger.info("processing batch-%d", ix)

2018-04-29 13:31:35,427 - memory_profile6_log - INFO -    317                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 13:31:35,427 - memory_profile6_log - INFO -    318                                                 logger.info("creating list history data...")

2018-04-29 13:31:35,427 - memory_profile6_log - INFO -    319                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 13:31:35,430 - memory_profile6_log - INFO -    320                                                 lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 13:31:35,431 - memory_profile6_log - INFO -    321                             

2018-04-29 13:31:35,433 - memory_profile6_log - INFO -    322                                                 logger.info("call history data...")

2018-04-29 13:31:35,434 - memory_profile6_log - INFO -    323                                                 h_frame = mh.loadDSHistory(lhistory)

2018-04-29 13:31:35,434 - memory_profile6_log - INFO -    324                             

2018-04-29 13:31:35,436 - memory_profile6_log - INFO -    325                                                 # me = os.getpid()

2018-04-29 13:31:35,437 - memory_profile6_log - INFO -    326                                                 # kill_proc_tree(me)

2018-04-29 13:31:35,438 - memory_profile6_log - INFO -    327                             

2018-04-29 13:31:35,440 - memory_profile6_log - INFO -    328                                                 logger.info("done collecting history data, appending now...")

2018-04-29 13:31:35,443 - memory_profile6_log - INFO -    329                                                 for m in h_frame:

2018-04-29 13:31:35,444 - memory_profile6_log - INFO -    330                                                     if m is not None:

2018-04-29 13:31:35,444 - memory_profile6_log - INFO -    331                                                         if len(m) > 0:

2018-04-29 13:31:35,447 - memory_profile6_log - INFO -    332                                                             datalist_hist.append(pd.DataFrame(m))

2018-04-29 13:31:35,447 - memory_profile6_log - INFO -    333                                                 del h_frame

2018-04-29 13:31:35,448 - memory_profile6_log - INFO -    334                                                 del lhistory

2018-04-29 13:31:35,450 - memory_profile6_log - INFO -    335                             

2018-04-29 13:31:35,450 - memory_profile6_log - INFO -    336                                             logger.info("Appending training data...")

2018-04-29 13:31:35,450 - memory_profile6_log - INFO -    337                                             datalist.append(tframe)

2018-04-29 13:31:35,451 - memory_profile6_log - INFO -    338                                     else: 

2018-04-29 13:31:35,454 - memory_profile6_log - INFO -    339     90.2 MiB      0.0 MiB               logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 13:31:35,456 - memory_profile6_log - INFO -    340     90.2 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 13:31:35,457 - memory_profile6_log - INFO -    341     90.2 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 13:31:35,457 - memory_profile6_log - INFO -    342                             

2018-04-29 13:31:35,457 - memory_profile6_log - INFO -    343     90.2 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 13:31:35,459 - memory_profile6_log - INFO - 


2018-04-29 13:31:35,460 - memory_profile6_log - INFO - Training cannot be empty..
2018-04-29 13:31:35,460 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 13:31:35,460 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 13:31:35,461 - memory_profile6_log - INFO - ================================================

2018-04-29 13:31:35,464 - memory_profile6_log - INFO -    345     86.6 MiB     86.6 MiB   @profile

2018-04-29 13:31:35,464 - memory_profile6_log - INFO -    346                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 13:31:35,467 - memory_profile6_log - INFO -    347     86.7 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 13:31:35,467 - memory_profile6_log - INFO -    348     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 13:31:35,467 - memory_profile6_log - INFO -    349                             

2018-04-29 13:31:35,467 - memory_profile6_log - INFO -    350                                 # ~~~ Begin collecting data ~~~

2018-04-29 13:31:35,469 - memory_profile6_log - INFO -    351     86.7 MiB      0.0 MiB       t0 = time.time()

2018-04-29 13:31:35,470 - memory_profile6_log - INFO -    352     90.2 MiB      3.6 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 13:31:35,470 - memory_profile6_log - INFO -    353     90.2 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 13:31:35,471 - memory_profile6_log - INFO -    354     90.2 MiB      0.0 MiB           logger.info("Training cannot be empty..")

2018-04-29 13:31:35,473 - memory_profile6_log - INFO -    355     90.2 MiB      0.0 MiB           return False

2018-04-29 13:31:35,473 - memory_profile6_log - INFO -    356                                 big_frame_hist = pd.concat(datalist_hist)

2018-04-29 13:31:35,473 - memory_profile6_log - INFO -    357                                 print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 13:31:35,476 - memory_profile6_log - INFO -    358                                 logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 13:31:35,476 - memory_profile6_log - INFO -    359                             

2018-04-29 13:31:35,477 - memory_profile6_log - INFO -    360                                 big_frame = pd.concat(datalist)

2018-04-29 13:31:35,477 - memory_profile6_log - INFO -    361                                 logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 13:31:35,480 - memory_profile6_log - INFO -    362                                 del datalist

2018-04-29 13:31:35,480 - memory_profile6_log - INFO -    363                             

2018-04-29 13:31:35,480 - memory_profile6_log - INFO -    364                                 big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 13:31:35,482 - memory_profile6_log - INFO -    365                             

2018-04-29 13:31:35,482 - memory_profile6_log - INFO -    366                                 # ~ get current news interest ~

2018-04-29 13:31:35,483 - memory_profile6_log - INFO -    367                                 if not cd:

2018-04-29 13:31:35,483 - memory_profile6_log - INFO -    368                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 13:31:35,483 - memory_profile6_log - INFO -    369                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 13:31:35,486 - memory_profile6_log - INFO -    370                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 13:31:35,487 - memory_profile6_log - INFO -    371                                 else:

2018-04-29 13:31:35,490 - memory_profile6_log - INFO -    372                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 13:31:35,490 - memory_profile6_log - INFO -    373                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 13:31:35,490 - memory_profile6_log - INFO -    374                             

2018-04-29 13:31:35,492 - memory_profile6_log - INFO -    375                                     # safe handling of query parameter

2018-04-29 13:31:35,492 - memory_profile6_log - INFO -    376                                     query_params = [

2018-04-29 13:31:35,493 - memory_profile6_log - INFO -    377                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 13:31:35,493 - memory_profile6_log - INFO -    378                                     ]

2018-04-29 13:31:35,494 - memory_profile6_log - INFO -    379                             

2018-04-29 13:31:35,494 - memory_profile6_log - INFO -    380                                     job_config.query_parameters = query_params

2018-04-29 13:31:35,500 - memory_profile6_log - INFO -    381                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 13:31:35,500 - memory_profile6_log - INFO -    382                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 13:31:35,502 - memory_profile6_log - INFO -    383                             

2018-04-29 13:31:35,503 - memory_profile6_log - INFO -    384                                 current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 13:31:35,503 - memory_profile6_log - INFO -    385                                 current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 13:31:35,503 - memory_profile6_log - INFO -    386                                                                        format='%Y-%m-%d', errors='coerce')

2018-04-29 13:31:35,506 - memory_profile6_log - INFO -    387                                 logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 13:31:35,506 - memory_profile6_log - INFO -    388                                 train_time = time.time() - t0

2018-04-29 13:31:35,506 - memory_profile6_log - INFO -    389                                 logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 13:31:35,506 - memory_profile6_log - INFO -    390                             

2018-04-29 13:31:35,506 - memory_profile6_log - INFO -    391                                 return big_frame, current_frame, big_frame_hist

2018-04-29 13:31:35,507 - memory_profile6_log - INFO - 


2018-04-29 13:31:35,507 - memory_profile6_log - INFO - Train return False, please cek your data availability
2018-04-29 13:32:09,723 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:32:09,726 - memory_profile6_log - INFO - date_generated: 
2018-04-29 13:32:09,726 - memory_profile6_log - INFO -  
2018-04-29 13:32:09,727 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 28, 13, 32, 9, 727000)]
2018-04-29 13:32:09,727 - memory_profile6_log - INFO - 

2018-04-29 13:32:09,729 - memory_profile6_log - INFO - using current date: 2018-04-29 13:32:09.727000
2018-04-29 13:32:09,729 - memory_profile6_log - INFO - using start date: 2018-04-28 00:00:00
2018-04-29 13:32:09,730 - memory_profile6_log - INFO - using end date: 2018-04-28
2018-04-29 13:32:09,871 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 13:32:09,875 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-28
2018-04-29 13:32:13,559 - memory_profile6_log - INFO - size of df: 24.0 Byte
2018-04-29 13:32:13,561 - memory_profile6_log - INFO - 2018-04-28 data is empty!
2018-04-29 13:32:13,562 - memory_profile6_log - INFO - tframe for date: 2018-04-28 is empty
2018-04-29 13:32:13,562 - memory_profile6_log - INFO - len datalist: 0
2018-04-29 13:32:13,565 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 13:32:13,565 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 13:32:13,566 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 13:32:13,568 - memory_profile6_log - INFO - ================================================

2018-04-29 13:32:13,569 - memory_profile6_log - INFO -    295     86.4 MiB     86.4 MiB   @profile

2018-04-29 13:32:13,569 - memory_profile6_log - INFO -    296                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 13:32:13,569 - memory_profile6_log - INFO -    297     86.4 MiB      0.0 MiB       bq_client = client

2018-04-29 13:32:13,571 - memory_profile6_log - INFO -    298     86.4 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 13:32:13,572 - memory_profile6_log - INFO -    299                             

2018-04-29 13:32:13,572 - memory_profile6_log - INFO -    300     86.4 MiB      0.0 MiB       datalist = []

2018-04-29 13:32:13,572 - memory_profile6_log - INFO -    301     86.4 MiB      0.0 MiB       datalist_hist = []

2018-04-29 13:32:13,573 - memory_profile6_log - INFO -    302                             

2018-04-29 13:32:13,573 - memory_profile6_log - INFO -    303     86.4 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 13:32:13,575 - memory_profile6_log - INFO -    304     90.1 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 13:32:13,575 - memory_profile6_log - INFO -    305     90.1 MiB      3.7 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 13:32:13,576 - memory_profile6_log - INFO -    306     90.1 MiB      0.0 MiB           if tframe is not None:

2018-04-29 13:32:13,578 - memory_profile6_log - INFO -    307                                         if not tframe.empty:

2018-04-29 13:32:13,578 - memory_profile6_log - INFO -    308                                             X_split = np.array_split(tframe, 5)

2018-04-29 13:32:13,579 - memory_profile6_log - INFO -    309                                             logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 13:32:13,581 - memory_profile6_log - INFO -    310                                             logger.info("Appending history data...")

2018-04-29 13:32:13,582 - memory_profile6_log - INFO -    311                                             for ix in range(len(X_split)):

2018-04-29 13:32:13,582 - memory_profile6_log - INFO -    312                                                 # ~ loading history

2018-04-29 13:32:13,584 - memory_profile6_log - INFO -    313                                                 """

2018-04-29 13:32:13,585 - memory_profile6_log - INFO -    314                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 13:32:13,586 - memory_profile6_log - INFO -    315                                                 """

2018-04-29 13:32:13,589 - memory_profile6_log - INFO -    316                                                 logger.info("processing batch-%d", ix)

2018-04-29 13:32:13,592 - memory_profile6_log - INFO -    317                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 13:32:13,592 - memory_profile6_log - INFO -    318                                                 logger.info("creating list history data...")

2018-04-29 13:32:13,595 - memory_profile6_log - INFO -    319                                                 lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 13:32:13,595 - memory_profile6_log - INFO -    320                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 13:32:13,596 - memory_profile6_log - INFO -    321                             

2018-04-29 13:32:13,598 - memory_profile6_log - INFO -    322                                                 logger.info("call history data...")

2018-04-29 13:32:13,601 - memory_profile6_log - INFO -    323                                                 h_frame = mh.loadDSHistory(lhistory)

2018-04-29 13:32:13,602 - memory_profile6_log - INFO -    324                             

2018-04-29 13:32:13,604 - memory_profile6_log - INFO -    325                                                 # me = os.getpid()

2018-04-29 13:32:13,605 - memory_profile6_log - INFO -    326                                                 # kill_proc_tree(me)

2018-04-29 13:32:13,607 - memory_profile6_log - INFO -    327                             

2018-04-29 13:32:13,607 - memory_profile6_log - INFO -    328                                                 logger.info("done collecting history data, appending now...")

2018-04-29 13:32:13,608 - memory_profile6_log - INFO -    329                                                 for m in h_frame:

2018-04-29 13:32:13,608 - memory_profile6_log - INFO -    330                                                     if m is not None:

2018-04-29 13:32:13,611 - memory_profile6_log - INFO -    331                                                         if len(m) > 0:

2018-04-29 13:32:13,614 - memory_profile6_log - INFO -    332                                                             datalist_hist.append(pd.DataFrame(m))

2018-04-29 13:32:13,615 - memory_profile6_log - INFO -    333                                                 del h_frame

2018-04-29 13:32:13,615 - memory_profile6_log - INFO -    334                                                 del lhistory

2018-04-29 13:32:13,615 - memory_profile6_log - INFO -    335                             

2018-04-29 13:32:13,617 - memory_profile6_log - INFO -    336                                             logger.info("Appending training data...")

2018-04-29 13:32:13,618 - memory_profile6_log - INFO -    337                                             datalist.append(tframe)

2018-04-29 13:32:13,619 - memory_profile6_log - INFO -    338                                     else: 

2018-04-29 13:32:13,619 - memory_profile6_log - INFO -    339     90.1 MiB      0.0 MiB               logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 13:32:13,622 - memory_profile6_log - INFO -    340     90.1 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 13:32:13,624 - memory_profile6_log - INFO -    341     90.1 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 13:32:13,625 - memory_profile6_log - INFO -    342                             

2018-04-29 13:32:13,625 - memory_profile6_log - INFO -    343     90.1 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 13:32:13,628 - memory_profile6_log - INFO - 


2018-04-29 13:32:13,628 - memory_profile6_log - INFO - Training cannot be empty..
2018-04-29 13:32:13,628 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 13:32:13,628 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 13:32:13,630 - memory_profile6_log - INFO - ================================================

2018-04-29 13:32:13,630 - memory_profile6_log - INFO -    345     86.2 MiB     86.2 MiB   @profile

2018-04-29 13:32:13,631 - memory_profile6_log - INFO -    346                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 13:32:13,631 - memory_profile6_log - INFO -    347     86.4 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 13:32:13,631 - memory_profile6_log - INFO -    348     86.4 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 13:32:13,634 - memory_profile6_log - INFO -    349                             

2018-04-29 13:32:13,634 - memory_profile6_log - INFO -    350                                 # ~~~ Begin collecting data ~~~

2018-04-29 13:32:13,637 - memory_profile6_log - INFO -    351     86.4 MiB      0.0 MiB       t0 = time.time()

2018-04-29 13:32:13,638 - memory_profile6_log - INFO -    352     90.1 MiB      3.8 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 13:32:13,638 - memory_profile6_log - INFO -    353     90.1 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 13:32:13,638 - memory_profile6_log - INFO -    354     90.1 MiB      0.0 MiB           logger.info("Training cannot be empty..")

2018-04-29 13:32:13,638 - memory_profile6_log - INFO -    355     90.1 MiB      0.0 MiB           return False

2018-04-29 13:32:13,638 - memory_profile6_log - INFO -    356                                 big_frame_hist = pd.concat(datalist_hist)

2018-04-29 13:32:13,640 - memory_profile6_log - INFO -    357                                 print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 13:32:13,640 - memory_profile6_log - INFO -    358                                 logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 13:32:13,641 - memory_profile6_log - INFO -    359                             

2018-04-29 13:32:13,641 - memory_profile6_log - INFO -    360                                 big_frame = pd.concat(datalist)

2018-04-29 13:32:13,641 - memory_profile6_log - INFO -    361                                 logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 13:32:13,641 - memory_profile6_log - INFO -    362                                 del datalist

2018-04-29 13:32:13,641 - memory_profile6_log - INFO -    363                             

2018-04-29 13:32:13,648 - memory_profile6_log - INFO -    364                                 big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 13:32:13,651 - memory_profile6_log - INFO -    365                             

2018-04-29 13:32:13,653 - memory_profile6_log - INFO -    366                                 # ~ get current news interest ~

2018-04-29 13:32:13,654 - memory_profile6_log - INFO -    367                                 if not cd:

2018-04-29 13:32:13,657 - memory_profile6_log - INFO -    368                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 13:32:13,657 - memory_profile6_log - INFO -    369                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 13:32:13,658 - memory_profile6_log - INFO -    370                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 13:32:13,660 - memory_profile6_log - INFO -    371                                 else:

2018-04-29 13:32:13,661 - memory_profile6_log - INFO -    372                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 13:32:13,661 - memory_profile6_log - INFO -    373                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 13:32:13,663 - memory_profile6_log - INFO -    374                             

2018-04-29 13:32:13,664 - memory_profile6_log - INFO -    375                                     # safe handling of query parameter

2018-04-29 13:32:13,664 - memory_profile6_log - INFO -    376                                     query_params = [

2018-04-29 13:32:13,664 - memory_profile6_log - INFO -    377                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 13:32:13,667 - memory_profile6_log - INFO -    378                                     ]

2018-04-29 13:32:13,668 - memory_profile6_log - INFO -    379                             

2018-04-29 13:32:13,670 - memory_profile6_log - INFO -    380                                     job_config.query_parameters = query_params

2018-04-29 13:32:13,671 - memory_profile6_log - INFO -    381                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 13:32:13,671 - memory_profile6_log - INFO -    382                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 13:32:13,671 - memory_profile6_log - INFO -    383                             

2018-04-29 13:32:13,671 - memory_profile6_log - INFO -    384                                 current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 13:32:13,673 - memory_profile6_log - INFO -    385                                 current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 13:32:13,673 - memory_profile6_log - INFO -    386                                                                        format='%Y-%m-%d', errors='coerce')

2018-04-29 13:32:13,673 - memory_profile6_log - INFO -    387                                 logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 13:32:13,674 - memory_profile6_log - INFO -    388                                 train_time = time.time() - t0

2018-04-29 13:32:13,674 - memory_profile6_log - INFO -    389                                 logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 13:32:13,674 - memory_profile6_log - INFO -    390                             

2018-04-29 13:32:13,674 - memory_profile6_log - INFO -    391                                 return big_frame, current_frame, big_frame_hist

2018-04-29 13:32:13,680 - memory_profile6_log - INFO - 


2018-04-29 13:32:13,681 - memory_profile6_log - INFO - Train return False, please cek your data availability
2018-04-29 13:32:20,326 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:32:20,329 - memory_profile6_log - INFO - date_generated: 
2018-04-29 13:32:20,329 - memory_profile6_log - INFO -  
2018-04-29 13:32:20,331 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 28, 13, 32, 20, 330000)]
2018-04-29 13:32:20,332 - memory_profile6_log - INFO - 

2018-04-29 13:32:20,332 - memory_profile6_log - INFO - using current date: 2018-04-29 13:32:20.330000
2018-04-29 13:32:20,332 - memory_profile6_log - INFO - using start date: 2018-04-28 00:00:00
2018-04-29 13:32:20,334 - memory_profile6_log - INFO - using end date: 2018-04-28
2018-04-29 13:32:20,461 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 13:32:20,466 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-28
2018-04-29 13:32:24,036 - memory_profile6_log - INFO - size of df: 24.0 Byte
2018-04-29 13:32:24,038 - memory_profile6_log - INFO - 2018-04-28 data is empty!
2018-04-29 13:32:24,039 - memory_profile6_log - INFO - tframe for date: 2018-04-28 is empty
2018-04-29 13:32:24,042 - memory_profile6_log - INFO - len datalist: 0
2018-04-29 13:32:24,042 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 13:32:24,045 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 13:32:24,046 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 13:32:24,048 - memory_profile6_log - INFO - ================================================

2018-04-29 13:32:24,049 - memory_profile6_log - INFO -    295     86.4 MiB     86.4 MiB   @profile

2018-04-29 13:32:24,052 - memory_profile6_log - INFO -    296                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 13:32:24,053 - memory_profile6_log - INFO -    297     86.4 MiB      0.0 MiB       bq_client = client

2018-04-29 13:32:24,056 - memory_profile6_log - INFO -    298     86.4 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 13:32:24,059 - memory_profile6_log - INFO -    299                             

2018-04-29 13:32:24,061 - memory_profile6_log - INFO -    300     86.4 MiB      0.0 MiB       datalist = []

2018-04-29 13:32:24,062 - memory_profile6_log - INFO -    301     86.4 MiB      0.0 MiB       datalist_hist = []

2018-04-29 13:32:24,063 - memory_profile6_log - INFO -    302                             

2018-04-29 13:32:24,063 - memory_profile6_log - INFO -    303     86.4 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 13:32:24,065 - memory_profile6_log - INFO -    304     90.1 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 13:32:24,066 - memory_profile6_log - INFO -    305     90.1 MiB      3.7 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 13:32:24,069 - memory_profile6_log - INFO -    306     90.1 MiB      0.0 MiB           if tframe is not None:

2018-04-29 13:32:24,069 - memory_profile6_log - INFO -    307                                         if not tframe.empty:

2018-04-29 13:32:24,071 - memory_profile6_log - INFO -    308                                             X_split = np.array_split(tframe, 5)

2018-04-29 13:32:24,072 - memory_profile6_log - INFO -    309                                             logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 13:32:24,072 - memory_profile6_log - INFO -    310                                             logger.info("Appending history data...")

2018-04-29 13:32:24,072 - memory_profile6_log - INFO -    311                                             for ix in range(len(X_split)):

2018-04-29 13:32:24,073 - memory_profile6_log - INFO -    312                                                 # ~ loading history

2018-04-29 13:32:24,075 - memory_profile6_log - INFO -    313                                                 """

2018-04-29 13:32:24,075 - memory_profile6_log - INFO -    314                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 13:32:24,076 - memory_profile6_log - INFO -    315                                                 """

2018-04-29 13:32:24,076 - memory_profile6_log - INFO -    316                                                 logger.info("processing batch-%d", ix)

2018-04-29 13:32:24,079 - memory_profile6_log - INFO -    317                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 13:32:24,081 - memory_profile6_log - INFO -    318                                                 logger.info("creating list history data...")

2018-04-29 13:32:24,082 - memory_profile6_log - INFO -    319                                                 lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 13:32:24,082 - memory_profile6_log - INFO -    320                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 13:32:24,085 - memory_profile6_log - INFO -    321                             

2018-04-29 13:32:24,085 - memory_profile6_log - INFO -    322                                                 logger.info("call history data...")

2018-04-29 13:32:24,085 - memory_profile6_log - INFO -    323                                                 h_frame = mh.loadDSHistory(lhistory)

2018-04-29 13:32:24,086 - memory_profile6_log - INFO -    324                             

2018-04-29 13:32:24,086 - memory_profile6_log - INFO -    325                                                 # me = os.getpid()

2018-04-29 13:32:24,088 - memory_profile6_log - INFO -    326                                                 # kill_proc_tree(me)

2018-04-29 13:32:24,088 - memory_profile6_log - INFO -    327                             

2018-04-29 13:32:24,092 - memory_profile6_log - INFO -    328                                                 logger.info("done collecting history data, appending now...")

2018-04-29 13:32:24,094 - memory_profile6_log - INFO -    329                                                 for m in h_frame:

2018-04-29 13:32:24,095 - memory_profile6_log - INFO -    330                                                     if m is not None:

2018-04-29 13:32:24,095 - memory_profile6_log - INFO -    331                                                         if len(m) > 0:

2018-04-29 13:32:24,096 - memory_profile6_log - INFO -    332                                                             datalist_hist.append(pd.DataFrame(m))

2018-04-29 13:32:24,098 - memory_profile6_log - INFO -    333                                                 del h_frame

2018-04-29 13:32:24,098 - memory_profile6_log - INFO -    334                                                 del lhistory

2018-04-29 13:32:24,101 - memory_profile6_log - INFO -    335                             

2018-04-29 13:32:24,101 - memory_profile6_log - INFO -    336                                             logger.info("Appending training data...")

2018-04-29 13:32:24,104 - memory_profile6_log - INFO -    337                                             datalist.append(tframe)

2018-04-29 13:32:24,105 - memory_profile6_log - INFO -    338                                     else: 

2018-04-29 13:32:24,107 - memory_profile6_log - INFO -    339     90.1 MiB      0.0 MiB               logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 13:32:24,108 - memory_profile6_log - INFO -    340     90.1 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 13:32:24,111 - memory_profile6_log - INFO -    341     90.1 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 13:32:24,112 - memory_profile6_log - INFO -    342                             

2018-04-29 13:32:24,114 - memory_profile6_log - INFO -    343     90.1 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 13:32:24,115 - memory_profile6_log - INFO - 


2018-04-29 13:32:24,117 - memory_profile6_log - INFO - Training cannot be empty..
2018-04-29 13:32:24,118 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 13:32:24,118 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 13:32:24,118 - memory_profile6_log - INFO - ================================================

2018-04-29 13:32:24,118 - memory_profile6_log - INFO -    345     86.2 MiB     86.2 MiB   @profile

2018-04-29 13:32:24,121 - memory_profile6_log - INFO -    346                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 13:32:24,121 - memory_profile6_log - INFO -    347     86.4 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 13:32:24,124 - memory_profile6_log - INFO -    348     86.4 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 13:32:24,124 - memory_profile6_log - INFO -    349                             

2018-04-29 13:32:24,127 - memory_profile6_log - INFO -    350                                 # ~~~ Begin collecting data ~~~

2018-04-29 13:32:24,127 - memory_profile6_log - INFO -    351     86.4 MiB      0.0 MiB       t0 = time.time()

2018-04-29 13:32:24,128 - memory_profile6_log - INFO -    352     90.1 MiB      3.7 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 13:32:24,128 - memory_profile6_log - INFO -    353     90.1 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 13:32:24,128 - memory_profile6_log - INFO -    354     90.1 MiB      0.0 MiB           logger.info("Training cannot be empty..")

2018-04-29 13:32:24,128 - memory_profile6_log - INFO -    355     90.1 MiB      0.0 MiB           return False

2018-04-29 13:32:24,130 - memory_profile6_log - INFO -    356                                 big_frame_hist = pd.concat(datalist_hist)

2018-04-29 13:32:24,130 - memory_profile6_log - INFO -    357                                 print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 13:32:24,130 - memory_profile6_log - INFO -    358                                 logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 13:32:24,134 - memory_profile6_log - INFO -    359                             

2018-04-29 13:32:24,134 - memory_profile6_log - INFO -    360                                 big_frame = pd.concat(datalist)

2018-04-29 13:32:24,135 - memory_profile6_log - INFO -    361                                 logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 13:32:24,137 - memory_profile6_log - INFO -    362                                 del datalist

2018-04-29 13:32:24,137 - memory_profile6_log - INFO -    363                             

2018-04-29 13:32:24,138 - memory_profile6_log - INFO -    364                                 big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 13:32:24,138 - memory_profile6_log - INFO -    365                             

2018-04-29 13:32:24,138 - memory_profile6_log - INFO -    366                                 # ~ get current news interest ~

2018-04-29 13:32:24,138 - memory_profile6_log - INFO -    367                                 if not cd:

2018-04-29 13:32:24,140 - memory_profile6_log - INFO -    368                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 13:32:24,141 - memory_profile6_log - INFO -    369                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 13:32:24,141 - memory_profile6_log - INFO -    370                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 13:32:24,145 - memory_profile6_log - INFO -    371                                 else:

2018-04-29 13:32:24,148 - memory_profile6_log - INFO -    372                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 13:32:24,148 - memory_profile6_log - INFO -    373                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 13:32:24,148 - memory_profile6_log - INFO -    374                             

2018-04-29 13:32:24,150 - memory_profile6_log - INFO -    375                                     # safe handling of query parameter

2018-04-29 13:32:24,151 - memory_profile6_log - INFO -    376                                     query_params = [

2018-04-29 13:32:24,153 - memory_profile6_log - INFO -    377                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 13:32:24,153 - memory_profile6_log - INFO -    378                                     ]

2018-04-29 13:32:24,154 - memory_profile6_log - INFO -    379                             

2018-04-29 13:32:24,154 - memory_profile6_log - INFO -    380                                     job_config.query_parameters = query_params

2018-04-29 13:32:24,157 - memory_profile6_log - INFO -    381                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 13:32:24,158 - memory_profile6_log - INFO -    382                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 13:32:24,160 - memory_profile6_log - INFO -    383                             

2018-04-29 13:32:24,161 - memory_profile6_log - INFO -    384                                 current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 13:32:24,161 - memory_profile6_log - INFO -    385                                 current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 13:32:24,161 - memory_profile6_log - INFO -    386                                                                        format='%Y-%m-%d', errors='coerce')

2018-04-29 13:32:24,163 - memory_profile6_log - INFO -    387                                 logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 13:32:24,163 - memory_profile6_log - INFO -    388                                 train_time = time.time() - t0

2018-04-29 13:32:24,163 - memory_profile6_log - INFO -    389                                 logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 13:32:24,164 - memory_profile6_log - INFO -    390                             

2018-04-29 13:32:24,164 - memory_profile6_log - INFO -    391                                 return big_frame, current_frame, big_frame_hist

2018-04-29 13:32:24,164 - memory_profile6_log - INFO - 


2018-04-29 13:32:24,167 - memory_profile6_log - INFO - Train return False, please cek your data availability
2018-04-29 13:32:36,563 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:32:36,566 - memory_profile6_log - INFO - date_generated: 
2018-04-29 13:32:36,566 - memory_profile6_log - INFO -  
2018-04-29 13:32:36,566 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 28, 13, 32, 36, 567000)]
2018-04-29 13:32:36,568 - memory_profile6_log - INFO - 

2018-04-29 13:32:36,568 - memory_profile6_log - INFO - using current date: 2018-04-29 13:32:36.567000
2018-04-29 13:32:36,568 - memory_profile6_log - INFO - using start date: 2018-04-28 00:00:00
2018-04-29 13:32:36,568 - memory_profile6_log - INFO - using end date: 2018-04-28
2018-04-29 13:32:36,691 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 13:32:36,694 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-28
2018-04-29 13:32:39,292 - memory_profile6_log - INFO - size of df: 24.0 Byte
2018-04-29 13:32:39,292 - memory_profile6_log - INFO - 2018-04-28 data is empty!
2018-04-29 13:32:39,293 - memory_profile6_log - INFO - tframe for date: 2018-04-28 is empty
2018-04-29 13:32:39,296 - memory_profile6_log - INFO - len datalist: 0
2018-04-29 13:32:39,296 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 13:32:39,298 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 13:32:39,299 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 13:32:39,299 - memory_profile6_log - INFO - ================================================

2018-04-29 13:32:39,301 - memory_profile6_log - INFO -    295     86.4 MiB     86.4 MiB   @profile

2018-04-29 13:32:39,302 - memory_profile6_log - INFO -    296                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 13:32:39,302 - memory_profile6_log - INFO -    297     86.4 MiB      0.0 MiB       bq_client = client

2018-04-29 13:32:39,303 - memory_profile6_log - INFO -    298     86.4 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 13:32:39,303 - memory_profile6_log - INFO -    299                             

2018-04-29 13:32:39,305 - memory_profile6_log - INFO -    300     86.4 MiB      0.0 MiB       datalist = []

2018-04-29 13:32:39,305 - memory_profile6_log - INFO -    301     86.4 MiB      0.0 MiB       datalist_hist = []

2018-04-29 13:32:39,306 - memory_profile6_log - INFO -    302                             

2018-04-29 13:32:39,306 - memory_profile6_log - INFO -    303     86.4 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 13:32:39,308 - memory_profile6_log - INFO -    304     90.1 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 13:32:39,309 - memory_profile6_log - INFO -    305     90.1 MiB      3.7 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 13:32:39,309 - memory_profile6_log - INFO -    306     90.1 MiB      0.0 MiB           if tframe is not None:

2018-04-29 13:32:39,309 - memory_profile6_log - INFO -    307                                         if not tframe.empty:

2018-04-29 13:32:39,312 - memory_profile6_log - INFO -    308                                             X_split = np.array_split(tframe, 5)

2018-04-29 13:32:39,312 - memory_profile6_log - INFO -    309                                             logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 13:32:39,312 - memory_profile6_log - INFO -    310                                             logger.info("Appending history data...")

2018-04-29 13:32:39,313 - memory_profile6_log - INFO -    311                                             for ix in range(len(X_split)):

2018-04-29 13:32:39,315 - memory_profile6_log - INFO -    312                                                 # ~ loading history

2018-04-29 13:32:39,315 - memory_profile6_log - INFO -    313                                                 """

2018-04-29 13:32:39,316 - memory_profile6_log - INFO -    314                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 13:32:39,318 - memory_profile6_log - INFO -    315                                                 """

2018-04-29 13:32:39,319 - memory_profile6_log - INFO -    316                                                 logger.info("processing batch-%d", ix)

2018-04-29 13:32:39,319 - memory_profile6_log - INFO -    317                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 13:32:39,322 - memory_profile6_log - INFO -    318                                                 logger.info("creating list history data...")

2018-04-29 13:32:39,323 - memory_profile6_log - INFO -    319                                                 lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 13:32:39,325 - memory_profile6_log - INFO -    320                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 13:32:39,325 - memory_profile6_log - INFO -    321                             

2018-04-29 13:32:39,326 - memory_profile6_log - INFO -    322                                                 logger.info("call history data...")

2018-04-29 13:32:39,328 - memory_profile6_log - INFO -    323                                                 h_frame = mh.loadDSHistory(lhistory)

2018-04-29 13:32:39,328 - memory_profile6_log - INFO -    324                             

2018-04-29 13:32:39,329 - memory_profile6_log - INFO -    325                                                 # me = os.getpid()

2018-04-29 13:32:39,331 - memory_profile6_log - INFO -    326                                                 # kill_proc_tree(me)

2018-04-29 13:32:39,331 - memory_profile6_log - INFO -    327                             

2018-04-29 13:32:39,335 - memory_profile6_log - INFO -    328                                                 logger.info("done collecting history data, appending now...")

2018-04-29 13:32:39,336 - memory_profile6_log - INFO -    329                                                 for m in h_frame:

2018-04-29 13:32:39,338 - memory_profile6_log - INFO -    330                                                     if m is not None:

2018-04-29 13:32:39,338 - memory_profile6_log - INFO -    331                                                         if len(m) > 0:

2018-04-29 13:32:39,339 - memory_profile6_log - INFO -    332                                                             datalist_hist.append(pd.DataFrame(m))

2018-04-29 13:32:39,341 - memory_profile6_log - INFO -    333                                                 del h_frame

2018-04-29 13:32:39,341 - memory_profile6_log - INFO -    334                                                 del lhistory

2018-04-29 13:32:39,342 - memory_profile6_log - INFO -    335                             

2018-04-29 13:32:39,342 - memory_profile6_log - INFO -    336                                             logger.info("Appending training data...")

2018-04-29 13:32:39,345 - memory_profile6_log - INFO -    337                                             datalist.append(tframe)

2018-04-29 13:32:39,348 - memory_profile6_log - INFO -    338                                     else: 

2018-04-29 13:32:39,348 - memory_profile6_log - INFO -    339     90.1 MiB      0.0 MiB               logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 13:32:39,348 - memory_profile6_log - INFO -    340     90.1 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 13:32:39,349 - memory_profile6_log - INFO -    341     90.1 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 13:32:39,349 - memory_profile6_log - INFO -    342                             

2018-04-29 13:32:39,351 - memory_profile6_log - INFO -    343     90.1 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 13:32:39,351 - memory_profile6_log - INFO - 


2018-04-29 13:32:39,352 - memory_profile6_log - INFO - Training cannot be empty..
2018-04-29 13:32:39,355 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 13:32:39,355 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 13:32:39,358 - memory_profile6_log - INFO - ================================================

2018-04-29 13:32:39,358 - memory_profile6_log - INFO -    345     86.2 MiB     86.2 MiB   @profile

2018-04-29 13:32:39,358 - memory_profile6_log - INFO -    346                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 13:32:39,358 - memory_profile6_log - INFO -    347     86.4 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 13:32:39,359 - memory_profile6_log - INFO -    348     86.4 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 13:32:39,359 - memory_profile6_log - INFO -    349                             

2018-04-29 13:32:39,362 - memory_profile6_log - INFO -    350                                 # ~~~ Begin collecting data ~~~

2018-04-29 13:32:39,362 - memory_profile6_log - INFO -    351     86.4 MiB      0.0 MiB       t0 = time.time()

2018-04-29 13:32:39,364 - memory_profile6_log - INFO -    352     90.1 MiB      3.7 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 13:32:39,364 - memory_profile6_log - INFO -    353     90.1 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 13:32:39,367 - memory_profile6_log - INFO -    354     90.1 MiB      0.0 MiB           logger.info("Training cannot be empty..")

2018-04-29 13:32:39,368 - memory_profile6_log - INFO -    355     90.1 MiB      0.0 MiB           return False

2018-04-29 13:32:39,368 - memory_profile6_log - INFO -    356                                 big_frame_hist = pd.concat(datalist_hist)

2018-04-29 13:32:39,371 - memory_profile6_log - INFO -    357                                 print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 13:32:39,371 - memory_profile6_log - INFO -    358                                 logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 13:32:39,371 - memory_profile6_log - INFO -    359                             

2018-04-29 13:32:39,372 - memory_profile6_log - INFO -    360                                 big_frame = pd.concat(datalist)

2018-04-29 13:32:39,372 - memory_profile6_log - INFO -    361                                 logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 13:32:39,372 - memory_profile6_log - INFO -    362                                 del datalist

2018-04-29 13:32:39,372 - memory_profile6_log - INFO -    363                             

2018-04-29 13:32:39,374 - memory_profile6_log - INFO -    364                                 big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 13:32:39,374 - memory_profile6_log - INFO -    365                             

2018-04-29 13:32:39,374 - memory_profile6_log - INFO -    366                                 # ~ get current news interest ~

2018-04-29 13:32:39,375 - memory_profile6_log - INFO -    367                                 if not cd:

2018-04-29 13:32:39,375 - memory_profile6_log - INFO -    368                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 13:32:39,375 - memory_profile6_log - INFO -    369                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 13:32:39,375 - memory_profile6_log - INFO -    370                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 13:32:39,378 - memory_profile6_log - INFO -    371                                 else:

2018-04-29 13:32:39,381 - memory_profile6_log - INFO -    372                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 13:32:39,381 - memory_profile6_log - INFO -    373                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 13:32:39,381 - memory_profile6_log - INFO -    374                             

2018-04-29 13:32:39,381 - memory_profile6_log - INFO -    375                                     # safe handling of query parameter

2018-04-29 13:32:39,382 - memory_profile6_log - INFO -    376                                     query_params = [

2018-04-29 13:32:39,382 - memory_profile6_log - INFO -    377                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 13:32:39,384 - memory_profile6_log - INFO -    378                                     ]

2018-04-29 13:32:39,384 - memory_profile6_log - INFO -    379                             

2018-04-29 13:32:39,384 - memory_profile6_log - INFO -    380                                     job_config.query_parameters = query_params

2018-04-29 13:32:39,385 - memory_profile6_log - INFO -    381                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 13:32:39,387 - memory_profile6_log - INFO -    382                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 13:32:39,387 - memory_profile6_log - INFO -    383                             

2018-04-29 13:32:39,390 - memory_profile6_log - INFO -    384                                 current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 13:32:39,391 - memory_profile6_log - INFO -    385                                 current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 13:32:39,391 - memory_profile6_log - INFO -    386                                                                        format='%Y-%m-%d', errors='coerce')

2018-04-29 13:32:39,391 - memory_profile6_log - INFO -    387                                 logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 13:32:39,392 - memory_profile6_log - INFO -    388                                 train_time = time.time() - t0

2018-04-29 13:32:39,392 - memory_profile6_log - INFO -    389                                 logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 13:32:39,394 - memory_profile6_log - INFO -    390                             

2018-04-29 13:32:39,394 - memory_profile6_log - INFO -    391                                 return big_frame, current_frame, big_frame_hist

2018-04-29 13:32:39,394 - memory_profile6_log - INFO - 


2018-04-29 13:32:39,395 - memory_profile6_log - INFO - Train return False, please cek your data availability
2018-04-29 13:33:57,421 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:33:57,424 - memory_profile6_log - INFO - date_generated: 
2018-04-29 13:33:57,424 - memory_profile6_log - INFO -  
2018-04-29 13:33:57,426 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 9, 0, 0)]
2018-04-29 13:33:57,426 - memory_profile6_log - INFO - 

2018-04-29 13:33:57,427 - memory_profile6_log - INFO - using current date: 2018-04-29 13:33:57.424000
2018-04-29 13:33:57,427 - memory_profile6_log - INFO - using start date: 2018-04-09 00:00:00
2018-04-29 13:33:57,427 - memory_profile6_log - INFO - using end date: 2018-04-09
2018-04-29 13:33:57,562 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 13:33:57,566 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-09
2018-04-29 13:34:14,615 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:34:14,618 - memory_profile6_log - INFO - date_generated: 
2018-04-29 13:34:14,618 - memory_profile6_log - INFO -  
2018-04-29 13:34:14,619 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 13:34:14,619 - memory_profile6_log - INFO - 

2018-04-29 13:34:14,619 - memory_profile6_log - INFO - using current date: 2018-04-29 13:34:14.619000
2018-04-29 13:34:14,621 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 13:34:14,621 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 13:34:14,743 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 13:34:14,746 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 13:35:23,510 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 13:35:23,512 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 13:35:23,552 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 13:35:23,552 - memory_profile6_log - INFO - Appending history data...
2018-04-29 13:35:23,555 - memory_profile6_log - INFO - processing batch-0
2018-04-29 13:35:23,555 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:35:23,596 - memory_profile6_log - INFO - call history data...
2018-04-29 13:35:51,823 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:35:52,523 - memory_profile6_log - INFO - processing batch-1
2018-04-29 13:35:52,525 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:35:52,532 - memory_profile6_log - INFO - call history data...
2018-04-29 13:36:19,740 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:36:20,434 - memory_profile6_log - INFO - processing batch-2
2018-04-29 13:36:20,437 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:36:20,444 - memory_profile6_log - INFO - call history data...
2018-04-29 13:36:46,828 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:36:47,533 - memory_profile6_log - INFO - processing batch-3
2018-04-29 13:36:47,535 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:36:47,542 - memory_profile6_log - INFO - call history data...
2018-04-29 13:37:14,426 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:37:15,117 - memory_profile6_log - INFO - processing batch-4
2018-04-29 13:37:15,118 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:37:15,128 - memory_profile6_log - INFO - call history data...
2018-04-29 13:37:41,414 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:37:42,118 - memory_profile6_log - INFO - Appending training data...
2018-04-29 13:37:42,119 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 13:37:42,121 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 13:37:42,122 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 13:37:42,124 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 13:37:42,125 - memory_profile6_log - INFO - ================================================

2018-04-29 13:37:42,127 - memory_profile6_log - INFO -    295     87.2 MiB     87.2 MiB   @profile

2018-04-29 13:37:42,127 - memory_profile6_log - INFO -    296                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 13:37:42,128 - memory_profile6_log - INFO -    297     87.2 MiB      0.0 MiB       bq_client = client

2018-04-29 13:37:42,131 - memory_profile6_log - INFO -    298     87.2 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 13:37:42,131 - memory_profile6_log - INFO -    299                             

2018-04-29 13:37:42,134 - memory_profile6_log - INFO -    300     87.2 MiB      0.0 MiB       datalist = []

2018-04-29 13:37:42,135 - memory_profile6_log - INFO -    301     87.2 MiB      0.0 MiB       datalist_hist = []

2018-04-29 13:37:42,137 - memory_profile6_log - INFO -    302                             

2018-04-29 13:37:42,138 - memory_profile6_log - INFO -    303     87.2 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 13:37:42,138 - memory_profile6_log - INFO -    304    351.3 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 13:37:42,141 - memory_profile6_log - INFO -    305    338.7 MiB    251.5 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 13:37:42,144 - memory_profile6_log - INFO -    306    338.7 MiB      0.0 MiB           if tframe is not None:

2018-04-29 13:37:42,144 - memory_profile6_log - INFO -    307    338.7 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 13:37:42,145 - memory_profile6_log - INFO -    308    346.8 MiB      8.1 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 13:37:42,147 - memory_profile6_log - INFO -    309    346.8 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 13:37:42,147 - memory_profile6_log - INFO -    310    346.8 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 13:37:42,148 - memory_profile6_log - INFO -    311    351.3 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 13:37:42,148 - memory_profile6_log - INFO -    312                                                 # ~ loading history

2018-04-29 13:37:42,151 - memory_profile6_log - INFO -    313                                                 """

2018-04-29 13:37:42,151 - memory_profile6_log - INFO -    314                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 13:37:42,153 - memory_profile6_log - INFO -    315                                                 """

2018-04-29 13:37:42,154 - memory_profile6_log - INFO -    316    351.1 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 13:37:42,154 - memory_profile6_log - INFO -    317                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 13:37:42,154 - memory_profile6_log - INFO -    318    351.1 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 13:37:42,154 - memory_profile6_log - INFO -    319    351.1 MiB      0.5 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 13:37:42,155 - memory_profile6_log - INFO -    320                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 13:37:42,157 - memory_profile6_log - INFO -    321                             

2018-04-29 13:37:42,157 - memory_profile6_log - INFO -    322    351.1 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 13:37:42,158 - memory_profile6_log - INFO -    323    351.2 MiB      3.2 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 13:37:42,161 - memory_profile6_log - INFO -    324                             

2018-04-29 13:37:42,164 - memory_profile6_log - INFO -    325                                                 # me = os.getpid()

2018-04-29 13:37:42,164 - memory_profile6_log - INFO -    326                                                 # kill_proc_tree(me)

2018-04-29 13:37:42,164 - memory_profile6_log - INFO -    327                             

2018-04-29 13:37:42,165 - memory_profile6_log - INFO -    328    351.2 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 13:37:42,167 - memory_profile6_log - INFO -    329    351.3 MiB      0.0 MiB                       for m in h_frame:

2018-04-29 13:37:42,167 - memory_profile6_log - INFO -    330    351.3 MiB      0.0 MiB                           if m is not None:

2018-04-29 13:37:42,167 - memory_profile6_log - INFO -    331    351.3 MiB      0.0 MiB                               if len(m) > 0:

2018-04-29 13:37:42,168 - memory_profile6_log - INFO -    332    351.3 MiB      0.7 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 13:37:42,170 - memory_profile6_log - INFO -    333    351.3 MiB      0.0 MiB                       del h_frame

2018-04-29 13:37:42,173 - memory_profile6_log - INFO -    334    351.3 MiB      0.0 MiB                       del lhistory

2018-04-29 13:37:42,177 - memory_profile6_log - INFO -    335                             

2018-04-29 13:37:42,177 - memory_profile6_log - INFO -    336    351.3 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 13:37:42,178 - memory_profile6_log - INFO -    337    351.3 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 13:37:42,180 - memory_profile6_log - INFO -    338                                     else: 

2018-04-29 13:37:42,180 - memory_profile6_log - INFO -    339                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 13:37:42,184 - memory_profile6_log - INFO -    340    351.3 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 13:37:42,184 - memory_profile6_log - INFO -    341    351.3 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 13:37:42,187 - memory_profile6_log - INFO -    342                             

2018-04-29 13:37:42,187 - memory_profile6_log - INFO -    343    351.3 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 13:37:42,188 - memory_profile6_log - INFO - 


2018-04-29 13:37:43,318 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 13:37:43,388 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0   0.000646        1844.325967        32            1854.325967  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
1   0.000646        1844.325967        16            1854.325967  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
2   0.000646        1844.325967        32            1854.325967  10960288  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
3   0.000646        1844.325967        16            1854.325967  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
4   0.000646        1844.325967        32            1854.325967  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
5        NaN      333823.000000       256          333833.000000  10959239  161129f951435-0d660a18dbcd87-173a7640-100200-1...
6   0.000646        1844.325967        16            1854.325967  10960288  1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...
0   0.000646        1844.325967         8            1854.325967  10960288  161e219cd891c8-0e882943705df4-15290f5f-38400-1...
1   0.000646        1844.325967        16            1854.325967  10960288  1626fd160d9114-055d801ae6b39d-66265f05-38400-1...
2   0.000646        1844.325967         8            1854.325967  10960288  16169731ce618-00742c5e307396-5b4d6a36-38400-16...
3   0.000646        1844.325967        16            1854.325967  10960288  161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...
4   0.000646        1844.325967         8            1854.325967  10960288  1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...
5   0.000646        1844.325967        16            1854.325967  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
0   0.000646        1844.325967         4            1854.325967  10960288  1612d88e55ab8-09af9658f30a46-65117b23-38400-16...
1   0.000646        1844.325967         8            1854.325967  10960288  16149f0a1611-00a034775-63446032-38400-16149f0a...
2   0.000646        1844.325967         4            1854.325967  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
3   0.000646        1844.325967         4            1854.325967  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
4   0.000646        1844.325967         4            1854.325967  10960288  16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...
5   0.000646        1844.325967         8            1854.325967  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
6   0.000646        1844.325967         4            1854.325967  10960288  16132f1045f30b-0613b72b33445e-5d640103-38400-1...
2018-04-29 13:37:43,388 - memory_profile6_log - INFO - 

2018-04-29 13:37:43,398 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 13:37:43,469 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 13:37:43,480 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 13:45:24,711 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:45:24,714 - memory_profile6_log - INFO - date_generated: 
2018-04-29 13:45:24,716 - memory_profile6_log - INFO -  
2018-04-29 13:45:24,716 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 13:45:24,717 - memory_profile6_log - INFO - 

2018-04-29 13:45:24,717 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 13:45:24,717 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 13:45:24,719 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 13:45:24,862 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 13:45:24,865 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 13:45:51,473 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:47:02,813 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:47:23,256 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:47:55,134 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:48:42,676 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:49:07,651 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:49:07,654 - memory_profile6_log - INFO - 2018-04-29
2018-04-29 13:49:07,654 - memory_profile6_log - INFO - 

2018-04-29 13:49:44,980 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:49:44,983 - memory_profile6_log - INFO - date_generated: 
2018-04-29 13:49:44,983 - memory_profile6_log - INFO -  
2018-04-29 13:49:44,983 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 13:49:44,983 - memory_profile6_log - INFO - 

2018-04-29 13:49:44,984 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 13:49:44,984 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 13:49:44,986 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 13:49:45,119 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 13:49:45,124 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 13:50:56,196 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 13:50:56,197 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 13:50:56,240 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 13:50:56,240 - memory_profile6_log - INFO - Appending history data...
2018-04-29 13:50:56,242 - memory_profile6_log - INFO - processing batch-0
2018-04-29 13:50:56,242 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:50:56,285 - memory_profile6_log - INFO - call history data...
2018-04-29 13:51:26,808 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:51:27,529 - memory_profile6_log - INFO - processing batch-1
2018-04-29 13:51:27,530 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:51:27,539 - memory_profile6_log - INFO - call history data...
2018-04-29 13:51:58,483 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:51:59,184 - memory_profile6_log - INFO - processing batch-2
2018-04-29 13:51:59,184 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:51:59,193 - memory_profile6_log - INFO - call history data...
2018-04-29 13:52:27,194 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:52:27,888 - memory_profile6_log - INFO - processing batch-3
2018-04-29 13:52:27,890 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:52:27,898 - memory_profile6_log - INFO - call history data...
2018-04-29 13:52:55,315 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:52:56,012 - memory_profile6_log - INFO - processing batch-4
2018-04-29 13:52:56,013 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:52:56,022 - memory_profile6_log - INFO - call history data...
2018-04-29 13:53:23,374 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:53:24,069 - memory_profile6_log - INFO - Appending training data...
2018-04-29 13:53:24,072 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 13:53:24,072 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 13:53:24,075 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 13:53:24,075 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 13:53:24,076 - memory_profile6_log - INFO - ================================================

2018-04-29 13:53:24,078 - memory_profile6_log - INFO -    295     86.7 MiB     86.7 MiB   @profile

2018-04-29 13:53:24,079 - memory_profile6_log - INFO -    296                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 13:53:24,082 - memory_profile6_log - INFO -    297     86.7 MiB      0.0 MiB       bq_client = client

2018-04-29 13:53:24,085 - memory_profile6_log - INFO -    298     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 13:53:24,085 - memory_profile6_log - INFO -    299                             

2018-04-29 13:53:24,086 - memory_profile6_log - INFO -    300     86.7 MiB      0.0 MiB       datalist = []

2018-04-29 13:53:24,088 - memory_profile6_log - INFO -    301     86.7 MiB      0.0 MiB       datalist_hist = []

2018-04-29 13:53:24,088 - memory_profile6_log - INFO -    302                             

2018-04-29 13:53:24,088 - memory_profile6_log - INFO -    303     86.7 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 13:53:24,089 - memory_profile6_log - INFO -    304    351.5 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 13:53:24,089 - memory_profile6_log - INFO -    305    339.3 MiB    252.6 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 13:53:24,091 - memory_profile6_log - INFO -    306    339.3 MiB      0.0 MiB           if tframe is not None:

2018-04-29 13:53:24,094 - memory_profile6_log - INFO -    307    339.3 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 13:53:24,094 - memory_profile6_log - INFO -    308    347.1 MiB      7.8 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 13:53:24,095 - memory_profile6_log - INFO -    309    347.1 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 13:53:24,096 - memory_profile6_log - INFO -    310    347.1 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 13:53:24,096 - memory_profile6_log - INFO -    311    351.5 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 13:53:24,098 - memory_profile6_log - INFO -    312                                                 # ~ loading history

2018-04-29 13:53:24,098 - memory_profile6_log - INFO -    313                                                 """

2018-04-29 13:53:24,099 - memory_profile6_log - INFO -    314                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 13:53:24,101 - memory_profile6_log - INFO -    315                                                 """

2018-04-29 13:53:24,105 - memory_profile6_log - INFO -    316    351.4 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 13:53:24,105 - memory_profile6_log - INFO -    317                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 13:53:24,107 - memory_profile6_log - INFO -    318    351.4 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 13:53:24,108 - memory_profile6_log - INFO -    319    351.4 MiB      0.8 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 13:53:24,111 - memory_profile6_log - INFO -    320                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 13:53:24,111 - memory_profile6_log - INFO -    321                             

2018-04-29 13:53:24,115 - memory_profile6_log - INFO -    322    351.4 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 13:53:24,115 - memory_profile6_log - INFO -    323    351.5 MiB      2.9 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 13:53:24,118 - memory_profile6_log - INFO -    324                             

2018-04-29 13:53:24,118 - memory_profile6_log - INFO -    325                                                 # me = os.getpid()

2018-04-29 13:53:24,118 - memory_profile6_log - INFO -    326                                                 # kill_proc_tree(me)

2018-04-29 13:53:24,119 - memory_profile6_log - INFO -    327                             

2018-04-29 13:53:24,121 - memory_profile6_log - INFO -    328    351.5 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 13:53:24,121 - memory_profile6_log - INFO -    329    351.5 MiB      0.0 MiB                       for m in h_frame:

2018-04-29 13:53:24,121 - memory_profile6_log - INFO -    330    351.5 MiB      0.0 MiB                           if m is not None:

2018-04-29 13:53:24,125 - memory_profile6_log - INFO -    331    351.5 MiB      0.0 MiB                               if len(m) > 0:

2018-04-29 13:53:24,125 - memory_profile6_log - INFO -    332    351.5 MiB      0.8 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 13:53:24,128 - memory_profile6_log - INFO -    333    351.5 MiB      0.0 MiB                       del h_frame

2018-04-29 13:53:24,128 - memory_profile6_log - INFO -    334    351.5 MiB      0.0 MiB                       del lhistory

2018-04-29 13:53:24,128 - memory_profile6_log - INFO -    335                             

2018-04-29 13:53:24,130 - memory_profile6_log - INFO -    336    351.5 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 13:53:24,130 - memory_profile6_log - INFO -    337    351.5 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 13:53:24,131 - memory_profile6_log - INFO -    338                                     else: 

2018-04-29 13:53:24,131 - memory_profile6_log - INFO -    339                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 13:53:24,132 - memory_profile6_log - INFO -    340    351.5 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 13:53:24,134 - memory_profile6_log - INFO -    341    351.5 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 13:53:24,138 - memory_profile6_log - INFO -    342                             

2018-04-29 13:53:24,138 - memory_profile6_log - INFO -    343    351.5 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 13:53:24,140 - memory_profile6_log - INFO - 


2018-04-29 13:53:25,259 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 13:53:25,328 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0   0.000646        1844.325967        16            1854.325967  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
1   0.000646        1844.325967        32            1854.325967  10960288  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
2   0.000646        1844.325967        16            1854.325967  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
3   0.000646        1844.325967        16            1854.325967  10960288  1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...
4        NaN      333823.000000       256          333833.000000  10959239  161129f951435-0d660a18dbcd87-173a7640-100200-1...
5   0.000646        1844.325967        32            1854.325967  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
6   0.000646        1844.325967        32            1854.325967  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
0   0.000646        1844.325967        16            1854.325967  10960288  1626fd160d9114-055d801ae6b39d-66265f05-38400-1...
1   0.000646        1844.325967        16            1854.325967  10960288  161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...
2   0.000646        1844.325967         8            1854.325967  10960288  161e219cd891c8-0e882943705df4-15290f5f-38400-1...
3   0.000646        1844.325967        16            1854.325967  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
4   0.000646        1844.325967         8            1854.325967  10960288  1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...
5   0.000646        1844.325967         8            1854.325967  10960288  16169731ce618-00742c5e307396-5b4d6a36-38400-16...
0   0.000646        1844.325967         4            1854.325967  10960288  16132f1045f30b-0613b72b33445e-5d640103-38400-1...
1   0.000646        1844.325967         4            1854.325967  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
2   0.000646        1844.325967         4            1854.325967  10960288  16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...
3   0.000646        1844.325967         8            1854.325967  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
4   0.000646        1844.325967         8            1854.325967  10960288  16149f0a1611-00a034775-63446032-38400-16149f0a...
5   0.000646        1844.325967         4            1854.325967  10960288  1612d88e55ab8-09af9658f30a46-65117b23-38400-16...
6   0.000646        1844.325967         4            1854.325967  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
2018-04-29 13:53:25,332 - memory_profile6_log - INFO - 

2018-04-29 13:53:25,342 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 13:53:25,411 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 13:53:25,424 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 13:54:15,888 - memory_profile6_log - INFO - size of df: 58.30 MB
2018-04-29 13:54:15,890 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-29 13:54:15,969 - memory_profile6_log - INFO - size of current_frame: 58.30 MB
2018-04-29 13:54:15,970 - memory_profile6_log - INFO - loading time of: 484420 total genuine-current interest data ~ take 270.875s
2018-04-29 13:54:15,980 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 13:54:15,982 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 13:54:15,983 - memory_profile6_log - INFO - ================================================

2018-04-29 13:54:15,987 - memory_profile6_log - INFO -    345     86.6 MiB     86.6 MiB   @profile

2018-04-29 13:54:15,989 - memory_profile6_log - INFO -    346                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 13:54:15,989 - memory_profile6_log - INFO -    347     86.7 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 13:54:15,992 - memory_profile6_log - INFO -    348     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 13:54:15,993 - memory_profile6_log - INFO -    349                             

2018-04-29 13:54:15,996 - memory_profile6_log - INFO -    350                                 # ~~~ Begin collecting data ~~~

2018-04-29 13:54:15,997 - memory_profile6_log - INFO -    351     86.7 MiB      0.0 MiB       t0 = time.time()

2018-04-29 13:54:16,000 - memory_profile6_log - INFO -    352    351.5 MiB    264.8 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 13:54:16,002 - memory_profile6_log - INFO -    353    351.5 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 13:54:16,003 - memory_profile6_log - INFO -    354                                     logger.info("Training cannot be empty..")

2018-04-29 13:54:16,005 - memory_profile6_log - INFO -    355                                     return False

2018-04-29 13:54:16,006 - memory_profile6_log - INFO -    356    350.9 MiB     -0.6 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 13:54:16,006 - memory_profile6_log - INFO -    357    351.1 MiB      0.2 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 13:54:16,006 - memory_profile6_log - INFO -    358    351.1 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 13:54:16,006 - memory_profile6_log - INFO -    359                             

2018-04-29 13:54:16,006 - memory_profile6_log - INFO -    360    356.9 MiB      5.8 MiB       big_frame = pd.concat(datalist)

2018-04-29 13:54:16,007 - memory_profile6_log - INFO -    361    356.9 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 13:54:16,007 - memory_profile6_log - INFO -    362    351.1 MiB     -5.8 MiB       del datalist

2018-04-29 13:54:16,009 - memory_profile6_log - INFO -    363                             

2018-04-29 13:54:16,013 - memory_profile6_log - INFO -    364    351.1 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 13:54:16,013 - memory_profile6_log - INFO -    365                             

2018-04-29 13:54:16,015 - memory_profile6_log - INFO -    366                                 # ~ get current news interest ~

2018-04-29 13:54:16,016 - memory_profile6_log - INFO -    367    351.1 MiB      0.0 MiB       if not cd:

2018-04-29 13:54:16,016 - memory_profile6_log - INFO -    368                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 13:54:16,017 - memory_profile6_log - INFO -    369                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 13:54:16,019 - memory_profile6_log - INFO -    370                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 13:54:16,020 - memory_profile6_log - INFO -    371                                 else:

2018-04-29 13:54:16,020 - memory_profile6_log - INFO -    372    351.1 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 13:54:16,023 - memory_profile6_log - INFO -    373    351.1 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 13:54:16,023 - memory_profile6_log - INFO -    374                             

2018-04-29 13:54:16,028 - memory_profile6_log - INFO -    375                                     # safe handling of query parameter

2018-04-29 13:54:16,029 - memory_profile6_log - INFO -    376                                     query_params = [

2018-04-29 13:54:16,029 - memory_profile6_log - INFO -    377    351.1 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 13:54:16,030 - memory_profile6_log - INFO -    378                                     ]

2018-04-29 13:54:16,030 - memory_profile6_log - INFO -    379                             

2018-04-29 13:54:16,032 - memory_profile6_log - INFO -    380    351.1 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-29 13:54:16,032 - memory_profile6_log - INFO -    381    419.4 MiB     68.3 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 13:54:16,035 - memory_profile6_log - INFO -    382    419.4 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 13:54:16,036 - memory_profile6_log - INFO -    383                             

2018-04-29 13:54:16,038 - memory_profile6_log - INFO -    384    419.5 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 13:54:16,039 - memory_profile6_log - INFO -    385    419.5 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 13:54:16,039 - memory_profile6_log - INFO -    386    419.5 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 13:54:16,039 - memory_profile6_log - INFO -    387    419.5 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 13:54:16,040 - memory_profile6_log - INFO -    388    419.5 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 13:54:16,040 - memory_profile6_log - INFO -    389    419.5 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 13:54:16,042 - memory_profile6_log - INFO -    390                             

2018-04-29 13:54:16,046 - memory_profile6_log - INFO -    391    419.5 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 13:54:16,046 - memory_profile6_log - INFO - 


2018-04-29 13:54:16,052 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 13:54:16,085 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 13:54:16,085 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-29 13:54:16,086 - memory_profile6_log - INFO - apply on: 5000 total history data(D(t))
2018-04-29 13:54:17,134 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 13:54:17,134 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 13:55:00,436 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-29 13:55:00,464 - memory_profile6_log - INFO - Total train time: 44.380s
2018-04-29 13:55:00,467 - memory_profile6_log - INFO - memory left before cleaning: 70.200 percent memory...
2018-04-29 13:55:00,467 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-29 13:55:00,469 - memory_profile6_log - INFO - deleting df_dut...
2018-04-29 13:55:00,470 - memory_profile6_log - INFO - deleting df_dt...
2018-04-29 13:55:00,470 - memory_profile6_log - INFO - deleting df_input...
2018-04-29 13:55:00,479 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-29 13:55:00,480 - memory_profile6_log - INFO - deleting df_current...
2018-04-29 13:55:00,482 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-29 13:55:00,493 - memory_profile6_log - INFO - deleting model_fit...
2018-04-29 13:55:00,493 - memory_profile6_log - INFO - deleting result...
2018-04-29 13:55:00,513 - memory_profile6_log - INFO - deleting model_transform...
2018-04-29 13:55:00,513 - memory_profile6_log - INFO - memory left after cleaning: 70.000 percent memory...
2018-04-29 13:55:00,516 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-29 13:55:00,516 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-29 13:55:00,684 - memory_profile6_log - INFO - Saving fitted_models as history...
2018-04-29 13:55:00,767 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 10
2018-04-29 13:55:00,769 - memory_profile6_log - INFO - processing batch-0
2018-04-29 13:55:00,964 - memory_profile6_log - INFO - processing batch-1
2018-04-29 13:55:01,153 - memory_profile6_log - INFO - processing batch-2
2018-04-29 13:55:01,331 - memory_profile6_log - INFO - processing batch-3
2018-04-29 13:55:01,516 - memory_profile6_log - INFO - processing batch-4
2018-04-29 13:55:01,703 - memory_profile6_log - INFO - processing batch-5
2018-04-29 13:55:01,884 - memory_profile6_log - INFO - processing batch-6
2018-04-29 13:55:02,066 - memory_profile6_log - INFO - processing batch-7
2018-04-29 13:55:02,260 - memory_profile6_log - INFO - processing batch-8
2018-04-29 13:55:02,453 - memory_profile6_log - INFO - processing batch-9
2018-04-29 13:55:02,637 - memory_profile6_log - INFO - deleting BR...
2018-04-29 13:55:02,638 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-04-29 13:55:02,648 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 13:55:02,648 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 13:55:02,648 - memory_profile6_log - INFO - ================================================

2018-04-29 13:55:02,650 - memory_profile6_log - INFO -    113    419.5 MiB    419.5 MiB   @profile

2018-04-29 13:55:02,651 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-29 13:55:02,651 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-29 13:55:02,651 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-29 13:55:02,651 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-29 13:55:02,651 - memory_profile6_log - INFO -    118                                      saveto="datastore"):

2018-04-29 13:55:02,654 - memory_profile6_log - INFO -    119                                 """

2018-04-29 13:55:02,654 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-29 13:55:02,657 - memory_profile6_log - INFO -    121                                 """

2018-04-29 13:55:02,658 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-29 13:55:02,660 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-29 13:55:02,661 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-29 13:55:02,661 - memory_profile6_log - INFO -    125    419.5 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-29 13:55:02,661 - memory_profile6_log - INFO -    126    427.5 MiB      8.0 MiB       df_dut = df_input.copy(deep=True)

2018-04-29 13:55:02,665 - memory_profile6_log - INFO -    127    427.5 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 13:55:02,667 - memory_profile6_log - INFO -    128                             

2018-04-29 13:55:02,667 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-29 13:55:02,668 - memory_profile6_log - INFO -    130    434.7 MiB      7.3 MiB       df_dt = df_current.copy(deep=True)

2018-04-29 13:55:02,668 - memory_profile6_log - INFO -    131    434.7 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 13:55:02,670 - memory_profile6_log - INFO -    132                             

2018-04-29 13:55:02,671 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-29 13:55:02,671 - memory_profile6_log - INFO -    134    434.7 MiB      0.0 MiB       t0 = time.time()

2018-04-29 13:55:02,671 - memory_profile6_log - INFO -    135    434.7 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-29 13:55:02,673 - memory_profile6_log - INFO -    136    434.7 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-29 13:55:02,678 - memory_profile6_log - INFO -    137    434.7 MiB      0.0 MiB       logger.info("apply on: %d total history data(D(t))", len(df_hist))

2018-04-29 13:55:02,680 - memory_profile6_log - INFO -    138                             

2018-04-29 13:55:02,681 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-29 13:55:02,681 - memory_profile6_log - INFO -    140    434.7 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-29 13:55:02,684 - memory_profile6_log - INFO -    141                             

2018-04-29 13:55:02,684 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-29 13:55:02,686 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-29 13:55:02,686 - memory_profile6_log - INFO -    144    436.8 MiB      2.1 MiB       NB = BR.processX(df_dut)

2018-04-29 13:55:02,687 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-29 13:55:02,690 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-29 13:55:02,691 - memory_profile6_log - INFO -    147    446.7 MiB      9.8 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-29 13:55:02,693 - memory_profile6_log - INFO -    148                                 """

2018-04-29 13:55:02,694 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-29 13:55:02,694 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-29 13:55:02,696 - memory_profile6_log - INFO -    151                                 """

2018-04-29 13:55:02,697 - memory_profile6_log - INFO -    152    446.7 MiB      0.0 MiB       fitby_sigmant = True

2018-04-29 13:55:02,697 - memory_profile6_log - INFO -    153    446.7 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-29 13:55:02,697 - memory_profile6_log - INFO -    154    446.7 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-29 13:55:02,698 - memory_profile6_log - INFO -    155    456.4 MiB      9.7 MiB                            'is_general']]

2018-04-29 13:55:02,703 - memory_profile6_log - INFO -    156    456.4 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-29 13:55:02,704 - memory_profile6_log - INFO -    157    456.4 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-29 13:55:02,707 - memory_profile6_log - INFO -    158    484.7 MiB     28.3 MiB                          verbose=False)

2018-04-29 13:55:02,709 - memory_profile6_log - INFO -    159    484.7 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-29 13:55:02,710 - memory_profile6_log - INFO -    160    484.7 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-29 13:55:02,710 - memory_profile6_log - INFO -    161                             

2018-04-29 13:55:02,711 - memory_profile6_log - INFO -    162                                 # ~~ and Transform ~~

2018-04-29 13:55:02,711 - memory_profile6_log - INFO -    163                                 #   handling current news interest == current date

2018-04-29 13:55:02,716 - memory_profile6_log - INFO -    164    484.7 MiB      0.0 MiB       if df_dt.empty:

2018-04-29 13:55:02,717 - memory_profile6_log - INFO -    165                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-29 13:55:02,719 - memory_profile6_log - INFO -    166                                     return None

2018-04-29 13:55:02,719 - memory_profile6_log - INFO -    167    484.9 MiB      0.2 MiB       NB = BR.processX(df_dt)

2018-04-29 13:55:02,720 - memory_profile6_log - INFO -    168    495.4 MiB     10.6 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-29 13:55:02,720 - memory_profile6_log - INFO -    169                             

2018-04-29 13:55:02,721 - memory_profile6_log - INFO -    170    495.4 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-29 13:55:02,721 - memory_profile6_log - INFO -    171    494.5 MiB     -0.9 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-29 13:55:02,723 - memory_profile6_log - INFO -    172    494.5 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-29 13:55:02,726 - memory_profile6_log - INFO -    173    494.5 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-29 13:55:02,726 - memory_profile6_log - INFO -    174    513.5 MiB     19.0 MiB                                                     verbose=False)

2018-04-29 13:55:02,729 - memory_profile6_log - INFO -    175                                 # ~~~ filter is general and specific topic ~~~

2018-04-29 13:55:02,729 - memory_profile6_log - INFO -    176                                 # the idea is just we need to rerank every topic according

2018-04-29 13:55:02,730 - memory_profile6_log - INFO -    177                                 # user_id and and is_general by p0_posterior

2018-04-29 13:55:02,730 - memory_profile6_log - INFO -    178    513.5 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-29 13:55:02,730 - memory_profile6_log - INFO -    179    515.4 MiB      1.9 MiB                                     'is_general']].groupby(['topic_id',

2018-04-29 13:55:02,730 - memory_profile6_log - INFO -    180    513.5 MiB     -1.9 MiB                                                             'is_general']

2018-04-29 13:55:02,730 - memory_profile6_log - INFO -    181                                                                                      ).size().to_frame().reset_index()

2018-04-29 13:55:02,730 - memory_profile6_log - INFO -    182                             

2018-04-29 13:55:02,733 - memory_profile6_log - INFO -    183                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-29 13:55:02,733 - memory_profile6_log - INFO -    184                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-29 13:55:02,734 - memory_profile6_log - INFO -    185    513.5 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-29 13:55:02,734 - memory_profile6_log - INFO -    186                             

2018-04-29 13:55:02,734 - memory_profile6_log - INFO -    187                                 # ~ start by provide rank for each topic type ~

2018-04-29 13:55:02,734 - memory_profile6_log - INFO -    188    534.6 MiB     21.1 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-29 13:55:02,739 - memory_profile6_log - INFO -    189    538.8 MiB      4.1 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-29 13:55:02,740 - memory_profile6_log - INFO -    190                             

2018-04-29 13:55:02,740 - memory_profile6_log - INFO -    191                                 # ~ set threshold to filter output

2018-04-29 13:55:02,743 - memory_profile6_log - INFO -    192    538.8 MiB      0.0 MiB       if threshold > 0:

2018-04-29 13:55:02,743 - memory_profile6_log - INFO -    193    538.8 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-29 13:55:02,743 - memory_profile6_log - INFO -    194    538.8 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-29 13:55:02,743 - memory_profile6_log - INFO -    195    537.7 MiB     -1.1 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-29 13:55:02,743 - memory_profile6_log - INFO -    196                             

2018-04-29 13:55:02,744 - memory_profile6_log - INFO -    197    537.7 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 13:55:02,744 - memory_profile6_log - INFO -    198    537.7 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-29 13:55:02,744 - memory_profile6_log - INFO -    199                             

2018-04-29 13:55:02,746 - memory_profile6_log - INFO -    200    537.7 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 13:55:02,746 - memory_profile6_log - INFO -    201                             

2018-04-29 13:55:02,746 - memory_profile6_log - INFO -    202    537.7 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-29 13:55:02,746 - memory_profile6_log - INFO -    203    537.7 MiB      0.0 MiB       del df_dut

2018-04-29 13:55:02,750 - memory_profile6_log - INFO -    204    537.7 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-29 13:55:02,750 - memory_profile6_log - INFO -    205    537.7 MiB      0.0 MiB       del df_dt

2018-04-29 13:55:02,752 - memory_profile6_log - INFO -    206    537.7 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-29 13:55:02,753 - memory_profile6_log - INFO -    207    537.7 MiB      0.0 MiB       del df_input

2018-04-29 13:55:02,753 - memory_profile6_log - INFO -    208    537.7 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-29 13:55:02,753 - memory_profile6_log - INFO -    209    528.9 MiB     -8.8 MiB       del df_input_X

2018-04-29 13:55:02,753 - memory_profile6_log - INFO -    210    528.9 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-29 13:55:02,753 - memory_profile6_log - INFO -    211    528.9 MiB      0.0 MiB       del df_current

2018-04-29 13:55:02,755 - memory_profile6_log - INFO -    212    528.9 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-29 13:55:02,755 - memory_profile6_log - INFO -    213    528.9 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-29 13:55:02,755 - memory_profile6_log - INFO -    214    528.9 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-29 13:55:02,756 - memory_profile6_log - INFO -    215    505.6 MiB    -23.3 MiB       del model_fit

2018-04-29 13:55:02,756 - memory_profile6_log - INFO -    216    505.6 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-29 13:55:02,756 - memory_profile6_log - INFO -    217    505.6 MiB      0.0 MiB       del result

2018-04-29 13:55:02,760 - memory_profile6_log - INFO -    218    505.6 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-29 13:55:02,763 - memory_profile6_log - INFO -    219                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 13:55:02,763 - memory_profile6_log - INFO -    220                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 13:55:02,763 - memory_profile6_log - INFO -    221                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 13:55:02,763 - memory_profile6_log - INFO -    222    505.6 MiB      0.0 MiB       if savetrain:

2018-04-29 13:55:02,765 - memory_profile6_log - INFO -    223    511.0 MiB      5.4 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-29 13:55:02,766 - memory_profile6_log - INFO -    224    511.0 MiB      0.0 MiB           del model_transform

2018-04-29 13:55:02,766 - memory_profile6_log - INFO -    225    511.0 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-29 13:55:02,767 - memory_profile6_log - INFO -    226    511.0 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 13:55:02,767 - memory_profile6_log - INFO -    227                             

2018-04-29 13:55:02,769 - memory_profile6_log - INFO -    228    511.0 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-29 13:55:02,769 - memory_profile6_log - INFO -    229                                     # ~ Place your code to save the training model here ~

2018-04-29 13:55:02,772 - memory_profile6_log - INFO -    230    511.0 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-29 13:55:02,773 - memory_profile6_log - INFO -    231    511.0 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-29 13:55:02,773 - memory_profile6_log - INFO -    232    511.0 MiB      0.0 MiB               if multproc:

2018-04-29 13:55:02,776 - memory_profile6_log - INFO -    233    488.8 MiB    -22.2 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-29 13:55:02,776 - memory_profile6_log - INFO -    234                             

2018-04-29 13:55:02,776 - memory_profile6_log - INFO -    235                                             # ~ save fitted models ~

2018-04-29 13:55:02,778 - memory_profile6_log - INFO -    236    488.8 MiB      0.0 MiB                   logger.info("Saving fitted_models as history...")

2018-04-29 13:55:02,778 - memory_profile6_log - INFO -    237    488.9 MiB      0.1 MiB                   save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 13:55:02,778 - memory_profile6_log - INFO -    238    502.6 MiB     13.7 MiB                   fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-29 13:55:02,779 - memory_profile6_log - INFO -    239                             

2018-04-29 13:55:02,779 - memory_profile6_log - INFO -    240    507.1 MiB      4.5 MiB                   X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-29 13:55:02,779 - memory_profile6_log - INFO -    241    507.1 MiB      0.0 MiB                   logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-29 13:55:02,779 - memory_profile6_log - INFO -    242    508.5 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 13:55:02,783 - memory_profile6_log - INFO -    243    508.5 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 13:55:02,786 - memory_profile6_log - INFO -    244    508.5 MiB      1.5 MiB                       mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-29 13:55:02,786 - memory_profile6_log - INFO -    245                             

2018-04-29 13:55:02,786 - memory_profile6_log - INFO -    246    508.5 MiB      0.0 MiB                   del X_split

2018-04-29 13:55:02,786 - memory_profile6_log - INFO -    247    508.5 MiB      0.0 MiB                   del BR

2018-04-29 13:55:02,788 - memory_profile6_log - INFO -    248    508.5 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-29 13:55:02,788 - memory_profile6_log - INFO -    249                             

2018-04-29 13:55:02,789 - memory_profile6_log - INFO -    250    508.5 MiB      0.0 MiB                   del save_sigma_nt

2018-04-29 13:55:02,789 - memory_profile6_log - INFO -    251    508.5 MiB      0.0 MiB                   logger.info("deleting save_sigma_nt...")

2018-04-29 13:55:02,793 - memory_profile6_log - INFO -    252                                             

2018-04-29 13:55:02,793 - memory_profile6_log - INFO -    253                                     elif str(saveto).lower() == "elastic":

2018-04-29 13:55:02,796 - memory_profile6_log - INFO -    254                                         logger.info("Using ElasticSearch as storage...")

2018-04-29 13:55:02,796 - memory_profile6_log - INFO -    255                                         mh.saveElasticS(model_transformsv)

2018-04-29 13:55:02,796 - memory_profile6_log - INFO -    256                             

2018-04-29 13:55:02,798 - memory_profile6_log - INFO -    257                                     # need save sigma_nt for daily train

2018-04-29 13:55:02,798 - memory_profile6_log - INFO -    258                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-29 13:55:02,798 - memory_profile6_log - INFO -    259                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-29 13:55:02,799 - memory_profile6_log - INFO -    260    508.5 MiB      0.0 MiB           if start_date and end_date:

2018-04-29 13:55:02,801 - memory_profile6_log - INFO -    261                                         if not fitby_sigmant:

2018-04-29 13:55:02,801 - memory_profile6_log - INFO -    262                                             logging.info("Saving sigma Nt...")

2018-04-29 13:55:02,802 - memory_profile6_log - INFO -    263                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 13:55:02,802 - memory_profile6_log - INFO -    264                                             save_sigma_nt['start_date'] = start_date

2018-04-29 13:55:02,802 - memory_profile6_log - INFO -    265                                             save_sigma_nt['end_date'] = end_date

2018-04-29 13:55:02,805 - memory_profile6_log - INFO -    266                                             print save_sigma_nt.head(5)

2018-04-29 13:55:02,809 - memory_profile6_log - INFO -    267                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-29 13:55:02,809 - memory_profile6_log - INFO -    268                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-29 13:55:02,811 - memory_profile6_log - INFO -    269    508.5 MiB      0.0 MiB       return

2018-04-29 13:55:02,812 - memory_profile6_log - INFO - 


2018-04-29 13:55:02,812 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-29 13:58:43,009 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:58:43,013 - memory_profile6_log - INFO - date_generated: 
2018-04-29 13:58:43,013 - memory_profile6_log - INFO -  
2018-04-29 13:58:43,013 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 13:58:43,013 - memory_profile6_log - INFO - 

2018-04-29 13:58:43,015 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 13:58:43,015 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 13:58:43,016 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 13:58:43,154 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 13:58:43,157 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 13:59:55,194 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 13:59:55,196 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 13:59:55,236 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 13:59:55,239 - memory_profile6_log - INFO - Appending history data...
2018-04-29 13:59:55,240 - memory_profile6_log - INFO - processing batch-0
2018-04-29 13:59:55,240 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:59:55,319 - memory_profile6_log - INFO - call history data...
2018-04-29 14:00:53,813 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:00:55,125 - memory_profile6_log - INFO - processing batch-1
2018-04-29 14:00:55,127 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:00:55,174 - memory_profile6_log - INFO - call history data...
2018-04-29 14:01:51,164 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:01:52,454 - memory_profile6_log - INFO - processing batch-2
2018-04-29 14:01:52,456 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:01:52,503 - memory_profile6_log - INFO - call history data...
2018-04-29 14:02:48,006 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:02:49,318 - memory_profile6_log - INFO - processing batch-3
2018-04-29 14:02:49,319 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:02:49,371 - memory_profile6_log - INFO - call history data...
2018-04-29 14:03:43,608 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:03:45,026 - memory_profile6_log - INFO - processing batch-4
2018-04-29 14:03:45,028 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:03:45,076 - memory_profile6_log - INFO - call history data...
2018-04-29 14:04:39,763 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:04:41,144 - memory_profile6_log - INFO - Appending training data...
2018-04-29 14:04:41,144 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 14:04:41,145 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 14:04:41,154 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 14:04:41,155 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 14:04:41,157 - memory_profile6_log - INFO - ================================================

2018-04-29 14:04:41,161 - memory_profile6_log - INFO -    295     86.7 MiB     86.7 MiB   @profile

2018-04-29 14:04:41,164 - memory_profile6_log - INFO -    296                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 14:04:41,165 - memory_profile6_log - INFO -    297     86.7 MiB      0.0 MiB       bq_client = client

2018-04-29 14:04:41,167 - memory_profile6_log - INFO -    298     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 14:04:41,168 - memory_profile6_log - INFO -    299                             

2018-04-29 14:04:41,170 - memory_profile6_log - INFO -    300     86.7 MiB      0.0 MiB       datalist = []

2018-04-29 14:04:41,171 - memory_profile6_log - INFO -    301     86.7 MiB      0.0 MiB       datalist_hist = []

2018-04-29 14:04:41,171 - memory_profile6_log - INFO -    302                             

2018-04-29 14:04:41,171 - memory_profile6_log - INFO -    303     86.7 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 14:04:41,174 - memory_profile6_log - INFO -    304    444.9 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 14:04:41,176 - memory_profile6_log - INFO -    305    339.0 MiB    252.3 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 14:04:41,178 - memory_profile6_log - INFO -    306    339.0 MiB      0.0 MiB           if tframe is not None:

2018-04-29 14:04:41,180 - memory_profile6_log - INFO -    307    339.0 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 14:04:41,180 - memory_profile6_log - INFO -    308    346.5 MiB      7.5 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 14:04:41,180 - memory_profile6_log - INFO -    309    346.5 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 14:04:41,181 - memory_profile6_log - INFO -    310    346.5 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 14:04:41,181 - memory_profile6_log - INFO -    311    444.9 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 14:04:41,184 - memory_profile6_log - INFO -    312                                                 # ~ loading history

2018-04-29 14:04:41,187 - memory_profile6_log - INFO -    313                                                 """

2018-04-29 14:04:41,188 - memory_profile6_log - INFO -    314                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 14:04:41,190 - memory_profile6_log - INFO -    315                                                 """

2018-04-29 14:04:41,190 - memory_profile6_log - INFO -    316    428.3 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 14:04:41,191 - memory_profile6_log - INFO -    317                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 14:04:41,191 - memory_profile6_log - INFO -    318    428.3 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 14:04:41,194 - memory_profile6_log - INFO -    319                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 14:04:41,194 - memory_profile6_log - INFO -    320    429.0 MiB      5.1 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 14:04:41,197 - memory_profile6_log - INFO -    321                             

2018-04-29 14:04:41,197 - memory_profile6_log - INFO -    322    429.0 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 14:04:41,197 - memory_profile6_log - INFO -    323    475.2 MiB    256.6 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 14:04:41,198 - memory_profile6_log - INFO -    324                             

2018-04-29 14:04:41,200 - memory_profile6_log - INFO -    325                                                 # me = os.getpid()

2018-04-29 14:04:41,200 - memory_profile6_log - INFO -    326                                                 # kill_proc_tree(me)

2018-04-29 14:04:41,201 - memory_profile6_log - INFO -    327                             

2018-04-29 14:04:41,201 - memory_profile6_log - INFO -    328    475.2 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 14:04:41,206 - memory_profile6_log - INFO -    329    476.4 MiB     -1.4 MiB                       for m in h_frame:

2018-04-29 14:04:41,207 - memory_profile6_log - INFO -    330    476.4 MiB     -1.4 MiB                           if m is not None:

2018-04-29 14:04:41,207 - memory_profile6_log - INFO -    331    476.4 MiB     -1.4 MiB                               if len(m) > 0:

2018-04-29 14:04:41,209 - memory_profile6_log - INFO -    332    476.4 MiB      3.0 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 14:04:41,210 - memory_profile6_log - INFO -    333    444.9 MiB   -166.6 MiB                       del h_frame

2018-04-29 14:04:41,210 - memory_profile6_log - INFO -    334    444.9 MiB     -1.2 MiB                       del lhistory

2018-04-29 14:04:41,211 - memory_profile6_log - INFO -    335                             

2018-04-29 14:04:41,213 - memory_profile6_log - INFO -    336    444.9 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 14:04:41,213 - memory_profile6_log - INFO -    337    444.9 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 14:04:41,217 - memory_profile6_log - INFO -    338                                     else: 

2018-04-29 14:04:41,217 - memory_profile6_log - INFO -    339                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 14:04:41,219 - memory_profile6_log - INFO -    340    444.9 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 14:04:41,220 - memory_profile6_log - INFO -    341    444.9 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 14:04:41,220 - memory_profile6_log - INFO -    342                             

2018-04-29 14:04:41,221 - memory_profile6_log - INFO -    343    444.9 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 14:04:41,223 - memory_profile6_log - INFO - 


2018-04-29 14:04:42,394 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 14:04:42,469 - memory_profile6_log - INFO -     p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0    0.001473         269.647011         4             279.647011  22291119  16174e7bea6105-01e4bf88dc23d8-80f0045-38400-16...
1    0.000646        3688.651934        48            3698.651934  10960288  1613f71c69c80-0616565e91cf83-6b6f613a-38400-16...
2    0.000646        3688.651934        61            3698.651934  10960288  1612d17ab2361-0a61468d795199-43426071-64320-16...
3    0.001473         269.647011        16             279.647011  22291119  1612d3cf24eb0-03b5c67043a8fb-4962427c-38400-16...
4    0.000055        5563.716667         4            5573.716667  11911567  1613746d53017-007a2bf6f22f28-70261016-38400-16...
5    0.000646        1844.325967         8            1854.325967  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
6    0.000055        5563.716667        57            5573.716667  11911567  162c08c688718-0a56519df6356a-776c4f30-38400-16...
7    0.000055        5563.716667        36            5573.716667  11911567  16135d850fb2e1-039c66e4be45428-4d707656-2c600-...
8    0.000055        5563.716667        15            5573.716667  11911567  161a8dd819211-08b69608c04698-15680144-4df28-16...
9    0.001473         269.647011         4             279.647011  22291119  161316a171411a-0feb060c12d58f-73261116-38400-1...
10   0.001473         269.647011         4             279.647011  22291119  16130674cb93f8-0e322948df367c-282b543f-38400-1...
11   0.001473         269.647011         4             279.647011  22291119  1611b7182ba1d8-09456ef3a53157-5f6c3a73-ff000-1...
12   0.001473         269.647011        32             279.647011  22291119  161a333c831e-00b875a76c6048-28313a6c-38400-161...
13   0.000646        7377.303867        74            7387.303867  10960288  1613335453c267-06848318f5de208-76313118-3d10d-...
14   0.000646        1844.325967         4            1854.325967  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
15   0.000646        1844.325967         4            1854.325967  10960288  161e5a454f6161-064229b4591715-61643d25-38400-1...
16   0.000646        3688.651934        24            3698.651934  10960288  1612d9f9772101-0b70a16f870af8-452b452b-38400-1...
17   0.000646        1844.325967        79            1854.325967  10960288  1612d0e57287b-00705a12b256f1-7666d1c-38400-161...
18   0.000646        1844.325967        16            1854.325967  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
19   0.000646        1844.325967        75            1854.325967  10960288  1612f72ab9c12f-067c6547446aa9-68600a3e-49a10-1...
2018-04-29 14:04:42,470 - memory_profile6_log - INFO - 

2018-04-29 14:04:42,553 - memory_profile6_log - INFO - size of big_frame_hist: 69.87 MB
2018-04-29 14:04:42,622 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 14:04:42,634 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 14:05:34,625 - memory_profile6_log - INFO - size of df: 58.30 MB
2018-04-29 14:05:34,627 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-29 14:05:34,727 - memory_profile6_log - INFO - size of current_frame: 58.30 MB
2018-04-29 14:05:34,730 - memory_profile6_log - INFO - loading time of: 484420 total genuine-current interest data ~ take 411.597s
2018-04-29 14:05:34,744 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 14:05:34,746 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 14:05:34,746 - memory_profile6_log - INFO - ================================================

2018-04-29 14:05:34,749 - memory_profile6_log - INFO -    345     86.6 MiB     86.6 MiB   @profile

2018-04-29 14:05:34,750 - memory_profile6_log - INFO -    346                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 14:05:34,753 - memory_profile6_log - INFO -    347     86.7 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 14:05:34,753 - memory_profile6_log - INFO -    348     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 14:05:34,755 - memory_profile6_log - INFO -    349                             

2018-04-29 14:05:34,756 - memory_profile6_log - INFO -    350                                 # ~~~ Begin collecting data ~~~

2018-04-29 14:05:34,756 - memory_profile6_log - INFO -    351     86.7 MiB      0.0 MiB       t0 = time.time()

2018-04-29 14:05:34,760 - memory_profile6_log - INFO -    352    441.8 MiB    355.1 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 14:05:34,762 - memory_profile6_log - INFO -    353    441.8 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 14:05:34,762 - memory_profile6_log - INFO -    354                                     logger.info("Training cannot be empty..")

2018-04-29 14:05:34,763 - memory_profile6_log - INFO -    355                                     return False

2018-04-29 14:05:34,763 - memory_profile6_log - INFO -    356    457.4 MiB     15.6 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 14:05:34,763 - memory_profile6_log - INFO -    357    457.7 MiB      0.2 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 14:05:34,763 - memory_profile6_log - INFO -    358    457.7 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 14:05:34,765 - memory_profile6_log - INFO -    359                             

2018-04-29 14:05:34,765 - memory_profile6_log - INFO -    360    463.7 MiB      6.1 MiB       big_frame = pd.concat(datalist)

2018-04-29 14:05:34,766 - memory_profile6_log - INFO -    361    463.7 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 14:05:34,766 - memory_profile6_log - INFO -    362    457.9 MiB     -5.8 MiB       del datalist

2018-04-29 14:05:34,769 - memory_profile6_log - INFO -    363                             

2018-04-29 14:05:34,770 - memory_profile6_log - INFO -    364    457.9 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 14:05:34,772 - memory_profile6_log - INFO -    365                             

2018-04-29 14:05:34,772 - memory_profile6_log - INFO -    366                                 # ~ get current news interest ~

2018-04-29 14:05:34,773 - memory_profile6_log - INFO -    367    457.9 MiB      0.0 MiB       if not cd:

2018-04-29 14:05:34,773 - memory_profile6_log - INFO -    368                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 14:05:34,773 - memory_profile6_log - INFO -    369                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 14:05:34,773 - memory_profile6_log - INFO -    370                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 14:05:34,773 - memory_profile6_log - INFO -    371                                 else:

2018-04-29 14:05:34,775 - memory_profile6_log - INFO -    372    457.9 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 14:05:34,776 - memory_profile6_log - INFO -    373    457.9 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 14:05:34,776 - memory_profile6_log - INFO -    374                             

2018-04-29 14:05:34,776 - memory_profile6_log - INFO -    375                                     # safe handling of query parameter

2018-04-29 14:05:34,778 - memory_profile6_log - INFO -    376                                     query_params = [

2018-04-29 14:05:34,782 - memory_profile6_log - INFO -    377    457.9 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 14:05:34,782 - memory_profile6_log - INFO -    378                                     ]

2018-04-29 14:05:34,783 - memory_profile6_log - INFO -    379                             

2018-04-29 14:05:34,783 - memory_profile6_log - INFO -    380    457.9 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-29 14:05:34,785 - memory_profile6_log - INFO -    381    515.6 MiB     57.8 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 14:05:34,786 - memory_profile6_log - INFO -    382    515.6 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 14:05:34,788 - memory_profile6_log - INFO -    383                             

2018-04-29 14:05:34,788 - memory_profile6_log - INFO -    384    515.7 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 14:05:34,789 - memory_profile6_log - INFO -    385    515.7 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 14:05:34,790 - memory_profile6_log - INFO -    386    515.7 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 14:05:34,792 - memory_profile6_log - INFO -    387    515.7 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 14:05:34,792 - memory_profile6_log - INFO -    388    515.7 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 14:05:34,793 - memory_profile6_log - INFO -    389    515.7 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 14:05:34,795 - memory_profile6_log - INFO -    390                             

2018-04-29 14:05:34,795 - memory_profile6_log - INFO -    391    515.7 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 14:05:34,796 - memory_profile6_log - INFO - 


2018-04-29 14:05:34,799 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 14:05:34,832 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 14:05:34,834 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-29 14:05:34,835 - memory_profile6_log - INFO - apply on: 253995 total history data(D(t))
2018-04-29 14:05:35,839 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 14:05:35,842 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 14:06:20,223 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-29 14:06:20,259 - memory_profile6_log - INFO - Total train time: 45.426s
2018-04-29 14:06:20,259 - memory_profile6_log - INFO - memory left before cleaning: 70.500 percent memory...
2018-04-29 14:06:20,260 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-29 14:06:20,263 - memory_profile6_log - INFO - deleting df_dut...
2018-04-29 14:06:20,263 - memory_profile6_log - INFO - deleting df_dt...
2018-04-29 14:06:20,266 - memory_profile6_log - INFO - deleting df_input...
2018-04-29 14:06:20,279 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-29 14:06:20,279 - memory_profile6_log - INFO - deleting df_current...
2018-04-29 14:06:20,282 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-29 14:06:20,295 - memory_profile6_log - INFO - deleting model_fit...
2018-04-29 14:06:20,296 - memory_profile6_log - INFO - deleting result...
2018-04-29 14:06:20,319 - memory_profile6_log - INFO - deleting model_transform...
2018-04-29 14:06:20,319 - memory_profile6_log - INFO - memory left after cleaning: 70.400 percent memory...
2018-04-29 14:06:20,321 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-29 14:06:20,322 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-29 14:06:20,515 - memory_profile6_log - INFO - Saving fitted_models as history...
2018-04-29 14:06:20,604 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 10
2018-04-29 14:06:20,605 - memory_profile6_log - INFO - processing batch-0
2018-04-29 14:06:20,796 - memory_profile6_log - INFO - processing batch-1
2018-04-29 14:06:20,986 - memory_profile6_log - INFO - processing batch-2
2018-04-29 14:06:21,171 - memory_profile6_log - INFO - processing batch-3
2018-04-29 14:06:21,364 - memory_profile6_log - INFO - processing batch-4
2018-04-29 14:06:21,555 - memory_profile6_log - INFO - processing batch-5
2018-04-29 14:06:21,737 - memory_profile6_log - INFO - processing batch-6
2018-04-29 14:06:21,921 - memory_profile6_log - INFO - processing batch-7
2018-04-29 14:06:22,112 - memory_profile6_log - INFO - processing batch-8
2018-04-29 14:06:22,306 - memory_profile6_log - INFO - processing batch-9
2018-04-29 14:06:22,503 - memory_profile6_log - INFO - deleting BR...
2018-04-29 14:06:22,505 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-04-29 14:06:22,515 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 14:06:22,515 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 14:06:22,516 - memory_profile6_log - INFO - ================================================

2018-04-29 14:06:22,516 - memory_profile6_log - INFO -    113    513.2 MiB    513.2 MiB   @profile

2018-04-29 14:06:22,517 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-29 14:06:22,517 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-29 14:06:22,523 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-29 14:06:22,523 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-29 14:06:22,525 - memory_profile6_log - INFO -    118                                      saveto="datastore"):

2018-04-29 14:06:22,525 - memory_profile6_log - INFO -    119                                 """

2018-04-29 14:06:22,525 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-29 14:06:22,526 - memory_profile6_log - INFO -    121                                 """

2018-04-29 14:06:22,526 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-29 14:06:22,526 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-29 14:06:22,528 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-29 14:06:22,528 - memory_profile6_log - INFO -    125    513.2 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-29 14:06:22,529 - memory_profile6_log - INFO -    126    521.2 MiB      8.0 MiB       df_dut = df_input.copy(deep=True)

2018-04-29 14:06:22,529 - memory_profile6_log - INFO -    127    521.2 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 14:06:22,529 - memory_profile6_log - INFO -    128                             

2018-04-29 14:06:22,529 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-29 14:06:22,529 - memory_profile6_log - INFO -    130    528.3 MiB      7.1 MiB       df_dt = df_current.copy(deep=True)

2018-04-29 14:06:22,533 - memory_profile6_log - INFO -    131    528.3 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 14:06:22,535 - memory_profile6_log - INFO -    132                             

2018-04-29 14:06:22,536 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-29 14:06:22,536 - memory_profile6_log - INFO -    134    528.3 MiB      0.0 MiB       t0 = time.time()

2018-04-29 14:06:22,536 - memory_profile6_log - INFO -    135    528.3 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-29 14:06:22,536 - memory_profile6_log - INFO -    136    528.3 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-29 14:06:22,536 - memory_profile6_log - INFO -    137    528.3 MiB      0.0 MiB       logger.info("apply on: %d total history data(D(t))", len(df_hist))

2018-04-29 14:06:22,538 - memory_profile6_log - INFO -    138                             

2018-04-29 14:06:22,538 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-29 14:06:22,539 - memory_profile6_log - INFO -    140    528.3 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-29 14:06:22,539 - memory_profile6_log - INFO -    141                             

2018-04-29 14:06:22,539 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-29 14:06:22,540 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-29 14:06:22,540 - memory_profile6_log - INFO -    144    530.2 MiB      1.9 MiB       NB = BR.processX(df_dut)

2018-04-29 14:06:22,540 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-29 14:06:22,542 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-29 14:06:22,542 - memory_profile6_log - INFO -    147    540.1 MiB      9.9 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-29 14:06:22,542 - memory_profile6_log - INFO -    148                                 """

2018-04-29 14:06:22,546 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-29 14:06:22,546 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-29 14:06:22,549 - memory_profile6_log - INFO -    151                                 """

2018-04-29 14:06:22,549 - memory_profile6_log - INFO -    152    540.1 MiB      0.0 MiB       fitby_sigmant = True

2018-04-29 14:06:22,549 - memory_profile6_log - INFO -    153    540.1 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-29 14:06:22,549 - memory_profile6_log - INFO -    154    540.1 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-29 14:06:22,549 - memory_profile6_log - INFO -    155    549.8 MiB      9.7 MiB                            'is_general']]

2018-04-29 14:06:22,551 - memory_profile6_log - INFO -    156    549.8 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-29 14:06:22,551 - memory_profile6_log - INFO -    157    549.8 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-29 14:06:22,551 - memory_profile6_log - INFO -    158    575.9 MiB     26.1 MiB                          verbose=False)

2018-04-29 14:06:22,552 - memory_profile6_log - INFO -    159    575.9 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-29 14:06:22,552 - memory_profile6_log - INFO -    160    575.9 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-29 14:06:22,552 - memory_profile6_log - INFO -    161                             

2018-04-29 14:06:22,552 - memory_profile6_log - INFO -    162                                 # ~~ and Transform ~~

2018-04-29 14:06:22,552 - memory_profile6_log - INFO -    163                                 #   handling current news interest == current date

2018-04-29 14:06:22,553 - memory_profile6_log - INFO -    164    575.9 MiB      0.0 MiB       if df_dt.empty:

2018-04-29 14:06:22,553 - memory_profile6_log - INFO -    165                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-29 14:06:22,553 - memory_profile6_log - INFO -    166                                     return None

2018-04-29 14:06:22,555 - memory_profile6_log - INFO -    167    576.2 MiB      0.3 MiB       NB = BR.processX(df_dt)

2018-04-29 14:06:22,558 - memory_profile6_log - INFO -    168    586.7 MiB     10.6 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-29 14:06:22,559 - memory_profile6_log - INFO -    169                             

2018-04-29 14:06:22,561 - memory_profile6_log - INFO -    170    586.7 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-29 14:06:22,561 - memory_profile6_log - INFO -    171    585.8 MiB     -0.9 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-29 14:06:22,561 - memory_profile6_log - INFO -    172    585.8 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-29 14:06:22,562 - memory_profile6_log - INFO -    173    585.8 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-29 14:06:22,562 - memory_profile6_log - INFO -    174    604.5 MiB     18.7 MiB                                                     verbose=False)

2018-04-29 14:06:22,562 - memory_profile6_log - INFO -    175                                 # ~~~ filter is general and specific topic ~~~

2018-04-29 14:06:22,562 - memory_profile6_log - INFO -    176                                 # the idea is just we need to rerank every topic according

2018-04-29 14:06:22,562 - memory_profile6_log - INFO -    177                                 # user_id and and is_general by p0_posterior

2018-04-29 14:06:22,563 - memory_profile6_log - INFO -    178    604.5 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-29 14:06:22,563 - memory_profile6_log - INFO -    179    606.5 MiB      1.9 MiB                                     'is_general']].groupby(['topic_id',

2018-04-29 14:06:22,565 - memory_profile6_log - INFO -    180    604.6 MiB     -1.9 MiB                                                             'is_general']

2018-04-29 14:06:22,565 - memory_profile6_log - INFO -    181                                                                                      ).size().to_frame().reset_index()

2018-04-29 14:06:22,565 - memory_profile6_log - INFO -    182                             

2018-04-29 14:06:22,565 - memory_profile6_log - INFO -    183                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-29 14:06:22,566 - memory_profile6_log - INFO -    184                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-29 14:06:22,569 - memory_profile6_log - INFO -    185    604.6 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-29 14:06:22,572 - memory_profile6_log - INFO -    186                             

2018-04-29 14:06:22,572 - memory_profile6_log - INFO -    187                                 # ~ start by provide rank for each topic type ~

2018-04-29 14:06:22,573 - memory_profile6_log - INFO -    188    608.9 MiB      4.4 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-29 14:06:22,573 - memory_profile6_log - INFO -    189    614.6 MiB      5.6 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-29 14:06:22,573 - memory_profile6_log - INFO -    190                             

2018-04-29 14:06:22,575 - memory_profile6_log - INFO -    191                                 # ~ set threshold to filter output

2018-04-29 14:06:22,575 - memory_profile6_log - INFO -    192    614.6 MiB      0.0 MiB       if threshold > 0:

2018-04-29 14:06:22,575 - memory_profile6_log - INFO -    193    614.6 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-29 14:06:22,575 - memory_profile6_log - INFO -    194    614.6 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-29 14:06:22,575 - memory_profile6_log - INFO -    195    614.6 MiB      0.0 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-29 14:06:22,575 - memory_profile6_log - INFO -    196                             

2018-04-29 14:06:22,576 - memory_profile6_log - INFO -    197    614.6 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 14:06:22,576 - memory_profile6_log - INFO -    198    614.6 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-29 14:06:22,578 - memory_profile6_log - INFO -    199                             

2018-04-29 14:06:22,582 - memory_profile6_log - INFO -    200    614.6 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 14:06:22,582 - memory_profile6_log - INFO -    201                             

2018-04-29 14:06:22,584 - memory_profile6_log - INFO -    202    614.6 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-29 14:06:22,584 - memory_profile6_log - INFO -    203    614.6 MiB      0.0 MiB       del df_dut

2018-04-29 14:06:22,584 - memory_profile6_log - INFO -    204    614.6 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-29 14:06:22,585 - memory_profile6_log - INFO -    205    614.6 MiB      0.0 MiB       del df_dt

2018-04-29 14:06:22,585 - memory_profile6_log - INFO -    206    614.6 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-29 14:06:22,585 - memory_profile6_log - INFO -    207    614.6 MiB      0.0 MiB       del df_input

2018-04-29 14:06:22,585 - memory_profile6_log - INFO -    208    614.6 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-29 14:06:22,585 - memory_profile6_log - INFO -    209    605.8 MiB     -8.8 MiB       del df_input_X

2018-04-29 14:06:22,585 - memory_profile6_log - INFO -    210    605.8 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-29 14:06:22,586 - memory_profile6_log - INFO -    211    605.8 MiB      0.0 MiB       del df_current

2018-04-29 14:06:22,586 - memory_profile6_log - INFO -    212    605.8 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-29 14:06:22,586 - memory_profile6_log - INFO -    213    605.8 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-29 14:06:22,588 - memory_profile6_log - INFO -    214    605.8 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-29 14:06:22,588 - memory_profile6_log - INFO -    215    582.6 MiB    -23.3 MiB       del model_fit

2018-04-29 14:06:22,589 - memory_profile6_log - INFO -    216    582.6 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-29 14:06:22,592 - memory_profile6_log - INFO -    217    582.6 MiB      0.0 MiB       del result

2018-04-29 14:06:22,594 - memory_profile6_log - INFO -    218    582.6 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-29 14:06:22,595 - memory_profile6_log - INFO -    219                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 14:06:22,595 - memory_profile6_log - INFO -    220                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 14:06:22,595 - memory_profile6_log - INFO -    221                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 14:06:22,595 - memory_profile6_log - INFO -    222    582.6 MiB      0.0 MiB       if savetrain:

2018-04-29 14:06:22,595 - memory_profile6_log - INFO -    223    588.0 MiB      5.4 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-29 14:06:22,596 - memory_profile6_log - INFO -    224    588.0 MiB      0.0 MiB           del model_transform

2018-04-29 14:06:22,598 - memory_profile6_log - INFO -    225    588.0 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-29 14:06:22,598 - memory_profile6_log - INFO -    226    588.0 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 14:06:22,598 - memory_profile6_log - INFO -    227                             

2018-04-29 14:06:22,599 - memory_profile6_log - INFO -    228    588.0 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-29 14:06:22,599 - memory_profile6_log - INFO -    229                                     # ~ Place your code to save the training model here ~

2018-04-29 14:06:22,601 - memory_profile6_log - INFO -    230    588.0 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-29 14:06:22,601 - memory_profile6_log - INFO -    231    588.0 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-29 14:06:22,605 - memory_profile6_log - INFO -    232    588.0 MiB      0.0 MiB               if multproc:

2018-04-29 14:06:22,605 - memory_profile6_log - INFO -    233    566.4 MiB    -21.6 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-29 14:06:22,605 - memory_profile6_log - INFO -    234                             

2018-04-29 14:06:22,607 - memory_profile6_log - INFO -    235                                             # ~ save fitted models ~

2018-04-29 14:06:22,608 - memory_profile6_log - INFO -    236    566.4 MiB      0.0 MiB                   logger.info("Saving fitted_models as history...")

2018-04-29 14:06:22,608 - memory_profile6_log - INFO -    237    566.6 MiB      0.2 MiB                   save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 14:06:22,608 - memory_profile6_log - INFO -    238    581.0 MiB     14.4 MiB                   fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-29 14:06:22,608 - memory_profile6_log - INFO -    239                             

2018-04-29 14:06:22,608 - memory_profile6_log - INFO -    240    591.4 MiB     10.4 MiB                   X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-29 14:06:22,609 - memory_profile6_log - INFO -    241    591.4 MiB      0.0 MiB                   logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-29 14:06:22,609 - memory_profile6_log - INFO -    242    594.1 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 14:06:22,609 - memory_profile6_log - INFO -    243    594.0 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 14:06:22,611 - memory_profile6_log - INFO -    244    594.1 MiB      2.7 MiB                       mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-29 14:06:22,611 - memory_profile6_log - INFO -    245                             

2018-04-29 14:06:22,611 - memory_profile6_log - INFO -    246    592.5 MiB     -1.6 MiB                   del X_split

2018-04-29 14:06:22,611 - memory_profile6_log - INFO -    247    592.5 MiB      0.0 MiB                   del BR

2018-04-29 14:06:22,617 - memory_profile6_log - INFO -    248    592.5 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-29 14:06:22,618 - memory_profile6_log - INFO -    249                             

2018-04-29 14:06:22,618 - memory_profile6_log - INFO -    250    592.5 MiB      0.0 MiB                   del save_sigma_nt

2018-04-29 14:06:22,619 - memory_profile6_log - INFO -    251    592.5 MiB      0.0 MiB                   logger.info("deleting save_sigma_nt...")

2018-04-29 14:06:22,619 - memory_profile6_log - INFO -    252                                             

2018-04-29 14:06:22,621 - memory_profile6_log - INFO -    253                                     elif str(saveto).lower() == "elastic":

2018-04-29 14:06:22,621 - memory_profile6_log - INFO -    254                                         logger.info("Using ElasticSearch as storage...")

2018-04-29 14:06:22,625 - memory_profile6_log - INFO -    255                                         mh.saveElasticS(model_transformsv)

2018-04-29 14:06:22,628 - memory_profile6_log - INFO -    256                             

2018-04-29 14:06:22,630 - memory_profile6_log - INFO -    257                                     # need save sigma_nt for daily train

2018-04-29 14:06:22,634 - memory_profile6_log - INFO -    258                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-29 14:06:22,634 - memory_profile6_log - INFO -    259                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-29 14:06:22,638 - memory_profile6_log - INFO -    260    592.5 MiB      0.0 MiB           if start_date and end_date:

2018-04-29 14:06:22,642 - memory_profile6_log - INFO -    261                                         if not fitby_sigmant:

2018-04-29 14:06:22,642 - memory_profile6_log - INFO -    262                                             logging.info("Saving sigma Nt...")

2018-04-29 14:06:22,644 - memory_profile6_log - INFO -    263                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 14:06:22,644 - memory_profile6_log - INFO -    264                                             save_sigma_nt['start_date'] = start_date

2018-04-29 14:06:22,644 - memory_profile6_log - INFO -    265                                             save_sigma_nt['end_date'] = end_date

2018-04-29 14:06:22,645 - memory_profile6_log - INFO -    266                                             print save_sigma_nt.head(5)

2018-04-29 14:06:22,645 - memory_profile6_log - INFO -    267                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-29 14:06:22,647 - memory_profile6_log - INFO -    268                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-29 14:06:22,650 - memory_profile6_log - INFO -    269    592.5 MiB      0.0 MiB       return

2018-04-29 14:06:22,651 - memory_profile6_log - INFO - 


2018-04-29 14:06:22,654 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-29 14:06:35,957 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 14:06:35,960 - memory_profile6_log - INFO - date_generated: 
2018-04-29 14:06:35,960 - memory_profile6_log - INFO -  
2018-04-29 14:06:35,960 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 14:06:35,960 - memory_profile6_log - INFO - 

2018-04-29 14:06:35,961 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 14:06:35,961 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 14:06:35,963 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 14:06:36,118 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 14:06:36,122 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 14:07:48,753 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 14:07:48,756 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 14:07:48,805 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 14:07:48,805 - memory_profile6_log - INFO - Appending history data...
2018-04-29 14:07:48,806 - memory_profile6_log - INFO - processing batch-0
2018-04-29 14:07:48,809 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:07:48,891 - memory_profile6_log - INFO - call history data...
2018-04-29 14:08:43,818 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:08:45,161 - memory_profile6_log - INFO - processing batch-1
2018-04-29 14:08:45,163 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:08:45,207 - memory_profile6_log - INFO - call history data...
2018-04-29 14:09:37,678 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:09:39,020 - memory_profile6_log - INFO - processing batch-2
2018-04-29 14:09:39,022 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:09:39,069 - memory_profile6_log - INFO - call history data...
2018-04-29 14:10:34,839 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:10:36,135 - memory_profile6_log - INFO - processing batch-3
2018-04-29 14:10:36,137 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:10:36,184 - memory_profile6_log - INFO - call history data...
2018-04-29 14:11:33,484 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:11:34,808 - memory_profile6_log - INFO - processing batch-4
2018-04-29 14:11:34,809 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:11:34,857 - memory_profile6_log - INFO - call history data...
2018-04-29 14:12:30,743 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:12:32,032 - memory_profile6_log - INFO - Appending training data...
2018-04-29 14:12:32,035 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 14:12:32,036 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 14:12:32,045 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 14:12:32,046 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 14:12:32,046 - memory_profile6_log - INFO - ================================================

2018-04-29 14:12:32,052 - memory_profile6_log - INFO -    295     86.6 MiB     86.6 MiB   @profile

2018-04-29 14:12:32,053 - memory_profile6_log - INFO -    296                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 14:12:32,055 - memory_profile6_log - INFO -    297     86.6 MiB      0.0 MiB       bq_client = client

2018-04-29 14:12:32,056 - memory_profile6_log - INFO -    298     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 14:12:32,058 - memory_profile6_log - INFO -    299                             

2018-04-29 14:12:32,059 - memory_profile6_log - INFO -    300     86.6 MiB      0.0 MiB       datalist = []

2018-04-29 14:12:32,059 - memory_profile6_log - INFO -    301     86.6 MiB      0.0 MiB       datalist_hist = []

2018-04-29 14:12:32,059 - memory_profile6_log - INFO -    302                             

2018-04-29 14:12:32,061 - memory_profile6_log - INFO -    303     86.6 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 14:12:32,062 - memory_profile6_log - INFO -    304    445.6 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 14:12:32,065 - memory_profile6_log - INFO -    305    339.1 MiB    252.5 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 14:12:32,065 - memory_profile6_log - INFO -    306    339.1 MiB      0.0 MiB           if tframe is not None:

2018-04-29 14:12:32,068 - memory_profile6_log - INFO -    307    339.1 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 14:12:32,068 - memory_profile6_log - INFO -    308    346.5 MiB      7.4 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 14:12:32,069 - memory_profile6_log - INFO -    309    346.5 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 14:12:32,069 - memory_profile6_log - INFO -    310    346.5 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 14:12:32,071 - memory_profile6_log - INFO -    311    445.6 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 14:12:32,072 - memory_profile6_log - INFO -    312                                                 # ~ loading history

2018-04-29 14:12:32,072 - memory_profile6_log - INFO -    313                                                 """

2018-04-29 14:12:32,072 - memory_profile6_log - INFO -    314                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 14:12:32,076 - memory_profile6_log - INFO -    315                                                 """

2018-04-29 14:12:32,078 - memory_profile6_log - INFO -    316    430.1 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 14:12:32,078 - memory_profile6_log - INFO -    317                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 14:12:32,078 - memory_profile6_log - INFO -    318    430.1 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 14:12:32,079 - memory_profile6_log - INFO -    319                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 14:12:32,081 - memory_profile6_log - INFO -    320    430.4 MiB      4.1 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 14:12:32,082 - memory_profile6_log - INFO -    321                             

2018-04-29 14:12:32,082 - memory_profile6_log - INFO -    322    430.4 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 14:12:32,084 - memory_profile6_log - INFO -    323    475.9 MiB    254.1 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 14:12:32,084 - memory_profile6_log - INFO -    324                             

2018-04-29 14:12:32,085 - memory_profile6_log - INFO -    325                                                 # me = os.getpid()

2018-04-29 14:12:32,088 - memory_profile6_log - INFO -    326                                                 # kill_proc_tree(me)

2018-04-29 14:12:32,089 - memory_profile6_log - INFO -    327                             

2018-04-29 14:12:32,091 - memory_profile6_log - INFO -    328    475.9 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 14:12:32,092 - memory_profile6_log - INFO -    329    476.7 MiB     -1.2 MiB                       for m in h_frame:

2018-04-29 14:12:32,092 - memory_profile6_log - INFO -    330    476.7 MiB     -1.2 MiB                           if m is not None:

2018-04-29 14:12:32,092 - memory_profile6_log - INFO -    331    476.7 MiB     -1.2 MiB                               if len(m) > 0:

2018-04-29 14:12:32,094 - memory_profile6_log - INFO -    332    476.7 MiB      3.3 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 14:12:32,095 - memory_profile6_log - INFO -    333    445.6 MiB   -162.3 MiB                       del h_frame

2018-04-29 14:12:32,095 - memory_profile6_log - INFO -    334    445.6 MiB     -1.3 MiB                       del lhistory

2018-04-29 14:12:32,095 - memory_profile6_log - INFO -    335                             

2018-04-29 14:12:32,098 - memory_profile6_log - INFO -    336    445.6 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 14:12:32,099 - memory_profile6_log - INFO -    337    445.6 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 14:12:32,101 - memory_profile6_log - INFO -    338                                     else: 

2018-04-29 14:12:32,101 - memory_profile6_log - INFO -    339                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 14:12:32,102 - memory_profile6_log - INFO -    340    445.6 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 14:12:32,104 - memory_profile6_log - INFO -    341    445.6 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 14:12:32,105 - memory_profile6_log - INFO -    342                             

2018-04-29 14:12:32,107 - memory_profile6_log - INFO -    343    445.6 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 14:12:32,109 - memory_profile6_log - INFO - 


2018-04-29 14:12:33,322 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 14:12:33,391 - memory_profile6_log - INFO -     p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0    0.000055        5563.716667        27            5573.716667  11911567  1612d9eb7fb333-0a1c17d0ba82e08-2c5f3268-2c600-...
1    0.000646        1844.325967         4            1854.325967  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
2    0.000646        1844.325967        28            1854.325967  10960288  1613cff5e9a99-028b356bcbf06b-734c435e-38400-16...
3    0.000646        1844.325967        16            1854.325967  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
4    0.001473         269.647011         4             279.647011  22291119  1612da8805b14b-0d324c1d36a4c7-39626377-2c740-1...
5    0.000646        1844.325967        25            1854.325967  10960288  161300e0f14a6-098c84d7864a32-57436a33-49a10-16...
6    0.000646        1844.325967       471            1854.325967  10960288  1610ca98d49149-0e61ac652d8ef-4323461-100200-16...
7    0.001473         269.647011        32             279.647011  22291119  1612d5d4a71111-05f5aba4c6e5bd-4a21472c-38400-1...
8    0.000646        1844.325967        23            1854.325967  10960288  161ab955e681ea-00cd2d4e919ec9-1e613a1c-38400-1...
9    0.001473         269.647011         4             279.647011  22291119  161add17c1de-03b340f8c7ee56-100d451f-38400-161...
10   0.000646        3688.651934        14            3698.651934  10960288  1617d6299713e-0638ae7886797c-44432908-38400-16...
11   0.000646        1844.325967        15            1854.325967  10960288  1612e8ce767c6-0e0b4eacdd5b13-45464657-38400-16...
12   0.000646        3688.651934        48            3698.651934  10960288  1613f71c69c80-0616565e91cf83-6b6f613a-38400-16...
13   0.001473         269.647011         8             279.647011  22291119  1612da003b5c7-09558ff4a105eb-134d5241-38400-16...
14   0.001473         269.647011         4             279.647011  22291119  16131b0ebd884-03aa3df1384c1c-7465070d-29b80-16...
15   0.000055        5563.716667         8            5573.716667  11911567  16130e2fea06e-08a4d37af-5c717549-38400-16130e2...
16   0.001473         269.647011         8             279.647011  22291119  161e954cb63110-0ad144257-227b4115-38400-161e95...
17   0.000646        1844.325967        32            1854.325967  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
18   0.001473         269.647011         4             279.647011  22291119  1614aa078305e-0fbe0328ace70f-7b261512-29b80-16...
19   0.000646        1844.325967        38            1854.325967  10960288  161316bf16fdb-0abca8bfb1328c-6e69682e-38400-16...
2018-04-29 14:12:33,394 - memory_profile6_log - INFO - 

2018-04-29 14:12:33,476 - memory_profile6_log - INFO - size of big_frame_hist: 69.87 MB
2018-04-29 14:12:33,546 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 14:12:33,559 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 14:13:21,872 - memory_profile6_log - INFO - size of df: 58.30 MB
2018-04-29 14:13:21,874 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-29 14:13:21,960 - memory_profile6_log - INFO - size of current_frame: 58.30 MB
2018-04-29 14:13:21,961 - memory_profile6_log - INFO - loading time of: 484420 total genuine-current interest data ~ take 405.873s
2018-04-29 14:13:21,979 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 14:13:21,980 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 14:13:21,982 - memory_profile6_log - INFO - ================================================

2018-04-29 14:13:21,983 - memory_profile6_log - INFO -    345     86.5 MiB     86.5 MiB   @profile

2018-04-29 14:13:21,983 - memory_profile6_log - INFO -    346                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 14:13:21,984 - memory_profile6_log - INFO -    347     86.6 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 14:13:21,986 - memory_profile6_log - INFO -    348     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 14:13:21,987 - memory_profile6_log - INFO -    349                             

2018-04-29 14:13:21,990 - memory_profile6_log - INFO -    350                                 # ~~~ Begin collecting data ~~~

2018-04-29 14:13:21,993 - memory_profile6_log - INFO -    351     86.6 MiB      0.0 MiB       t0 = time.time()

2018-04-29 14:13:21,994 - memory_profile6_log - INFO -    352    442.0 MiB    355.4 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 14:13:21,996 - memory_profile6_log - INFO -    353    442.0 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 14:13:21,996 - memory_profile6_log - INFO -    354                                     logger.info("Training cannot be empty..")

2018-04-29 14:13:21,996 - memory_profile6_log - INFO -    355                                     return False

2018-04-29 14:13:21,997 - memory_profile6_log - INFO -    356    456.7 MiB     14.8 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 14:13:21,997 - memory_profile6_log - INFO -    357    457.0 MiB      0.3 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 14:13:21,997 - memory_profile6_log - INFO -    358    457.0 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 14:13:21,999 - memory_profile6_log - INFO -    359                             

2018-04-29 14:13:21,999 - memory_profile6_log - INFO -    360    463.1 MiB      6.1 MiB       big_frame = pd.concat(datalist)

2018-04-29 14:13:22,003 - memory_profile6_log - INFO -    361    463.1 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 14:13:22,005 - memory_profile6_log - INFO -    362    457.3 MiB     -5.8 MiB       del datalist

2018-04-29 14:13:22,005 - memory_profile6_log - INFO -    363                             

2018-04-29 14:13:22,006 - memory_profile6_log - INFO -    364    457.3 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 14:13:22,006 - memory_profile6_log - INFO -    365                             

2018-04-29 14:13:22,006 - memory_profile6_log - INFO -    366                                 # ~ get current news interest ~

2018-04-29 14:13:22,006 - memory_profile6_log - INFO -    367    457.3 MiB      0.0 MiB       if not cd:

2018-04-29 14:13:22,006 - memory_profile6_log - INFO -    368                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 14:13:22,007 - memory_profile6_log - INFO -    369                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 14:13:22,009 - memory_profile6_log - INFO -    370                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 14:13:22,009 - memory_profile6_log - INFO -    371                                 else:

2018-04-29 14:13:22,010 - memory_profile6_log - INFO -    372    457.3 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 14:13:22,015 - memory_profile6_log - INFO -    373    457.3 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 14:13:22,016 - memory_profile6_log - INFO -    374                             

2018-04-29 14:13:22,016 - memory_profile6_log - INFO -    375                                     # safe handling of query parameter

2018-04-29 14:13:22,016 - memory_profile6_log - INFO -    376                                     query_params = [

2018-04-29 14:13:22,019 - memory_profile6_log - INFO -    377    457.3 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 14:13:22,020 - memory_profile6_log - INFO -    378                                     ]

2018-04-29 14:13:22,022 - memory_profile6_log - INFO -    379                             

2018-04-29 14:13:22,022 - memory_profile6_log - INFO -    380    457.3 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-29 14:13:22,022 - memory_profile6_log - INFO -    381    516.1 MiB     58.8 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 14:13:22,025 - memory_profile6_log - INFO -    382    516.1 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 14:13:22,026 - memory_profile6_log - INFO -    383                             

2018-04-29 14:13:22,026 - memory_profile6_log - INFO -    384    516.1 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 14:13:22,029 - memory_profile6_log - INFO -    385    516.1 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 14:13:22,029 - memory_profile6_log - INFO -    386    516.1 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 14:13:22,029 - memory_profile6_log - INFO -    387    516.1 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 14:13:22,029 - memory_profile6_log - INFO -    388    516.1 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 14:13:22,030 - memory_profile6_log - INFO -    389    516.1 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 14:13:22,030 - memory_profile6_log - INFO -    390                             

2018-04-29 14:13:22,032 - memory_profile6_log - INFO -    391    516.1 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 14:13:22,032 - memory_profile6_log - INFO - 


2018-04-29 14:13:22,036 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 14:13:22,065 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 14:13:22,066 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-29 14:13:22,069 - memory_profile6_log - INFO - apply on: 253995 total history data(D(t))
2018-04-29 14:13:23,122 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 14:13:23,124 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 14:13:23,221 - memory_profile6_log - INFO - Unexpected error:
2018-04-29 14:13:23,226 - memory_profile6_log - INFO -  
2018-04-29 14:13:23,226 - memory_profile6_log - INFO - cannot do a non-empty take from an empty axes.
2018-04-29 14:13:23,229 - memory_profile6_log - INFO - 

2018-04-29 14:15:18,052 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 14:15:18,055 - memory_profile6_log - INFO - date_generated: 
2018-04-29 14:15:18,055 - memory_profile6_log - INFO -  
2018-04-29 14:15:18,055 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 14:15:18,055 - memory_profile6_log - INFO - 

2018-04-29 14:15:18,055 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 14:15:18,056 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 14:15:18,058 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 14:15:18,198 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 14:15:18,201 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 14:16:29,242 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 14:16:29,243 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 14:16:29,282 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 14:16:29,285 - memory_profile6_log - INFO - Appending history data...
2018-04-29 14:16:29,286 - memory_profile6_log - INFO - processing batch-0
2018-04-29 14:16:29,286 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:16:29,364 - memory_profile6_log - INFO - call history data...
2018-04-29 14:17:27,111 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:17:28,398 - memory_profile6_log - INFO - processing batch-1
2018-04-29 14:17:28,398 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:17:28,450 - memory_profile6_log - INFO - call history data...
2018-04-29 14:18:23,572 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:18:24,868 - memory_profile6_log - INFO - processing batch-2
2018-04-29 14:18:24,868 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:18:24,917 - memory_profile6_log - INFO - call history data...
2018-04-29 14:19:18,844 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:19:20,194 - memory_profile6_log - INFO - processing batch-3
2018-04-29 14:19:20,194 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:19:20,246 - memory_profile6_log - INFO - call history data...
2018-04-29 14:20:14,894 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:20:16,197 - memory_profile6_log - INFO - processing batch-4
2018-04-29 14:20:16,197 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:20:16,246 - memory_profile6_log - INFO - call history data...
2018-04-29 14:21:09,266 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:21:10,602 - memory_profile6_log - INFO - Appending training data...
2018-04-29 14:21:10,604 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 14:21:10,604 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 14:21:10,614 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 14:21:10,615 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 14:21:10,615 - memory_profile6_log - INFO - ================================================

2018-04-29 14:21:10,617 - memory_profile6_log - INFO -    295     86.7 MiB     86.7 MiB   @profile

2018-04-29 14:21:10,618 - memory_profile6_log - INFO -    296                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 14:21:10,618 - memory_profile6_log - INFO -    297     86.7 MiB      0.0 MiB       bq_client = client

2018-04-29 14:21:10,619 - memory_profile6_log - INFO -    298     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 14:21:10,621 - memory_profile6_log - INFO -    299                             

2018-04-29 14:21:10,621 - memory_profile6_log - INFO -    300     86.7 MiB      0.0 MiB       datalist = []

2018-04-29 14:21:10,624 - memory_profile6_log - INFO -    301     86.7 MiB      0.0 MiB       datalist_hist = []

2018-04-29 14:21:10,625 - memory_profile6_log - INFO -    302                             

2018-04-29 14:21:10,627 - memory_profile6_log - INFO -    303     86.7 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 14:21:10,628 - memory_profile6_log - INFO -    304    446.9 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 14:21:10,628 - memory_profile6_log - INFO -    305    339.0 MiB    252.3 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 14:21:10,628 - memory_profile6_log - INFO -    306    339.0 MiB      0.0 MiB           if tframe is not None:

2018-04-29 14:21:10,630 - memory_profile6_log - INFO -    307    339.0 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 14:21:10,630 - memory_profile6_log - INFO -    308    346.6 MiB      7.6 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 14:21:10,631 - memory_profile6_log - INFO -    309    346.6 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 14:21:10,631 - memory_profile6_log - INFO -    310    346.6 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 14:21:10,632 - memory_profile6_log - INFO -    311    446.9 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 14:21:10,637 - memory_profile6_log - INFO -    312                                                 # ~ loading history

2018-04-29 14:21:10,638 - memory_profile6_log - INFO -    313                                                 """

2018-04-29 14:21:10,640 - memory_profile6_log - INFO -    314                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 14:21:10,641 - memory_profile6_log - INFO -    315                                                 """

2018-04-29 14:21:10,641 - memory_profile6_log - INFO -    316    429.0 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 14:21:10,642 - memory_profile6_log - INFO -    317                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 14:21:10,644 - memory_profile6_log - INFO -    318    429.0 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 14:21:10,644 - memory_profile6_log - INFO -    319                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 14:21:10,644 - memory_profile6_log - INFO -    320    429.4 MiB      4.4 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 14:21:10,644 - memory_profile6_log - INFO -    321                             

2018-04-29 14:21:10,648 - memory_profile6_log - INFO -    322    429.4 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 14:21:10,648 - memory_profile6_log - INFO -    323    475.8 MiB    252.4 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 14:21:10,651 - memory_profile6_log - INFO -    324                             

2018-04-29 14:21:10,653 - memory_profile6_log - INFO -    325                                                 # me = os.getpid()

2018-04-29 14:21:10,653 - memory_profile6_log - INFO -    326                                                 # kill_proc_tree(me)

2018-04-29 14:21:10,654 - memory_profile6_log - INFO -    327                             

2018-04-29 14:21:10,654 - memory_profile6_log - INFO -    328    475.8 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 14:21:10,655 - memory_profile6_log - INFO -    329    477.0 MiB     -1.4 MiB                       for m in h_frame:

2018-04-29 14:21:10,657 - memory_profile6_log - INFO -    330    476.9 MiB     -1.4 MiB                           if m is not None:

2018-04-29 14:21:10,658 - memory_profile6_log - INFO -    331    476.9 MiB     -1.4 MiB                               if len(m) > 0:

2018-04-29 14:21:10,661 - memory_profile6_log - INFO -    332    477.0 MiB      2.4 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 14:21:10,661 - memory_profile6_log - INFO -    333    446.9 MiB   -160.4 MiB                       del h_frame

2018-04-29 14:21:10,663 - memory_profile6_log - INFO -    334    446.9 MiB      0.0 MiB                       del lhistory

2018-04-29 14:21:10,664 - memory_profile6_log - INFO -    335                             

2018-04-29 14:21:10,664 - memory_profile6_log - INFO -    336    446.9 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 14:21:10,664 - memory_profile6_log - INFO -    337    446.9 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 14:21:10,665 - memory_profile6_log - INFO -    338                                     else: 

2018-04-29 14:21:10,668 - memory_profile6_log - INFO -    339                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 14:21:10,670 - memory_profile6_log - INFO -    340    446.9 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 14:21:10,671 - memory_profile6_log - INFO -    341    446.9 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 14:21:10,671 - memory_profile6_log - INFO -    342                             

2018-04-29 14:21:10,673 - memory_profile6_log - INFO -    343    446.9 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 14:21:10,674 - memory_profile6_log - INFO - 


2018-04-29 14:21:11,822 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 14:21:11,892 - memory_profile6_log - INFO -     p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0    0.001473         269.647011         8             279.647011  22291119  1613ad7e0b851-0fc417524ceb79-70261314-4df28-16...
1    0.001473         269.647011         8             279.647011  22291119  1612db41e4cd-0b9acce13-5c71754f-38400-1612db41...
2    0.000646        1844.325967         9            1854.325967  10960288  161378cbe8ea3-04c9e0e16b5ee-510e3b3d-38400-161...
3    0.000646        3688.651934         8            3698.651934  10960288  161976e0d293a-0b965799941492-164f5c5c-55188-16...
4    0.000646        1844.325967        14            1854.325967  10960288  16241fb0c0f0-0ed5c3ec3e65be-7821727e-55188-162...
5    0.001473         269.647011         8             279.647011  22291119  16276de434b17-03652a4cf66c21-75640303-38400-16...
6    0.000055        5563.716667        21            5573.716667  11911567  161382058d651-0055fb1f849841-4d61342f-38400-16...
7    0.000055        5563.716667       471            5573.716667  11911567  1610ca98d49149-0e61ac652d8ef-4323461-100200-16...
8    0.001473         269.647011        16             279.647011  22291119  16130deb9eab4-0f795f55592649-d4f4822-38400-161...
9    0.000646        1844.325967        16            1854.325967  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
10   0.000055        5563.716667        50            5573.716667  11911567  1612d71088d1c3-083723c07e39b5-48616602-38400-1...
11   0.000646        1844.325967        18            1854.325967  10960288  1618f963fcd60-05fe908efd3acd-282b503d-38400-16...
12   0.000646        1844.325967        16            1854.325967  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
13   0.000646        1844.325967       335            1854.325967  10960288  1614a78deff48f-03731aa5504b8a-4353468-100200-1...
14   0.001473         269.647011         4             279.647011  22291119  16130c70ae446-0b4f4fedaf144b-797d262d-38400-16...
15   0.001473         269.647011         4             279.647011  22291119  1616efe1de7365-024418da2c9cdb-667e1364-3d10d-1...
16   0.000646        1844.325967        21            1854.325967  10960288  1615c1a1aab234-08acb35bb06629-57416238-38400-1...
17   0.001473         269.647011        32             279.647011  22291119  1612d5d4a71111-05f5aba4c6e5bd-4a21472c-38400-1...
18   0.001473         269.647011         4             279.647011  22291119  16131b0ebd884-03aa3df1384c1c-7465070d-29b80-16...
19   0.000646        3688.651934         8            3698.651934  10960288  161bdf8fd45b1-07281a95fb4268-a6c5a75-38400-161...
2018-04-29 14:21:11,894 - memory_profile6_log - INFO - 

2018-04-29 14:21:11,994 - memory_profile6_log - INFO - size of big_frame_hist: 69.87 MB
2018-04-29 14:21:12,065 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 14:21:12,076 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-04-29 14:27:32,642 - memory_profile6_log - INFO - size of df: 509.09 MB
2018-04-29 14:27:32,644 - memory_profile6_log - INFO - getting total: 2090090 training data(current date interest)
2018-04-29 14:27:33,273 - memory_profile6_log - INFO - size of current_frame: 525.04 MB
2018-04-29 14:27:33,275 - memory_profile6_log - INFO - loading time of: 2344085 total genuine-current interest data ~ take 735.097s
2018-04-29 14:27:33,292 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 14:27:33,293 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 14:27:33,295 - memory_profile6_log - INFO - ================================================

2018-04-29 14:27:33,296 - memory_profile6_log - INFO -    345     86.6 MiB     86.6 MiB   @profile

2018-04-29 14:27:33,296 - memory_profile6_log - INFO -    346                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 14:27:33,299 - memory_profile6_log - INFO -    347     86.7 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 14:27:33,299 - memory_profile6_log - INFO -    348     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 14:27:33,299 - memory_profile6_log - INFO -    349                             

2018-04-29 14:27:33,301 - memory_profile6_log - INFO -    350                                 # ~~~ Begin collecting data ~~~

2018-04-29 14:27:33,302 - memory_profile6_log - INFO -    351     86.7 MiB      0.0 MiB       t0 = time.time()

2018-04-29 14:27:33,302 - memory_profile6_log - INFO -    352    442.2 MiB    355.5 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 14:27:33,305 - memory_profile6_log - INFO -    353    442.2 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 14:27:33,305 - memory_profile6_log - INFO -    354                                     logger.info("Training cannot be empty..")

2018-04-29 14:27:33,308 - memory_profile6_log - INFO -    355                                     return False

2018-04-29 14:27:33,309 - memory_profile6_log - INFO -    356    457.5 MiB     15.3 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 14:27:33,309 - memory_profile6_log - INFO -    357    457.8 MiB      0.3 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 14:27:33,309 - memory_profile6_log - INFO -    358    457.8 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 14:27:33,309 - memory_profile6_log - INFO -    359                             

2018-04-29 14:27:33,309 - memory_profile6_log - INFO -    360    463.8 MiB      6.1 MiB       big_frame = pd.concat(datalist)

2018-04-29 14:27:33,311 - memory_profile6_log - INFO -    361    463.8 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 14:27:33,312 - memory_profile6_log - INFO -    362    458.0 MiB     -5.8 MiB       del datalist

2018-04-29 14:27:33,312 - memory_profile6_log - INFO -    363                             

2018-04-29 14:27:33,312 - memory_profile6_log - INFO -    364    458.0 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 14:27:33,318 - memory_profile6_log - INFO -    365                             

2018-04-29 14:27:33,319 - memory_profile6_log - INFO -    366                                 # ~ get current news interest ~

2018-04-29 14:27:33,319 - memory_profile6_log - INFO -    367    458.0 MiB      0.0 MiB       if not cd:

2018-04-29 14:27:33,321 - memory_profile6_log - INFO -    368    458.0 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-04-29 14:27:33,321 - memory_profile6_log - INFO -    369    963.7 MiB    505.7 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 14:27:33,323 - memory_profile6_log - INFO -    370    963.7 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 14:27:33,323 - memory_profile6_log - INFO -    371                                 else:

2018-04-29 14:27:33,323 - memory_profile6_log - INFO -    372                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 14:27:33,325 - memory_profile6_log - INFO -    373                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 14:27:33,325 - memory_profile6_log - INFO -    374                             

2018-04-29 14:27:33,325 - memory_profile6_log - INFO -    375                                     # safe handling of query parameter

2018-04-29 14:27:33,329 - memory_profile6_log - INFO -    376                                     query_params = [

2018-04-29 14:27:33,329 - memory_profile6_log - INFO -    377                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 14:27:33,332 - memory_profile6_log - INFO -    378                                     ]

2018-04-29 14:27:33,332 - memory_profile6_log - INFO -    379                             

2018-04-29 14:27:33,334 - memory_profile6_log - INFO -    380                                     job_config.query_parameters = query_params

2018-04-29 14:27:33,334 - memory_profile6_log - INFO -    381                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 14:27:33,335 - memory_profile6_log - INFO -    382                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 14:27:33,335 - memory_profile6_log - INFO -    383                             

2018-04-29 14:27:33,335 - memory_profile6_log - INFO -    384    979.7 MiB     16.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 14:27:33,335 - memory_profile6_log - INFO -    385    979.7 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 14:27:33,336 - memory_profile6_log - INFO -    386    979.7 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 14:27:33,338 - memory_profile6_log - INFO -    387    979.7 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 14:27:33,342 - memory_profile6_log - INFO -    388    979.7 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 14:27:33,342 - memory_profile6_log - INFO -    389    979.7 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 14:27:33,342 - memory_profile6_log - INFO -    390                             

2018-04-29 14:27:33,344 - memory_profile6_log - INFO -    391    979.7 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 14:27:33,344 - memory_profile6_log - INFO - 


2018-04-29 14:27:33,348 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 14:27:33,467 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 14:27:33,469 - memory_profile6_log - INFO - transform on: 2090090 total current data(D(t))
2018-04-29 14:27:33,470 - memory_profile6_log - INFO - apply on: 253995 total history data(D(t))
2018-04-29 14:27:34,509 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 14:27:34,510 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 14:28:20,569 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-29 14:28:20,598 - memory_profile6_log - INFO - Total train time: 47.131s
2018-04-29 14:28:20,599 - memory_profile6_log - INFO - memory left before cleaning: 75.000 percent memory...
2018-04-29 14:28:20,601 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-29 14:28:20,602 - memory_profile6_log - INFO - deleting df_dut...
2018-04-29 14:28:20,604 - memory_profile6_log - INFO - deleting df_dt...
2018-04-29 14:28:20,605 - memory_profile6_log - INFO - deleting df_input...
2018-04-29 14:28:20,690 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-29 14:28:20,690 - memory_profile6_log - INFO - deleting df_current...
2018-04-29 14:28:20,693 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-29 14:28:20,704 - memory_profile6_log - INFO - deleting model_fit...
2018-04-29 14:28:20,706 - memory_profile6_log - INFO - deleting result...
2018-04-29 14:28:20,723 - memory_profile6_log - INFO - deleting model_transform...
2018-04-29 14:28:20,724 - memory_profile6_log - INFO - memory left after cleaning: 74.300 percent memory...
2018-04-29 14:28:20,726 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-29 14:28:20,727 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-29 14:28:20,891 - memory_profile6_log - INFO - Saving fitted_models as history...
2018-04-29 14:28:20,980 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 10
2018-04-29 14:28:20,982 - memory_profile6_log - INFO - processing batch-0
2018-04-29 14:28:21,183 - memory_profile6_log - INFO - processing batch-1
2018-04-29 14:28:21,365 - memory_profile6_log - INFO - processing batch-2
2018-04-29 14:28:21,548 - memory_profile6_log - INFO - processing batch-3
2018-04-29 14:28:21,726 - memory_profile6_log - INFO - processing batch-4
2018-04-29 14:28:21,913 - memory_profile6_log - INFO - processing batch-5
2018-04-29 14:28:22,101 - memory_profile6_log - INFO - processing batch-6
2018-04-29 14:28:22,292 - memory_profile6_log - INFO - processing batch-7
2018-04-29 14:28:22,484 - memory_profile6_log - INFO - processing batch-8
2018-04-29 14:28:22,671 - memory_profile6_log - INFO - processing batch-9
2018-04-29 14:28:22,859 - memory_profile6_log - INFO - deleting BR...
2018-04-29 14:28:22,861 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-04-29 14:28:22,871 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 14:28:22,872 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 14:28:22,875 - memory_profile6_log - INFO - ================================================

2018-04-29 14:28:22,875 - memory_profile6_log - INFO -    113    974.3 MiB    974.3 MiB   @profile

2018-04-29 14:28:22,878 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-29 14:28:22,878 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-29 14:28:22,878 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-29 14:28:22,880 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-29 14:28:22,880 - memory_profile6_log - INFO -    118                                      saveto="datastore"):

2018-04-29 14:28:22,881 - memory_profile6_log - INFO -    119                                 """

2018-04-29 14:28:22,881 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-29 14:28:22,881 - memory_profile6_log - INFO -    121                                 """

2018-04-29 14:28:22,885 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-29 14:28:22,887 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-29 14:28:22,888 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-29 14:28:22,888 - memory_profile6_log - INFO -    125    974.3 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-29 14:28:22,888 - memory_profile6_log - INFO -    126    982.5 MiB      8.1 MiB       df_dut = df_input.copy(deep=True)

2018-04-29 14:28:22,890 - memory_profile6_log - INFO -    127    982.5 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 14:28:22,890 - memory_profile6_log - INFO -    128                             

2018-04-29 14:28:22,891 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-29 14:28:22,892 - memory_profile6_log - INFO -    130   1048.2 MiB     65.8 MiB       df_dt = df_current.copy(deep=True)

2018-04-29 14:28:22,892 - memory_profile6_log - INFO -    131   1048.2 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 14:28:22,892 - memory_profile6_log - INFO -    132                             

2018-04-29 14:28:22,894 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-29 14:28:22,894 - memory_profile6_log - INFO -    134   1048.2 MiB      0.0 MiB       t0 = time.time()

2018-04-29 14:28:22,894 - memory_profile6_log - INFO -    135   1048.2 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-29 14:28:22,898 - memory_profile6_log - INFO -    136   1048.2 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-29 14:28:22,898 - memory_profile6_log - INFO -    137   1048.2 MiB      0.0 MiB       logger.info("apply on: %d total history data(D(t))", len(df_hist))

2018-04-29 14:28:22,901 - memory_profile6_log - INFO -    138                             

2018-04-29 14:28:22,901 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-29 14:28:22,903 - memory_profile6_log - INFO -    140   1048.2 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-29 14:28:22,903 - memory_profile6_log - INFO -    141                             

2018-04-29 14:28:22,904 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-29 14:28:22,904 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-29 14:28:22,904 - memory_profile6_log - INFO -    144   1048.8 MiB      0.6 MiB       NB = BR.processX(df_dut)

2018-04-29 14:28:22,904 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-29 14:28:22,905 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-29 14:28:22,905 - memory_profile6_log - INFO -    147   1058.7 MiB      9.9 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-29 14:28:22,908 - memory_profile6_log - INFO -    148                                 """

2018-04-29 14:28:22,911 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-29 14:28:22,911 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-29 14:28:22,913 - memory_profile6_log - INFO -    151                                 """

2018-04-29 14:28:22,914 - memory_profile6_log - INFO -    152   1058.7 MiB      0.0 MiB       fitby_sigmant = True

2018-04-29 14:28:22,914 - memory_profile6_log - INFO -    153   1058.7 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-29 14:28:22,917 - memory_profile6_log - INFO -    154   1058.7 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-29 14:28:22,921 - memory_profile6_log - INFO -    155   1068.4 MiB      9.7 MiB                            'is_general']]

2018-04-29 14:28:22,924 - memory_profile6_log - INFO -    156   1068.4 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-29 14:28:22,926 - memory_profile6_log - INFO -    157   1068.4 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-29 14:28:22,927 - memory_profile6_log - INFO -    158   1096.0 MiB     27.6 MiB                          verbose=False)

2018-04-29 14:28:22,928 - memory_profile6_log - INFO -    159   1096.0 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-29 14:28:22,930 - memory_profile6_log - INFO -    160   1096.0 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-29 14:28:22,930 - memory_profile6_log - INFO -    161                             

2018-04-29 14:28:22,934 - memory_profile6_log - INFO -    162                                 # ~~ and Transform ~~

2018-04-29 14:28:22,934 - memory_profile6_log - INFO -    163                                 #   handling current news interest == current date

2018-04-29 14:28:22,937 - memory_profile6_log - INFO -    164   1096.0 MiB      0.0 MiB       if df_dt.empty:

2018-04-29 14:28:22,937 - memory_profile6_log - INFO -    165                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-29 14:28:22,937 - memory_profile6_log - INFO -    166                                     return None

2018-04-29 14:28:22,938 - memory_profile6_log - INFO -    167   1102.5 MiB      6.5 MiB       NB = BR.processX(df_dt)

2018-04-29 14:28:22,940 - memory_profile6_log - INFO -    168   1200.5 MiB     98.0 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-29 14:28:22,940 - memory_profile6_log - INFO -    169                             

2018-04-29 14:28:22,944 - memory_profile6_log - INFO -    170   1200.5 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-29 14:28:22,946 - memory_profile6_log - INFO -    171   1272.6 MiB     72.0 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-29 14:28:22,947 - memory_profile6_log - INFO -    172   1272.6 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-29 14:28:22,947 - memory_profile6_log - INFO -    173   1272.6 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-29 14:28:22,948 - memory_profile6_log - INFO -    174   1287.4 MiB     14.9 MiB                                                     verbose=False)

2018-04-29 14:28:22,950 - memory_profile6_log - INFO -    175                                 # ~~~ filter is general and specific topic ~~~

2018-04-29 14:28:22,950 - memory_profile6_log - INFO -    176                                 # the idea is just we need to rerank every topic according

2018-04-29 14:28:22,950 - memory_profile6_log - INFO -    177                                 # user_id and and is_general by p0_posterior

2018-04-29 14:28:22,950 - memory_profile6_log - INFO -    178   1287.4 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-29 14:28:22,951 - memory_profile6_log - INFO -    179   1289.4 MiB      1.9 MiB                                     'is_general']].groupby(['topic_id',

2018-04-29 14:28:22,951 - memory_profile6_log - INFO -    180   1287.4 MiB     -1.9 MiB                                                             'is_general']

2018-04-29 14:28:22,956 - memory_profile6_log - INFO -    181                                                                                      ).size().to_frame().reset_index()

2018-04-29 14:28:22,957 - memory_profile6_log - INFO -    182                             

2018-04-29 14:28:22,959 - memory_profile6_log - INFO -    183                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-29 14:28:22,961 - memory_profile6_log - INFO -    184                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-29 14:28:22,966 - memory_profile6_log - INFO -    185   1287.5 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-29 14:28:22,967 - memory_profile6_log - INFO -    186                             

2018-04-29 14:28:22,967 - memory_profile6_log - INFO -    187                                 # ~ start by provide rank for each topic type ~

2018-04-29 14:28:22,967 - memory_profile6_log - INFO -    188   1290.6 MiB      3.1 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-29 14:28:22,969 - memory_profile6_log - INFO -    189   1296.4 MiB      5.8 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-29 14:28:22,969 - memory_profile6_log - INFO -    190                             

2018-04-29 14:28:22,970 - memory_profile6_log - INFO -    191                                 # ~ set threshold to filter output

2018-04-29 14:28:22,970 - memory_profile6_log - INFO -    192   1296.4 MiB      0.0 MiB       if threshold > 0:

2018-04-29 14:28:22,970 - memory_profile6_log - INFO -    193   1296.4 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-29 14:28:22,970 - memory_profile6_log - INFO -    194   1296.4 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-29 14:28:22,970 - memory_profile6_log - INFO -    195   1296.0 MiB     -0.3 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-29 14:28:22,970 - memory_profile6_log - INFO -    196                             

2018-04-29 14:28:22,971 - memory_profile6_log - INFO -    197   1296.0 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 14:28:22,971 - memory_profile6_log - INFO -    198   1296.0 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-29 14:28:22,971 - memory_profile6_log - INFO -    199                             

2018-04-29 14:28:22,973 - memory_profile6_log - INFO -    200   1296.0 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 14:28:22,974 - memory_profile6_log - INFO -    201                             

2018-04-29 14:28:22,983 - memory_profile6_log - INFO -    202   1296.0 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-29 14:28:22,984 - memory_profile6_log - INFO -    203   1296.0 MiB      0.0 MiB       del df_dut

2018-04-29 14:28:22,986 - memory_profile6_log - INFO -    204   1296.0 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-29 14:28:22,986 - memory_profile6_log - INFO -    205   1296.0 MiB      0.0 MiB       del df_dt

2018-04-29 14:28:22,987 - memory_profile6_log - INFO -    206   1296.0 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-29 14:28:22,989 - memory_profile6_log - INFO -    207   1296.0 MiB      0.0 MiB       del df_input

2018-04-29 14:28:22,992 - memory_profile6_log - INFO -    208   1296.0 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-29 14:28:22,993 - memory_profile6_log - INFO -    209   1214.3 MiB    -81.7 MiB       del df_input_X

2018-04-29 14:28:22,994 - memory_profile6_log - INFO -    210   1214.3 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-29 14:28:22,994 - memory_profile6_log - INFO -    211   1214.3 MiB      0.0 MiB       del df_current

2018-04-29 14:28:22,996 - memory_profile6_log - INFO -    212   1214.3 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-29 14:28:22,996 - memory_profile6_log - INFO -    213   1214.3 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-29 14:28:22,996 - memory_profile6_log - INFO -    214   1214.3 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-29 14:28:22,996 - memory_profile6_log - INFO -    215   1191.0 MiB    -23.3 MiB       del model_fit

2018-04-29 14:28:22,997 - memory_profile6_log - INFO -    216   1191.0 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-29 14:28:22,997 - memory_profile6_log - INFO -    217   1191.0 MiB      0.0 MiB       del result

2018-04-29 14:28:23,000 - memory_profile6_log - INFO -    218   1191.0 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-29 14:28:23,003 - memory_profile6_log - INFO -    219                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 14:28:23,005 - memory_profile6_log - INFO -    220                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 14:28:23,006 - memory_profile6_log - INFO -    221                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 14:28:23,007 - memory_profile6_log - INFO -    222   1191.0 MiB      0.0 MiB       if savetrain:

2018-04-29 14:28:23,009 - memory_profile6_log - INFO -    223   1196.4 MiB      5.4 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-29 14:28:23,010 - memory_profile6_log - INFO -    224   1196.4 MiB      0.0 MiB           del model_transform

2018-04-29 14:28:23,010 - memory_profile6_log - INFO -    225   1196.4 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-29 14:28:23,010 - memory_profile6_log - INFO -    226   1196.4 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 14:28:23,012 - memory_profile6_log - INFO -    227                             

2018-04-29 14:28:23,015 - memory_profile6_log - INFO -    228   1196.4 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-29 14:28:23,016 - memory_profile6_log - INFO -    229                                     # ~ Place your code to save the training model here ~

2018-04-29 14:28:23,017 - memory_profile6_log - INFO -    230   1196.4 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-29 14:28:23,019 - memory_profile6_log - INFO -    231   1196.4 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-29 14:28:23,019 - memory_profile6_log - INFO -    232   1196.4 MiB      0.0 MiB               if multproc:

2018-04-29 14:28:23,020 - memory_profile6_log - INFO -    233   1176.9 MiB    -19.6 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-29 14:28:23,020 - memory_profile6_log - INFO -    234                             

2018-04-29 14:28:23,022 - memory_profile6_log - INFO -    235                                             # ~ save fitted models ~

2018-04-29 14:28:23,023 - memory_profile6_log - INFO -    236   1176.9 MiB      0.0 MiB                   logger.info("Saving fitted_models as history...")

2018-04-29 14:28:23,023 - memory_profile6_log - INFO -    237   1177.1 MiB      0.2 MiB                   save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 14:28:23,029 - memory_profile6_log - INFO -    238   1192.3 MiB     15.2 MiB                   fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-29 14:28:23,029 - memory_profile6_log - INFO -    239                             

2018-04-29 14:28:23,030 - memory_profile6_log - INFO -    240   1203.1 MiB     10.9 MiB                   X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-29 14:28:23,032 - memory_profile6_log - INFO -    241   1203.1 MiB      0.0 MiB                   logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-29 14:28:23,035 - memory_profile6_log - INFO -    242   1205.1 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 14:28:23,036 - memory_profile6_log - INFO -    243   1205.1 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 14:28:23,036 - memory_profile6_log - INFO -    244   1205.1 MiB      1.9 MiB                       mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-29 14:28:23,036 - memory_profile6_log - INFO -    245                             

2018-04-29 14:28:23,036 - memory_profile6_log - INFO -    246   1205.1 MiB      0.0 MiB                   del X_split

2018-04-29 14:28:23,036 - memory_profile6_log - INFO -    247   1205.1 MiB      0.0 MiB                   del BR

2018-04-29 14:28:23,036 - memory_profile6_log - INFO -    248   1205.1 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-29 14:28:23,036 - memory_profile6_log - INFO -    249                             

2018-04-29 14:28:23,036 - memory_profile6_log - INFO -    250   1205.1 MiB      0.0 MiB                   del save_sigma_nt

2018-04-29 14:28:23,036 - memory_profile6_log - INFO -    251   1205.1 MiB      0.0 MiB                   logger.info("deleting save_sigma_nt...")

2018-04-29 14:28:23,036 - memory_profile6_log - INFO -    252                                             

2018-04-29 14:28:23,042 - memory_profile6_log - INFO -    253                                     elif str(saveto).lower() == "elastic":

2018-04-29 14:28:23,046 - memory_profile6_log - INFO -    254                                         logger.info("Using ElasticSearch as storage...")

2018-04-29 14:28:23,046 - memory_profile6_log - INFO -    255                                         mh.saveElasticS(model_transformsv)

2018-04-29 14:28:23,046 - memory_profile6_log - INFO -    256                             

2018-04-29 14:28:23,048 - memory_profile6_log - INFO -    257                                     # need save sigma_nt for daily train

2018-04-29 14:28:23,049 - memory_profile6_log - INFO -    258                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-29 14:28:23,049 - memory_profile6_log - INFO -    259                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-29 14:28:23,052 - memory_profile6_log - INFO -    260   1205.1 MiB      0.0 MiB           if start_date and end_date:

2018-04-29 14:28:23,053 - memory_profile6_log - INFO -    261                                         if not fitby_sigmant:

2018-04-29 14:28:23,053 - memory_profile6_log - INFO -    262                                             logging.info("Saving sigma Nt...")

2018-04-29 14:28:23,055 - memory_profile6_log - INFO -    263                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 14:28:23,056 - memory_profile6_log - INFO -    264                                             save_sigma_nt['start_date'] = start_date

2018-04-29 14:28:23,056 - memory_profile6_log - INFO -    265                                             save_sigma_nt['end_date'] = end_date

2018-04-29 14:28:23,059 - memory_profile6_log - INFO -    266                                             print save_sigma_nt.head(5)

2018-04-29 14:28:23,061 - memory_profile6_log - INFO -    267                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-29 14:28:23,061 - memory_profile6_log - INFO -    268                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-29 14:28:23,062 - memory_profile6_log - INFO -    269   1205.1 MiB      0.0 MiB       return

2018-04-29 14:28:23,062 - memory_profile6_log - INFO - 


2018-04-29 14:28:23,065 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-29 14:33:34,835 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 14:33:34,839 - memory_profile6_log - INFO - date_generated: 
2018-04-29 14:33:34,839 - memory_profile6_log - INFO -  
2018-04-29 14:33:34,841 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 9, 0, 0)]
2018-04-29 14:33:34,841 - memory_profile6_log - INFO - 

2018-04-29 14:33:34,841 - memory_profile6_log - INFO - using current date: 2018-04-10
2018-04-29 14:33:34,842 - memory_profile6_log - INFO - using start date: 2018-04-09 00:00:00
2018-04-29 14:33:34,842 - memory_profile6_log - INFO - using end date: 2018-04-09
2018-04-29 14:33:34,967 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 14:33:34,971 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-09
2018-04-29 14:35:28,645 - memory_profile6_log - INFO - size of df: 102.25 MB
2018-04-29 14:35:28,647 - memory_profile6_log - INFO - getting total: 403532 training data(genuine interest) for date: 2018-04-09
2018-04-29 14:35:28,707 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 14:35:28,707 - memory_profile6_log - INFO - Appending history data...
2018-04-29 14:35:28,709 - memory_profile6_log - INFO - processing batch-0
2018-04-29 14:35:28,710 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:35:28,815 - memory_profile6_log - INFO - call history data...
2018-04-29 14:36:39,262 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:36:41,038 - memory_profile6_log - INFO - processing batch-1
2018-04-29 14:36:41,039 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:36:41,117 - memory_profile6_log - INFO - call history data...
2018-04-29 14:37:49,342 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:37:50,984 - memory_profile6_log - INFO - processing batch-2
2018-04-29 14:37:50,986 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:37:51,061 - memory_profile6_log - INFO - call history data...
2018-04-29 14:40:24,117 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 14:40:24,119 - memory_profile6_log - INFO - date_generated: 
2018-04-29 14:40:24,121 - memory_profile6_log - INFO -  
2018-04-29 14:40:24,121 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 9, 0, 0)]
2018-04-29 14:40:24,121 - memory_profile6_log - INFO - 

2018-04-29 14:40:24,121 - memory_profile6_log - INFO - using current date: 2018-04-10
2018-04-29 14:40:24,121 - memory_profile6_log - INFO - using start date: 2018-04-09 00:00:00
2018-04-29 14:40:24,121 - memory_profile6_log - INFO - using end date: 2018-04-09
2018-04-29 14:40:24,256 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 14:40:24,259 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-09
2018-04-29 14:42:17,904 - memory_profile6_log - INFO - size of df: 102.25 MB
2018-04-29 14:42:17,905 - memory_profile6_log - INFO - getting total: 403532 training data(genuine interest) for date: 2018-04-09
2018-04-29 14:42:17,967 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 14:42:17,967 - memory_profile6_log - INFO - Appending history data...
2018-04-29 14:42:17,969 - memory_profile6_log - INFO - processing batch-0
2018-04-29 14:42:17,970 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:42:18,075 - memory_profile6_log - INFO - call history data...
2018-04-29 14:43:27,187 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:43:28,825 - memory_profile6_log - INFO - processing batch-1
2018-04-29 14:43:28,826 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:43:28,900 - memory_profile6_log - INFO - call history data...
2018-04-29 14:44:36,298 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:44:37,921 - memory_profile6_log - INFO - processing batch-2
2018-04-29 14:44:37,923 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:44:37,999 - memory_profile6_log - INFO - call history data...
2018-04-29 14:45:53,348 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:45:55,006 - memory_profile6_log - INFO - processing batch-3
2018-04-29 14:45:55,006 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:45:55,085 - memory_profile6_log - INFO - call history data...
2018-04-29 14:47:08,486 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:47:10,188 - memory_profile6_log - INFO - processing batch-4
2018-04-29 14:47:10,190 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:47:10,259 - memory_profile6_log - INFO - call history data...
2018-04-29 14:48:27,240 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:48:28,890 - memory_profile6_log - INFO - Appending training data...
2018-04-29 14:48:28,891 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 14:48:28,892 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 14:48:28,905 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 14:48:28,907 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 14:48:28,908 - memory_profile6_log - INFO - ================================================

2018-04-29 14:48:28,913 - memory_profile6_log - INFO -    295     86.8 MiB     86.8 MiB   @profile

2018-04-29 14:48:28,914 - memory_profile6_log - INFO -    296                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 14:48:28,915 - memory_profile6_log - INFO -    297     86.8 MiB      0.0 MiB       bq_client = client

2018-04-29 14:48:28,917 - memory_profile6_log - INFO -    298     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 14:48:28,917 - memory_profile6_log - INFO -    299                             

2018-04-29 14:48:28,918 - memory_profile6_log - INFO -    300     86.8 MiB      0.0 MiB       datalist = []

2018-04-29 14:48:28,920 - memory_profile6_log - INFO -    301     86.8 MiB      0.0 MiB       datalist_hist = []

2018-04-29 14:48:28,921 - memory_profile6_log - INFO -    302                             

2018-04-29 14:48:28,921 - memory_profile6_log - INFO -    303     86.8 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 14:48:28,924 - memory_profile6_log - INFO -    304    654.5 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 14:48:28,924 - memory_profile6_log - INFO -    305    393.2 MiB    306.4 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 14:48:28,927 - memory_profile6_log - INFO -    306    393.2 MiB      0.0 MiB           if tframe is not None:

2018-04-29 14:48:28,930 - memory_profile6_log - INFO -    307    393.2 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 14:48:28,931 - memory_profile6_log - INFO -    308    406.3 MiB     13.1 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 14:48:28,933 - memory_profile6_log - INFO -    309    406.3 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 14:48:28,936 - memory_profile6_log - INFO -    310    406.3 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 14:48:28,937 - memory_profile6_log - INFO -    311    654.5 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 14:48:28,937 - memory_profile6_log - INFO -    312                                                 # ~ loading history

2018-04-29 14:48:28,938 - memory_profile6_log - INFO -    313                                                 """

2018-04-29 14:48:28,940 - memory_profile6_log - INFO -    314                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 14:48:28,940 - memory_profile6_log - INFO -    315                                                 """

2018-04-29 14:48:28,943 - memory_profile6_log - INFO -    316    630.0 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 14:48:28,944 - memory_profile6_log - INFO -    317                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 14:48:28,946 - memory_profile6_log - INFO -    318    630.0 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 14:48:28,947 - memory_profile6_log - INFO -    319                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 14:48:28,947 - memory_profile6_log - INFO -    320    631.4 MiB      8.1 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 14:48:28,948 - memory_profile6_log - INFO -    321                             

2018-04-29 14:48:28,950 - memory_profile6_log - INFO -    322    631.4 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 14:48:28,950 - memory_profile6_log - INFO -    323    706.2 MiB    536.0 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 14:48:28,950 - memory_profile6_log - INFO -    324                             

2018-04-29 14:48:28,953 - memory_profile6_log - INFO -    325                                                 # me = os.getpid()

2018-04-29 14:48:28,953 - memory_profile6_log - INFO -    326                                                 # kill_proc_tree(me)

2018-04-29 14:48:28,956 - memory_profile6_log - INFO -    327                             

2018-04-29 14:48:28,956 - memory_profile6_log - INFO -    328    706.2 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 14:48:28,957 - memory_profile6_log - INFO -    329    708.4 MiB     -1.0 MiB                       for m in h_frame:

2018-04-29 14:48:28,959 - memory_profile6_log - INFO -    330    708.4 MiB     -1.0 MiB                           if m is not None:

2018-04-29 14:48:28,960 - memory_profile6_log - INFO -    331    708.4 MiB     -1.0 MiB                               if len(m) > 0:

2018-04-29 14:48:28,964 - memory_profile6_log - INFO -    332    708.4 MiB      9.6 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 14:48:28,964 - memory_profile6_log - INFO -    333    655.1 MiB   -291.4 MiB                       del h_frame

2018-04-29 14:48:28,967 - memory_profile6_log - INFO -    334    654.5 MiB    -15.0 MiB                       del lhistory

2018-04-29 14:48:28,967 - memory_profile6_log - INFO -    335                             

2018-04-29 14:48:28,969 - memory_profile6_log - INFO -    336    654.5 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 14:48:28,970 - memory_profile6_log - INFO -    337    654.5 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 14:48:28,970 - memory_profile6_log - INFO -    338                                     else: 

2018-04-29 14:48:28,970 - memory_profile6_log - INFO -    339                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 14:48:28,971 - memory_profile6_log - INFO -    340    654.5 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 14:48:28,971 - memory_profile6_log - INFO -    341    654.5 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 14:48:28,979 - memory_profile6_log - INFO -    342                             

2018-04-29 14:48:28,980 - memory_profile6_log - INFO -    343    654.5 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 14:48:28,982 - memory_profile6_log - INFO - 


2018-04-29 14:48:30,170 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 14:48:30,242 - memory_profile6_log - INFO -     p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0    0.001543         376.207606       303             386.207606  10960288  1610d37360f208-09f8dc8d4f6437-4323461-100200-1...
1    0.000646       18443.259669        47           18453.259669  10960288  161ac1f818c291-0dd09ed3def2f7-3b60490d-100200-...
2    0.001543         115.330640       143             125.330640  10960288  16215b1fa84f4-0244194f6a3c3f-c343567-e1000-162...
3    0.001543         276.015458       675             286.015458  10960288  16281b8933da15-0391070f0a64e6-b34356b-1fa400-1...
4    0.000088        3435.617685      2635            3445.617685  11911567  16116a35abf7b-02ebd04ab0ab75-4323461-100200-16...
5    0.001543        3007.139656       234            3017.139656  10960288  161e4fe8217546-0d3d07a15ce9da-b353461-100200-1...
6    0.000088        6740.892857        15            6750.892857  11911567  162294766a319c-0d16b05bb32617-18741b1f-38400-1...
7    0.001543         445.737339       185             455.737339  10960288  16137e7988e400-0e4f9d96053ce-2c5f3268-4a640-16...
8    0.001543         381.996451       331             391.996451  10960288  16130047ada52-0ada44117-14d422a-29b80-16130047...
9    0.001543         105.812247       762             115.812247  10960288  1612d18da84ab-04e53015f12048-770c003f-38400-16...
10   0.001543        1148.403873      2879            1158.403873  10960288  1610ca10ab6151-09e6699899a0ec-4323461-100200-1...
11   0.004470         382.074899        32             392.074899  22291119  1626b3c178312f-08f9ac741a7b3f-71261e1a-38400-1...
12   0.001543         451.839870        99             461.839870  10960288  16113e91df674a-0aa88425265e92-4323461-100200-1...
13   0.001543         882.303128        54             892.303128  10960288  161f466e32911-0b032e180a22a5-444f4d5b-38400-16...
14   0.000088        6169.630751        59            6179.630751  11911567  162a944840211a-008b91166f77268-17347840-c0000-...
15   0.001543       15204.105398       176           15214.105398  10960288  162413b89ac2fc-0d7cbff1c76a7-4323461-100200-16...
16   0.001543         257.539864       417             267.539864  10960288  161b69c3161ab-0448ac078c2e7-3a2a047e-38400-161...
17   0.000088        2414.648188       134            2424.648188  11911567  161fb5419215b-09e7c04c9d1dc-103b0a74-38400-161...
18   0.000646       11065.955801       250           11075.955801  10960288  1610c92e11ec4a-0dcda7bb5f421e-4323461-100200-1...
19   0.001543        3664.951456        33            3674.951456  10960288  16223975899307-07f89d079f2be9-3e3d5f01-100200-...
2018-04-29 14:48:30,243 - memory_profile6_log - INFO - 

2018-04-29 14:48:30,368 - memory_profile6_log - INFO - size of big_frame_hist: 111.10 MB
2018-04-29 14:48:30,474 - memory_profile6_log - INFO - size of big_frame: 102.25 MB
2018-04-29 14:48:30,494 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-04-29 14:54:33,487 - memory_profile6_log - INFO - size of df: 509.09 MB
2018-04-29 14:54:33,490 - memory_profile6_log - INFO - getting total: 2090090 training data(current date interest)
2018-04-29 14:54:34,127 - memory_profile6_log - INFO - size of current_frame: 525.04 MB
2018-04-29 14:54:34,128 - memory_profile6_log - INFO - loading time of: 2493622 total genuine-current interest data ~ take 849.894s
2018-04-29 14:54:34,157 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 14:54:34,157 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 14:54:34,158 - memory_profile6_log - INFO - ================================================

2018-04-29 14:54:34,161 - memory_profile6_log - INFO -    345     86.7 MiB     86.7 MiB   @profile

2018-04-29 14:54:34,161 - memory_profile6_log - INFO -    346                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 14:54:34,163 - memory_profile6_log - INFO -    347     86.8 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 14:54:34,164 - memory_profile6_log - INFO -    348     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 14:54:34,164 - memory_profile6_log - INFO -    349                             

2018-04-29 14:54:34,165 - memory_profile6_log - INFO -    350                                 # ~~~ Begin collecting data ~~~

2018-04-29 14:54:34,167 - memory_profile6_log - INFO -    351     86.8 MiB      0.0 MiB       t0 = time.time()

2018-04-29 14:54:34,168 - memory_profile6_log - INFO -    352    646.5 MiB    559.7 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 14:54:34,170 - memory_profile6_log - INFO -    353    646.5 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 14:54:34,171 - memory_profile6_log - INFO -    354                                     logger.info("Training cannot be empty..")

2018-04-29 14:54:34,171 - memory_profile6_log - INFO -    355                                     return False

2018-04-29 14:54:34,173 - memory_profile6_log - INFO -    356    671.0 MiB     24.6 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 14:54:34,174 - memory_profile6_log - INFO -    357    671.3 MiB      0.3 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 14:54:34,174 - memory_profile6_log - INFO -    358    671.3 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 14:54:34,174 - memory_profile6_log - INFO -    359                             

2018-04-29 14:54:34,174 - memory_profile6_log - INFO -    360    680.8 MiB      9.5 MiB       big_frame = pd.concat(datalist)

2018-04-29 14:54:34,176 - memory_profile6_log - INFO -    361    680.8 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 14:54:34,176 - memory_profile6_log - INFO -    362    671.6 MiB     -9.2 MiB       del datalist

2018-04-29 14:54:34,177 - memory_profile6_log - INFO -    363                             

2018-04-29 14:54:34,177 - memory_profile6_log - INFO -    364    671.6 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 14:54:34,180 - memory_profile6_log - INFO -    365                             

2018-04-29 14:54:34,181 - memory_profile6_log - INFO -    366                                 # ~ get current news interest ~

2018-04-29 14:54:34,184 - memory_profile6_log - INFO -    367    671.6 MiB      0.0 MiB       if not cd:

2018-04-29 14:54:34,184 - memory_profile6_log - INFO -    368    671.6 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-04-29 14:54:34,184 - memory_profile6_log - INFO -    369   1098.3 MiB    426.7 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 14:54:34,186 - memory_profile6_log - INFO -    370   1098.3 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 14:54:34,186 - memory_profile6_log - INFO -    371                                 else:

2018-04-29 14:54:34,187 - memory_profile6_log - INFO -    372                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 14:54:34,187 - memory_profile6_log - INFO -    373                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 14:54:34,187 - memory_profile6_log - INFO -    374                             

2018-04-29 14:54:34,188 - memory_profile6_log - INFO -    375                                     # safe handling of query parameter

2018-04-29 14:54:34,193 - memory_profile6_log - INFO -    376                                     query_params = [

2018-04-29 14:54:34,194 - memory_profile6_log - INFO -    377                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 14:54:34,194 - memory_profile6_log - INFO -    378                                     ]

2018-04-29 14:54:34,194 - memory_profile6_log - INFO -    379                             

2018-04-29 14:54:34,196 - memory_profile6_log - INFO -    380                                     job_config.query_parameters = query_params

2018-04-29 14:54:34,197 - memory_profile6_log - INFO -    381                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 14:54:34,197 - memory_profile6_log - INFO -    382                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 14:54:34,197 - memory_profile6_log - INFO -    383                             

2018-04-29 14:54:34,197 - memory_profile6_log - INFO -    384   1114.2 MiB     16.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 14:54:34,198 - memory_profile6_log - INFO -    385   1114.2 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 14:54:34,198 - memory_profile6_log - INFO -    386   1114.3 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 14:54:34,200 - memory_profile6_log - INFO -    387   1114.3 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 14:54:34,203 - memory_profile6_log - INFO -    388   1114.3 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 14:54:34,203 - memory_profile6_log - INFO -    389   1114.3 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 14:54:34,204 - memory_profile6_log - INFO -    390                             

2018-04-29 14:54:34,204 - memory_profile6_log - INFO -    391   1114.3 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 14:54:34,206 - memory_profile6_log - INFO - 


2018-04-29 14:54:34,210 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 14:54:34,344 - memory_profile6_log - INFO - train on: 403532 total genuine interest data(D(u, t))
2018-04-29 14:54:34,345 - memory_profile6_log - INFO - transform on: 2090090 total current data(D(t))
2018-04-29 14:54:34,346 - memory_profile6_log - INFO - apply on: 403532 total history data(D(t))
2018-04-29 14:54:35,934 - memory_profile6_log - INFO - Len of model_fit: 403532
2018-04-29 14:54:35,936 - memory_profile6_log - INFO - Len of df_dut: 403532
2018-04-29 14:55:37,686 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-29 14:55:37,730 - memory_profile6_log - INFO - Total train time: 63.386s
2018-04-29 14:55:37,732 - memory_profile6_log - INFO - memory left before cleaning: 75.200 percent memory...
2018-04-29 14:55:37,733 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-29 14:55:37,733 - memory_profile6_log - INFO - deleting df_dut...
2018-04-29 14:55:37,734 - memory_profile6_log - INFO - deleting df_dt...
2018-04-29 14:55:37,736 - memory_profile6_log - INFO - deleting df_input...
2018-04-29 14:55:37,819 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-29 14:55:37,821 - memory_profile6_log - INFO - deleting df_current...
2018-04-29 14:55:37,822 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-29 14:55:37,841 - memory_profile6_log - INFO - deleting model_fit...
2018-04-29 14:55:37,842 - memory_profile6_log - INFO - deleting result...
2018-04-29 14:55:37,871 - memory_profile6_log - INFO - deleting model_transform...
2018-04-29 14:55:37,871 - memory_profile6_log - INFO - memory left after cleaning: 74.600 percent memory...
2018-04-29 14:55:37,874 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-29 14:55:37,875 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-29 14:55:38,059 - memory_profile6_log - INFO - Saving fitted_models as history...
2018-04-29 14:55:38,183 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 10
2018-04-29 14:55:38,184 - memory_profile6_log - INFO - processing batch-0
2018-04-29 14:55:38,372 - memory_profile6_log - INFO - processing batch-1
2018-04-29 14:55:38,569 - memory_profile6_log - INFO - processing batch-2
2018-04-29 14:55:38,752 - memory_profile6_log - INFO - processing batch-3
2018-04-29 14:55:38,937 - memory_profile6_log - INFO - processing batch-4
2018-04-29 14:55:39,131 - memory_profile6_log - INFO - processing batch-5
2018-04-29 14:55:39,312 - memory_profile6_log - INFO - processing batch-6
2018-04-29 14:55:39,497 - memory_profile6_log - INFO - processing batch-7
2018-04-29 14:55:39,683 - memory_profile6_log - INFO - processing batch-8
2018-04-29 14:55:39,871 - memory_profile6_log - INFO - processing batch-9
2018-04-29 14:55:40,062 - memory_profile6_log - INFO - deleting BR...
2018-04-29 14:55:40,065 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-04-29 14:55:40,098 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 14:55:40,098 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 14:55:40,098 - memory_profile6_log - INFO - ================================================

2018-04-29 14:55:40,098 - memory_profile6_log - INFO -    113   1104.2 MiB   1104.2 MiB   @profile

2018-04-29 14:55:40,099 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-29 14:55:40,099 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-29 14:55:40,102 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-29 14:55:40,102 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-29 14:55:40,105 - memory_profile6_log - INFO -    118                                      saveto="datastore"):

2018-04-29 14:55:40,105 - memory_profile6_log - INFO -    119                                 """

2018-04-29 14:55:40,107 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-29 14:55:40,108 - memory_profile6_log - INFO -    121                                 """

2018-04-29 14:55:40,108 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-29 14:55:40,108 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-29 14:55:40,109 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-29 14:55:40,111 - memory_profile6_log - INFO -    125   1104.2 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-29 14:55:40,114 - memory_profile6_log - INFO -    126   1116.6 MiB     12.4 MiB       df_dut = df_input.copy(deep=True)

2018-04-29 14:55:40,115 - memory_profile6_log - INFO -    127   1116.6 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 14:55:40,117 - memory_profile6_log - INFO -    128                             

2018-04-29 14:55:40,117 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-29 14:55:40,118 - memory_profile6_log - INFO -    130   1182.4 MiB     65.8 MiB       df_dt = df_current.copy(deep=True)

2018-04-29 14:55:40,118 - memory_profile6_log - INFO -    131   1182.4 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 14:55:40,119 - memory_profile6_log - INFO -    132                             

2018-04-29 14:55:40,119 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-29 14:55:40,121 - memory_profile6_log - INFO -    134   1182.4 MiB      0.0 MiB       t0 = time.time()

2018-04-29 14:55:40,121 - memory_profile6_log - INFO -    135   1182.4 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-29 14:55:40,121 - memory_profile6_log - INFO -    136   1182.4 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-29 14:55:40,130 - memory_profile6_log - INFO -    137   1182.4 MiB      0.0 MiB       logger.info("apply on: %d total history data(D(t))", len(df_hist))

2018-04-29 14:55:40,130 - memory_profile6_log - INFO -    138                             

2018-04-29 14:55:40,131 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-29 14:55:40,131 - memory_profile6_log - INFO -    140   1182.4 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-29 14:55:40,131 - memory_profile6_log - INFO -    141                             

2018-04-29 14:55:40,131 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-29 14:55:40,132 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-29 14:55:40,132 - memory_profile6_log - INFO -    144   1183.9 MiB      1.4 MiB       NB = BR.processX(df_dut)

2018-04-29 14:55:40,134 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-29 14:55:40,134 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-29 14:55:40,134 - memory_profile6_log - INFO -    147   1199.5 MiB     15.6 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-29 14:55:40,135 - memory_profile6_log - INFO -    148                                 """

2018-04-29 14:55:40,137 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-29 14:55:40,137 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-29 14:55:40,138 - memory_profile6_log - INFO -    151                                 """

2018-04-29 14:55:40,138 - memory_profile6_log - INFO -    152   1199.5 MiB      0.0 MiB       fitby_sigmant = True

2018-04-29 14:55:40,141 - memory_profile6_log - INFO -    153   1199.5 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-29 14:55:40,142 - memory_profile6_log - INFO -    154   1199.5 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-29 14:55:40,144 - memory_profile6_log - INFO -    155   1214.9 MiB     15.4 MiB                            'is_general']]

2018-04-29 14:55:40,145 - memory_profile6_log - INFO -    156   1214.9 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-29 14:55:40,147 - memory_profile6_log - INFO -    157   1214.9 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-29 14:55:40,148 - memory_profile6_log - INFO -    158   1255.9 MiB     41.0 MiB                          verbose=False)

2018-04-29 14:55:40,148 - memory_profile6_log - INFO -    159   1255.9 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-29 14:55:40,148 - memory_profile6_log - INFO -    160   1255.9 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-29 14:55:40,148 - memory_profile6_log - INFO -    161                             

2018-04-29 14:55:40,150 - memory_profile6_log - INFO -    162                                 # ~~ and Transform ~~

2018-04-29 14:55:40,150 - memory_profile6_log - INFO -    163                                 #   handling current news interest == current date

2018-04-29 14:55:40,154 - memory_profile6_log - INFO -    164   1255.9 MiB      0.0 MiB       if df_dt.empty:

2018-04-29 14:55:40,155 - memory_profile6_log - INFO -    165                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-29 14:55:40,155 - memory_profile6_log - INFO -    166                                     return None

2018-04-29 14:55:40,157 - memory_profile6_log - INFO -    167   1262.3 MiB      6.4 MiB       NB = BR.processX(df_dt)

2018-04-29 14:55:40,157 - memory_profile6_log - INFO -    168   1360.1 MiB     97.7 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-29 14:55:40,157 - memory_profile6_log - INFO -    169                             

2018-04-29 14:55:40,157 - memory_profile6_log - INFO -    170   1360.1 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-29 14:55:40,157 - memory_profile6_log - INFO -    171   1426.4 MiB     66.3 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-29 14:55:40,157 - memory_profile6_log - INFO -    172   1426.4 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-29 14:55:40,158 - memory_profile6_log - INFO -    173   1426.4 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-29 14:55:40,158 - memory_profile6_log - INFO -    174   1449.7 MiB     23.4 MiB                                                     verbose=False)

2018-04-29 14:55:40,160 - memory_profile6_log - INFO -    175                                 # ~~~ filter is general and specific topic ~~~

2018-04-29 14:55:40,161 - memory_profile6_log - INFO -    176                                 # the idea is just we need to rerank every topic according

2018-04-29 14:55:40,161 - memory_profile6_log - INFO -    177                                 # user_id and and is_general by p0_posterior

2018-04-29 14:55:40,161 - memory_profile6_log - INFO -    178   1449.7 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-29 14:55:40,161 - memory_profile6_log - INFO -    179   1452.8 MiB      3.1 MiB                                     'is_general']].groupby(['topic_id',

2018-04-29 14:55:40,165 - memory_profile6_log - INFO -    180   1449.8 MiB     -3.1 MiB                                                             'is_general']

2018-04-29 14:55:40,165 - memory_profile6_log - INFO -    181                                                                                      ).size().to_frame().reset_index()

2018-04-29 14:55:40,167 - memory_profile6_log - INFO -    182                             

2018-04-29 14:55:40,167 - memory_profile6_log - INFO -    183                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-29 14:55:40,168 - memory_profile6_log - INFO -    184                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-29 14:55:40,168 - memory_profile6_log - INFO -    185   1449.8 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-29 14:55:40,170 - memory_profile6_log - INFO -    186                             

2018-04-29 14:55:40,170 - memory_profile6_log - INFO -    187                                 # ~ start by provide rank for each topic type ~

2018-04-29 14:55:40,171 - memory_profile6_log - INFO -    188   1451.7 MiB      1.9 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-29 14:55:40,171 - memory_profile6_log - INFO -    189   1459.5 MiB      7.8 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-29 14:55:40,171 - memory_profile6_log - INFO -    190                             

2018-04-29 14:55:40,171 - memory_profile6_log - INFO -    191                                 # ~ set threshold to filter output

2018-04-29 14:55:40,171 - memory_profile6_log - INFO -    192   1459.5 MiB      0.0 MiB       if threshold > 0:

2018-04-29 14:55:40,173 - memory_profile6_log - INFO -    193   1459.5 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-29 14:55:40,173 - memory_profile6_log - INFO -    194   1459.5 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-29 14:55:40,173 - memory_profile6_log - INFO -    195   1457.6 MiB     -1.9 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-29 14:55:40,174 - memory_profile6_log - INFO -    196                             

2018-04-29 14:55:40,174 - memory_profile6_log - INFO -    197   1457.6 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 14:55:40,174 - memory_profile6_log - INFO -    198   1457.6 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-29 14:55:40,177 - memory_profile6_log - INFO -    199                             

2018-04-29 14:55:40,178 - memory_profile6_log - INFO -    200   1457.6 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 14:55:40,180 - memory_profile6_log - INFO -    201                             

2018-04-29 14:55:40,180 - memory_profile6_log - INFO -    202   1457.6 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-29 14:55:40,180 - memory_profile6_log - INFO -    203   1457.6 MiB      0.0 MiB       del df_dut

2018-04-29 14:55:40,181 - memory_profile6_log - INFO -    204   1457.6 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-29 14:55:40,181 - memory_profile6_log - INFO -    205   1457.6 MiB      0.0 MiB       del df_dt

2018-04-29 14:55:40,181 - memory_profile6_log - INFO -    206   1457.6 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-29 14:55:40,223 - memory_profile6_log - INFO -    207   1457.6 MiB      0.0 MiB       del df_input

2018-04-29 14:55:40,226 - memory_profile6_log - INFO -    208   1457.6 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-29 14:55:40,232 - memory_profile6_log - INFO -    209   1375.9 MiB    -81.7 MiB       del df_input_X

2018-04-29 14:55:40,232 - memory_profile6_log - INFO -    210   1375.9 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-29 14:55:40,233 - memory_profile6_log - INFO -    211   1375.9 MiB      0.0 MiB       del df_current

2018-04-29 14:55:40,236 - memory_profile6_log - INFO -    212   1375.9 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-29 14:55:40,237 - memory_profile6_log - INFO -    213   1375.9 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-29 14:55:40,237 - memory_profile6_log - INFO -    214   1375.9 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-29 14:55:40,239 - memory_profile6_log - INFO -    215   1338.9 MiB    -37.0 MiB       del model_fit

2018-04-29 14:55:40,240 - memory_profile6_log - INFO -    216   1338.9 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-29 14:55:40,240 - memory_profile6_log - INFO -    217   1338.9 MiB      0.0 MiB       del result

2018-04-29 14:55:40,240 - memory_profile6_log - INFO -    218   1338.9 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-29 14:55:40,240 - memory_profile6_log - INFO -    219                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 14:55:40,240 - memory_profile6_log - INFO -    220                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 14:55:40,240 - memory_profile6_log - INFO -    221                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 14:55:40,242 - memory_profile6_log - INFO -    222   1338.9 MiB      0.0 MiB       if savetrain:

2018-04-29 14:55:40,242 - memory_profile6_log - INFO -    223   1347.4 MiB      8.5 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-29 14:55:40,242 - memory_profile6_log - INFO -    224   1347.4 MiB      0.0 MiB           del model_transform

2018-04-29 14:55:40,243 - memory_profile6_log - INFO -    225   1347.4 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-29 14:55:40,246 - memory_profile6_log - INFO -    226   1347.4 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 14:55:40,246 - memory_profile6_log - INFO -    227                             

2018-04-29 14:55:40,247 - memory_profile6_log - INFO -    228   1347.4 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-29 14:55:40,247 - memory_profile6_log - INFO -    229                                     # ~ Place your code to save the training model here ~

2018-04-29 14:55:40,249 - memory_profile6_log - INFO -    230   1347.4 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-29 14:55:40,249 - memory_profile6_log - INFO -    231   1347.4 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-29 14:55:40,250 - memory_profile6_log - INFO -    232   1347.4 MiB      0.0 MiB               if multproc:

2018-04-29 14:55:40,250 - memory_profile6_log - INFO -    233   1318.4 MiB    -29.0 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-29 14:55:40,250 - memory_profile6_log - INFO -    234                             

2018-04-29 14:55:40,250 - memory_profile6_log - INFO -    235                                             # ~ save fitted models ~

2018-04-29 14:55:40,253 - memory_profile6_log - INFO -    236   1318.4 MiB      0.0 MiB                   logger.info("Saving fitted_models as history...")

2018-04-29 14:55:40,253 - memory_profile6_log - INFO -    237   1318.5 MiB      0.0 MiB                   save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 14:55:40,253 - memory_profile6_log - INFO -    238   1341.0 MiB     22.6 MiB                   fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-29 14:55:40,253 - memory_profile6_log - INFO -    239                             

2018-04-29 14:55:40,257 - memory_profile6_log - INFO -    240   1356.0 MiB     14.9 MiB                   X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-29 14:55:40,257 - memory_profile6_log - INFO -    241   1356.0 MiB      0.0 MiB                   logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-29 14:55:40,259 - memory_profile6_log - INFO -    242   1357.6 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 14:55:40,259 - memory_profile6_log - INFO -    243   1357.6 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 14:55:40,260 - memory_profile6_log - INFO -    244   1357.6 MiB      1.6 MiB                       mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-29 14:55:40,262 - memory_profile6_log - INFO -    245                             

2018-04-29 14:55:40,262 - memory_profile6_log - INFO -    246   1353.8 MiB     -3.7 MiB                   del X_split

2018-04-29 14:55:40,263 - memory_profile6_log - INFO -    247   1353.8 MiB      0.0 MiB                   del BR

2018-04-29 14:55:40,263 - memory_profile6_log - INFO -    248   1353.8 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-29 14:55:40,263 - memory_profile6_log - INFO -    249                             

2018-04-29 14:55:40,263 - memory_profile6_log - INFO -    250   1353.8 MiB      0.0 MiB                   del save_sigma_nt

2018-04-29 14:55:40,263 - memory_profile6_log - INFO -    251   1353.8 MiB      0.0 MiB                   logger.info("deleting save_sigma_nt...")

2018-04-29 14:55:40,265 - memory_profile6_log - INFO -    252                                             

2018-04-29 14:55:40,265 - memory_profile6_log - INFO -    253                                     elif str(saveto).lower() == "elastic":

2018-04-29 14:55:40,267 - memory_profile6_log - INFO -    254                                         logger.info("Using ElasticSearch as storage...")

2018-04-29 14:55:40,269 - memory_profile6_log - INFO -    255                                         mh.saveElasticS(model_transformsv)

2018-04-29 14:55:40,270 - memory_profile6_log - INFO -    256                             

2018-04-29 14:55:40,272 - memory_profile6_log - INFO -    257                                     # need save sigma_nt for daily train

2018-04-29 14:55:40,272 - memory_profile6_log - INFO -    258                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-29 14:55:40,273 - memory_profile6_log - INFO -    259                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-29 14:55:40,273 - memory_profile6_log - INFO -    260   1353.8 MiB      0.0 MiB           if start_date and end_date:

2018-04-29 14:55:40,273 - memory_profile6_log - INFO -    261                                         if not fitby_sigmant:

2018-04-29 14:55:40,273 - memory_profile6_log - INFO -    262                                             logging.info("Saving sigma Nt...")

2018-04-29 14:55:40,273 - memory_profile6_log - INFO -    263                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 14:55:40,275 - memory_profile6_log - INFO -    264                                             save_sigma_nt['start_date'] = start_date

2018-04-29 14:55:40,275 - memory_profile6_log - INFO -    265                                             save_sigma_nt['end_date'] = end_date

2018-04-29 14:55:40,276 - memory_profile6_log - INFO -    266                                             print save_sigma_nt.head(5)

2018-04-29 14:55:40,276 - memory_profile6_log - INFO -    267                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-29 14:55:40,279 - memory_profile6_log - INFO -    268                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-29 14:55:40,280 - memory_profile6_log - INFO -    269   1353.8 MiB      0.0 MiB       return

2018-04-29 14:55:40,282 - memory_profile6_log - INFO - 


2018-04-29 14:55:40,282 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-29 14:57:45,928 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 14:57:45,931 - memory_profile6_log - INFO - date_generated: 
2018-04-29 14:57:45,931 - memory_profile6_log - INFO -  
2018-04-29 14:57:45,933 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 9, 0, 0)]
2018-04-29 14:57:45,933 - memory_profile6_log - INFO - 

2018-04-29 14:57:45,934 - memory_profile6_log - INFO - using current date: 2018-04-10
2018-04-29 14:57:45,934 - memory_profile6_log - INFO - using start date: 2018-04-09 00:00:00
2018-04-29 14:57:45,934 - memory_profile6_log - INFO - using end date: 2018-04-09
2018-04-29 14:57:46,069 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 14:57:46,072 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-09
2018-04-29 14:59:39,032 - memory_profile6_log - INFO - size of df: 102.25 MB
2018-04-29 14:59:39,032 - memory_profile6_log - INFO - getting total: 403532 training data(genuine interest) for date: 2018-04-09
2018-04-29 14:59:39,088 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 14:59:39,088 - memory_profile6_log - INFO - Appending history data...
2018-04-29 14:59:39,091 - memory_profile6_log - INFO - processing batch-0
2018-04-29 14:59:39,092 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:59:39,197 - memory_profile6_log - INFO - call history data...
2018-04-29 15:00:55,723 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 15:00:57,375 - memory_profile6_log - INFO - processing batch-1
2018-04-29 15:00:57,377 - memory_profile6_log - INFO - creating list history data...
2018-04-29 15:00:57,450 - memory_profile6_log - INFO - call history data...
2018-04-29 15:02:13,332 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 15:02:14,957 - memory_profile6_log - INFO - processing batch-2
2018-04-29 15:02:14,959 - memory_profile6_log - INFO - creating list history data...
2018-04-29 15:02:15,042 - memory_profile6_log - INFO - call history data...
2018-04-29 15:03:30,634 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 15:03:32,263 - memory_profile6_log - INFO - processing batch-3
2018-04-29 15:03:32,265 - memory_profile6_log - INFO - creating list history data...
2018-04-29 15:03:32,338 - memory_profile6_log - INFO - call history data...
2018-04-29 15:04:48,194 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 15:04:49,855 - memory_profile6_log - INFO - processing batch-4
2018-04-29 15:04:49,857 - memory_profile6_log - INFO - creating list history data...
2018-04-29 15:04:49,930 - memory_profile6_log - INFO - call history data...
2018-04-29 15:06:11,232 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 15:06:12,865 - memory_profile6_log - INFO - Appending training data...
2018-04-29 15:06:12,868 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 15:06:12,868 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 15:06:12,881 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 15:06:12,882 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 15:06:12,884 - memory_profile6_log - INFO - ================================================

2018-04-29 15:06:12,884 - memory_profile6_log - INFO -    295     86.8 MiB     86.8 MiB   @profile

2018-04-29 15:06:12,888 - memory_profile6_log - INFO -    296                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 15:06:12,888 - memory_profile6_log - INFO -    297     86.8 MiB      0.0 MiB       bq_client = client

2018-04-29 15:06:12,891 - memory_profile6_log - INFO -    298     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 15:06:12,891 - memory_profile6_log - INFO -    299                             

2018-04-29 15:06:12,891 - memory_profile6_log - INFO -    300     86.8 MiB      0.0 MiB       datalist = []

2018-04-29 15:06:12,892 - memory_profile6_log - INFO -    301     86.8 MiB      0.0 MiB       datalist_hist = []

2018-04-29 15:06:12,894 - memory_profile6_log - INFO -    302                             

2018-04-29 15:06:12,894 - memory_profile6_log - INFO -    303     86.8 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 15:06:12,898 - memory_profile6_log - INFO -    304    655.4 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 15:06:12,898 - memory_profile6_log - INFO -    305    392.9 MiB    306.1 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 15:06:12,900 - memory_profile6_log - INFO -    306    392.9 MiB      0.0 MiB           if tframe is not None:

2018-04-29 15:06:12,901 - memory_profile6_log - INFO -    307    392.9 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 15:06:12,901 - memory_profile6_log - INFO -    308    406.1 MiB     13.1 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 15:06:12,903 - memory_profile6_log - INFO -    309    406.1 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 15:06:12,904 - memory_profile6_log - INFO -    310    406.1 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 15:06:12,905 - memory_profile6_log - INFO -    311    655.4 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 15:06:12,907 - memory_profile6_log - INFO -    312                                                 # ~ loading history

2018-04-29 15:06:12,908 - memory_profile6_log - INFO -    313                                                 """

2018-04-29 15:06:12,910 - memory_profile6_log - INFO -    314                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 15:06:12,911 - memory_profile6_log - INFO -    315                                                 """

2018-04-29 15:06:12,913 - memory_profile6_log - INFO -    316    628.7 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 15:06:12,913 - memory_profile6_log - INFO -    317                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 15:06:12,914 - memory_profile6_log - INFO -    318    628.7 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 15:06:12,914 - memory_profile6_log - INFO -    319                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 15:06:12,917 - memory_profile6_log - INFO -    320    629.5 MiB      8.5 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 15:06:12,918 - memory_profile6_log - INFO -    321                             

2018-04-29 15:06:12,920 - memory_profile6_log - INFO -    322    629.5 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 15:06:12,921 - memory_profile6_log - INFO -    323    704.9 MiB    534.5 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 15:06:12,921 - memory_profile6_log - INFO -    324                             

2018-04-29 15:06:12,923 - memory_profile6_log - INFO -    325                                                 # me = os.getpid()

2018-04-29 15:06:12,924 - memory_profile6_log - INFO -    326                                                 # kill_proc_tree(me)

2018-04-29 15:06:12,924 - memory_profile6_log - INFO -    327                             

2018-04-29 15:06:12,926 - memory_profile6_log - INFO -    328    704.9 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 15:06:12,928 - memory_profile6_log - INFO -    329    707.0 MiB     -0.9 MiB                       for m in h_frame:

2018-04-29 15:06:12,930 - memory_profile6_log - INFO -    330    707.0 MiB     -0.9 MiB                           if m is not None:

2018-04-29 15:06:12,930 - memory_profile6_log - INFO -    331    707.0 MiB     -0.9 MiB                               if len(m) > 0:

2018-04-29 15:06:12,931 - memory_profile6_log - INFO -    332    707.0 MiB      9.7 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 15:06:12,933 - memory_profile6_log - INFO -    333    655.4 MiB   -290.5 MiB                       del h_frame

2018-04-29 15:06:12,934 - memory_profile6_log - INFO -    334    655.4 MiB    -13.7 MiB                       del lhistory

2018-04-29 15:06:12,934 - memory_profile6_log - INFO -    335                             

2018-04-29 15:06:12,936 - memory_profile6_log - INFO -    336    655.4 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 15:06:12,937 - memory_profile6_log - INFO -    337    655.4 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 15:06:12,938 - memory_profile6_log - INFO -    338                                     else: 

2018-04-29 15:06:12,940 - memory_profile6_log - INFO -    339                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 15:06:12,940 - memory_profile6_log - INFO -    340    655.4 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 15:06:12,941 - memory_profile6_log - INFO -    341    655.4 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 15:06:12,943 - memory_profile6_log - INFO -    342                             

2018-04-29 15:06:12,943 - memory_profile6_log - INFO -    343    655.4 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 15:06:12,944 - memory_profile6_log - INFO - 


2018-04-29 15:06:14,108 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 15:06:14,207 - memory_profile6_log - INFO -     p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0    0.001543         394.687612       718             404.687612  10960288  1612f7bb03410e-0524566d1e88ff-2a0b185d-38400-1...
1    0.001543         502.346467       311             512.346467  10960288  16275ef9b5ad-0d25ed18f50a1c-3b670e20-4df28-162...
2    0.001543        1610.180397        59            1620.180397  10960288  161f36169fce02-03becb8d4c95b1-b353461-15f900-1...
3    0.000088       13481.785714        51           13491.785714  11911567  161c17f16b2aea-07a42b07ed0344-a35346f-15f900-1...
4    0.004470         104.752541       220             114.752541  22291119  1612de3ae04200-06b613a5de42a28-2c5f3268-2c600-...
5    0.001543        1086.583319      1401            1096.583319  10960288  1616ac78a3e5b5-09b19c5de4820b-4c534c69-100200-...
6    0.004470         182.437807       178             192.437807  22291119  161abf106841e-0a1f11083ce818-6a6d3d24-38400-16...
7    0.001543        7329.902913        88            7339.902913  10960288  162a823de4d3a-0188a0cd2dbc66-556b3f73-100200-1...
8    0.001543         137.700488       173             147.700488  10960288  161a8d02707a-00e15b9bbe51de-7023786e-38400-161...
9    0.001543         757.502989        83             767.502989  10960288  162000bced2108-0b3e765f648e59-5e615b69-38400-1...
10   0.004470          73.211957       167              83.211957  22291119  1620fd2047f140-01c64ab93e299-7126121a-38400-16...
11   0.000088         793.046218        85             803.046218  11911567  1617837a69987-0c6dc2d5fbd368-367b2a70-38400-16...
12   0.001543         291.317353       500             301.317353  10960288  161e597faed5c7-0a6a04984b835d8-1362684a-100200...
13   0.001543        2670.969656       226            2680.969656  10960288               9b58f159-de79-43c4-a658-b6d19f4e0e28
14   0.001543        5530.427277       944            5540.427277  10960288  162188fedc4102f-08353c9a5317cc-b353461-100200-...
15   0.004470         191.037449        64             201.037449  22291119  1613090be5a44b-0c62feb84-2f10403b-13c680-16130...
16   0.000088        2106.529018        32            2116.529018  11911567  16162a4f8de32-00d4e9007f223f-1a520605-38400-16...
17   0.001543          91.623786        60             101.623786  10960288  1612dcfd0171f5-00dd8987eb7be2-5c7f6035-38400-1...
18   0.001543         137.060738       728             147.060738  10960288  1610cafeaae255-08b80070e796c6-4323461-100200-1...
19   0.000088        1053.264509        32            1063.264509  11911567  1622d0156341d8-00514d785e0b658-17357b40-c0000-...
2018-04-29 15:06:14,209 - memory_profile6_log - INFO - 

2018-04-29 15:06:14,335 - memory_profile6_log - INFO - size of big_frame_hist: 111.10 MB
2018-04-29 15:06:14,451 - memory_profile6_log - INFO - size of big_frame: 102.25 MB
2018-04-29 15:06:14,470 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-04-29 15:12:32,046 - memory_profile6_log - INFO - size of df: 509.09 MB
2018-04-29 15:12:32,048 - memory_profile6_log - INFO - getting total: 2090090 training data(current date interest)
2018-04-29 15:12:32,690 - memory_profile6_log - INFO - size of current_frame: 525.04 MB
2018-04-29 15:12:32,693 - memory_profile6_log - INFO - loading time of: 2493622 total genuine-current interest data ~ take 886.646s
2018-04-29 15:12:32,717 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 15:12:32,717 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 15:12:32,719 - memory_profile6_log - INFO - ================================================

2018-04-29 15:12:32,720 - memory_profile6_log - INFO -    345     86.7 MiB     86.7 MiB   @profile

2018-04-29 15:12:32,720 - memory_profile6_log - INFO -    346                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 15:12:32,721 - memory_profile6_log - INFO -    347     86.8 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 15:12:32,721 - memory_profile6_log - INFO -    348     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 15:12:32,723 - memory_profile6_log - INFO -    349                             

2018-04-29 15:12:32,723 - memory_profile6_log - INFO -    350                                 # ~~~ Begin collecting data ~~~

2018-04-29 15:12:32,723 - memory_profile6_log - INFO -    351     86.8 MiB      0.0 MiB       t0 = time.time()

2018-04-29 15:12:32,724 - memory_profile6_log - INFO -    352    648.7 MiB    561.8 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 15:12:32,724 - memory_profile6_log - INFO -    353    648.7 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 15:12:32,726 - memory_profile6_log - INFO -    354                                     logger.info("Training cannot be empty..")

2018-04-29 15:12:32,726 - memory_profile6_log - INFO -    355                                     return False

2018-04-29 15:12:32,730 - memory_profile6_log - INFO -    356    673.1 MiB     24.4 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 15:12:32,730 - memory_profile6_log - INFO -    357    673.2 MiB      0.1 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 15:12:32,730 - memory_profile6_log - INFO -    358    673.2 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 15:12:32,732 - memory_profile6_log - INFO -    359                             

2018-04-29 15:12:32,732 - memory_profile6_log - INFO -    360    682.8 MiB      9.6 MiB       big_frame = pd.concat(datalist)

2018-04-29 15:12:32,733 - memory_profile6_log - INFO -    361    682.8 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 15:12:32,733 - memory_profile6_log - INFO -    362    673.5 MiB     -9.2 MiB       del datalist

2018-04-29 15:12:32,733 - memory_profile6_log - INFO -    363                             

2018-04-29 15:12:32,733 - memory_profile6_log - INFO -    364    673.5 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 15:12:32,734 - memory_profile6_log - INFO -    365                             

2018-04-29 15:12:32,734 - memory_profile6_log - INFO -    366                                 # ~ get current news interest ~

2018-04-29 15:12:32,736 - memory_profile6_log - INFO -    367    673.5 MiB      0.0 MiB       if not cd:

2018-04-29 15:12:32,736 - memory_profile6_log - INFO -    368    673.5 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-04-29 15:12:32,736 - memory_profile6_log - INFO -    369   1098.8 MiB    425.3 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 15:12:32,736 - memory_profile6_log - INFO -    370   1098.8 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 15:12:32,736 - memory_profile6_log - INFO -    371                                 else:

2018-04-29 15:12:32,736 - memory_profile6_log - INFO -    372                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 15:12:32,737 - memory_profile6_log - INFO -    373                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 15:12:32,737 - memory_profile6_log - INFO -    374                             

2018-04-29 15:12:32,739 - memory_profile6_log - INFO -    375                                     # safe handling of query parameter

2018-04-29 15:12:32,743 - memory_profile6_log - INFO -    376                                     query_params = [

2018-04-29 15:12:32,743 - memory_profile6_log - INFO -    377                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 15:12:32,743 - memory_profile6_log - INFO -    378                                     ]

2018-04-29 15:12:32,744 - memory_profile6_log - INFO -    379                             

2018-04-29 15:12:32,744 - memory_profile6_log - INFO -    380                                     job_config.query_parameters = query_params

2018-04-29 15:12:32,746 - memory_profile6_log - INFO -    381                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 15:12:32,746 - memory_profile6_log - INFO -    382                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 15:12:32,746 - memory_profile6_log - INFO -    383                             

2018-04-29 15:12:32,746 - memory_profile6_log - INFO -    384   1114.8 MiB     16.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 15:12:32,746 - memory_profile6_log - INFO -    385   1114.8 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 15:12:32,747 - memory_profile6_log - INFO -    386   1114.8 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 15:12:32,749 - memory_profile6_log - INFO -    387   1114.8 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 15:12:32,750 - memory_profile6_log - INFO -    388   1114.8 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 15:12:32,750 - memory_profile6_log - INFO -    389   1114.8 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 15:12:32,755 - memory_profile6_log - INFO -    390                             

2018-04-29 15:12:32,756 - memory_profile6_log - INFO -    391   1114.8 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 15:12:32,756 - memory_profile6_log - INFO - 


2018-04-29 15:12:32,759 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 15:12:32,905 - memory_profile6_log - INFO - train on: 403532 total genuine interest data(D(u, t))
2018-04-29 15:12:32,907 - memory_profile6_log - INFO - transform on: 2090090 total current data(D(t))
2018-04-29 15:12:32,907 - memory_profile6_log - INFO - apply on: 403532 total history data(D(t))
2018-04-29 15:12:34,657 - memory_profile6_log - INFO - Len of model_fit: 403532
2018-04-29 15:12:34,657 - memory_profile6_log - INFO - Len of df_dut: 403532
2018-04-29 15:13:35,778 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-29 15:13:35,819 - memory_profile6_log - INFO - Total train time: 62.913s
2018-04-29 15:13:35,819 - memory_profile6_log - INFO - memory left before cleaning: 76.700 percent memory...
2018-04-29 15:13:35,822 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-29 15:13:35,822 - memory_profile6_log - INFO - deleting df_dut...
2018-04-29 15:13:35,825 - memory_profile6_log - INFO - deleting df_dt...
2018-04-29 15:13:35,825 - memory_profile6_log - INFO - deleting df_input...
2018-04-29 15:13:35,910 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-29 15:13:35,911 - memory_profile6_log - INFO - deleting df_current...
2018-04-29 15:13:35,913 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-29 15:13:35,933 - memory_profile6_log - INFO - deleting model_fit...
2018-04-29 15:13:35,934 - memory_profile6_log - INFO - deleting result...
2018-04-29 15:13:35,963 - memory_profile6_log - INFO - deleting model_transform...
2018-04-29 15:13:35,966 - memory_profile6_log - INFO - memory left after cleaning: 76.000 percent memory...
2018-04-29 15:13:35,967 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-29 15:13:35,969 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-29 15:13:36,176 - memory_profile6_log - INFO - Saving fitted_models as history...
2018-04-29 15:13:36,292 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 10
2018-04-29 15:13:36,293 - memory_profile6_log - INFO - processing batch-0
2018-04-29 15:13:36,484 - memory_profile6_log - INFO - processing batch-1
2018-04-29 15:13:36,671 - memory_profile6_log - INFO - processing batch-2
2018-04-29 15:13:36,858 - memory_profile6_log - INFO - processing batch-3
2018-04-29 15:13:37,039 - memory_profile6_log - INFO - processing batch-4
2018-04-29 15:13:37,239 - memory_profile6_log - INFO - processing batch-5
2018-04-29 15:13:37,428 - memory_profile6_log - INFO - processing batch-6
2018-04-29 15:13:37,609 - memory_profile6_log - INFO - processing batch-7
2018-04-29 15:13:37,795 - memory_profile6_log - INFO - processing batch-8
2018-04-29 15:13:37,980 - memory_profile6_log - INFO - processing batch-9
2018-04-29 15:13:38,176 - memory_profile6_log - INFO - deleting BR...
2018-04-29 15:13:38,177 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-04-29 15:13:38,193 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 15:13:38,193 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 15:13:38,193 - memory_profile6_log - INFO - ================================================

2018-04-29 15:13:38,194 - memory_profile6_log - INFO -    113   1105.3 MiB   1105.3 MiB   @profile

2018-04-29 15:13:38,194 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-29 15:13:38,194 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-29 15:13:38,194 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-29 15:13:38,196 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-29 15:13:38,196 - memory_profile6_log - INFO -    118                                      saveto="datastore"):

2018-04-29 15:13:38,196 - memory_profile6_log - INFO -    119                                 """

2018-04-29 15:13:38,197 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-29 15:13:38,197 - memory_profile6_log - INFO -    121                                 """

2018-04-29 15:13:38,197 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-29 15:13:38,197 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-29 15:13:38,197 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-29 15:13:38,200 - memory_profile6_log - INFO -    125   1105.3 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-29 15:13:38,200 - memory_profile6_log - INFO -    126   1118.0 MiB     12.7 MiB       df_dut = df_input.copy(deep=True)

2018-04-29 15:13:38,204 - memory_profile6_log - INFO -    127   1118.0 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 15:13:38,204 - memory_profile6_log - INFO -    128                             

2018-04-29 15:13:38,207 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-29 15:13:38,207 - memory_profile6_log - INFO -    130   1183.8 MiB     65.8 MiB       df_dt = df_current.copy(deep=True)

2018-04-29 15:13:38,209 - memory_profile6_log - INFO -    131   1183.8 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 15:13:38,210 - memory_profile6_log - INFO -    132                             

2018-04-29 15:13:38,210 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-29 15:13:38,210 - memory_profile6_log - INFO -    134   1183.8 MiB      0.0 MiB       t0 = time.time()

2018-04-29 15:13:38,210 - memory_profile6_log - INFO -    135   1183.8 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-29 15:13:38,211 - memory_profile6_log - INFO -    136   1183.8 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-29 15:13:38,211 - memory_profile6_log - INFO -    137   1183.8 MiB      0.0 MiB       logger.info("apply on: %d total history data(D(t))", len(df_hist))

2018-04-29 15:13:38,213 - memory_profile6_log - INFO -    138                             

2018-04-29 15:13:38,213 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-29 15:13:38,213 - memory_profile6_log - INFO -    140   1183.8 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-29 15:13:38,217 - memory_profile6_log - INFO -    141                             

2018-04-29 15:13:38,217 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-29 15:13:38,219 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-29 15:13:38,219 - memory_profile6_log - INFO -    144   1186.2 MiB      2.4 MiB       NB = BR.processX(df_dut)

2018-04-29 15:13:38,220 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-29 15:13:38,220 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-29 15:13:38,220 - memory_profile6_log - INFO -    147   1202.1 MiB     16.0 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-29 15:13:38,220 - memory_profile6_log - INFO -    148                                 """

2018-04-29 15:13:38,221 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-29 15:13:38,221 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-29 15:13:38,223 - memory_profile6_log - INFO -    151                                 """

2018-04-29 15:13:38,223 - memory_profile6_log - INFO -    152   1202.1 MiB      0.0 MiB       fitby_sigmant = True

2018-04-29 15:13:38,223 - memory_profile6_log - INFO -    153   1202.1 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-29 15:13:38,223 - memory_profile6_log - INFO -    154   1202.1 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-29 15:13:38,227 - memory_profile6_log - INFO -    155   1217.5 MiB     15.4 MiB                            'is_general']]

2018-04-29 15:13:38,230 - memory_profile6_log - INFO -    156   1217.5 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-29 15:13:38,230 - memory_profile6_log - INFO -    157   1217.5 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-29 15:13:38,230 - memory_profile6_log - INFO -    158   1259.0 MiB     41.5 MiB                          verbose=False)

2018-04-29 15:13:38,232 - memory_profile6_log - INFO -    159   1259.0 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-29 15:13:38,232 - memory_profile6_log - INFO -    160   1259.0 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-29 15:13:38,233 - memory_profile6_log - INFO -    161                             

2018-04-29 15:13:38,233 - memory_profile6_log - INFO -    162                                 # ~~ and Transform ~~

2018-04-29 15:13:38,234 - memory_profile6_log - INFO -    163                                 #   handling current news interest == current date

2018-04-29 15:13:38,234 - memory_profile6_log - INFO -    164   1259.0 MiB      0.0 MiB       if df_dt.empty:

2018-04-29 15:13:38,236 - memory_profile6_log - INFO -    165                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-29 15:13:38,236 - memory_profile6_log - INFO -    166                                     return None

2018-04-29 15:13:38,236 - memory_profile6_log - INFO -    167   1265.3 MiB      6.3 MiB       NB = BR.processX(df_dt)

2018-04-29 15:13:38,239 - memory_profile6_log - INFO -    168   1363.7 MiB     98.5 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-29 15:13:38,240 - memory_profile6_log - INFO -    169                             

2018-04-29 15:13:38,240 - memory_profile6_log - INFO -    170   1363.7 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-29 15:13:38,243 - memory_profile6_log - INFO -    171   1430.1 MiB     66.3 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-29 15:13:38,243 - memory_profile6_log - INFO -    172   1430.1 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-29 15:13:38,243 - memory_profile6_log - INFO -    173   1430.1 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-29 15:13:38,244 - memory_profile6_log - INFO -    174   1454.3 MiB     24.2 MiB                                                     verbose=False)

2018-04-29 15:13:38,244 - memory_profile6_log - INFO -    175                                 # ~~~ filter is general and specific topic ~~~

2018-04-29 15:13:38,246 - memory_profile6_log - INFO -    176                                 # the idea is just we need to rerank every topic according

2018-04-29 15:13:38,246 - memory_profile6_log - INFO -    177                                 # user_id and and is_general by p0_posterior

2018-04-29 15:13:38,246 - memory_profile6_log - INFO -    178   1454.3 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-29 15:13:38,246 - memory_profile6_log - INFO -    179   1457.4 MiB      3.1 MiB                                     'is_general']].groupby(['topic_id',

2018-04-29 15:13:38,246 - memory_profile6_log - INFO -    180   1454.3 MiB     -3.1 MiB                                                             'is_general']

2018-04-29 15:13:38,250 - memory_profile6_log - INFO -    181                                                                                      ).size().to_frame().reset_index()

2018-04-29 15:13:38,253 - memory_profile6_log - INFO -    182                             

2018-04-29 15:13:38,253 - memory_profile6_log - INFO -    183                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-29 15:13:38,253 - memory_profile6_log - INFO -    184                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-29 15:13:38,255 - memory_profile6_log - INFO -    185   1454.4 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-29 15:13:38,255 - memory_profile6_log - INFO -    186                             

2018-04-29 15:13:38,256 - memory_profile6_log - INFO -    187                                 # ~ start by provide rank for each topic type ~

2018-04-29 15:13:38,256 - memory_profile6_log - INFO -    188   1450.3 MiB     -4.1 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-29 15:13:38,257 - memory_profile6_log - INFO -    189   1456.8 MiB      6.5 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-29 15:13:38,260 - memory_profile6_log - INFO -    190                             

2018-04-29 15:13:38,262 - memory_profile6_log - INFO -    191                                 # ~ set threshold to filter output

2018-04-29 15:13:38,263 - memory_profile6_log - INFO -    192   1456.8 MiB      0.0 MiB       if threshold > 0:

2018-04-29 15:13:38,263 - memory_profile6_log - INFO -    193   1456.8 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-29 15:13:38,263 - memory_profile6_log - INFO -    194   1456.8 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-29 15:13:38,263 - memory_profile6_log - INFO -    195   1454.7 MiB     -2.1 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-29 15:13:38,263 - memory_profile6_log - INFO -    196                             

2018-04-29 15:13:38,265 - memory_profile6_log - INFO -    197   1454.7 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 15:13:38,265 - memory_profile6_log - INFO -    198   1454.7 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-29 15:13:38,266 - memory_profile6_log - INFO -    199                             

2018-04-29 15:13:38,267 - memory_profile6_log - INFO -    200   1454.7 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 15:13:38,267 - memory_profile6_log - INFO -    201                             

2018-04-29 15:13:38,269 - memory_profile6_log - INFO -    202   1454.7 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-29 15:13:38,269 - memory_profile6_log - INFO -    203   1454.7 MiB      0.0 MiB       del df_dut

2018-04-29 15:13:38,269 - memory_profile6_log - INFO -    204   1454.7 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-29 15:13:38,269 - memory_profile6_log - INFO -    205   1454.7 MiB      0.0 MiB       del df_dt

2018-04-29 15:13:38,273 - memory_profile6_log - INFO -    206   1454.7 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-29 15:13:38,273 - memory_profile6_log - INFO -    207   1454.7 MiB      0.0 MiB       del df_input

2018-04-29 15:13:38,275 - memory_profile6_log - INFO -    208   1454.7 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-29 15:13:38,276 - memory_profile6_log - INFO -    209   1373.0 MiB    -81.7 MiB       del df_input_X

2018-04-29 15:13:38,276 - memory_profile6_log - INFO -    210   1373.0 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-29 15:13:38,276 - memory_profile6_log - INFO -    211   1373.0 MiB      0.0 MiB       del df_current

2018-04-29 15:13:38,278 - memory_profile6_log - INFO -    212   1373.0 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-29 15:13:38,278 - memory_profile6_log - INFO -    213   1373.0 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-29 15:13:38,279 - memory_profile6_log - INFO -    214   1373.0 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-29 15:13:38,279 - memory_profile6_log - INFO -    215   1336.0 MiB    -37.0 MiB       del model_fit

2018-04-29 15:13:38,279 - memory_profile6_log - INFO -    216   1336.0 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-29 15:13:38,279 - memory_profile6_log - INFO -    217   1336.0 MiB      0.0 MiB       del result

2018-04-29 15:13:38,279 - memory_profile6_log - INFO -    218   1336.0 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-29 15:13:38,279 - memory_profile6_log - INFO -    219                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 15:13:38,280 - memory_profile6_log - INFO -    220                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 15:13:38,280 - memory_profile6_log - INFO -    221                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 15:13:38,282 - memory_profile6_log - INFO -    222   1336.0 MiB      0.0 MiB       if savetrain:

2018-04-29 15:13:38,286 - memory_profile6_log - INFO -    223   1344.5 MiB      8.5 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-29 15:13:38,286 - memory_profile6_log - INFO -    224   1344.5 MiB      0.0 MiB           del model_transform

2018-04-29 15:13:38,288 - memory_profile6_log - INFO -    225   1344.5 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-29 15:13:38,288 - memory_profile6_log - INFO -    226   1344.5 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 15:13:38,289 - memory_profile6_log - INFO -    227                             

2018-04-29 15:13:38,289 - memory_profile6_log - INFO -    228   1344.5 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-29 15:13:38,289 - memory_profile6_log - INFO -    229                                     # ~ Place your code to save the training model here ~

2018-04-29 15:13:38,290 - memory_profile6_log - INFO -    230   1344.5 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-29 15:13:38,292 - memory_profile6_log - INFO -    231   1344.5 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-29 15:13:38,292 - memory_profile6_log - INFO -    232   1344.5 MiB      0.0 MiB               if multproc:

2018-04-29 15:13:38,296 - memory_profile6_log - INFO -    233   1315.3 MiB    -29.2 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-29 15:13:38,296 - memory_profile6_log - INFO -    234                             

2018-04-29 15:13:38,299 - memory_profile6_log - INFO -    235                                             # ~ save fitted models ~

2018-04-29 15:13:38,299 - memory_profile6_log - INFO -    236   1315.3 MiB      0.0 MiB                   logger.info("Saving fitted_models as history...")

2018-04-29 15:13:38,301 - memory_profile6_log - INFO -    237   1315.4 MiB      0.0 MiB                   save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 15:13:38,302 - memory_profile6_log - INFO -    238   1338.4 MiB     23.1 MiB                   fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-29 15:13:38,302 - memory_profile6_log - INFO -    239                             

2018-04-29 15:13:38,302 - memory_profile6_log - INFO -    240   1351.9 MiB     13.5 MiB                   X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-29 15:13:38,302 - memory_profile6_log - INFO -    241   1351.9 MiB      0.0 MiB                   logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-29 15:13:38,302 - memory_profile6_log - INFO -    242   1353.2 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 15:13:38,303 - memory_profile6_log - INFO -    243   1353.2 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 15:13:38,306 - memory_profile6_log - INFO -    244   1353.2 MiB      1.2 MiB                       mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-29 15:13:38,308 - memory_profile6_log - INFO -    245                             

2018-04-29 15:13:38,309 - memory_profile6_log - INFO -    246   1349.5 MiB     -3.7 MiB                   del X_split

2018-04-29 15:13:38,309 - memory_profile6_log - INFO -    247   1349.5 MiB      0.0 MiB                   del BR

2018-04-29 15:13:38,311 - memory_profile6_log - INFO -    248   1349.5 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-29 15:13:38,311 - memory_profile6_log - INFO -    249                             

2018-04-29 15:13:38,312 - memory_profile6_log - INFO -    250   1349.5 MiB      0.0 MiB                   del save_sigma_nt

2018-04-29 15:13:38,312 - memory_profile6_log - INFO -    251   1349.5 MiB      0.0 MiB                   logger.info("deleting save_sigma_nt...")

2018-04-29 15:13:38,312 - memory_profile6_log - INFO -    252                                             

2018-04-29 15:13:38,312 - memory_profile6_log - INFO -    253                                     elif str(saveto).lower() == "elastic":

2018-04-29 15:13:38,312 - memory_profile6_log - INFO -    254                                         logger.info("Using ElasticSearch as storage...")

2018-04-29 15:13:38,312 - memory_profile6_log - INFO -    255                                         mh.saveElasticS(model_transformsv)

2018-04-29 15:13:38,313 - memory_profile6_log - INFO -    256                             

2018-04-29 15:13:38,313 - memory_profile6_log - INFO -    257                                     # need save sigma_nt for daily train

2018-04-29 15:13:38,313 - memory_profile6_log - INFO -    258                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-29 15:13:38,315 - memory_profile6_log - INFO -    259                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-29 15:13:38,315 - memory_profile6_log - INFO -    260   1349.5 MiB      0.0 MiB           if start_date and end_date:

2018-04-29 15:13:38,319 - memory_profile6_log - INFO -    261                                         if not fitby_sigmant:

2018-04-29 15:13:38,321 - memory_profile6_log - INFO -    262                                             logging.info("Saving sigma Nt...")

2018-04-29 15:13:38,322 - memory_profile6_log - INFO -    263                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 15:13:38,323 - memory_profile6_log - INFO -    264                                             save_sigma_nt['start_date'] = start_date

2018-04-29 15:13:38,325 - memory_profile6_log - INFO -    265                                             save_sigma_nt['end_date'] = end_date

2018-04-29 15:13:38,326 - memory_profile6_log - INFO -    266                                             print save_sigma_nt.head(5)

2018-04-29 15:13:38,326 - memory_profile6_log - INFO -    267                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-29 15:13:38,328 - memory_profile6_log - INFO -    268                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-29 15:13:38,328 - memory_profile6_log - INFO -    269   1349.5 MiB      0.0 MiB       return

2018-04-29 15:13:38,331 - memory_profile6_log - INFO - 


2018-04-29 15:13:38,331 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-29 15:55:57,732 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 15:55:57,733 - memory_profile6_log - INFO - date_generated: 
2018-04-29 15:55:57,734 - memory_profile6_log - INFO -  
2018-04-29 15:55:57,734 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 15:55:57,736 - memory_profile6_log - INFO - 

2018-04-29 15:55:57,736 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 15:55:57,736 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 15:55:57,736 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 15:55:57,857 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 15:55:57,859 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 15:56:34,727 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 15:56:34,732 - memory_profile6_log - INFO - date_generated: 
2018-04-29 15:56:34,732 - memory_profile6_log - INFO -  
2018-04-29 15:56:34,732 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 15:56:34,733 - memory_profile6_log - INFO - 

2018-04-29 15:56:34,733 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 15:56:34,733 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 15:56:34,733 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 15:56:34,854 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 15:56:34,857 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 15:57:40,752 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 15:57:40,755 - memory_profile6_log - INFO - date_generated: 
2018-04-29 15:57:40,755 - memory_profile6_log - INFO -  
2018-04-29 15:57:40,756 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 15:57:40,756 - memory_profile6_log - INFO - 

2018-04-29 15:57:40,756 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 15:57:40,756 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 15:57:40,757 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 15:57:40,881 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 15:57:40,884 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 15:58:48,711 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 15:58:48,713 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 15:58:48,760 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 15:58:48,762 - memory_profile6_log - INFO - Appending history data...
2018-04-29 15:58:48,763 - memory_profile6_log - INFO - processing batch-0
2018-04-29 15:58:48,765 - memory_profile6_log - INFO - creating list history data...
2018-04-29 15:58:48,844 - memory_profile6_log - INFO - call history data...
2018-04-29 15:59:45,088 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 15:59:46,328 - memory_profile6_log - INFO - processing batch-1
2018-04-29 15:59:46,328 - memory_profile6_log - INFO - creating list history data...
2018-04-29 15:59:46,378 - memory_profile6_log - INFO - call history data...
2018-04-29 16:00:40,875 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:00:42,095 - memory_profile6_log - INFO - processing batch-2
2018-04-29 16:00:42,096 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:00:42,151 - memory_profile6_log - INFO - call history data...
2018-04-29 16:01:39,690 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:01:40,913 - memory_profile6_log - INFO - processing batch-3
2018-04-29 16:01:40,914 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:01:40,966 - memory_profile6_log - INFO - call history data...
2018-04-29 16:02:36,318 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:02:37,552 - memory_profile6_log - INFO - processing batch-4
2018-04-29 16:02:37,555 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:02:37,605 - memory_profile6_log - INFO - call history data...
2018-04-29 16:03:32,013 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:03:33,252 - memory_profile6_log - INFO - Appending training data...
2018-04-29 16:03:33,253 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 16:03:33,255 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 16:03:33,263 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 16:03:33,263 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 16:03:33,266 - memory_profile6_log - INFO - ================================================

2018-04-29 16:03:33,266 - memory_profile6_log - INFO -    298     86.9 MiB     86.9 MiB   @profile

2018-04-29 16:03:33,267 - memory_profile6_log - INFO -    299                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 16:03:33,269 - memory_profile6_log - INFO -    300     86.9 MiB      0.0 MiB       bq_client = client

2018-04-29 16:03:33,269 - memory_profile6_log - INFO -    301     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 16:03:33,270 - memory_profile6_log - INFO -    302                             

2018-04-29 16:03:33,273 - memory_profile6_log - INFO -    303     86.9 MiB      0.0 MiB       datalist = []

2018-04-29 16:03:33,273 - memory_profile6_log - INFO -    304     86.9 MiB      0.0 MiB       datalist_hist = []

2018-04-29 16:03:33,276 - memory_profile6_log - INFO -    305                             

2018-04-29 16:03:33,276 - memory_profile6_log - INFO -    306     86.9 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 16:03:33,278 - memory_profile6_log - INFO -    307    445.9 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 16:03:33,279 - memory_profile6_log - INFO -    308    339.6 MiB    252.6 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 16:03:33,280 - memory_profile6_log - INFO -    309    339.6 MiB      0.0 MiB           if tframe is not None:

2018-04-29 16:03:33,282 - memory_profile6_log - INFO -    310    339.6 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 16:03:33,283 - memory_profile6_log - INFO -    311    347.1 MiB      7.5 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 16:03:33,285 - memory_profile6_log - INFO -    312    347.1 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 16:03:33,286 - memory_profile6_log - INFO -    313    347.1 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 16:03:33,286 - memory_profile6_log - INFO -    314    445.9 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 16:03:33,289 - memory_profile6_log - INFO -    315                                                 # ~ loading history

2018-04-29 16:03:33,289 - memory_profile6_log - INFO -    316                                                 """

2018-04-29 16:03:33,292 - memory_profile6_log - INFO -    317                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 16:03:33,292 - memory_profile6_log - INFO -    318                                                 """

2018-04-29 16:03:33,295 - memory_profile6_log - INFO -    319    430.2 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 16:03:33,296 - memory_profile6_log - INFO -    320                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 16:03:33,298 - memory_profile6_log - INFO -    321    430.2 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 16:03:33,299 - memory_profile6_log - INFO -    322                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 16:03:33,299 - memory_profile6_log - INFO -    323    430.8 MiB      4.0 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 16:03:33,301 - memory_profile6_log - INFO -    324                             

2018-04-29 16:03:33,302 - memory_profile6_log - INFO -    325    430.8 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 16:03:33,302 - memory_profile6_log - INFO -    326    475.4 MiB    251.8 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 16:03:33,305 - memory_profile6_log - INFO -    327                             

2018-04-29 16:03:33,306 - memory_profile6_log - INFO -    328                                                 # me = os.getpid()

2018-04-29 16:03:33,308 - memory_profile6_log - INFO -    329                                                 # kill_proc_tree(me)

2018-04-29 16:03:33,309 - memory_profile6_log - INFO -    330                             

2018-04-29 16:03:33,309 - memory_profile6_log - INFO -    331    475.4 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 16:03:33,309 - memory_profile6_log - INFO -    332    476.4 MiB     -0.9 MiB                       for m in h_frame:

2018-04-29 16:03:33,311 - memory_profile6_log - INFO -    333    476.4 MiB     -0.9 MiB                           if m is not None:

2018-04-29 16:03:33,312 - memory_profile6_log - INFO -    334    476.4 MiB     -0.9 MiB                               if len(m) > 0:

2018-04-29 16:03:33,312 - memory_profile6_log - INFO -    335    476.4 MiB      2.8 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 16:03:33,315 - memory_profile6_log - INFO -    336    445.9 MiB   -160.7 MiB                       del h_frame

2018-04-29 16:03:33,315 - memory_profile6_log - INFO -    337    445.9 MiB      0.0 MiB                       del lhistory

2018-04-29 16:03:33,318 - memory_profile6_log - INFO -    338                             

2018-04-29 16:03:33,319 - memory_profile6_log - INFO -    339    445.9 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 16:03:33,319 - memory_profile6_log - INFO -    340    445.9 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 16:03:33,319 - memory_profile6_log - INFO -    341                                     else: 

2018-04-29 16:03:33,321 - memory_profile6_log - INFO -    342                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 16:03:33,322 - memory_profile6_log - INFO -    343    445.9 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 16:03:33,322 - memory_profile6_log - INFO -    344    445.9 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 16:03:33,325 - memory_profile6_log - INFO -    345                             

2018-04-29 16:03:33,326 - memory_profile6_log - INFO -    346    445.9 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 16:03:33,328 - memory_profile6_log - INFO - 


2018-04-29 16:03:34,430 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 16:03:34,500 - memory_profile6_log - INFO -     p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0    0.000646        1844.325967        72            1854.325967  10960288  1627c8436d652-070435fe3262f2-15606626-38400-16...
1    0.000055        5563.716667        29            5573.716667  11911567  1622db65f7379-02d6dcab4ac38a-6b3d6c-2c880-1622...
2    0.000646        1844.325967        21            1854.325967  10960288  1625b257b8a6c-0032929dc13715-6e17495f-29b80-16...
3    0.000646        1844.325967        48            1854.325967  10960288  1613c2f0cd9da-032377262d89-467c6e3a-38400-1613...
4    0.000646        1844.325967         4            1854.325967  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
5    0.001473         269.647011         8             279.647011  22291119  1612dd4baba55-0e3d53e928bbdd-1a331550-38400-16...
6    0.000055        5563.716667        79            5573.716667  11911567  161a2573db38a-0834b483951503-7023786e-38400-16...
7    0.001473         269.647011         4             279.647011  22291119  1612d7e198144-0691c46a6-7a720f25-38400-1612d7e...
8    0.001473         269.647011         4             279.647011  22291119  1612d10d34c0-0f502e986b12-73261116-38400-1612d...
9    0.001473         269.647011        32             279.647011  22291119  161a333c831e-00b875a76c6048-28313a6c-38400-161...
10   0.000646        1844.325967        18            1854.325967  10960288  1612d67486c12-0240b99fbdedf6-3a0e0140-38400-16...
11   0.000055        5563.716667         4            5573.716667  11911567  1613746d53017-007a2bf6f22f28-70261016-38400-16...
12   0.000646        1844.325967        23            1854.325967  10960288  161abd353a066-0eb2535f986277-4e684743-38400-16...
13   0.000646        1844.325967        34            1854.325967  10960288  161b31ba3c0146-091f7870d34799-5f47693b-38400-1...
14   0.000055        5563.716667        18            5573.716667  11911567  1612dc48c2f6c-02034120b12382-5944693b-38400-16...
15   0.001473         269.647011        16             279.647011  22291119  1622eb5b2bc8d-08b70a147f1bcf-655a7c2d-38400-16...
16   0.000646        1844.325967        17            1854.325967  10960288  1610cb732313c-0c6252e2dec4ba-4323461-100200-16...
17   0.001473         269.647011         4             279.647011  22291119  1616efe1de7365-024418da2c9cdb-667e1364-3d10d-1...
18   0.000646        1844.325967         9            1854.325967  10960288  1613026b3ea26e-03f94b4cbf2756-7703033f-38400-1...
19   0.001473         269.647011        32             279.647011  22291119  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
2018-04-29 16:03:34,502 - memory_profile6_log - INFO - 

2018-04-29 16:03:34,584 - memory_profile6_log - INFO - size of big_frame_hist: 69.87 MB
2018-04-29 16:03:34,653 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 16:03:34,665 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 16:04:24,134 - memory_profile6_log - INFO - size of df: 58.30 MB
2018-04-29 16:04:24,135 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-29 16:04:24,216 - memory_profile6_log - INFO - size of current_frame: 58.30 MB
2018-04-29 16:04:24,217 - memory_profile6_log - INFO - loading time of: 484420 total genuine-current interest data ~ take 403.358s
2018-04-29 16:04:24,233 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 16:04:24,233 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 16:04:24,234 - memory_profile6_log - INFO - ================================================

2018-04-29 16:04:24,236 - memory_profile6_log - INFO -    348     86.8 MiB     86.8 MiB   @profile

2018-04-29 16:04:24,236 - memory_profile6_log - INFO -    349                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 16:04:24,239 - memory_profile6_log - INFO -    350     86.9 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 16:04:24,239 - memory_profile6_log - INFO -    351     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 16:04:24,240 - memory_profile6_log - INFO -    352                             

2018-04-29 16:04:24,242 - memory_profile6_log - INFO -    353                                 # ~~~ Begin collecting data ~~~

2018-04-29 16:04:24,243 - memory_profile6_log - INFO -    354     86.9 MiB      0.0 MiB       t0 = time.time()

2018-04-29 16:04:24,243 - memory_profile6_log - INFO -    355    444.7 MiB    357.8 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 16:04:24,246 - memory_profile6_log - INFO -    356    444.7 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 16:04:24,247 - memory_profile6_log - INFO -    357                                     logger.info("Training cannot be empty..")

2018-04-29 16:04:24,250 - memory_profile6_log - INFO -    358                                     return False

2018-04-29 16:04:24,250 - memory_profile6_log - INFO -    359    460.0 MiB     15.3 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 16:04:24,252 - memory_profile6_log - INFO -    360    460.3 MiB      0.3 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 16:04:24,253 - memory_profile6_log - INFO -    361    460.3 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 16:04:24,253 - memory_profile6_log - INFO -    362                             

2018-04-29 16:04:24,253 - memory_profile6_log - INFO -    363    466.4 MiB      6.1 MiB       big_frame = pd.concat(datalist)

2018-04-29 16:04:24,256 - memory_profile6_log - INFO -    364    466.4 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 16:04:24,256 - memory_profile6_log - INFO -    365    460.6 MiB     -5.8 MiB       del datalist

2018-04-29 16:04:24,257 - memory_profile6_log - INFO -    366                             

2018-04-29 16:04:24,257 - memory_profile6_log - INFO -    367    460.6 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 16:04:24,259 - memory_profile6_log - INFO -    368                             

2018-04-29 16:04:24,259 - memory_profile6_log - INFO -    369                                 # ~ get current news interest ~

2018-04-29 16:04:24,259 - memory_profile6_log - INFO -    370    460.6 MiB      0.0 MiB       if not cd:

2018-04-29 16:04:24,259 - memory_profile6_log - INFO -    371                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 16:04:24,259 - memory_profile6_log - INFO -    372                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 16:04:24,260 - memory_profile6_log - INFO -    373                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 16:04:24,260 - memory_profile6_log - INFO -    374                                 else:

2018-04-29 16:04:24,262 - memory_profile6_log - INFO -    375    460.6 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 16:04:24,262 - memory_profile6_log - INFO -    376    460.6 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 16:04:24,263 - memory_profile6_log - INFO -    377                             

2018-04-29 16:04:24,263 - memory_profile6_log - INFO -    378                                     # safe handling of query parameter

2018-04-29 16:04:24,263 - memory_profile6_log - INFO -    379                                     query_params = [

2018-04-29 16:04:24,263 - memory_profile6_log - INFO -    380    460.6 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 16:04:24,263 - memory_profile6_log - INFO -    381                                     ]

2018-04-29 16:04:24,269 - memory_profile6_log - INFO -    382                             

2018-04-29 16:04:24,269 - memory_profile6_log - INFO -    383    460.6 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-29 16:04:24,270 - memory_profile6_log - INFO -    384    516.1 MiB     55.5 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 16:04:24,270 - memory_profile6_log - INFO -    385    516.1 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 16:04:24,272 - memory_profile6_log - INFO -    386                             

2018-04-29 16:04:24,273 - memory_profile6_log - INFO -    387    516.1 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 16:04:24,273 - memory_profile6_log - INFO -    388    516.1 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 16:04:24,275 - memory_profile6_log - INFO -    389    516.1 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 16:04:24,275 - memory_profile6_log - INFO -    390    516.1 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 16:04:24,276 - memory_profile6_log - INFO -    391    516.1 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 16:04:24,276 - memory_profile6_log - INFO -    392    516.1 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 16:04:24,279 - memory_profile6_log - INFO -    393                             

2018-04-29 16:04:24,279 - memory_profile6_log - INFO -    394    516.1 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 16:04:24,280 - memory_profile6_log - INFO - 


2018-04-29 16:04:24,283 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 16:04:24,315 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 16:04:24,315 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-29 16:04:24,316 - memory_profile6_log - INFO - apply on: 253995 total history...)
2018-04-29 16:04:25,375 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 16:04:25,377 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 16:04:25,891 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-29 16:04:25,892 - memory_profile6_log - INFO - 

2018-04-29 16:04:25,892 - memory_profile6_log - INFO - len of current fitted models: 253995
2018-04-29 16:04:25,894 - memory_profile6_log - INFO - 

2018-04-29 16:04:25,894 - memory_profile6_log - INFO - len of history fitted models: 253995
2018-04-29 16:04:25,895 - memory_profile6_log - INFO - 

2018-04-29 16:11:05,427 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 16:11:05,430 - memory_profile6_log - INFO - date_generated: 
2018-04-29 16:11:05,430 - memory_profile6_log - INFO -  
2018-04-29 16:11:05,430 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 16:11:05,430 - memory_profile6_log - INFO - 

2018-04-29 16:11:05,431 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 16:11:05,433 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 16:11:05,433 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 16:11:05,575 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 16:11:05,578 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 16:12:14,964 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 16:12:14,966 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 16:12:15,009 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 16:12:15,009 - memory_profile6_log - INFO - Appending history data...
2018-04-29 16:12:15,012 - memory_profile6_log - INFO - processing batch-0
2018-04-29 16:12:15,013 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:12:15,055 - memory_profile6_log - INFO - call history data...
2018-04-29 16:12:43,990 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:12:44,632 - memory_profile6_log - INFO - processing batch-1
2018-04-29 16:12:44,634 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:12:44,641 - memory_profile6_log - INFO - call history data...
2018-04-29 16:13:12,963 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:13:13,635 - memory_profile6_log - INFO - processing batch-2
2018-04-29 16:13:13,637 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:13:13,644 - memory_profile6_log - INFO - call history data...
2018-04-29 16:13:42,684 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:13:43,338 - memory_profile6_log - INFO - processing batch-3
2018-04-29 16:13:43,341 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:13:43,348 - memory_profile6_log - INFO - call history data...
2018-04-29 16:14:14,796 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:14:15,448 - memory_profile6_log - INFO - processing batch-4
2018-04-29 16:14:15,450 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:14:15,457 - memory_profile6_log - INFO - call history data...
2018-04-29 16:14:47,148 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:14:47,799 - memory_profile6_log - INFO - Appending training data...
2018-04-29 16:14:47,801 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 16:14:47,802 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 16:14:47,803 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 16:14:47,805 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 16:14:47,805 - memory_profile6_log - INFO - ================================================

2018-04-29 16:14:47,806 - memory_profile6_log - INFO -    302     86.6 MiB     86.6 MiB   @profile

2018-04-29 16:14:47,808 - memory_profile6_log - INFO -    303                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 16:14:47,809 - memory_profile6_log - INFO -    304     86.6 MiB      0.0 MiB       bq_client = client

2018-04-29 16:14:47,812 - memory_profile6_log - INFO -    305     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 16:14:47,813 - memory_profile6_log - INFO -    306                             

2018-04-29 16:14:47,815 - memory_profile6_log - INFO -    307     86.6 MiB      0.0 MiB       datalist = []

2018-04-29 16:14:47,815 - memory_profile6_log - INFO -    308     86.6 MiB      0.0 MiB       datalist_hist = []

2018-04-29 16:14:47,816 - memory_profile6_log - INFO -    309                             

2018-04-29 16:14:47,818 - memory_profile6_log - INFO -    310     86.6 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 16:14:47,818 - memory_profile6_log - INFO -    311    351.7 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 16:14:47,822 - memory_profile6_log - INFO -    312    339.7 MiB    253.1 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 16:14:47,825 - memory_profile6_log - INFO -    313    339.7 MiB      0.0 MiB           if tframe is not None:

2018-04-29 16:14:47,825 - memory_profile6_log - INFO -    314    339.7 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 16:14:47,826 - memory_profile6_log - INFO -    315    347.3 MiB      7.6 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 16:14:47,828 - memory_profile6_log - INFO -    316    347.3 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 16:14:47,829 - memory_profile6_log - INFO -    317    347.3 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 16:14:47,832 - memory_profile6_log - INFO -    318    351.7 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 16:14:47,834 - memory_profile6_log - INFO -    319                                                 # ~ loading history

2018-04-29 16:14:47,835 - memory_profile6_log - INFO -    320                                                 """

2018-04-29 16:14:47,835 - memory_profile6_log - INFO -    321                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 16:14:47,836 - memory_profile6_log - INFO -    322                                                 """

2018-04-29 16:14:47,838 - memory_profile6_log - INFO -    323    351.4 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 16:14:47,838 - memory_profile6_log - INFO -    324                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 16:14:47,839 - memory_profile6_log - INFO -    325    351.4 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 16:14:47,842 - memory_profile6_log - INFO -    326    351.4 MiB      0.9 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 16:14:47,844 - memory_profile6_log - INFO -    327                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 16:14:47,845 - memory_profile6_log - INFO -    328                             

2018-04-29 16:14:47,845 - memory_profile6_log - INFO -    329    351.4 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 16:14:47,846 - memory_profile6_log - INFO -    330    351.6 MiB      2.8 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 16:14:47,848 - memory_profile6_log - INFO -    331                             

2018-04-29 16:14:47,848 - memory_profile6_log - INFO -    332                                                 # me = os.getpid()

2018-04-29 16:14:47,849 - memory_profile6_log - INFO -    333                                                 # kill_proc_tree(me)

2018-04-29 16:14:47,852 - memory_profile6_log - INFO -    334                             

2018-04-29 16:14:47,854 - memory_profile6_log - INFO -    335    351.6 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 16:14:47,855 - memory_profile6_log - INFO -    336    351.7 MiB      0.0 MiB                       for m in h_frame:

2018-04-29 16:14:47,857 - memory_profile6_log - INFO -    337    351.7 MiB      0.0 MiB                           if m is not None:

2018-04-29 16:14:47,858 - memory_profile6_log - INFO -    338    351.7 MiB      0.0 MiB                               if len(m) > 0:

2018-04-29 16:14:47,858 - memory_profile6_log - INFO -    339    351.7 MiB      0.7 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 16:14:47,859 - memory_profile6_log - INFO -    340    351.7 MiB      0.0 MiB                       del h_frame

2018-04-29 16:14:47,862 - memory_profile6_log - INFO -    341    351.7 MiB      0.0 MiB                       del lhistory

2018-04-29 16:14:47,865 - memory_profile6_log - INFO -    342                             

2018-04-29 16:14:47,865 - memory_profile6_log - INFO -    343    351.7 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 16:14:47,867 - memory_profile6_log - INFO -    344    351.7 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 16:14:47,868 - memory_profile6_log - INFO -    345                                     else: 

2018-04-29 16:14:47,869 - memory_profile6_log - INFO -    346                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 16:14:47,871 - memory_profile6_log - INFO -    347    351.7 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 16:14:47,874 - memory_profile6_log - INFO -    348    351.7 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 16:14:47,875 - memory_profile6_log - INFO -    349                             

2018-04-29 16:14:47,875 - memory_profile6_log - INFO -    350    351.7 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 16:14:47,877 - memory_profile6_log - INFO - 


2018-04-29 16:14:48,982 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 16:14:49,051 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0        NaN      333823.000000       256          333833.000000  10959239  161129f951435-0d660a18dbcd87-173a7640-100200-1...
1   0.000646        1844.325967        32            1854.325967  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
2   0.000646        1844.325967        16            1854.325967  10960288  1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...
3   0.000646        1844.325967        32            1854.325967  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
4   0.000646        1844.325967        32            1854.325967  10960288  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
5   0.000646        1844.325967        16            1854.325967  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
6   0.000646        1844.325967        16            1854.325967  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
0   0.000646        1844.325967         8            1854.325967  10960288  16169731ce618-00742c5e307396-5b4d6a36-38400-16...
1   0.000646        1844.325967        16            1854.325967  10960288  161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...
2   0.000646        1844.325967        16            1854.325967  10960288  1626fd160d9114-055d801ae6b39d-66265f05-38400-1...
3   0.000646        1844.325967        16            1854.325967  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
4   0.000646        1844.325967         8            1854.325967  10960288  1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...
5   0.000646        1844.325967         8            1854.325967  10960288  161e219cd891c8-0e882943705df4-15290f5f-38400-1...
0   0.000646        1844.325967         4            1854.325967  10960288  16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...
1   0.000646        1844.325967         4            1854.325967  10960288  1612d88e55ab8-09af9658f30a46-65117b23-38400-16...
2   0.000646        1844.325967         8            1854.325967  10960288  16149f0a1611-00a034775-63446032-38400-16149f0a...
3   0.000646        1844.325967         8            1854.325967  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
4   0.000646        1844.325967         4            1854.325967  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
5   0.000646        1844.325967         4            1854.325967  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
6   0.000646        1844.325967         4            1854.325967  10960288  16132f1045f30b-0613b72b33445e-5d640103-38400-1...
2018-04-29 16:14:49,052 - memory_profile6_log - INFO - 

2018-04-29 16:14:49,062 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 16:14:49,134 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 16:14:49,148 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 16:15:39,512 - memory_profile6_log - INFO - size of df: 58.30 MB
2018-04-29 16:15:39,513 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-29 16:15:39,575 - memory_profile6_log - INFO - size of current_frame: 58.30 MB
2018-04-29 16:15:39,575 - memory_profile6_log - INFO - loading time of: 484420 total genuine-current interest data ~ take 274.026s
2018-04-29 16:15:39,582 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 16:15:39,582 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 16:15:39,582 - memory_profile6_log - INFO - ================================================

2018-04-29 16:15:39,585 - memory_profile6_log - INFO -    352     86.5 MiB     86.5 MiB   @profile

2018-04-29 16:15:39,586 - memory_profile6_log - INFO -    353                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 16:15:39,588 - memory_profile6_log - INFO -    354     86.6 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 16:15:39,588 - memory_profile6_log - INFO -    355     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 16:15:39,589 - memory_profile6_log - INFO -    356                             

2018-04-29 16:15:39,589 - memory_profile6_log - INFO -    357                                 # ~~~ Begin collecting data ~~~

2018-04-29 16:15:39,591 - memory_profile6_log - INFO -    358     86.6 MiB      0.0 MiB       t0 = time.time()

2018-04-29 16:15:39,592 - memory_profile6_log - INFO -    359    351.7 MiB    265.1 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 16:15:39,592 - memory_profile6_log - INFO -    360    351.7 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 16:15:39,592 - memory_profile6_log - INFO -    361                                     logger.info("Training cannot be empty..")

2018-04-29 16:15:39,594 - memory_profile6_log - INFO -    362                                     return False

2018-04-29 16:15:39,595 - memory_profile6_log - INFO -    363    349.8 MiB     -1.9 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 16:15:39,598 - memory_profile6_log - INFO -    364    350.0 MiB      0.2 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 16:15:39,598 - memory_profile6_log - INFO -    365    350.0 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 16:15:39,598 - memory_profile6_log - INFO -    366                             

2018-04-29 16:15:39,599 - memory_profile6_log - INFO -    367    355.8 MiB      5.8 MiB       big_frame = pd.concat(datalist)

2018-04-29 16:15:39,601 - memory_profile6_log - INFO -    368    355.8 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 16:15:39,601 - memory_profile6_log - INFO -    369    350.0 MiB     -5.8 MiB       del datalist

2018-04-29 16:15:39,601 - memory_profile6_log - INFO -    370                             

2018-04-29 16:15:39,605 - memory_profile6_log - INFO -    371    350.0 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 16:15:39,605 - memory_profile6_log - INFO -    372                             

2018-04-29 16:15:39,608 - memory_profile6_log - INFO -    373                                 # ~ get current news interest ~

2018-04-29 16:15:39,608 - memory_profile6_log - INFO -    374    350.0 MiB      0.0 MiB       if not cd:

2018-04-29 16:15:39,609 - memory_profile6_log - INFO -    375                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 16:15:39,609 - memory_profile6_log - INFO -    376                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 16:15:39,611 - memory_profile6_log - INFO -    377                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 16:15:39,612 - memory_profile6_log - INFO -    378                                 else:

2018-04-29 16:15:39,612 - memory_profile6_log - INFO -    379    350.0 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 16:15:39,615 - memory_profile6_log - INFO -    380    350.0 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 16:15:39,618 - memory_profile6_log - INFO -    381                             

2018-04-29 16:15:39,619 - memory_profile6_log - INFO -    382                                     # safe handling of query parameter

2018-04-29 16:15:39,621 - memory_profile6_log - INFO -    383                                     query_params = [

2018-04-29 16:15:39,621 - memory_profile6_log - INFO -    384    350.0 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 16:15:39,621 - memory_profile6_log - INFO -    385                                     ]

2018-04-29 16:15:39,621 - memory_profile6_log - INFO -    386                             

2018-04-29 16:15:39,621 - memory_profile6_log - INFO -    387    350.0 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-29 16:15:39,622 - memory_profile6_log - INFO -    388    420.0 MiB     70.0 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 16:15:39,622 - memory_profile6_log - INFO -    389    420.0 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 16:15:39,622 - memory_profile6_log - INFO -    390                             

2018-04-29 16:15:39,625 - memory_profile6_log - INFO -    391    420.1 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 16:15:39,625 - memory_profile6_log - INFO -    392    420.1 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 16:15:39,628 - memory_profile6_log - INFO -    393    420.1 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 16:15:39,628 - memory_profile6_log - INFO -    394    420.1 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 16:15:39,630 - memory_profile6_log - INFO -    395    420.1 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 16:15:39,630 - memory_profile6_log - INFO -    396    420.1 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 16:15:39,631 - memory_profile6_log - INFO -    397                             

2018-04-29 16:15:39,631 - memory_profile6_log - INFO -    398    420.1 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 16:15:39,631 - memory_profile6_log - INFO - 


2018-04-29 16:15:39,634 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 16:15:39,665 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 16:15:39,667 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-29 16:15:39,668 - memory_profile6_log - INFO - apply on: 5000 total history...)
2018-04-29 16:15:40,720 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 16:15:40,720 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 16:15:40,940 - memory_profile6_log - INFO - model_fit dtypes:

2018-04-29 16:15:40,946 - memory_profile6_log - INFO - date              datetime64[ns, UTC]
user_id                        object
topic_id                       object
num_x                           int64
num_y                           int64
is_general                       bool
date_all_click                  int64
Ntotal                          int64
sigma_Nt                        int64
joinprob_ci                   float64
p_cat_ci                      float64
posterior                     float64
dtype: object
2018-04-29 16:15:40,947 - memory_profile6_log - INFO - 

2018-04-29 16:15:40,947 - memory_profile6_log - INFO - fitted_models_hist dtypes:

2018-04-29 16:15:40,953 - memory_profile6_log - INFO - p0_cat_ci                float64
pt_posterior_x_Nt        float64
sigma_Nt                   int64
smoothed_pt_posterior    float64
topic_id                  object
user_id                   object
dtype: object
2018-04-29 16:15:40,956 - memory_profile6_log - INFO - 

2018-04-29 16:15:41,191 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-29 16:15:41,193 - memory_profile6_log - INFO - 

2018-04-29 16:15:41,194 - memory_profile6_log - INFO - len of current fitted models: 253995
2018-04-29 16:15:41,194 - memory_profile6_log - INFO - 

2018-04-29 16:15:41,196 - memory_profile6_log - INFO - len of history fitted models: 5000
2018-04-29 16:15:41,197 - memory_profile6_log - INFO - 

2018-04-29 16:29:00,503 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 16:29:00,505 - memory_profile6_log - INFO - date_generated: 
2018-04-29 16:29:00,506 - memory_profile6_log - INFO -  
2018-04-29 16:29:00,506 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 16:29:00,506 - memory_profile6_log - INFO - 

2018-04-29 16:29:00,506 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 16:29:00,507 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 16:29:00,507 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 16:29:00,637 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 16:29:00,641 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 16:29:22,244 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 16:29:22,249 - memory_profile6_log - INFO - date_generated: 
2018-04-29 16:29:22,249 - memory_profile6_log - INFO -  
2018-04-29 16:29:22,250 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 16:29:22,250 - memory_profile6_log - INFO - 

2018-04-29 16:29:22,250 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 16:29:22,250 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 16:29:22,252 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 16:29:22,381 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 16:29:22,388 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 16:30:30,180 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 16:30:30,180 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 16:30:30,224 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 16:30:30,226 - memory_profile6_log - INFO - Appending history data...
2018-04-29 16:30:30,227 - memory_profile6_log - INFO - processing batch-0
2018-04-29 16:30:30,229 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:30:30,270 - memory_profile6_log - INFO - call history data...
2018-04-29 16:31:00,420 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:31:01,144 - memory_profile6_log - INFO - processing batch-1
2018-04-29 16:31:01,145 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:31:01,154 - memory_profile6_log - INFO - call history data...
2018-04-29 16:31:29,710 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:31:30,361 - memory_profile6_log - INFO - processing batch-2
2018-04-29 16:31:30,362 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:31:30,371 - memory_profile6_log - INFO - call history data...
2018-04-29 16:32:01,180 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:32:01,851 - memory_profile6_log - INFO - processing batch-3
2018-04-29 16:32:01,852 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:32:01,859 - memory_profile6_log - INFO - call history data...
2018-04-29 16:32:30,960 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:32:31,664 - memory_profile6_log - INFO - processing batch-4
2018-04-29 16:32:31,665 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:32:31,674 - memory_profile6_log - INFO - call history data...
2018-04-29 16:33:01,066 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:33:01,744 - memory_profile6_log - INFO - Appending training data...
2018-04-29 16:33:01,746 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 16:33:01,747 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 16:33:01,749 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 16:33:01,750 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 16:33:01,750 - memory_profile6_log - INFO - ================================================

2018-04-29 16:33:01,753 - memory_profile6_log - INFO -    302     86.9 MiB     86.9 MiB   @profile

2018-04-29 16:33:01,753 - memory_profile6_log - INFO -    303                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 16:33:01,756 - memory_profile6_log - INFO -    304     86.9 MiB      0.0 MiB       bq_client = client

2018-04-29 16:33:01,757 - memory_profile6_log - INFO -    305     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 16:33:01,759 - memory_profile6_log - INFO -    306                             

2018-04-29 16:33:01,759 - memory_profile6_log - INFO -    307     86.9 MiB      0.0 MiB       datalist = []

2018-04-29 16:33:01,762 - memory_profile6_log - INFO -    308     86.9 MiB      0.0 MiB       datalist_hist = []

2018-04-29 16:33:01,762 - memory_profile6_log - INFO -    309                             

2018-04-29 16:33:01,763 - memory_profile6_log - INFO -    310     86.9 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 16:33:01,763 - memory_profile6_log - INFO -    311    349.9 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 16:33:01,766 - memory_profile6_log - INFO -    312    338.0 MiB    251.0 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 16:33:01,767 - memory_profile6_log - INFO -    313    338.0 MiB      0.0 MiB           if tframe is not None:

2018-04-29 16:33:01,769 - memory_profile6_log - INFO -    314    338.0 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 16:33:01,769 - memory_profile6_log - INFO -    315    346.2 MiB      8.3 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 16:33:01,769 - memory_profile6_log - INFO -    316    346.2 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 16:33:01,770 - memory_profile6_log - INFO -    317    346.2 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 16:33:01,772 - memory_profile6_log - INFO -    318    349.8 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 16:33:01,773 - memory_profile6_log - INFO -    319                                                 # ~ loading history

2018-04-29 16:33:01,773 - memory_profile6_log - INFO -    320                                                 """

2018-04-29 16:33:01,775 - memory_profile6_log - INFO -    321                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 16:33:01,776 - memory_profile6_log - INFO -    322                                                 """

2018-04-29 16:33:01,779 - memory_profile6_log - INFO -    323    349.6 MiB      0.1 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 16:33:01,779 - memory_profile6_log - INFO -    324                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 16:33:01,779 - memory_profile6_log - INFO -    325    349.6 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 16:33:01,780 - memory_profile6_log - INFO -    326    349.6 MiB      0.5 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 16:33:01,782 - memory_profile6_log - INFO -    327                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 16:33:01,782 - memory_profile6_log - INFO -    328                             

2018-04-29 16:33:01,783 - memory_profile6_log - INFO -    329    349.6 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 16:33:01,786 - memory_profile6_log - INFO -    330    349.7 MiB      2.3 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 16:33:01,786 - memory_profile6_log - INFO -    331                             

2018-04-29 16:33:01,788 - memory_profile6_log - INFO -    332                                                 # me = os.getpid()

2018-04-29 16:33:01,789 - memory_profile6_log - INFO -    333                                                 # kill_proc_tree(me)

2018-04-29 16:33:01,789 - memory_profile6_log - INFO -    334                             

2018-04-29 16:33:01,790 - memory_profile6_log - INFO -    335    349.7 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 16:33:01,790 - memory_profile6_log - INFO -    336    349.8 MiB      0.0 MiB                       for m in h_frame:

2018-04-29 16:33:01,792 - memory_profile6_log - INFO -    337    349.8 MiB      0.0 MiB                           if m is not None:

2018-04-29 16:33:01,792 - memory_profile6_log - INFO -    338    349.8 MiB      0.0 MiB                               if len(m) > 0:

2018-04-29 16:33:01,792 - memory_profile6_log - INFO -    339    349.8 MiB      0.6 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 16:33:01,793 - memory_profile6_log - INFO -    340    349.8 MiB      0.0 MiB                       del h_frame

2018-04-29 16:33:01,795 - memory_profile6_log - INFO -    341    349.8 MiB      0.0 MiB                       del lhistory

2018-04-29 16:33:01,798 - memory_profile6_log - INFO -    342                             

2018-04-29 16:33:01,799 - memory_profile6_log - INFO -    343    349.9 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 16:33:01,801 - memory_profile6_log - INFO -    344    349.9 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 16:33:01,802 - memory_profile6_log - INFO -    345                                     else: 

2018-04-29 16:33:01,803 - memory_profile6_log - INFO -    346                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 16:33:01,803 - memory_profile6_log - INFO -    347    349.9 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 16:33:01,805 - memory_profile6_log - INFO -    348    349.9 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 16:33:01,806 - memory_profile6_log - INFO -    349                             

2018-04-29 16:33:01,809 - memory_profile6_log - INFO -    350    349.9 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 16:33:01,809 - memory_profile6_log - INFO - 


2018-04-29 16:33:02,957 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 16:33:03,026 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0   0.000646        1844.325967        16            1854.325967  10960288  1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...
1   0.000646        1844.325967        32            1854.325967  10960288  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
2   0.000646        1844.325967        16            1854.325967  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
3   0.000646        1844.325967        32            1854.325967  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
4        NaN      333823.000000       256          333833.000000  10959239  161129f951435-0d660a18dbcd87-173a7640-100200-1...
5   0.000646        1844.325967        16            1854.325967  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
6   0.000646        1844.325967        32            1854.325967  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
0   0.000646        1844.325967         8            1854.325967  10960288  1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...
1   0.000646        1844.325967         8            1854.325967  10960288  161e219cd891c8-0e882943705df4-15290f5f-38400-1...
2   0.000646        1844.325967        16            1854.325967  10960288  1626fd160d9114-055d801ae6b39d-66265f05-38400-1...
3   0.000646        1844.325967        16            1854.325967  10960288  161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...
4   0.000646        1844.325967         8            1854.325967  10960288  16169731ce618-00742c5e307396-5b4d6a36-38400-16...
5   0.000646        1844.325967        16            1854.325967  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
0   0.000646        1844.325967         4            1854.325967  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
1   0.000646        1844.325967         4            1854.325967  10960288  1612d88e55ab8-09af9658f30a46-65117b23-38400-16...
2   0.000646        1844.325967         4            1854.325967  10960288  16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...
3   0.000646        1844.325967         4            1854.325967  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
4   0.000646        1844.325967         8            1854.325967  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
5   0.000646        1844.325967         8            1854.325967  10960288  16149f0a1611-00a034775-63446032-38400-16149f0a...
6   0.000646        1844.325967         4            1854.325967  10960288  16132f1045f30b-0613b72b33445e-5d640103-38400-1...
2018-04-29 16:33:03,026 - memory_profile6_log - INFO - 

2018-04-29 16:33:03,036 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 16:33:03,105 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 16:33:03,124 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 16:34:06,776 - memory_profile6_log - INFO - size of df: 58.30 MB
2018-04-29 16:34:06,776 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-29 16:34:06,841 - memory_profile6_log - INFO - size of current_frame: 58.30 MB
2018-04-29 16:34:06,842 - memory_profile6_log - INFO - loading time of: 484420 total genuine-current interest data ~ take 284.482s
2018-04-29 16:34:06,848 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 16:34:06,849 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 16:34:06,851 - memory_profile6_log - INFO - ================================================

2018-04-29 16:34:06,851 - memory_profile6_log - INFO -    352     86.8 MiB     86.8 MiB   @profile

2018-04-29 16:34:06,854 - memory_profile6_log - INFO -    353                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 16:34:06,855 - memory_profile6_log - INFO -    354     86.9 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 16:34:06,855 - memory_profile6_log - INFO -    355     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 16:34:06,855 - memory_profile6_log - INFO -    356                             

2018-04-29 16:34:06,857 - memory_profile6_log - INFO -    357                                 # ~~~ Begin collecting data ~~~

2018-04-29 16:34:06,858 - memory_profile6_log - INFO -    358     86.9 MiB      0.0 MiB       t0 = time.time()

2018-04-29 16:34:06,858 - memory_profile6_log - INFO -    359    349.9 MiB    262.9 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 16:34:06,858 - memory_profile6_log - INFO -    360    349.9 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 16:34:06,859 - memory_profile6_log - INFO -    361                                     logger.info("Training cannot be empty..")

2018-04-29 16:34:06,861 - memory_profile6_log - INFO -    362                                     return False

2018-04-29 16:34:06,865 - memory_profile6_log - INFO -    363    349.0 MiB     -0.8 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 16:34:06,865 - memory_profile6_log - INFO -    364    349.1 MiB      0.1 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 16:34:06,867 - memory_profile6_log - INFO -    365    349.1 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 16:34:06,868 - memory_profile6_log - INFO -    366                             

2018-04-29 16:34:06,868 - memory_profile6_log - INFO -    367    354.9 MiB      5.8 MiB       big_frame = pd.concat(datalist)

2018-04-29 16:34:06,868 - memory_profile6_log - INFO -    368    354.9 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 16:34:06,868 - memory_profile6_log - INFO -    369    349.1 MiB     -5.8 MiB       del datalist

2018-04-29 16:34:06,868 - memory_profile6_log - INFO -    370                             

2018-04-29 16:34:06,871 - memory_profile6_log - INFO -    371    349.1 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 16:34:06,871 - memory_profile6_log - INFO -    372                             

2018-04-29 16:34:06,872 - memory_profile6_log - INFO -    373                                 # ~ get current news interest ~

2018-04-29 16:34:06,874 - memory_profile6_log - INFO -    374    349.1 MiB      0.0 MiB       if not cd:

2018-04-29 16:34:06,875 - memory_profile6_log - INFO -    375                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 16:34:06,875 - memory_profile6_log - INFO -    376                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 16:34:06,875 - memory_profile6_log - INFO -    377                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 16:34:06,875 - memory_profile6_log - INFO -    378                                 else:

2018-04-29 16:34:06,877 - memory_profile6_log - INFO -    379    349.1 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 16:34:06,878 - memory_profile6_log - INFO -    380    349.1 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 16:34:06,878 - memory_profile6_log - INFO -    381                             

2018-04-29 16:34:06,878 - memory_profile6_log - INFO -    382                                     # safe handling of query parameter

2018-04-29 16:34:06,878 - memory_profile6_log - INFO -    383                                     query_params = [

2018-04-29 16:34:06,881 - memory_profile6_log - INFO -    384    349.1 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 16:34:06,884 - memory_profile6_log - INFO -    385                                     ]

2018-04-29 16:34:06,884 - memory_profile6_log - INFO -    386                             

2018-04-29 16:34:06,884 - memory_profile6_log - INFO -    387    349.1 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-29 16:34:06,885 - memory_profile6_log - INFO -    388    420.1 MiB     70.9 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 16:34:06,887 - memory_profile6_log - INFO -    389    420.1 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 16:34:06,888 - memory_profile6_log - INFO -    390                             

2018-04-29 16:34:06,888 - memory_profile6_log - INFO -    391    420.1 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 16:34:06,888 - memory_profile6_log - INFO -    392    420.1 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 16:34:06,890 - memory_profile6_log - INFO -    393    420.1 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 16:34:06,891 - memory_profile6_log - INFO -    394    420.1 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 16:34:06,892 - memory_profile6_log - INFO -    395    420.1 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 16:34:06,894 - memory_profile6_log - INFO -    396    420.1 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 16:34:06,894 - memory_profile6_log - INFO -    397                             

2018-04-29 16:34:06,895 - memory_profile6_log - INFO -    398    420.1 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 16:34:06,897 - memory_profile6_log - INFO - 


2018-04-29 16:34:06,900 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 16:34:06,931 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 16:34:06,934 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-29 16:34:06,934 - memory_profile6_log - INFO - apply on: 5000 total history...)
2018-04-29 16:34:08,019 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 16:34:08,020 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 16:34:08,250 - memory_profile6_log - INFO - model_fit dtypes:

2018-04-29 16:34:08,256 - memory_profile6_log - INFO - date              datetime64[ns, UTC]
user_id                        object
topic_id                       object
num_x                           int64
num_y                           int64
is_general                       bool
date_all_click                  int64
Ntotal                          int64
sigma_Nt                        int64
joinprob_ci                   float64
p_cat_ci                      float64
posterior                     float64
dtype: object
2018-04-29 16:34:08,259 - memory_profile6_log - INFO - 

2018-04-29 16:34:08,259 - memory_profile6_log - INFO - fitted_models_hist dtypes:

2018-04-29 16:34:08,266 - memory_profile6_log - INFO - p0_cat_ci                float64
pt_posterior_x_Nt        float64
sigma_Nt                   int64
smoothed_pt_posterior    float64
topic_id                  object
user_id                   object
dtype: object
2018-04-29 16:34:08,266 - memory_profile6_log - INFO - 

2018-04-29 16:34:08,841 - memory_profile6_log - INFO - Len of fitted_models on main class: 253995
2018-04-29 16:34:08,842 - memory_profile6_log - INFO - 

2018-04-29 16:34:08,844 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-29 16:34:08,845 - memory_profile6_log - INFO - 

2018-04-29 16:34:08,845 - memory_profile6_log - INFO - Fitted models before concat:

2018-04-29 16:34:08,871 - memory_profile6_log - INFO -                                 user_id    topic_id  pt_posterior_x_Nt  p0_cat_ci  smoothed_pt_posterior
0  107e02cd-54af-40fd-a602-e6105250ae8c  1046474724        1902.125356   0.000090            1912.125356
1  107e02cd-54af-40fd-a602-e6105250ae8c  1060896301        1346.060484   0.000065            1356.060484
2  107e02cd-54af-40fd-a602-e6105250ae8c   188400148         144.387111   0.036200             154.387111
3  107e02cd-54af-40fd-a602-e6105250ae8c    22553543          71.961844   0.107319              81.961844
4  107e02cd-54af-40fd-a602-e6105250ae8c    22596701         122.085700   0.040442             132.085700
2018-04-29 16:34:08,871 - memory_profile6_log - INFO - 

2018-04-29 16:34:08,872 - memory_profile6_log - INFO - len of current fitted models: 253995
2018-04-29 16:34:08,874 - memory_profile6_log - INFO - 

2018-04-29 16:34:08,875 - memory_profile6_log - INFO - len of history fitted models: 5000
2018-04-29 16:34:08,875 - memory_profile6_log - INFO - 

2018-04-29 16:34:08,918 - memory_profile6_log - INFO - len of fitted models after concat: 258995
2018-04-29 16:34:08,920 - memory_profile6_log - INFO - 

2018-04-29 16:34:08,921 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-29 16:34:08,921 - memory_profile6_log - INFO - 

2018-04-29 16:39:51,529 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 16:39:51,532 - memory_profile6_log - INFO - date_generated: 
2018-04-29 16:39:51,532 - memory_profile6_log - INFO -  
2018-04-29 16:39:51,532 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 16:39:51,532 - memory_profile6_log - INFO - 

2018-04-29 16:39:51,532 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 16:39:51,533 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 16:39:51,535 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 16:39:51,654 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 16:39:51,657 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 16:40:59,770 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 16:40:59,772 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 16:40:59,815 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 16:40:59,815 - memory_profile6_log - INFO - Appending history data...
2018-04-29 16:40:59,816 - memory_profile6_log - INFO - processing batch-0
2018-04-29 16:40:59,819 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:40:59,864 - memory_profile6_log - INFO - call history data...
2018-04-29 16:41:28,305 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:41:30,196 - memory_profile6_log - INFO - processing batch-1
2018-04-29 16:41:30,197 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:41:30,216 - memory_profile6_log - INFO - call history data...
2018-04-29 16:42:02,934 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:42:04,561 - memory_profile6_log - INFO - processing batch-2
2018-04-29 16:42:04,562 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:42:04,582 - memory_profile6_log - INFO - call history data...
2018-04-29 16:42:37,641 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:42:39,323 - memory_profile6_log - INFO - processing batch-3
2018-04-29 16:42:39,326 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:42:39,345 - memory_profile6_log - INFO - call history data...
2018-04-29 16:43:12,641 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:43:14,309 - memory_profile6_log - INFO - processing batch-4
2018-04-29 16:43:14,312 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:43:14,332 - memory_profile6_log - INFO - call history data...
2018-04-29 16:43:47,201 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:43:48,819 - memory_profile6_log - INFO - Appending training data...
2018-04-29 16:43:48,821 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 16:43:48,823 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 16:43:48,826 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 16:43:48,828 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 16:43:48,832 - memory_profile6_log - INFO - ================================================

2018-04-29 16:43:48,835 - memory_profile6_log - INFO -    310     87.0 MiB     87.0 MiB   @profile

2018-04-29 16:43:48,838 - memory_profile6_log - INFO -    311                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 16:43:48,844 - memory_profile6_log - INFO -    312     87.0 MiB      0.0 MiB       bq_client = client

2018-04-29 16:43:48,846 - memory_profile6_log - INFO -    313     87.0 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 16:43:48,848 - memory_profile6_log - INFO -    314                             

2018-04-29 16:43:48,851 - memory_profile6_log - INFO -    315     87.0 MiB      0.0 MiB       datalist = []

2018-04-29 16:43:48,855 - memory_profile6_log - INFO -    316     87.0 MiB      0.0 MiB       datalist_hist = []

2018-04-29 16:43:48,858 - memory_profile6_log - INFO -    317                             

2018-04-29 16:43:48,859 - memory_profile6_log - INFO -    318     87.0 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 16:43:48,861 - memory_profile6_log - INFO -    319    349.5 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 16:43:48,867 - memory_profile6_log - INFO -    320    338.2 MiB    251.1 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 16:43:48,868 - memory_profile6_log - INFO -    321    338.2 MiB      0.0 MiB           if tframe is not None:

2018-04-29 16:43:48,871 - memory_profile6_log - INFO -    322    338.2 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 16:43:48,872 - memory_profile6_log - INFO -    323    346.4 MiB      8.3 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 16:43:48,878 - memory_profile6_log - INFO -    324    346.4 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 16:43:48,880 - memory_profile6_log - INFO -    325    346.4 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 16:43:48,882 - memory_profile6_log - INFO -    326    349.5 MiB     -0.3 MiB                   for ix in range(len(X_split)):

2018-04-29 16:43:48,884 - memory_profile6_log - INFO -    327                                                 # ~ loading history

2018-04-29 16:43:48,888 - memory_profile6_log - INFO -    328                                                 """

2018-04-29 16:43:48,891 - memory_profile6_log - INFO -    329                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 16:43:48,892 - memory_profile6_log - INFO -    330                                                 """

2018-04-29 16:43:48,894 - memory_profile6_log - INFO -    331    349.4 MiB     -0.2 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 16:43:48,898 - memory_profile6_log - INFO -    332                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 16:43:48,903 - memory_profile6_log - INFO -    333    349.4 MiB     -0.3 MiB                       logger.info("creating list history data...")

2018-04-29 16:43:48,904 - memory_profile6_log - INFO -    334    349.4 MiB      0.2 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 16:43:48,911 - memory_profile6_log - INFO -    335                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 16:43:48,917 - memory_profile6_log - INFO -    336                             

2018-04-29 16:43:48,921 - memory_profile6_log - INFO -    337    349.4 MiB     -0.3 MiB                       logger.info("call history data...")

2018-04-29 16:43:48,923 - memory_profile6_log - INFO -    338    349.5 MiB      1.3 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 16:43:48,926 - memory_profile6_log - INFO -    339                             

2018-04-29 16:43:48,930 - memory_profile6_log - INFO -    340                                                 # me = os.getpid()

2018-04-29 16:43:48,934 - memory_profile6_log - INFO -    341                                                 # kill_proc_tree(me)

2018-04-29 16:43:48,937 - memory_profile6_log - INFO -    342                             

2018-04-29 16:43:48,941 - memory_profile6_log - INFO -    343    349.5 MiB     -0.2 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 16:43:48,944 - memory_profile6_log - INFO -    344    349.5 MiB    -83.3 MiB                       for m in h_frame:

2018-04-29 16:43:48,946 - memory_profile6_log - INFO -    345    349.5 MiB    -83.4 MiB                           if m is not None:

2018-04-29 16:43:48,947 - memory_profile6_log - INFO -    346    349.5 MiB    -83.4 MiB                               if len(m) > 0:

2018-04-29 16:43:48,953 - memory_profile6_log - INFO -    347    349.5 MiB    -82.6 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 16:43:48,954 - memory_profile6_log - INFO -    348    349.5 MiB     -0.3 MiB                       del h_frame

2018-04-29 16:43:48,957 - memory_profile6_log - INFO -    349    349.5 MiB     -0.3 MiB                       del lhistory

2018-04-29 16:43:48,959 - memory_profile6_log - INFO -    350                             

2018-04-29 16:43:48,964 - memory_profile6_log - INFO -    351    349.5 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 16:43:48,967 - memory_profile6_log - INFO -    352    349.5 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 16:43:48,970 - memory_profile6_log - INFO -    353                                     else: 

2018-04-29 16:43:48,971 - memory_profile6_log - INFO -    354                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 16:43:48,976 - memory_profile6_log - INFO -    355    349.5 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 16:43:48,979 - memory_profile6_log - INFO -    356    349.5 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 16:43:48,982 - memory_profile6_log - INFO -    357                             

2018-04-29 16:43:48,983 - memory_profile6_log - INFO -    358    349.5 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 16:43:48,987 - memory_profile6_log - INFO - 


2018-04-29 16:43:51,846 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 16:43:52,006 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0   0.000646        1844.325967        16            1854.325967  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
1   0.000646        1844.325967        16            1854.325967  10960288  1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...
2   0.000646        1844.325967        16            1854.325967  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
3   0.000646        1844.325967        32            1854.325967  10960288  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
4   0.000646        1844.325967        32            1854.325967  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
5        NaN      333823.000000       256          333833.000000  10959239  161129f951435-0d660a18dbcd87-173a7640-100200-1...
6   0.000646        1844.325967        32            1854.325967  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
0   0.000646        1844.325967         8            1854.325967  10960288  1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...
1   0.000646        1844.325967        16            1854.325967  10960288  161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...
2   0.000646        1844.325967         8            1854.325967  10960288  161e219cd891c8-0e882943705df4-15290f5f-38400-1...
3   0.000646        1844.325967         8            1854.325967  10960288  16169731ce618-00742c5e307396-5b4d6a36-38400-16...
4   0.000646        1844.325967        16            1854.325967  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
5   0.000646        1844.325967        16            1854.325967  10960288  1626fd160d9114-055d801ae6b39d-66265f05-38400-1...
0   0.000646        1844.325967         4            1854.325967  10960288  16132f1045f30b-0613b72b33445e-5d640103-38400-1...
1   0.000646        1844.325967         8            1854.325967  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
2   0.000646        1844.325967         4            1854.325967  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
3   0.000646        1844.325967         4            1854.325967  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
4   0.000646        1844.325967         4            1854.325967  10960288  16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...
5   0.000646        1844.325967         4            1854.325967  10960288  1612d88e55ab8-09af9658f30a46-65117b23-38400-16...
6   0.000646        1844.325967         8            1854.325967  10960288  16149f0a1611-00a034775-63446032-38400-16149f0a...
2018-04-29 16:43:52,009 - memory_profile6_log - INFO - 

2018-04-29 16:43:52,045 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 16:43:52,226 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 16:43:52,250 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 16:45:25,917 - memory_profile6_log - INFO - size of df: 58.30 MB
2018-04-29 16:45:25,921 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-29 16:45:26,062 - memory_profile6_log - INFO - size of current_frame: 58.30 MB
2018-04-29 16:45:26,065 - memory_profile6_log - INFO - loading time of: 484420 total genuine-current interest data ~ take 334.430s
2018-04-29 16:45:26,075 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 16:45:26,076 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 16:45:26,078 - memory_profile6_log - INFO - ================================================

2018-04-29 16:45:26,078 - memory_profile6_log - INFO -    360     86.9 MiB     86.9 MiB   @profile

2018-04-29 16:45:26,078 - memory_profile6_log - INFO -    361                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 16:45:26,079 - memory_profile6_log - INFO -    362     87.0 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 16:45:26,081 - memory_profile6_log - INFO -    363     87.0 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 16:45:26,086 - memory_profile6_log - INFO -    364                             

2018-04-29 16:45:26,091 - memory_profile6_log - INFO -    365                                 # ~~~ Begin collecting data ~~~

2018-04-29 16:45:26,092 - memory_profile6_log - INFO -    366     87.0 MiB      0.0 MiB       t0 = time.time()

2018-04-29 16:45:26,092 - memory_profile6_log - INFO -    367    349.5 MiB    262.5 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 16:45:26,096 - memory_profile6_log - INFO -    368    349.5 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 16:45:26,098 - memory_profile6_log - INFO -    369                                     logger.info("Training cannot be empty..")

2018-04-29 16:45:26,108 - memory_profile6_log - INFO -    370                                     return False

2018-04-29 16:45:26,111 - memory_profile6_log - INFO -    371    348.6 MiB     -1.0 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 16:45:26,114 - memory_profile6_log - INFO -    372    348.6 MiB      0.1 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 16:45:26,125 - memory_profile6_log - INFO -    373    348.6 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 16:45:26,140 - memory_profile6_log - INFO -    374                             

2018-04-29 16:45:26,141 - memory_profile6_log - INFO -    375    354.7 MiB      6.1 MiB       big_frame = pd.concat(datalist)

2018-04-29 16:45:26,154 - memory_profile6_log - INFO -    376    354.7 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 16:45:26,157 - memory_profile6_log - INFO -    377    348.9 MiB     -5.8 MiB       del datalist

2018-04-29 16:45:26,160 - memory_profile6_log - INFO -    378                             

2018-04-29 16:45:26,164 - memory_profile6_log - INFO -    379    348.9 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 16:45:26,167 - memory_profile6_log - INFO -    380                             

2018-04-29 16:45:26,167 - memory_profile6_log - INFO -    381                                 # ~ get current news interest ~

2018-04-29 16:45:26,170 - memory_profile6_log - INFO -    382    348.9 MiB      0.0 MiB       if not cd:

2018-04-29 16:45:26,171 - memory_profile6_log - INFO -    383                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 16:45:26,177 - memory_profile6_log - INFO -    384                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 16:45:26,180 - memory_profile6_log - INFO -    385                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 16:45:26,181 - memory_profile6_log - INFO -    386                                 else:

2018-04-29 16:45:26,184 - memory_profile6_log - INFO -    387    348.9 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 16:45:26,187 - memory_profile6_log - INFO -    388    348.9 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 16:45:26,190 - memory_profile6_log - INFO -    389                             

2018-04-29 16:45:26,190 - memory_profile6_log - INFO -    390                                     # safe handling of query parameter

2018-04-29 16:45:26,193 - memory_profile6_log - INFO -    391                                     query_params = [

2018-04-29 16:45:26,194 - memory_profile6_log - INFO -    392    348.9 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 16:45:26,194 - memory_profile6_log - INFO -    393                                     ]

2018-04-29 16:45:26,196 - memory_profile6_log - INFO -    394                             

2018-04-29 16:45:26,201 - memory_profile6_log - INFO -    395    348.9 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-29 16:45:26,203 - memory_profile6_log - INFO -    396    419.7 MiB     70.8 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 16:45:26,204 - memory_profile6_log - INFO -    397    419.7 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 16:45:26,207 - memory_profile6_log - INFO -    398                             

2018-04-29 16:45:26,209 - memory_profile6_log - INFO -    399    419.7 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 16:45:26,214 - memory_profile6_log - INFO -    400    419.7 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 16:45:26,216 - memory_profile6_log - INFO -    401    419.7 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 16:45:26,217 - memory_profile6_log - INFO -    402    419.7 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 16:45:26,217 - memory_profile6_log - INFO -    403    419.7 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 16:45:26,220 - memory_profile6_log - INFO -    404    419.7 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 16:45:26,220 - memory_profile6_log - INFO -    405                             

2018-04-29 16:45:26,223 - memory_profile6_log - INFO -    406    419.7 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 16:45:26,227 - memory_profile6_log - INFO - 


2018-04-29 16:45:26,237 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 16:45:26,299 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 16:45:26,302 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-29 16:45:26,303 - memory_profile6_log - INFO - apply on: 5000 total history...)
2018-04-29 16:48:57,874 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 16:48:57,881 - memory_profile6_log - INFO - date_generated: 
2018-04-29 16:48:57,881 - memory_profile6_log - INFO -  
2018-04-29 16:48:57,881 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 16:48:57,884 - memory_profile6_log - INFO - 

2018-04-29 16:48:57,884 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 16:48:57,885 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 16:48:57,887 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 16:48:58,243 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 16:48:58,252 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 16:51:22,716 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 16:51:22,719 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 16:51:22,799 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 16:51:22,802 - memory_profile6_log - INFO - Appending history data...
2018-04-29 16:51:22,805 - memory_profile6_log - INFO - processing batch-0
2018-04-29 16:51:22,806 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:51:22,900 - memory_profile6_log - INFO - call history data...
2018-04-29 16:52:00,471 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:52:02,181 - memory_profile6_log - INFO - processing batch-1
2018-04-29 16:52:02,184 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:52:02,203 - memory_profile6_log - INFO - call history data...
2018-04-29 16:52:34,967 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:52:36,621 - memory_profile6_log - INFO - processing batch-2
2018-04-29 16:52:36,624 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:52:36,641 - memory_profile6_log - INFO - call history data...
2018-04-29 16:53:07,974 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:53:09,618 - memory_profile6_log - INFO - processing batch-3
2018-04-29 16:53:09,619 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:53:09,638 - memory_profile6_log - INFO - call history data...
2018-04-29 16:53:41,885 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:53:43,536 - memory_profile6_log - INFO - processing batch-4
2018-04-29 16:53:43,539 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:53:43,562 - memory_profile6_log - INFO - call history data...
2018-04-29 16:54:13,644 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:54:14,295 - memory_profile6_log - INFO - Appending training data...
2018-04-29 16:54:14,296 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 16:54:14,299 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 16:54:14,299 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 16:54:14,301 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 16:54:14,302 - memory_profile6_log - INFO - ================================================

2018-04-29 16:54:14,302 - memory_profile6_log - INFO -    311     86.5 MiB     86.5 MiB   @profile

2018-04-29 16:54:14,303 - memory_profile6_log - INFO -    312                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 16:54:14,306 - memory_profile6_log - INFO -    313     86.5 MiB      0.0 MiB       bq_client = client

2018-04-29 16:54:14,306 - memory_profile6_log - INFO -    314     86.5 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 16:54:14,309 - memory_profile6_log - INFO -    315                             

2018-04-29 16:54:14,309 - memory_profile6_log - INFO -    316     86.5 MiB      0.0 MiB       datalist = []

2018-04-29 16:54:14,311 - memory_profile6_log - INFO -    317     86.5 MiB      0.0 MiB       datalist_hist = []

2018-04-29 16:54:14,312 - memory_profile6_log - INFO -    318                             

2018-04-29 16:54:14,312 - memory_profile6_log - INFO -    319     86.5 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 16:54:14,312 - memory_profile6_log - INFO -    320    350.5 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 16:54:14,313 - memory_profile6_log - INFO -    321    338.8 MiB    252.3 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 16:54:14,316 - memory_profile6_log - INFO -    322    338.8 MiB      0.0 MiB           if tframe is not None:

2018-04-29 16:54:14,316 - memory_profile6_log - INFO -    323    338.8 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 16:54:14,318 - memory_profile6_log - INFO -    324    346.3 MiB      7.6 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 16:54:14,319 - memory_profile6_log - INFO -    325    346.3 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 16:54:14,319 - memory_profile6_log - INFO -    326    346.3 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 16:54:14,321 - memory_profile6_log - INFO -    327    350.4 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 16:54:14,321 - memory_profile6_log - INFO -    328                                                 # ~ loading history

2018-04-29 16:54:14,322 - memory_profile6_log - INFO -    329                                                 """

2018-04-29 16:54:14,322 - memory_profile6_log - INFO -    330                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 16:54:14,323 - memory_profile6_log - INFO -    331                                                 """

2018-04-29 16:54:14,325 - memory_profile6_log - INFO -    332    350.2 MiB      0.1 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 16:54:14,326 - memory_profile6_log - INFO -    333                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 16:54:14,328 - memory_profile6_log - INFO -    334    350.2 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 16:54:14,328 - memory_profile6_log - INFO -    335    350.2 MiB      1.1 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 16:54:14,329 - memory_profile6_log - INFO -    336                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 16:54:14,329 - memory_profile6_log - INFO -    337                             

2018-04-29 16:54:14,332 - memory_profile6_log - INFO -    338    350.2 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 16:54:14,332 - memory_profile6_log - INFO -    339    350.4 MiB      2.2 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 16:54:14,332 - memory_profile6_log - INFO -    340                             

2018-04-29 16:54:14,334 - memory_profile6_log - INFO -    341                                                 # me = os.getpid()

2018-04-29 16:54:14,338 - memory_profile6_log - INFO -    342                                                 # kill_proc_tree(me)

2018-04-29 16:54:14,338 - memory_profile6_log - INFO -    343                             

2018-04-29 16:54:14,342 - memory_profile6_log - INFO -    344    350.4 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 16:54:14,342 - memory_profile6_log - INFO -    345    350.4 MiB      0.0 MiB                       for m in h_frame:

2018-04-29 16:54:14,342 - memory_profile6_log - INFO -    346    350.4 MiB      0.0 MiB                           if m is not None:

2018-04-29 16:54:14,344 - memory_profile6_log - INFO -    347    350.4 MiB      0.0 MiB                               if len(m) > 0:

2018-04-29 16:54:14,345 - memory_profile6_log - INFO -    348    350.4 MiB      0.8 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 16:54:14,346 - memory_profile6_log - INFO -    349    350.4 MiB      0.0 MiB                       del h_frame

2018-04-29 16:54:14,348 - memory_profile6_log - INFO -    350    350.4 MiB      0.0 MiB                       del lhistory

2018-04-29 16:54:14,349 - memory_profile6_log - INFO -    351                             

2018-04-29 16:54:14,351 - memory_profile6_log - INFO -    352    350.5 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 16:54:14,351 - memory_profile6_log - INFO -    353    350.5 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 16:54:14,352 - memory_profile6_log - INFO -    354                                     else: 

2018-04-29 16:54:14,354 - memory_profile6_log - INFO -    355                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 16:54:14,355 - memory_profile6_log - INFO -    356    350.5 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 16:54:14,358 - memory_profile6_log - INFO -    357    350.5 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 16:54:14,358 - memory_profile6_log - INFO -    358                             

2018-04-29 16:54:14,359 - memory_profile6_log - INFO -    359    350.5 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 16:54:14,359 - memory_profile6_log - INFO - 


2018-04-29 16:54:15,466 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 16:54:15,536 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0   0.000646        1844.325967        16            1854.325967  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
1   0.000646        1844.325967        16            1854.325967  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
2   0.000646        1844.325967        32            1854.325967  10960288  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
3   0.000646        1844.325967        32            1854.325967  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
4   0.000646        1844.325967        32            1854.325967  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
5   0.000646        1844.325967        16            1854.325967  10960288  1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...
6        NaN      333823.000000       256          333833.000000  10959239  161129f951435-0d660a18dbcd87-173a7640-100200-1...
0   0.000646        1844.325967         8            1854.325967  10960288  161e219cd891c8-0e882943705df4-15290f5f-38400-1...
1   0.000646        1844.325967        16            1854.325967  10960288  1626fd160d9114-055d801ae6b39d-66265f05-38400-1...
2   0.000646        1844.325967         8            1854.325967  10960288  1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...
3   0.000646        1844.325967         8            1854.325967  10960288  16169731ce618-00742c5e307396-5b4d6a36-38400-16...
4   0.000646        1844.325967        16            1854.325967  10960288  161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...
5   0.000646        1844.325967        16            1854.325967  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
0   0.000646        1844.325967         8            1854.325967  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
1   0.000646        1844.325967         4            1854.325967  10960288  1612d88e55ab8-09af9658f30a46-65117b23-38400-16...
2   0.000646        1844.325967         8            1854.325967  10960288  16149f0a1611-00a034775-63446032-38400-16149f0a...
3   0.000646        1844.325967         4            1854.325967  10960288  16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...
4   0.000646        1844.325967         4            1854.325967  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
5   0.000646        1844.325967         4            1854.325967  10960288  16132f1045f30b-0613b72b33445e-5d640103-38400-1...
6   0.000646        1844.325967         4            1854.325967  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
2018-04-29 16:54:15,536 - memory_profile6_log - INFO - 

2018-04-29 16:54:15,546 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 16:54:15,621 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 16:54:15,634 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 16:55:04,036 - memory_profile6_log - INFO - size of df: 58.30 MB
2018-04-29 16:55:04,038 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-29 16:55:04,099 - memory_profile6_log - INFO - size of current_frame: 58.30 MB
2018-04-29 16:55:04,101 - memory_profile6_log - INFO - loading time of: 484420 total genuine-current interest data ~ take 365.913s
2018-04-29 16:55:04,111 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 16:55:04,112 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 16:55:04,114 - memory_profile6_log - INFO - ================================================

2018-04-29 16:55:04,115 - memory_profile6_log - INFO -    361     86.4 MiB     86.4 MiB   @profile

2018-04-29 16:55:04,117 - memory_profile6_log - INFO -    362                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 16:55:04,118 - memory_profile6_log - INFO -    363     86.5 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 16:55:04,121 - memory_profile6_log - INFO -    364     86.5 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 16:55:04,121 - memory_profile6_log - INFO -    365                             

2018-04-29 16:55:04,121 - memory_profile6_log - INFO -    366                                 # ~~~ Begin collecting data ~~~

2018-04-29 16:55:04,122 - memory_profile6_log - INFO -    367     86.5 MiB      0.0 MiB       t0 = time.time()

2018-04-29 16:55:04,124 - memory_profile6_log - INFO -    368    350.5 MiB    264.0 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 16:55:04,124 - memory_profile6_log - INFO -    369    350.5 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 16:55:04,125 - memory_profile6_log - INFO -    370                                     logger.info("Training cannot be empty..")

2018-04-29 16:55:04,127 - memory_profile6_log - INFO -    371                                     return False

2018-04-29 16:55:04,127 - memory_profile6_log - INFO -    372    349.1 MiB     -1.3 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 16:55:04,128 - memory_profile6_log - INFO -    373    349.3 MiB      0.2 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 16:55:04,128 - memory_profile6_log - INFO -    374    349.3 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 16:55:04,131 - memory_profile6_log - INFO -    375                             

2018-04-29 16:55:04,134 - memory_profile6_log - INFO -    376    355.1 MiB      5.8 MiB       big_frame = pd.concat(datalist)

2018-04-29 16:55:04,134 - memory_profile6_log - INFO -    377    355.1 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 16:55:04,135 - memory_profile6_log - INFO -    378    349.3 MiB     -5.8 MiB       del datalist

2018-04-29 16:55:04,138 - memory_profile6_log - INFO -    379                             

2018-04-29 16:55:04,138 - memory_profile6_log - INFO -    380    349.3 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 16:55:04,140 - memory_profile6_log - INFO -    381                             

2018-04-29 16:55:04,141 - memory_profile6_log - INFO -    382                                 # ~ get current news interest ~

2018-04-29 16:55:04,142 - memory_profile6_log - INFO -    383    349.3 MiB      0.0 MiB       if not cd:

2018-04-29 16:55:04,144 - memory_profile6_log - INFO -    384                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 16:55:04,144 - memory_profile6_log - INFO -    385                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 16:55:04,144 - memory_profile6_log - INFO -    386                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 16:55:04,145 - memory_profile6_log - INFO -    387                                 else:

2018-04-29 16:55:04,147 - memory_profile6_log - INFO -    388    349.3 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 16:55:04,147 - memory_profile6_log - INFO -    389    349.3 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 16:55:04,148 - memory_profile6_log - INFO -    390                             

2018-04-29 16:55:04,148 - memory_profile6_log - INFO -    391                                     # safe handling of query parameter

2018-04-29 16:55:04,148 - memory_profile6_log - INFO -    392                                     query_params = [

2018-04-29 16:55:04,148 - memory_profile6_log - INFO -    393    349.3 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 16:55:04,150 - memory_profile6_log - INFO -    394                                     ]

2018-04-29 16:55:04,154 - memory_profile6_log - INFO -    395                             

2018-04-29 16:55:04,154 - memory_profile6_log - INFO -    396    349.3 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-29 16:55:04,155 - memory_profile6_log - INFO -    397    420.1 MiB     70.8 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 16:55:04,157 - memory_profile6_log - INFO -    398    420.1 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 16:55:04,157 - memory_profile6_log - INFO -    399                             

2018-04-29 16:55:04,157 - memory_profile6_log - INFO -    400    420.1 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 16:55:04,157 - memory_profile6_log - INFO -    401    420.1 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 16:55:04,160 - memory_profile6_log - INFO -    402    420.1 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 16:55:04,161 - memory_profile6_log - INFO -    403    420.1 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 16:55:04,161 - memory_profile6_log - INFO -    404    420.1 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 16:55:04,161 - memory_profile6_log - INFO -    405    420.1 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 16:55:04,164 - memory_profile6_log - INFO -    406                             

2018-04-29 16:55:04,164 - memory_profile6_log - INFO -    407    420.1 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 16:55:04,168 - memory_profile6_log - INFO - 


2018-04-29 16:55:04,171 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 16:55:04,200 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 16:55:04,200 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-29 16:55:04,203 - memory_profile6_log - INFO - apply on: 5000 total history...)
2018-04-29 16:55:04,428 - memory_profile6_log - INFO - len of uniques_fit_hist: 
2018-04-29 16:55:04,430 - memory_profile6_log - INFO -  
2018-04-29 16:55:04,431 - memory_profile6_log - INFO - 5000
2018-04-29 16:55:04,434 - memory_profile6_log - INFO - 

2018-04-29 16:55:04,436 - memory_profile6_log - INFO - uniques_fit_hist:

2018-04-29 16:55:04,509 - memory_profile6_log - INFO -                                               user_id  sigma_Nt
0   16290be16f412b-0b174766972ba1-78227b65-38400-1...        16
1   1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...        16
2   161890d29c1d5-0e69b9c4731723-2406423c-38400-16...        32
3   1612da17a01324-0b4ae4e577dca7-58596970-38400-1...        32
4   16140595aef60-0f425b909e43ea-5768397b-100200-1...        32
5   1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...        16
6   161129f951435-0d660a18dbcd87-173a7640-100200-1...       256
0   161e219cd891c8-0e882943705df4-15290f5f-38400-1...         8
1   1626fd160d9114-055d801ae6b39d-66265f05-38400-1...        16
2   1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...         8
3   16169731ce618-00742c5e307396-5b4d6a36-38400-16...         8
4   161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...        16
5   1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...        16
0   1613f4d321b10-00841d4071241b-42686c23-38400-16...         8
1   1612d88e55ab8-09af9658f30a46-65117b23-38400-16...         4
2   16149f0a1611-00a034775-63446032-38400-16149f0a...         8
3   16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...         4
4   16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...         4
5   16132f1045f30b-0613b72b33445e-5d640103-38400-1...         4
6   1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...         4
0   1610ca98d49149-0e61ac652d8ef-4323461-100200-16...       471
1   16273bde6812b-084c1eff4c3363-42584954-38400-16...        33
2   161e5a454f6161-064229b4591715-61643d25-38400-1...         4
3   1626193fe8054-0dba227898b329-75650b03-38400-16...         4
4   1621f5b8e9b1-0128731ade18b5-2e483333-38400-162...        66
5   1624301817b205-0f6738c6297604-684f2d56-38400-1...         4
6   1612e03d3fd5d-08eab60d8f3d32-a4e5b49-38400-161...        33
0   16257d034e22b8-0cb575c67a4be78-1d451b27-fa000-...        15
1   1621e49418f49-0d6f8597daa543-4b496639-38400-16...        52
2   1612e8ce767c6-0e0b4eacdd5b13-45464657-38400-16...        15
..                                                ...       ...
3   1612fb1a15735-020f35175f6dd5-45654c3a-38400-16...        43
4   1613186a6bebe-0899e00791177-2c18273a-38400-161...        30
5   161d23e558715-09f1e96db21b58-1e061919-38400-16...        41
0   16141b03760d-0228f75551037a-726b623e-38400-161...        42
1   161376faec61f-0d2c1d9ede3adc-5c617622-38400-16...        44
2   161d1b2e498308-0f48d8032a1c19-2620205a-3d10d-1...        15
3   1612d5b869d12-02f1a1eaffe4d-4b643e7e-38400-161...        17
4   1613fdfbed91a2-04890805f9eed18-6f30253b-2c600-...        18
5   1613a92918a8-0d33d223cef553-114e2d5d-64140-161...        19
6   16131273eeea55-0bc3d7d469b3f-15297b10-3d10d-16...        21
0   1616ad1a959555-0d9ab606c6b589-453c4d32-4a640-1...        22
1   16141c4bcefc9-0958eed353b901-646e0a3e-38400-16...        22
2   1612da9b46c118-0b9b0791246fad-4b476b3b-38400-1...        22
3   161422bc4861f-052e3f9302d3b3-7d26745b-38400-16...        24
4   1612d14cd3653-0e00119350fb19-7703023a-38400-16...        46
5   16183eaa8bb80-03fa1ac4c491db-592b3b5c-38400-16...        24
6   16155d99105143-05bfbfdb8-6a697342-38400-16155d...        23
0   1612dc9c1d04e-0b4b0ed06c0eff-4d60643c-29b80-16...        14
1   161dc51f5b82-0c490fa7e3be9f-16702366-38400-161...        25
2   161325c8c3f26-04c492744c92c4-6e38331d-2c880-16...        14
3   1615a3074e06-0ae8be715-f400144-38400-1615a3074e3e        14
4   161413ffbfa100-01159c226cdab5-f465175-38400-16...        14
5   1614be2c2f30-01d03541d14aec-50655a08-38400-161...        14
0   162418d41739d-07c0823ef9d4a58-2f222d58-2c600-1...        25
1   1614aa25de1609-0d17261ba945a3-33b0722-4a640-16...        10
2   1619caa77db166-03b18ccc6-6e5d1c4a-38400-1619ca...        10
3   1613fba31b8c3-0690745c06a243-6e55342d-38400-16...        37
4   1612d2b4f371a-024ccb56cb028d-22a5812-38400-161...        64
5   1612db724803d-0872de26b-59712a6c-38400-1612db7...        29
6   1612d64081b0-028625bf07ab7-5978623d-38400-1612...        20

[5000 rows x 2 columns]
2018-04-29 16:55:04,510 - memory_profile6_log - INFO - 

2018-04-29 16:55:04,517 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate: 
2018-04-29 16:55:04,519 - memory_profile6_log - INFO -  
2018-04-29 16:55:04,523 - memory_profile6_log - INFO - 3753
2018-04-29 16:55:04,528 - memory_profile6_log - INFO - 

2018-04-29 16:55:04,529 - memory_profile6_log - INFO - uniques_fit_hist:

2018-04-29 16:55:04,601 - memory_profile6_log - INFO -                                               user_id  sigma_Nt
0   16290be16f412b-0b174766972ba1-78227b65-38400-1...        16
1   1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...        16
2   161890d29c1d5-0e69b9c4731723-2406423c-38400-16...        32
3   1612da17a01324-0b4ae4e577dca7-58596970-38400-1...        32
4   16140595aef60-0f425b909e43ea-5768397b-100200-1...        32
5   1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...        16
6   161129f951435-0d660a18dbcd87-173a7640-100200-1...       256
0   161e219cd891c8-0e882943705df4-15290f5f-38400-1...         8
1   1626fd160d9114-055d801ae6b39d-66265f05-38400-1...        16
2   1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...         8
3   16169731ce618-00742c5e307396-5b4d6a36-38400-16...         8
4   161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...        16
5   1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...        16
0   1613f4d321b10-00841d4071241b-42686c23-38400-16...         8
1   1612d88e55ab8-09af9658f30a46-65117b23-38400-16...         4
2   16149f0a1611-00a034775-63446032-38400-16149f0a...         8
3   16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...         4
4   16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...         4
5   16132f1045f30b-0613b72b33445e-5d640103-38400-1...         4
6   1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...         4
0   1610ca98d49149-0e61ac652d8ef-4323461-100200-16...       471
1   16273bde6812b-084c1eff4c3363-42584954-38400-16...        33
2   161e5a454f6161-064229b4591715-61643d25-38400-1...         4
3   1626193fe8054-0dba227898b329-75650b03-38400-16...         4
4   1621f5b8e9b1-0128731ade18b5-2e483333-38400-162...        66
5   1624301817b205-0f6738c6297604-684f2d56-38400-1...         4
6   1612e03d3fd5d-08eab60d8f3d32-a4e5b49-38400-161...        33
0   16257d034e22b8-0cb575c67a4be78-1d451b27-fa000-...        15
1   1621e49418f49-0d6f8597daa543-4b496639-38400-16...        52
2   1612e8ce767c6-0e0b4eacdd5b13-45464657-38400-16...        15
..                                                ...       ...
2   1617393822170-0072ba8096a026-4507440e-2c880-16...         8
4   16258005d03d1-07d7d0a4afb7c5-6c585013-38400-16...         8
5   161471adf26c7-03b31f4e1da6d7-7933452b-49a10-16...         8
2   1612d9378ceb1-0fccf82b476fd9-282b543f-38400-16...         4
3   1613830c19746-0bbc73dc2-10157579-38400-1613830...         4
2   161494230c766-07e2dfc2f38f0b-6f337f6d-38400-16...        33
3   16137ace4ed1ff-0a7c55846c7e79-59706a3f-38400-1...        30
0   16131e09fdbb6-08b5ba401ad8d8-4346500b-38400-16...        26
4   1617f7e7d6765-0df630faf-410a670f-38400-1617f7e...        13
2   161477a820960-0456909f230cbc-5b65171a-38400-16...        13
6   1613028a106128-0169ca14063a61-58596970-38400-1...        13
0   1627b8912298c-05b1a0e9ebe016-6149071e-96000-16...        13
1   1615fdf1b7b305-0bab7f28a269c68-6b7e027f-3d10d-...        13
4   162000bced2108-0b3e765f648e59-5e615b69-38400-1...        13
3   1612d396ff3b9-09cc160072e2ae-524a6333-55188-16...         9
0   1612fb32d0db3-05006fe1eb0964-f4b0b15-38400-161...        22
2   16266151da687-05fe10a72-221f3b01-3d10d-1626615...        22
5   1616a422ea2a3-08ebd602631f82-47144065-38400-16...        11
1   162a542b04319f-03ce6f10c22dd6-7c277a75-49a10-1...        12
2   1613c72d67cf8-00ac6c573a9be6-13365f51-38400-16...        12
6   16249414f9719b-0593b54318f6e9-5e4d6f38-38400-1...        12
6   1612d9c84d05b-078aa885ec464-1456653c-38400-161...        92
4   161874b1a4414-0f718a13161522-124e2a53-38400-16...        27
1   1618eeefe1e9b-0595b2e4c-3730a44-38400-1618eeef...        62
4   1612cf2255851-02c696926-69287075-38400-1612cf2...        25
3   1619c7df013112-07d824e63cc2db-204a2f45-38400-1...        56
5   161318a0b7bbb-061acaca0b5b7e-362a0770-38400-16...        14
5   16270725c4fa8-0d14bbbfae6e24-67265a0f-38400-16...        20
3   16242598f9a188-0e5551733cd82e-5e426d3d-38400-1...        10
4   1612d2b4f371a-024ccb56cb028d-22a5812-38400-161...        64

[3753 rows x 2 columns]
2018-04-29 16:55:04,604 - memory_profile6_log - INFO - 

2018-04-29 16:55:05,443 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 16:55:05,444 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 16:55:05,663 - memory_profile6_log - INFO - model_fit dtypes:

2018-04-29 16:55:05,668 - memory_profile6_log - INFO - date              datetime64[ns, UTC]
user_id                        object
topic_id                       object
num_x                           int64
num_y                           int64
is_general                       bool
date_all_click                  int64
Ntotal                          int64
sigma_Nt                        int64
joinprob_ci                   float64
p_cat_ci                      float64
posterior                     float64
dtype: object
2018-04-29 16:55:05,671 - memory_profile6_log - INFO - 

2018-04-29 16:55:05,671 - memory_profile6_log - INFO - fitted_models_hist dtypes:

2018-04-29 16:55:05,677 - memory_profile6_log - INFO - p0_cat_ci                float64
pt_posterior_x_Nt        float64
sigma_Nt                   int64
smoothed_pt_posterior    float64
topic_id                  object
user_id                   object
dtype: object
2018-04-29 16:55:05,680 - memory_profile6_log - INFO - 

2018-04-29 16:55:06,213 - memory_profile6_log - INFO - Len of fitted_models on main class: 253995
2018-04-29 16:55:06,213 - memory_profile6_log - INFO - 

2018-04-29 16:55:06,216 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-29 16:55:06,217 - memory_profile6_log - INFO - 

2018-04-29 16:55:06,219 - memory_profile6_log - INFO - Fitted models before concat:

2018-04-29 16:55:06,244 - memory_profile6_log - INFO -                                 user_id    topic_id  pt_posterior_x_Nt  p0_cat_ci  smoothed_pt_posterior
0  107e02cd-54af-40fd-a602-e6105250ae8c  1046474724        1902.125356   0.000090            1912.125356
1  107e02cd-54af-40fd-a602-e6105250ae8c  1060896301        1346.060484   0.000065            1356.060484
2  107e02cd-54af-40fd-a602-e6105250ae8c   188400148         144.387111   0.036200             154.387111
3  107e02cd-54af-40fd-a602-e6105250ae8c    22553543          71.961844   0.107319              81.961844
4  107e02cd-54af-40fd-a602-e6105250ae8c    22596701         122.085700   0.040442             132.085700
2018-04-29 16:55:06,246 - memory_profile6_log - INFO - 

2018-04-29 16:55:06,246 - memory_profile6_log - INFO - len of current fitted models: 253995
2018-04-29 16:55:06,247 - memory_profile6_log - INFO - 

2018-04-29 16:55:06,249 - memory_profile6_log - INFO - len of history fitted models: 5000
2018-04-29 16:55:06,250 - memory_profile6_log - INFO - 

2018-04-29 16:55:06,289 - memory_profile6_log - INFO - len of fitted models after concat: 258995
2018-04-29 16:55:06,292 - memory_profile6_log - INFO - 

2018-04-29 16:55:06,292 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-29 16:55:06,293 - memory_profile6_log - INFO - 

2018-04-29 17:05:49,724 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 17:05:49,727 - memory_profile6_log - INFO - date_generated: 
2018-04-29 17:05:49,729 - memory_profile6_log - INFO -  
2018-04-29 17:05:49,729 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 17:05:49,729 - memory_profile6_log - INFO - 

2018-04-29 17:05:49,729 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 17:05:49,730 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 17:05:49,730 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 17:05:49,852 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 17:05:49,857 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 17:06:59,009 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 17:06:59,012 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 17:06:59,053 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 17:06:59,055 - memory_profile6_log - INFO - Appending history data...
2018-04-29 17:06:59,056 - memory_profile6_log - INFO - processing batch-0
2018-04-29 17:06:59,056 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:06:59,095 - memory_profile6_log - INFO - call history data...
2018-04-29 17:07:26,825 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:07:27,505 - memory_profile6_log - INFO - processing batch-1
2018-04-29 17:07:27,506 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:07:27,513 - memory_profile6_log - INFO - call history data...
2018-04-29 17:07:56,138 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:07:56,802 - memory_profile6_log - INFO - processing batch-2
2018-04-29 17:07:56,803 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:07:56,809 - memory_profile6_log - INFO - call history data...
2018-04-29 17:08:24,138 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:08:24,789 - memory_profile6_log - INFO - processing batch-3
2018-04-29 17:08:24,790 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:08:24,799 - memory_profile6_log - INFO - call history data...
2018-04-29 17:08:55,641 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:08:56,342 - memory_profile6_log - INFO - processing batch-4
2018-04-29 17:08:56,345 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:08:56,352 - memory_profile6_log - INFO - call history data...
2018-04-29 17:09:29,720 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:09:30,401 - memory_profile6_log - INFO - Appending training data...
2018-04-29 17:09:30,401 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 17:09:30,404 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 17:09:30,405 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 17:09:30,407 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 17:09:30,407 - memory_profile6_log - INFO - ================================================

2018-04-29 17:09:30,408 - memory_profile6_log - INFO -    309     86.7 MiB     86.7 MiB   @profile

2018-04-29 17:09:30,410 - memory_profile6_log - INFO -    310                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 17:09:30,411 - memory_profile6_log - INFO -    311     86.7 MiB      0.0 MiB       bq_client = client

2018-04-29 17:09:30,413 - memory_profile6_log - INFO -    312     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 17:09:30,414 - memory_profile6_log - INFO -    313                             

2018-04-29 17:09:30,414 - memory_profile6_log - INFO -    314     86.7 MiB      0.0 MiB       datalist = []

2018-04-29 17:09:30,415 - memory_profile6_log - INFO -    315     86.7 MiB      0.0 MiB       datalist_hist = []

2018-04-29 17:09:30,417 - memory_profile6_log - INFO -    316                             

2018-04-29 17:09:30,417 - memory_profile6_log - INFO -    317     86.7 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 17:09:30,418 - memory_profile6_log - INFO -    318    351.4 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 17:09:30,420 - memory_profile6_log - INFO -    319    338.8 MiB    252.1 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 17:09:30,421 - memory_profile6_log - INFO -    320    338.8 MiB      0.0 MiB           if tframe is not None:

2018-04-29 17:09:30,423 - memory_profile6_log - INFO -    321    338.8 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 17:09:30,424 - memory_profile6_log - INFO -    322    347.1 MiB      8.3 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 17:09:30,424 - memory_profile6_log - INFO -    323    347.1 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 17:09:30,426 - memory_profile6_log - INFO -    324    347.1 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 17:09:30,426 - memory_profile6_log - INFO -    325    351.4 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 17:09:30,427 - memory_profile6_log - INFO -    326                                                 # ~ loading history

2018-04-29 17:09:30,427 - memory_profile6_log - INFO -    327                                                 """

2018-04-29 17:09:30,430 - memory_profile6_log - INFO -    328                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 17:09:30,430 - memory_profile6_log - INFO -    329                                                 """

2018-04-29 17:09:30,434 - memory_profile6_log - INFO -    330    351.2 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 17:09:30,436 - memory_profile6_log - INFO -    331                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 17:09:30,437 - memory_profile6_log - INFO -    332    351.2 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 17:09:30,437 - memory_profile6_log - INFO -    333    351.2 MiB      0.5 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 17:09:30,438 - memory_profile6_log - INFO -    334                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 17:09:30,440 - memory_profile6_log - INFO -    335                             

2018-04-29 17:09:30,440 - memory_profile6_log - INFO -    336    351.2 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 17:09:30,441 - memory_profile6_log - INFO -    337    351.3 MiB      3.1 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 17:09:30,443 - memory_profile6_log - INFO -    338                             

2018-04-29 17:09:30,446 - memory_profile6_log - INFO -    339                                                 # me = os.getpid()

2018-04-29 17:09:30,447 - memory_profile6_log - INFO -    340                                                 # kill_proc_tree(me)

2018-04-29 17:09:30,447 - memory_profile6_log - INFO -    341                             

2018-04-29 17:09:30,448 - memory_profile6_log - INFO -    342    351.3 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 17:09:30,450 - memory_profile6_log - INFO -    343    351.4 MiB     -0.1 MiB                       for m in h_frame:

2018-04-29 17:09:30,450 - memory_profile6_log - INFO -    344    351.4 MiB     -0.1 MiB                           if m is not None:

2018-04-29 17:09:30,451 - memory_profile6_log - INFO -    345    351.4 MiB     -0.1 MiB                               if len(m) > 0:

2018-04-29 17:09:30,453 - memory_profile6_log - INFO -    346    351.4 MiB      0.6 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 17:09:30,453 - memory_profile6_log - INFO -    347    351.4 MiB      0.0 MiB                       del h_frame

2018-04-29 17:09:30,456 - memory_profile6_log - INFO -    348    351.4 MiB      0.0 MiB                       del lhistory

2018-04-29 17:09:30,457 - memory_profile6_log - INFO -    349                             

2018-04-29 17:09:30,459 - memory_profile6_log - INFO -    350    351.4 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 17:09:30,459 - memory_profile6_log - INFO -    351    351.4 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 17:09:30,460 - memory_profile6_log - INFO -    352                                     else: 

2018-04-29 17:09:30,460 - memory_profile6_log - INFO -    353                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 17:09:30,461 - memory_profile6_log - INFO -    354    351.4 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 17:09:30,461 - memory_profile6_log - INFO -    355    351.4 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 17:09:30,463 - memory_profile6_log - INFO -    356                             

2018-04-29 17:09:30,464 - memory_profile6_log - INFO -    357    351.4 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 17:09:30,466 - memory_profile6_log - INFO - 


2018-04-29 17:09:31,588 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 17:09:31,661 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0        NaN      333823.000000       256          333833.000000  10959239  161129f951435-0d660a18dbcd87-173a7640-100200-1...
1   0.000646        1844.325967        16            1854.325967  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
2   0.000646        1844.325967        16            1854.325967  10960288  1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...
3   0.000646        1844.325967        32            1854.325967  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
4   0.000646        1844.325967        32            1854.325967  10960288  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
5   0.000646        1844.325967        16            1854.325967  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
6   0.000646        1844.325967        32            1854.325967  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
0   0.000646        1844.325967        16            1854.325967  10960288  161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...
1   0.000646        1844.325967        16            1854.325967  10960288  1626fd160d9114-055d801ae6b39d-66265f05-38400-1...
2   0.000646        1844.325967         8            1854.325967  10960288  161e219cd891c8-0e882943705df4-15290f5f-38400-1...
3   0.000646        1844.325967         8            1854.325967  10960288  16169731ce618-00742c5e307396-5b4d6a36-38400-16...
4   0.000646        1844.325967        16            1854.325967  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
5   0.000646        1844.325967         8            1854.325967  10960288  1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...
0   0.000646        1844.325967         4            1854.325967  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
1   0.000646        1844.325967         4            1854.325967  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
2   0.000646        1844.325967         8            1854.325967  10960288  16149f0a1611-00a034775-63446032-38400-16149f0a...
3   0.000646        1844.325967         8            1854.325967  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
4   0.000646        1844.325967         4            1854.325967  10960288  16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...
5   0.000646        1844.325967         4            1854.325967  10960288  16132f1045f30b-0613b72b33445e-5d640103-38400-1...
6   0.000646        1844.325967         4            1854.325967  10960288  1612d88e55ab8-09af9658f30a46-65117b23-38400-16...
2018-04-29 17:09:31,661 - memory_profile6_log - INFO - 

2018-04-29 17:09:31,671 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 17:09:31,742 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 17:09:31,756 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 17:10:20,163 - memory_profile6_log - INFO - size of df: 58.30 MB
2018-04-29 17:10:20,164 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-29 17:10:20,230 - memory_profile6_log - INFO - size of current_frame: 58.30 MB
2018-04-29 17:10:20,230 - memory_profile6_log - INFO - loading time of: 484420 total genuine-current interest data ~ take 270.399s
2018-04-29 17:10:20,237 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 17:10:20,239 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 17:10:20,240 - memory_profile6_log - INFO - ================================================

2018-04-29 17:10:20,243 - memory_profile6_log - INFO -    359     86.6 MiB     86.6 MiB   @profile

2018-04-29 17:10:20,244 - memory_profile6_log - INFO -    360                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 17:10:20,246 - memory_profile6_log - INFO -    361     86.7 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 17:10:20,246 - memory_profile6_log - INFO -    362     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 17:10:20,247 - memory_profile6_log - INFO -    363                             

2018-04-29 17:10:20,247 - memory_profile6_log - INFO -    364                                 # ~~~ Begin collecting data ~~~

2018-04-29 17:10:20,249 - memory_profile6_log - INFO -    365     86.7 MiB      0.0 MiB       t0 = time.time()

2018-04-29 17:10:20,250 - memory_profile6_log - INFO -    366    351.4 MiB    264.7 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 17:10:20,252 - memory_profile6_log - INFO -    367    351.4 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 17:10:20,253 - memory_profile6_log - INFO -    368                                     logger.info("Training cannot be empty..")

2018-04-29 17:10:20,253 - memory_profile6_log - INFO -    369                                     return False

2018-04-29 17:10:20,255 - memory_profile6_log - INFO -    370    348.0 MiB     -3.3 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 17:10:20,255 - memory_profile6_log - INFO -    371    348.1 MiB      0.1 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 17:10:20,256 - memory_profile6_log - INFO -    372    348.1 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 17:10:20,256 - memory_profile6_log - INFO -    373                             

2018-04-29 17:10:20,256 - memory_profile6_log - INFO -    374    353.9 MiB      5.8 MiB       big_frame = pd.concat(datalist)

2018-04-29 17:10:20,256 - memory_profile6_log - INFO -    375    353.9 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 17:10:20,257 - memory_profile6_log - INFO -    376    348.1 MiB     -5.8 MiB       del datalist

2018-04-29 17:10:20,259 - memory_profile6_log - INFO -    377                             

2018-04-29 17:10:20,259 - memory_profile6_log - INFO -    378    348.1 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 17:10:20,263 - memory_profile6_log - INFO -    379                             

2018-04-29 17:10:20,263 - memory_profile6_log - INFO -    380                                 # ~ get current news interest ~

2018-04-29 17:10:20,265 - memory_profile6_log - INFO -    381    348.1 MiB      0.0 MiB       if not cd:

2018-04-29 17:10:20,266 - memory_profile6_log - INFO -    382                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 17:10:20,266 - memory_profile6_log - INFO -    383                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 17:10:20,266 - memory_profile6_log - INFO -    384                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 17:10:20,269 - memory_profile6_log - INFO -    385                                 else:

2018-04-29 17:10:20,269 - memory_profile6_log - INFO -    386    348.1 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 17:10:20,270 - memory_profile6_log - INFO -    387    348.1 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 17:10:20,270 - memory_profile6_log - INFO -    388                             

2018-04-29 17:10:20,275 - memory_profile6_log - INFO -    389                                     # safe handling of query parameter

2018-04-29 17:10:20,276 - memory_profile6_log - INFO -    390                                     query_params = [

2018-04-29 17:10:20,276 - memory_profile6_log - INFO -    391    348.1 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 17:10:20,279 - memory_profile6_log - INFO -    392                                     ]

2018-04-29 17:10:20,279 - memory_profile6_log - INFO -    393                             

2018-04-29 17:10:20,279 - memory_profile6_log - INFO -    394    348.1 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-29 17:10:20,279 - memory_profile6_log - INFO -    395    419.3 MiB     71.1 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 17:10:20,279 - memory_profile6_log - INFO -    396    419.3 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 17:10:20,280 - memory_profile6_log - INFO -    397                             

2018-04-29 17:10:20,280 - memory_profile6_log - INFO -    398    419.3 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 17:10:20,280 - memory_profile6_log - INFO -    399    419.3 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 17:10:20,282 - memory_profile6_log - INFO -    400    419.3 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 17:10:20,282 - memory_profile6_log - INFO -    401    419.3 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 17:10:20,282 - memory_profile6_log - INFO -    402    419.3 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 17:10:20,285 - memory_profile6_log - INFO -    403    419.3 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 17:10:20,286 - memory_profile6_log - INFO -    404                             

2018-04-29 17:10:20,288 - memory_profile6_log - INFO -    405    419.3 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 17:10:20,290 - memory_profile6_log - INFO - 


2018-04-29 17:10:20,293 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 17:10:20,325 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 17:10:20,325 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-29 17:10:20,326 - memory_profile6_log - INFO - apply on: 5000 total history...)
2018-04-29 17:10:20,562 - memory_profile6_log - INFO - len of uniques_fit_hist:5000
2018-04-29 17:10:20,571 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:3753
2018-04-29 17:10:21,431 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 17:10:21,433 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 17:10:21,644 - memory_profile6_log - INFO - model_fit dtypes:

2018-04-29 17:10:21,650 - memory_profile6_log - INFO - date              datetime64[ns, UTC]
user_id                        object
topic_id                       object
num_x                           int64
num_y                           int64
is_general                       bool
date_all_click                  int64
Ntotal                          int64
sigma_Nt                        int64
joinprob_ci                   float64
p_cat_ci                      float64
posterior                     float64
dtype: object
2018-04-29 17:10:21,651 - memory_profile6_log - INFO - 

2018-04-29 17:10:21,651 - memory_profile6_log - INFO - fitted_models_hist dtypes:

2018-04-29 17:10:21,657 - memory_profile6_log - INFO - p0_cat_ci                float64
pt_posterior_x_Nt        float64
sigma_Nt                   int64
smoothed_pt_posterior    float64
topic_id                  object
user_id                   object
dtype: object
2018-04-29 17:10:21,658 - memory_profile6_log - INFO - 

2018-04-29 17:10:22,230 - memory_profile6_log - INFO - Len of fitted_models on main class: 253995
2018-04-29 17:10:22,230 - memory_profile6_log - INFO - 

2018-04-29 17:10:22,232 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-29 17:10:22,233 - memory_profile6_log - INFO - 

2018-04-29 17:10:22,234 - memory_profile6_log - INFO - Fitted models before concat:

2018-04-29 17:10:22,259 - memory_profile6_log - INFO -                                 user_id    topic_id  pt_posterior_x_Nt  p0_cat_ci  smoothed_pt_posterior
0  107e02cd-54af-40fd-a602-e6105250ae8c  1046474724         951.062678   0.000090             961.062678
1  107e02cd-54af-40fd-a602-e6105250ae8c  1060896301         673.030242   0.000065             683.030242
2  107e02cd-54af-40fd-a602-e6105250ae8c   188400148          72.193555   0.036200              82.193555
3  107e02cd-54af-40fd-a602-e6105250ae8c    22553543          35.980922   0.107319              45.980922
4  107e02cd-54af-40fd-a602-e6105250ae8c    22596701          61.042850   0.040442              71.042850
2018-04-29 17:10:22,260 - memory_profile6_log - INFO - 

2018-04-29 17:10:22,262 - memory_profile6_log - INFO - len of current fitted models: 253995
2018-04-29 17:10:22,263 - memory_profile6_log - INFO - 

2018-04-29 17:10:22,263 - memory_profile6_log - INFO - len of history fitted models: 5000
2018-04-29 17:10:22,265 - memory_profile6_log - INFO - 

2018-04-29 17:10:22,305 - memory_profile6_log - INFO - len of fitted models after concat: 258995
2018-04-29 17:10:22,306 - memory_profile6_log - INFO - 

2018-04-29 17:10:22,308 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-29 17:10:22,309 - memory_profile6_log - INFO - 

2018-04-29 17:10:22,466 - memory_profile6_log - INFO - Simplifying current fitted models...
2018-04-29 17:10:22,467 - memory_profile6_log - INFO - 

2018-04-29 17:15:48,315 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 17:15:48,318 - memory_profile6_log - INFO - date_generated: 
2018-04-29 17:15:48,319 - memory_profile6_log - INFO -  
2018-04-29 17:15:48,319 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 17:15:48,319 - memory_profile6_log - INFO - 

2018-04-29 17:15:48,321 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 17:15:48,322 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 17:15:48,322 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 17:15:48,459 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 17:15:48,461 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 17:16:55,750 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 17:16:55,752 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 17:16:55,798 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 17:16:55,799 - memory_profile6_log - INFO - Appending history data...
2018-04-29 17:16:55,799 - memory_profile6_log - INFO - processing batch-0
2018-04-29 17:16:55,802 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:16:55,845 - memory_profile6_log - INFO - call history data...
2018-04-29 17:17:23,976 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:17:24,654 - memory_profile6_log - INFO - processing batch-1
2018-04-29 17:17:24,654 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:17:24,664 - memory_profile6_log - INFO - call history data...
2018-04-29 17:17:51,801 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:17:52,447 - memory_profile6_log - INFO - processing batch-2
2018-04-29 17:17:52,447 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:17:52,456 - memory_profile6_log - INFO - call history data...
2018-04-29 17:18:19,555 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:18:20,265 - memory_profile6_log - INFO - processing batch-3
2018-04-29 17:18:20,266 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:18:20,273 - memory_profile6_log - INFO - call history data...
2018-04-29 17:18:50,697 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:18:51,542 - memory_profile6_log - INFO - processing batch-4
2018-04-29 17:18:51,542 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:18:51,551 - memory_profile6_log - INFO - call history data...
2018-04-29 17:19:25,858 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:19:26,565 - memory_profile6_log - INFO - Appending training data...
2018-04-29 17:19:26,566 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 17:19:26,569 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 17:19:26,571 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 17:19:26,572 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 17:19:26,572 - memory_profile6_log - INFO - ================================================

2018-04-29 17:19:26,573 - memory_profile6_log - INFO -    309     86.9 MiB     86.9 MiB   @profile

2018-04-29 17:19:26,575 - memory_profile6_log - INFO -    310                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 17:19:26,578 - memory_profile6_log - INFO -    311     86.9 MiB      0.0 MiB       bq_client = client

2018-04-29 17:19:26,578 - memory_profile6_log - INFO -    312     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 17:19:26,581 - memory_profile6_log - INFO -    313                             

2018-04-29 17:19:26,582 - memory_profile6_log - INFO -    314     86.9 MiB      0.0 MiB       datalist = []

2018-04-29 17:19:26,582 - memory_profile6_log - INFO -    315     86.9 MiB      0.0 MiB       datalist_hist = []

2018-04-29 17:19:26,584 - memory_profile6_log - INFO -    316                             

2018-04-29 17:19:26,584 - memory_profile6_log - INFO -    317     86.9 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 17:19:26,585 - memory_profile6_log - INFO -    318    350.8 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 17:19:26,588 - memory_profile6_log - INFO -    319    338.5 MiB    251.6 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 17:19:26,588 - memory_profile6_log - INFO -    320    338.5 MiB      0.0 MiB           if tframe is not None:

2018-04-29 17:19:26,589 - memory_profile6_log - INFO -    321    338.5 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 17:19:26,589 - memory_profile6_log - INFO -    322    346.8 MiB      8.3 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 17:19:26,592 - memory_profile6_log - INFO -    323    346.8 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 17:19:26,592 - memory_profile6_log - INFO -    324    346.8 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 17:19:26,592 - memory_profile6_log - INFO -    325    350.8 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 17:19:26,595 - memory_profile6_log - INFO -    326                                                 # ~ loading history

2018-04-29 17:19:26,595 - memory_profile6_log - INFO -    327                                                 """

2018-04-29 17:19:26,596 - memory_profile6_log - INFO -    328                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 17:19:26,598 - memory_profile6_log - INFO -    329                                                 """

2018-04-29 17:19:26,599 - memory_profile6_log - INFO -    330    350.7 MiB      0.1 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 17:19:26,601 - memory_profile6_log - INFO -    331                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 17:19:26,601 - memory_profile6_log - INFO -    332    350.7 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 17:19:26,604 - memory_profile6_log - INFO -    333    350.7 MiB      0.5 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 17:19:26,605 - memory_profile6_log - INFO -    334                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 17:19:26,605 - memory_profile6_log - INFO -    335                             

2018-04-29 17:19:26,611 - memory_profile6_log - INFO -    336    350.7 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 17:19:26,611 - memory_profile6_log - INFO -    337    350.8 MiB      2.7 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 17:19:26,615 - memory_profile6_log - INFO -    338                             

2018-04-29 17:19:26,615 - memory_profile6_log - INFO -    339                                                 # me = os.getpid()

2018-04-29 17:19:26,617 - memory_profile6_log - INFO -    340                                                 # kill_proc_tree(me)

2018-04-29 17:19:26,618 - memory_profile6_log - INFO -    341                             

2018-04-29 17:19:26,618 - memory_profile6_log - INFO -    342    350.8 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 17:19:26,621 - memory_profile6_log - INFO -    343    350.8 MiB     -0.0 MiB                       for m in h_frame:

2018-04-29 17:19:26,621 - memory_profile6_log - INFO -    344    350.8 MiB     -0.0 MiB                           if m is not None:

2018-04-29 17:19:26,621 - memory_profile6_log - INFO -    345    350.8 MiB     -0.0 MiB                               if len(m) > 0:

2018-04-29 17:19:26,622 - memory_profile6_log - INFO -    346    350.8 MiB      0.7 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 17:19:26,624 - memory_profile6_log - INFO -    347    350.8 MiB     -0.0 MiB                       del h_frame

2018-04-29 17:19:26,625 - memory_profile6_log - INFO -    348    350.8 MiB      0.0 MiB                       del lhistory

2018-04-29 17:19:26,625 - memory_profile6_log - INFO -    349                             

2018-04-29 17:19:26,627 - memory_profile6_log - INFO -    350    350.8 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 17:19:26,630 - memory_profile6_log - INFO -    351    350.8 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 17:19:26,631 - memory_profile6_log - INFO -    352                                     else: 

2018-04-29 17:19:26,631 - memory_profile6_log - INFO -    353                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 17:19:26,631 - memory_profile6_log - INFO -    354    350.8 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 17:19:26,634 - memory_profile6_log - INFO -    355    350.8 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 17:19:26,634 - memory_profile6_log - INFO -    356                             

2018-04-29 17:19:26,634 - memory_profile6_log - INFO -    357    350.8 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 17:19:26,635 - memory_profile6_log - INFO - 


2018-04-29 17:19:27,713 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 17:19:27,792 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0   0.000646        1844.325967        16            1854.325967  10960288  1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...
1        NaN      333823.000000       256          333833.000000  10959239  161129f951435-0d660a18dbcd87-173a7640-100200-1...
2   0.000646        1844.325967        32            1854.325967  10960288  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
3   0.000646        1844.325967        32            1854.325967  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
4   0.000646        1844.325967        32            1854.325967  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
5   0.000646        1844.325967        16            1854.325967  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
6   0.000646        1844.325967        16            1854.325967  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
0   0.000646        1844.325967        16            1854.325967  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
1   0.000646        1844.325967        16            1854.325967  10960288  1626fd160d9114-055d801ae6b39d-66265f05-38400-1...
2   0.000646        1844.325967        16            1854.325967  10960288  161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...
3   0.000646        1844.325967         8            1854.325967  10960288  161e219cd891c8-0e882943705df4-15290f5f-38400-1...
4   0.000646        1844.325967         8            1854.325967  10960288  16169731ce618-00742c5e307396-5b4d6a36-38400-16...
5   0.000646        1844.325967         8            1854.325967  10960288  1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...
0   0.000646        1844.325967         8            1854.325967  10960288  16149f0a1611-00a034775-63446032-38400-16149f0a...
1   0.000646        1844.325967         4            1854.325967  10960288  16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...
2   0.000646        1844.325967         4            1854.325967  10960288  16132f1045f30b-0613b72b33445e-5d640103-38400-1...
3   0.000646        1844.325967         4            1854.325967  10960288  1612d88e55ab8-09af9658f30a46-65117b23-38400-16...
4   0.000646        1844.325967         4            1854.325967  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
5   0.000646        1844.325967         4            1854.325967  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
6   0.000646        1844.325967         8            1854.325967  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
2018-04-29 17:19:27,793 - memory_profile6_log - INFO - 

2018-04-29 17:19:27,802 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 17:19:27,875 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 17:19:27,887 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 17:20:16,663 - memory_profile6_log - INFO - size of df: 58.30 MB
2018-04-29 17:20:16,664 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-29 17:20:16,727 - memory_profile6_log - INFO - size of current_frame: 58.30 MB
2018-04-29 17:20:16,730 - memory_profile6_log - INFO - loading time of: 484420 total genuine-current interest data ~ take 268.293s
2018-04-29 17:20:16,736 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 17:20:16,736 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 17:20:16,739 - memory_profile6_log - INFO - ================================================

2018-04-29 17:20:16,740 - memory_profile6_log - INFO -    359     86.8 MiB     86.8 MiB   @profile

2018-04-29 17:20:16,742 - memory_profile6_log - INFO -    360                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 17:20:16,743 - memory_profile6_log - INFO -    361     86.9 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 17:20:16,743 - memory_profile6_log - INFO -    362     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 17:20:16,744 - memory_profile6_log - INFO -    363                             

2018-04-29 17:20:16,744 - memory_profile6_log - INFO -    364                                 # ~~~ Begin collecting data ~~~

2018-04-29 17:20:16,747 - memory_profile6_log - INFO -    365     86.9 MiB      0.0 MiB       t0 = time.time()

2018-04-29 17:20:16,750 - memory_profile6_log - INFO -    366    350.8 MiB    263.9 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 17:20:16,750 - memory_profile6_log - INFO -    367    350.8 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 17:20:16,752 - memory_profile6_log - INFO -    368                                     logger.info("Training cannot be empty..")

2018-04-29 17:20:16,753 - memory_profile6_log - INFO -    369                                     return False

2018-04-29 17:20:16,753 - memory_profile6_log - INFO -    370    349.7 MiB     -1.1 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 17:20:16,756 - memory_profile6_log - INFO -    371    349.9 MiB      0.1 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 17:20:16,756 - memory_profile6_log - INFO -    372    349.9 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 17:20:16,756 - memory_profile6_log - INFO -    373                             

2018-04-29 17:20:16,756 - memory_profile6_log - INFO -    374    355.7 MiB      5.8 MiB       big_frame = pd.concat(datalist)

2018-04-29 17:20:16,759 - memory_profile6_log - INFO -    375    355.7 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 17:20:16,759 - memory_profile6_log - INFO -    376    349.9 MiB     -5.8 MiB       del datalist

2018-04-29 17:20:16,766 - memory_profile6_log - INFO -    377                             

2018-04-29 17:20:16,766 - memory_profile6_log - INFO -    378    349.9 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 17:20:16,772 - memory_profile6_log - INFO -    379                             

2018-04-29 17:20:16,773 - memory_profile6_log - INFO -    380                                 # ~ get current news interest ~

2018-04-29 17:20:16,775 - memory_profile6_log - INFO -    381    349.9 MiB      0.0 MiB       if not cd:

2018-04-29 17:20:16,776 - memory_profile6_log - INFO -    382                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 17:20:16,776 - memory_profile6_log - INFO -    383                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 17:20:16,776 - memory_profile6_log - INFO -    384                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 17:20:16,776 - memory_profile6_log - INFO -    385                                 else:

2018-04-29 17:20:16,778 - memory_profile6_log - INFO -    386    349.9 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 17:20:16,779 - memory_profile6_log - INFO -    387    349.9 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 17:20:16,782 - memory_profile6_log - INFO -    388                             

2018-04-29 17:20:16,782 - memory_profile6_log - INFO -    389                                     # safe handling of query parameter

2018-04-29 17:20:16,783 - memory_profile6_log - INFO -    390                                     query_params = [

2018-04-29 17:20:16,785 - memory_profile6_log - INFO -    391    349.9 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 17:20:16,785 - memory_profile6_log - INFO -    392                                     ]

2018-04-29 17:20:16,786 - memory_profile6_log - INFO -    393                             

2018-04-29 17:20:16,786 - memory_profile6_log - INFO -    394    349.9 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-29 17:20:16,786 - memory_profile6_log - INFO -    395    420.6 MiB     70.7 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 17:20:16,789 - memory_profile6_log - INFO -    396    420.6 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 17:20:16,789 - memory_profile6_log - INFO -    397                             

2018-04-29 17:20:16,789 - memory_profile6_log - INFO -    398    420.6 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 17:20:16,790 - memory_profile6_log - INFO -    399    420.6 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 17:20:16,793 - memory_profile6_log - INFO -    400    420.6 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 17:20:16,795 - memory_profile6_log - INFO -    401    420.6 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 17:20:16,796 - memory_profile6_log - INFO -    402    420.6 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 17:20:16,798 - memory_profile6_log - INFO -    403    420.6 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 17:20:16,798 - memory_profile6_log - INFO -    404                             

2018-04-29 17:20:16,799 - memory_profile6_log - INFO -    405    420.6 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 17:20:16,799 - memory_profile6_log - INFO - 


2018-04-29 17:20:16,802 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 17:20:16,832 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 17:20:16,832 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-29 17:20:16,834 - memory_profile6_log - INFO - apply on: 5000 total history...)
2018-04-29 17:20:17,048 - memory_profile6_log - INFO - len of uniques_fit_hist:5000
2018-04-29 17:20:17,053 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:3753
2018-04-29 17:20:17,861 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 17:20:17,864 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 17:20:18,072 - memory_profile6_log - INFO - model_fit dtypes:

2018-04-29 17:20:18,078 - memory_profile6_log - INFO - date              datetime64[ns, UTC]
user_id                        object
topic_id                       object
num_x                           int64
num_y                           int64
is_general                       bool
date_all_click                  int64
Ntotal                          int64
sigma_Nt                        int64
joinprob_ci                   float64
p_cat_ci                      float64
posterior                     float64
dtype: object
2018-04-29 17:20:18,078 - memory_profile6_log - INFO - 

2018-04-29 17:20:18,079 - memory_profile6_log - INFO - fitted_models_hist dtypes:

2018-04-29 17:20:18,085 - memory_profile6_log - INFO - p0_cat_ci                float64
pt_posterior_x_Nt        float64
sigma_Nt                   int64
smoothed_pt_posterior    float64
topic_id                  object
user_id                   object
dtype: object
2018-04-29 17:20:18,088 - memory_profile6_log - INFO - 

2018-04-29 17:20:18,642 - memory_profile6_log - INFO - Len of fitted_models on main class: 253995
2018-04-29 17:20:18,644 - memory_profile6_log - INFO - 

2018-04-29 17:20:18,644 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-29 17:20:18,644 - memory_profile6_log - INFO - 

2018-04-29 17:20:18,645 - memory_profile6_log - INFO - Fitted models before concat:

2018-04-29 17:20:18,673 - memory_profile6_log - INFO -                                 user_id    topic_id  pt_posterior_x_Nt  p0_cat_ci  smoothed_pt_posterior
0  107e02cd-54af-40fd-a602-e6105250ae8c  1046474724         951.062678   0.000090             961.062678
1  107e02cd-54af-40fd-a602-e6105250ae8c  1060896301         673.030242   0.000065             683.030242
2  107e02cd-54af-40fd-a602-e6105250ae8c   188400148          72.193555   0.036200              82.193555
3  107e02cd-54af-40fd-a602-e6105250ae8c    22553543          35.980922   0.107319              45.980922
4  107e02cd-54af-40fd-a602-e6105250ae8c    22596701          61.042850   0.040442              71.042850
2018-04-29 17:20:18,697 - memory_profile6_log - INFO - 

2018-04-29 17:20:18,697 - memory_profile6_log - INFO - len of current fitted models: 253995
2018-04-29 17:20:18,698 - memory_profile6_log - INFO - 

2018-04-29 17:20:18,700 - memory_profile6_log - INFO - len of history fitted models: 5000
2018-04-29 17:20:18,700 - memory_profile6_log - INFO - 

2018-04-29 17:20:18,742 - memory_profile6_log - INFO - len of fitted models after concat: 258995
2018-04-29 17:20:18,743 - memory_profile6_log - INFO - 

2018-04-29 17:20:18,744 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-29 17:20:18,746 - memory_profile6_log - INFO - 

2018-04-29 17:20:18,905 - memory_profile6_log - INFO - Fitted models after concat:

2018-04-29 17:20:18,924 - memory_profile6_log - INFO -                                 user_id    topic_id  pt_posterior_x_Nt
0  107e02cd-54af-40fd-a602-e6105250ae8c  1046474724         951.062678
1  107e02cd-54af-40fd-a602-e6105250ae8c  1060896301         673.030242
2  107e02cd-54af-40fd-a602-e6105250ae8c   188400148          72.193555
3  107e02cd-54af-40fd-a602-e6105250ae8c    22553543          35.980922
4  107e02cd-54af-40fd-a602-e6105250ae8c    22596701          61.042850
2018-04-29 17:20:18,926 - memory_profile6_log - INFO - 

2018-04-29 17:20:18,927 - memory_profile6_log - INFO - Simplifying current fitted models...
2018-04-29 17:20:18,928 - memory_profile6_log - INFO - 

2018-04-29 17:26:16,216 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 17:26:16,220 - memory_profile6_log - INFO - date_generated: 
2018-04-29 17:26:16,220 - memory_profile6_log - INFO -  
2018-04-29 17:26:16,220 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 17:26:16,221 - memory_profile6_log - INFO - 

2018-04-29 17:26:16,221 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 17:26:16,221 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 17:26:16,223 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 17:26:16,348 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 17:26:16,352 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 17:27:26,122 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 17:27:26,124 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 17:27:26,170 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 17:27:26,171 - memory_profile6_log - INFO - Appending history data...
2018-04-29 17:27:26,171 - memory_profile6_log - INFO - processing batch-0
2018-04-29 17:27:26,174 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:27:26,216 - memory_profile6_log - INFO - call history data...
2018-04-29 17:27:56,654 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:27:57,328 - memory_profile6_log - INFO - processing batch-1
2018-04-29 17:27:57,331 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:27:57,338 - memory_profile6_log - INFO - call history data...
2018-04-29 17:28:25,072 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:28:25,723 - memory_profile6_log - INFO - processing batch-2
2018-04-29 17:28:25,724 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:28:25,732 - memory_profile6_log - INFO - call history data...
2018-04-29 17:28:54,874 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:28:55,533 - memory_profile6_log - INFO - processing batch-3
2018-04-29 17:28:55,536 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:28:55,543 - memory_profile6_log - INFO - call history data...
2018-04-29 17:29:22,792 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:29:23,447 - memory_profile6_log - INFO - processing batch-4
2018-04-29 17:29:23,450 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:29:23,457 - memory_profile6_log - INFO - call history data...
2018-04-29 17:29:50,767 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:29:51,453 - memory_profile6_log - INFO - Appending training data...
2018-04-29 17:29:51,454 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 17:29:51,457 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 17:29:51,459 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 17:29:51,460 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 17:29:51,460 - memory_profile6_log - INFO - ================================================

2018-04-29 17:29:51,463 - memory_profile6_log - INFO -    311     86.9 MiB     86.9 MiB   @profile

2018-04-29 17:29:51,466 - memory_profile6_log - INFO -    312                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 17:29:51,469 - memory_profile6_log - INFO -    313     86.9 MiB      0.0 MiB       bq_client = client

2018-04-29 17:29:51,470 - memory_profile6_log - INFO -    314     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 17:29:51,473 - memory_profile6_log - INFO -    315                             

2018-04-29 17:29:51,473 - memory_profile6_log - INFO -    316     86.9 MiB      0.0 MiB       datalist = []

2018-04-29 17:29:51,474 - memory_profile6_log - INFO -    317     86.9 MiB      0.0 MiB       datalist_hist = []

2018-04-29 17:29:51,476 - memory_profile6_log - INFO -    318                             

2018-04-29 17:29:51,477 - memory_profile6_log - INFO -    319     86.9 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 17:29:51,479 - memory_profile6_log - INFO -    320    351.5 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 17:29:51,480 - memory_profile6_log - INFO -    321    339.6 MiB    252.7 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 17:29:51,480 - memory_profile6_log - INFO -    322    339.6 MiB      0.0 MiB           if tframe is not None:

2018-04-29 17:29:51,482 - memory_profile6_log - INFO -    323    339.6 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 17:29:51,483 - memory_profile6_log - INFO -    324    347.9 MiB      8.3 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 17:29:51,483 - memory_profile6_log - INFO -    325    347.9 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 17:29:51,483 - memory_profile6_log - INFO -    326    347.9 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 17:29:51,484 - memory_profile6_log - INFO -    327    351.5 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 17:29:51,486 - memory_profile6_log - INFO -    328                                                 # ~ loading history

2018-04-29 17:29:51,487 - memory_profile6_log - INFO -    329                                                 """

2018-04-29 17:29:51,489 - memory_profile6_log - INFO -    330                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 17:29:51,490 - memory_profile6_log - INFO -    331                                                 """

2018-04-29 17:29:51,490 - memory_profile6_log - INFO -    332    351.3 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 17:29:51,492 - memory_profile6_log - INFO -    333                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 17:29:51,493 - memory_profile6_log - INFO -    334    351.3 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 17:29:51,493 - memory_profile6_log - INFO -    335    351.3 MiB      0.6 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 17:29:51,493 - memory_profile6_log - INFO -    336                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 17:29:51,494 - memory_profile6_log - INFO -    337                             

2018-04-29 17:29:51,497 - memory_profile6_log - INFO -    338    351.3 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 17:29:51,499 - memory_profile6_log - INFO -    339    351.4 MiB      2.2 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 17:29:51,499 - memory_profile6_log - INFO -    340                             

2018-04-29 17:29:51,500 - memory_profile6_log - INFO -    341                                                 # me = os.getpid()

2018-04-29 17:29:51,500 - memory_profile6_log - INFO -    342                                                 # kill_proc_tree(me)

2018-04-29 17:29:51,500 - memory_profile6_log - INFO -    343                             

2018-04-29 17:29:51,502 - memory_profile6_log - INFO -    344    351.4 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 17:29:51,503 - memory_profile6_log - INFO -    345    351.5 MiB      0.0 MiB                       for m in h_frame:

2018-04-29 17:29:51,503 - memory_profile6_log - INFO -    346    351.5 MiB      0.0 MiB                           if m is not None:

2018-04-29 17:29:51,503 - memory_profile6_log - INFO -    347    351.5 MiB      0.0 MiB                               if len(m) > 0:

2018-04-29 17:29:51,505 - memory_profile6_log - INFO -    348    351.5 MiB      0.8 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 17:29:51,506 - memory_profile6_log - INFO -    349    351.5 MiB      0.0 MiB                       del h_frame

2018-04-29 17:29:51,507 - memory_profile6_log - INFO -    350    351.5 MiB      0.0 MiB                       del lhistory

2018-04-29 17:29:51,509 - memory_profile6_log - INFO -    351                             

2018-04-29 17:29:51,510 - memory_profile6_log - INFO -    352    351.5 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 17:29:51,512 - memory_profile6_log - INFO -    353    351.5 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 17:29:51,513 - memory_profile6_log - INFO -    354                                     else: 

2018-04-29 17:29:51,513 - memory_profile6_log - INFO -    355                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 17:29:51,513 - memory_profile6_log - INFO -    356    351.5 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 17:29:51,515 - memory_profile6_log - INFO -    357    351.5 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 17:29:51,516 - memory_profile6_log - INFO -    358                             

2018-04-29 17:29:51,516 - memory_profile6_log - INFO -    359    351.5 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 17:29:51,519 - memory_profile6_log - INFO - 


2018-04-29 17:29:52,608 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 17:29:52,677 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0   0.000646        1844.325967        32            1854.325967  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
1   0.000646        1844.325967        16            1854.325967  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
2   0.000646        1844.325967        32            1854.325967  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
3   0.000646        1844.325967        16            1854.325967  10960288  1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...
4        NaN      333823.000000       256          333833.000000  10959239  161129f951435-0d660a18dbcd87-173a7640-100200-1...
5   0.000646        1844.325967        32            1854.325967  10960288  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
6   0.000646        1844.325967        16            1854.325967  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
0   0.000646        1844.325967        16            1854.325967  10960288  1626fd160d9114-055d801ae6b39d-66265f05-38400-1...
1   0.000646        1844.325967         8            1854.325967  10960288  16169731ce618-00742c5e307396-5b4d6a36-38400-16...
2   0.000646        1844.325967         8            1854.325967  10960288  161e219cd891c8-0e882943705df4-15290f5f-38400-1...
3   0.000646        1844.325967        16            1854.325967  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
4   0.000646        1844.325967        16            1854.325967  10960288  161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...
5   0.000646        1844.325967         8            1854.325967  10960288  1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...
0   0.000646        1844.325967         8            1854.325967  10960288  16149f0a1611-00a034775-63446032-38400-16149f0a...
1   0.000646        1844.325967         8            1854.325967  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
2   0.000646        1844.325967         4            1854.325967  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
3   0.000646        1844.325967         4            1854.325967  10960288  16132f1045f30b-0613b72b33445e-5d640103-38400-1...
4   0.000646        1844.325967         4            1854.325967  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
5   0.000646        1844.325967         4            1854.325967  10960288  1612d88e55ab8-09af9658f30a46-65117b23-38400-16...
6   0.000646        1844.325967         4            1854.325967  10960288  16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...
2018-04-29 17:29:52,680 - memory_profile6_log - INFO - 

2018-04-29 17:29:52,688 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 17:29:52,760 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 17:29:52,778 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 17:30:41,678 - memory_profile6_log - INFO - size of df: 58.30 MB
2018-04-29 17:30:41,680 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-29 17:30:41,743 - memory_profile6_log - INFO - size of current_frame: 58.30 MB
2018-04-29 17:30:41,743 - memory_profile6_log - INFO - loading time of: 484420 total genuine-current interest data ~ take 265.418s
2018-04-29 17:30:41,750 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 17:30:41,750 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 17:30:41,752 - memory_profile6_log - INFO - ================================================

2018-04-29 17:30:41,752 - memory_profile6_log - INFO -    361     86.7 MiB     86.7 MiB   @profile

2018-04-29 17:30:41,759 - memory_profile6_log - INFO -    362                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 17:30:41,759 - memory_profile6_log - INFO -    363     86.9 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 17:30:41,762 - memory_profile6_log - INFO -    364     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 17:30:41,763 - memory_profile6_log - INFO -    365                             

2018-04-29 17:30:41,763 - memory_profile6_log - INFO -    366                                 # ~~~ Begin collecting data ~~~

2018-04-29 17:30:41,765 - memory_profile6_log - INFO -    367     86.9 MiB      0.0 MiB       t0 = time.time()

2018-04-29 17:30:41,766 - memory_profile6_log - INFO -    368    351.5 MiB    264.6 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 17:30:41,766 - memory_profile6_log - INFO -    369    351.5 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 17:30:41,769 - memory_profile6_log - INFO -    370                                     logger.info("Training cannot be empty..")

2018-04-29 17:30:41,769 - memory_profile6_log - INFO -    371                                     return False

2018-04-29 17:30:41,772 - memory_profile6_log - INFO -    372    351.6 MiB      0.1 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 17:30:41,773 - memory_profile6_log - INFO -    373    351.7 MiB      0.1 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 17:30:41,773 - memory_profile6_log - INFO -    374    351.7 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 17:30:41,775 - memory_profile6_log - INFO -    375                             

2018-04-29 17:30:41,775 - memory_profile6_log - INFO -    376    357.5 MiB      5.8 MiB       big_frame = pd.concat(datalist)

2018-04-29 17:30:41,775 - memory_profile6_log - INFO -    377    357.5 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 17:30:41,776 - memory_profile6_log - INFO -    378    351.7 MiB     -5.8 MiB       del datalist

2018-04-29 17:30:41,776 - memory_profile6_log - INFO -    379                             

2018-04-29 17:30:41,779 - memory_profile6_log - INFO -    380    351.7 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 17:30:41,782 - memory_profile6_log - INFO -    381                             

2018-04-29 17:30:41,782 - memory_profile6_log - INFO -    382                                 # ~ get current news interest ~

2018-04-29 17:30:41,782 - memory_profile6_log - INFO -    383    351.7 MiB      0.0 MiB       if not cd:

2018-04-29 17:30:41,782 - memory_profile6_log - INFO -    384                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 17:30:41,783 - memory_profile6_log - INFO -    385                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 17:30:41,785 - memory_profile6_log - INFO -    386                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 17:30:41,785 - memory_profile6_log - INFO -    387                                 else:

2018-04-29 17:30:41,786 - memory_profile6_log - INFO -    388    351.7 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 17:30:41,786 - memory_profile6_log - INFO -    389    351.7 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 17:30:41,790 - memory_profile6_log - INFO -    390                             

2018-04-29 17:30:41,790 - memory_profile6_log - INFO -    391                                     # safe handling of query parameter

2018-04-29 17:30:41,792 - memory_profile6_log - INFO -    392                                     query_params = [

2018-04-29 17:30:41,792 - memory_profile6_log - INFO -    393    351.7 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 17:30:41,792 - memory_profile6_log - INFO -    394                                     ]

2018-04-29 17:30:41,793 - memory_profile6_log - INFO -    395                             

2018-04-29 17:30:41,795 - memory_profile6_log - INFO -    396    351.7 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-29 17:30:41,795 - memory_profile6_log - INFO -    397    418.3 MiB     66.6 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 17:30:41,796 - memory_profile6_log - INFO -    398    418.3 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 17:30:41,798 - memory_profile6_log - INFO -    399                             

2018-04-29 17:30:41,799 - memory_profile6_log - INFO -    400    418.3 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 17:30:41,799 - memory_profile6_log - INFO -    401    418.3 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 17:30:41,799 - memory_profile6_log - INFO -    402    418.3 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 17:30:41,802 - memory_profile6_log - INFO -    403    418.3 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 17:30:41,802 - memory_profile6_log - INFO -    404    418.3 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 17:30:41,803 - memory_profile6_log - INFO -    405    418.3 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 17:30:41,803 - memory_profile6_log - INFO -    406                             

2018-04-29 17:30:41,805 - memory_profile6_log - INFO -    407    418.3 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 17:30:41,805 - memory_profile6_log - INFO - 


2018-04-29 17:30:41,809 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 17:30:41,838 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 17:30:41,839 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-29 17:30:41,841 - memory_profile6_log - INFO - apply on: 5000 total history...)
2018-04-29 17:30:42,052 - memory_profile6_log - INFO - len of uniques_fit_hist:5000
2018-04-29 17:30:42,059 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:3753
2018-04-29 17:30:42,857 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 17:30:42,858 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 17:30:43,084 - memory_profile6_log - INFO - model_fit dtypes:

2018-04-29 17:30:43,089 - memory_profile6_log - INFO - date              datetime64[ns, UTC]
user_id                        object
topic_id                       object
num_x                           int64
num_y                           int64
is_general                       bool
date_all_click                  int64
Ntotal                          int64
sigma_Nt                        int64
joinprob_ci                   float64
p_cat_ci                      float64
posterior                     float64
dtype: object
2018-04-29 17:30:43,091 - memory_profile6_log - INFO - 

2018-04-29 17:30:43,092 - memory_profile6_log - INFO - fitted_models_hist dtypes:

2018-04-29 17:30:43,098 - memory_profile6_log - INFO - p0_cat_ci                float64
pt_posterior_x_Nt        float64
sigma_Nt                   int64
smoothed_pt_posterior    float64
topic_id                  object
user_id                   object
dtype: object
2018-04-29 17:30:43,098 - memory_profile6_log - INFO - 

2018-04-29 17:30:43,592 - memory_profile6_log - INFO - Len of fitted_models on main class: 253995
2018-04-29 17:30:43,595 - memory_profile6_log - INFO - 

2018-04-29 17:30:43,595 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-29 17:30:43,596 - memory_profile6_log - INFO - 

2018-04-29 17:30:43,598 - memory_profile6_log - INFO - Fitted models before concat:

2018-04-29 17:30:43,619 - memory_profile6_log - INFO -                                 user_id    topic_id  pt_posterior_x_Nt  smoothed_pt_posterior
0  107e02cd-54af-40fd-a602-e6105250ae8c  1046474724         951.062678             961.062678
1  107e02cd-54af-40fd-a602-e6105250ae8c  1060896301         673.030242             683.030242
2  107e02cd-54af-40fd-a602-e6105250ae8c   188400148          72.193555              82.193555
3  107e02cd-54af-40fd-a602-e6105250ae8c    22553543          35.980922              45.980922
4  107e02cd-54af-40fd-a602-e6105250ae8c    22596701          61.042850              71.042850
2018-04-29 17:30:43,621 - memory_profile6_log - INFO - 

2018-04-29 17:30:43,621 - memory_profile6_log - INFO - len of current fitted models: 253995
2018-04-29 17:30:43,624 - memory_profile6_log - INFO - 

2018-04-29 17:30:43,625 - memory_profile6_log - INFO - len of history fitted models: 5000
2018-04-29 17:30:43,625 - memory_profile6_log - INFO - 

2018-04-29 17:30:43,658 - memory_profile6_log - INFO - len of fitted models after concat: 258995
2018-04-29 17:30:43,660 - memory_profile6_log - INFO - 

2018-04-29 17:30:43,661 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-29 17:30:43,664 - memory_profile6_log - INFO - 

2018-04-29 17:30:43,819 - memory_profile6_log - INFO - Fitted models after concat:

2018-04-29 17:30:43,836 - memory_profile6_log - INFO -                                 user_id    topic_id  pt_posterior_x_Nt
0  107e02cd-54af-40fd-a602-e6105250ae8c  1046474724         951.062678
1  107e02cd-54af-40fd-a602-e6105250ae8c  1060896301         673.030242
2  107e02cd-54af-40fd-a602-e6105250ae8c   188400148          72.193555
3  107e02cd-54af-40fd-a602-e6105250ae8c    22553543          35.980922
4  107e02cd-54af-40fd-a602-e6105250ae8c    22596701          61.042850
2018-04-29 17:30:43,838 - memory_profile6_log - INFO - 

2018-04-29 17:30:43,838 - memory_profile6_log - INFO - Simplifying current fitted models...
2018-04-29 17:30:43,839 - memory_profile6_log - INFO - 

2018-04-29 17:33:33,459 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 17:33:33,460 - memory_profile6_log - INFO - date_generated: 
2018-04-29 17:33:33,460 - memory_profile6_log - INFO -  
2018-04-29 17:33:33,461 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 17:33:33,461 - memory_profile6_log - INFO - 

2018-04-29 17:33:33,461 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 17:33:33,463 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 17:33:33,463 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 17:33:33,586 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 17:33:33,591 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 17:34:47,931 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 17:34:47,933 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 17:34:47,973 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 17:34:47,974 - memory_profile6_log - INFO - Appending history data...
2018-04-29 17:34:47,976 - memory_profile6_log - INFO - processing batch-0
2018-04-29 17:34:47,976 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:34:48,019 - memory_profile6_log - INFO - call history data...
2018-04-29 17:35:16,384 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:35:17,078 - memory_profile6_log - INFO - processing batch-1
2018-04-29 17:35:17,078 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:35:17,086 - memory_profile6_log - INFO - call history data...
2018-04-29 17:35:45,230 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:35:45,890 - memory_profile6_log - INFO - processing batch-2
2018-04-29 17:35:45,891 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:35:45,898 - memory_profile6_log - INFO - call history data...
2018-04-29 17:36:13,418 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:36:14,125 - memory_profile6_log - INFO - processing batch-3
2018-04-29 17:36:14,127 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:36:14,134 - memory_profile6_log - INFO - call history data...
2018-04-29 17:36:41,668 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:36:42,447 - memory_profile6_log - INFO - processing batch-4
2018-04-29 17:36:42,448 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:36:42,457 - memory_profile6_log - INFO - call history data...
2018-04-29 17:37:10,122 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:37:10,829 - memory_profile6_log - INFO - Appending training data...
2018-04-29 17:37:10,832 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 17:37:10,835 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 17:37:10,835 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 17:37:10,836 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 17:37:10,838 - memory_profile6_log - INFO - ================================================

2018-04-29 17:37:10,838 - memory_profile6_log - INFO -    311     87.0 MiB     87.0 MiB   @profile

2018-04-29 17:37:10,838 - memory_profile6_log - INFO -    312                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 17:37:10,839 - memory_profile6_log - INFO -    313     87.0 MiB      0.0 MiB       bq_client = client

2018-04-29 17:37:10,842 - memory_profile6_log - INFO -    314     87.0 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 17:37:10,842 - memory_profile6_log - INFO -    315                             

2018-04-29 17:37:10,844 - memory_profile6_log - INFO -    316     87.0 MiB      0.0 MiB       datalist = []

2018-04-29 17:37:10,844 - memory_profile6_log - INFO -    317     87.0 MiB      0.0 MiB       datalist_hist = []

2018-04-29 17:37:10,845 - memory_profile6_log - INFO -    318                             

2018-04-29 17:37:10,845 - memory_profile6_log - INFO -    319     87.0 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 17:37:10,846 - memory_profile6_log - INFO -    320    351.4 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 17:37:10,846 - memory_profile6_log - INFO -    321    340.4 MiB    253.5 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 17:37:10,848 - memory_profile6_log - INFO -    322    340.4 MiB      0.0 MiB           if tframe is not None:

2018-04-29 17:37:10,848 - memory_profile6_log - INFO -    323    340.4 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 17:37:10,848 - memory_profile6_log - INFO -    324    347.6 MiB      7.2 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 17:37:10,851 - memory_profile6_log - INFO -    325    347.6 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 17:37:10,852 - memory_profile6_log - INFO -    326    347.6 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 17:37:10,855 - memory_profile6_log - INFO -    327    351.4 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 17:37:10,855 - memory_profile6_log - INFO -    328                                                 # ~ loading history

2018-04-29 17:37:10,857 - memory_profile6_log - INFO -    329                                                 """

2018-04-29 17:37:10,858 - memory_profile6_log - INFO -    330                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 17:37:10,858 - memory_profile6_log - INFO -    331                                                 """

2018-04-29 17:37:10,859 - memory_profile6_log - INFO -    332    351.2 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 17:37:10,861 - memory_profile6_log - INFO -    333                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 17:37:10,864 - memory_profile6_log - INFO -    334    351.2 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 17:37:10,865 - memory_profile6_log - INFO -    335    351.2 MiB      0.5 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 17:37:10,867 - memory_profile6_log - INFO -    336                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 17:37:10,868 - memory_profile6_log - INFO -    337                             

2018-04-29 17:37:10,874 - memory_profile6_log - INFO -    338    351.2 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 17:37:10,878 - memory_profile6_log - INFO -    339    351.3 MiB      2.3 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 17:37:10,881 - memory_profile6_log - INFO -    340                             

2018-04-29 17:37:10,885 - memory_profile6_log - INFO -    341                                                 # me = os.getpid()

2018-04-29 17:37:10,891 - memory_profile6_log - INFO -    342                                                 # kill_proc_tree(me)

2018-04-29 17:37:10,892 - memory_profile6_log - INFO -    343                             

2018-04-29 17:37:10,894 - memory_profile6_log - INFO -    344    351.3 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 17:37:10,897 - memory_profile6_log - INFO -    345    351.4 MiB      0.0 MiB                       for m in h_frame:

2018-04-29 17:37:10,901 - memory_profile6_log - INFO -    346    351.4 MiB      0.0 MiB                           if m is not None:

2018-04-29 17:37:10,901 - memory_profile6_log - INFO -    347    351.4 MiB      0.0 MiB                               if len(m) > 0:

2018-04-29 17:37:10,903 - memory_profile6_log - INFO -    348    351.4 MiB      0.9 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 17:37:10,904 - memory_profile6_log - INFO -    349    351.4 MiB      0.0 MiB                       del h_frame

2018-04-29 17:37:10,905 - memory_profile6_log - INFO -    350    351.4 MiB      0.0 MiB                       del lhistory

2018-04-29 17:37:10,907 - memory_profile6_log - INFO -    351                             

2018-04-29 17:37:10,907 - memory_profile6_log - INFO -    352    351.4 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 17:37:10,908 - memory_profile6_log - INFO -    353    351.4 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 17:37:10,908 - memory_profile6_log - INFO -    354                                     else: 

2018-04-29 17:37:10,911 - memory_profile6_log - INFO -    355                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 17:37:10,911 - memory_profile6_log - INFO -    356    351.4 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 17:37:10,915 - memory_profile6_log - INFO -    357    351.4 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 17:37:10,917 - memory_profile6_log - INFO -    358                             

2018-04-29 17:37:10,918 - memory_profile6_log - INFO -    359    351.4 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 17:37:10,920 - memory_profile6_log - INFO - 


2018-04-29 17:37:12,190 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 17:37:12,265 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0   0.000646        1844.325967        32            1854.325967  10960288  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
1   0.000646        1844.325967        16            1854.325967  10960288  1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...
2        NaN      333823.000000       256          333833.000000  10959239  161129f951435-0d660a18dbcd87-173a7640-100200-1...
3   0.000646        1844.325967        16            1854.325967  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
4   0.000646        1844.325967        32            1854.325967  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
5   0.000646        1844.325967        32            1854.325967  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
6   0.000646        1844.325967        16            1854.325967  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
0   0.000646        1844.325967        16            1854.325967  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
1   0.000646        1844.325967         8            1854.325967  10960288  1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...
2   0.000646        1844.325967         8            1854.325967  10960288  16169731ce618-00742c5e307396-5b4d6a36-38400-16...
3   0.000646        1844.325967        16            1854.325967  10960288  161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...
4   0.000646        1844.325967         8            1854.325967  10960288  161e219cd891c8-0e882943705df4-15290f5f-38400-1...
5   0.000646        1844.325967        16            1854.325967  10960288  1626fd160d9114-055d801ae6b39d-66265f05-38400-1...
0   0.000646        1844.325967         4            1854.325967  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
1   0.000646        1844.325967         8            1854.325967  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
2   0.000646        1844.325967         4            1854.325967  10960288  1612d88e55ab8-09af9658f30a46-65117b23-38400-16...
3   0.000646        1844.325967         8            1854.325967  10960288  16149f0a1611-00a034775-63446032-38400-16149f0a...
4   0.000646        1844.325967         4            1854.325967  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
5   0.000646        1844.325967         4            1854.325967  10960288  16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...
6   0.000646        1844.325967         4            1854.325967  10960288  16132f1045f30b-0613b72b33445e-5d640103-38400-1...
2018-04-29 17:37:12,266 - memory_profile6_log - INFO - 

2018-04-29 17:37:12,275 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 17:37:12,344 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 17:37:12,355 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 17:38:09,230 - memory_profile6_log - INFO - size of df: 58.30 MB
2018-04-29 17:38:09,232 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-29 17:38:09,292 - memory_profile6_log - INFO - size of current_frame: 58.30 MB
2018-04-29 17:38:09,293 - memory_profile6_log - INFO - loading time of: 484420 total genuine-current interest data ~ take 275.728s
2018-04-29 17:38:09,299 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 17:38:09,301 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 17:38:09,302 - memory_profile6_log - INFO - ================================================

2018-04-29 17:38:09,303 - memory_profile6_log - INFO -    361     86.9 MiB     86.9 MiB   @profile

2018-04-29 17:38:09,305 - memory_profile6_log - INFO -    362                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 17:38:09,305 - memory_profile6_log - INFO -    363     87.0 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 17:38:09,305 - memory_profile6_log - INFO -    364     87.0 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 17:38:09,308 - memory_profile6_log - INFO -    365                             

2018-04-29 17:38:09,309 - memory_profile6_log - INFO -    366                                 # ~~~ Begin collecting data ~~~

2018-04-29 17:38:09,309 - memory_profile6_log - INFO -    367     87.0 MiB      0.0 MiB       t0 = time.time()

2018-04-29 17:38:09,309 - memory_profile6_log - INFO -    368    351.4 MiB    264.4 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 17:38:09,311 - memory_profile6_log - INFO -    369    351.4 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 17:38:09,311 - memory_profile6_log - INFO -    370                                     logger.info("Training cannot be empty..")

2018-04-29 17:38:09,312 - memory_profile6_log - INFO -    371                                     return False

2018-04-29 17:38:09,312 - memory_profile6_log - INFO -    372    350.9 MiB     -0.5 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 17:38:09,312 - memory_profile6_log - INFO -    373    351.0 MiB      0.1 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 17:38:09,315 - memory_profile6_log - INFO -    374    351.0 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 17:38:09,315 - memory_profile6_log - INFO -    375                             

2018-04-29 17:38:09,316 - memory_profile6_log - INFO -    376    356.8 MiB      5.8 MiB       big_frame = pd.concat(datalist)

2018-04-29 17:38:09,318 - memory_profile6_log - INFO -    377    356.8 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 17:38:09,318 - memory_profile6_log - INFO -    378    351.0 MiB     -5.8 MiB       del datalist

2018-04-29 17:38:09,319 - memory_profile6_log - INFO -    379                             

2018-04-29 17:38:09,319 - memory_profile6_log - INFO -    380    351.0 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 17:38:09,319 - memory_profile6_log - INFO -    381                             

2018-04-29 17:38:09,319 - memory_profile6_log - INFO -    382                                 # ~ get current news interest ~

2018-04-29 17:38:09,319 - memory_profile6_log - INFO -    383    351.0 MiB      0.0 MiB       if not cd:

2018-04-29 17:38:09,321 - memory_profile6_log - INFO -    384                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 17:38:09,322 - memory_profile6_log - INFO -    385                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 17:38:09,322 - memory_profile6_log - INFO -    386                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 17:38:09,326 - memory_profile6_log - INFO -    387                                 else:

2018-04-29 17:38:09,328 - memory_profile6_log - INFO -    388    351.0 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 17:38:09,328 - memory_profile6_log - INFO -    389    351.0 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 17:38:09,328 - memory_profile6_log - INFO -    390                             

2018-04-29 17:38:09,328 - memory_profile6_log - INFO -    391                                     # safe handling of query parameter

2018-04-29 17:38:09,329 - memory_profile6_log - INFO -    392                                     query_params = [

2018-04-29 17:38:09,329 - memory_profile6_log - INFO -    393    351.0 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 17:38:09,331 - memory_profile6_log - INFO -    394                                     ]

2018-04-29 17:38:09,332 - memory_profile6_log - INFO -    395                             

2018-04-29 17:38:09,334 - memory_profile6_log - INFO -    396    351.0 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-29 17:38:09,334 - memory_profile6_log - INFO -    397    418.8 MiB     67.7 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 17:38:09,336 - memory_profile6_log - INFO -    398    418.8 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 17:38:09,336 - memory_profile6_log - INFO -    399                             

2018-04-29 17:38:09,338 - memory_profile6_log - INFO -    400    418.8 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 17:38:09,339 - memory_profile6_log - INFO -    401    418.8 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 17:38:09,341 - memory_profile6_log - INFO -    402    418.8 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 17:38:09,342 - memory_profile6_log - INFO -    403    418.8 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 17:38:09,342 - memory_profile6_log - INFO -    404    418.8 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 17:38:09,342 - memory_profile6_log - INFO -    405    418.8 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 17:38:09,344 - memory_profile6_log - INFO -    406                             

2018-04-29 17:38:09,344 - memory_profile6_log - INFO -    407    418.8 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 17:38:09,345 - memory_profile6_log - INFO - 


2018-04-29 17:38:09,348 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 17:38:09,380 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 17:38:09,381 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-29 17:38:09,381 - memory_profile6_log - INFO - apply on: 5000 total history...)
2018-04-29 17:38:09,598 - memory_profile6_log - INFO - len of uniques_fit_hist:5000
2018-04-29 17:38:09,604 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:3753
2018-04-29 17:38:10,436 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 17:38:10,437 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 17:38:10,645 - memory_profile6_log - INFO - model_fit dtypes:

2018-04-29 17:38:10,651 - memory_profile6_log - INFO - date              datetime64[ns, UTC]
user_id                        object
topic_id                       object
num_x                           int64
num_y                           int64
is_general                       bool
date_all_click                  int64
Ntotal                          int64
sigma_Nt                        int64
joinprob_ci                   float64
p_cat_ci                      float64
posterior                     float64
dtype: object
2018-04-29 17:38:10,651 - memory_profile6_log - INFO - 

2018-04-29 17:38:10,653 - memory_profile6_log - INFO - fitted_models_hist dtypes:

2018-04-29 17:38:10,657 - memory_profile6_log - INFO - p0_cat_ci                float64
pt_posterior_x_Nt        float64
sigma_Nt                   int64
smoothed_pt_posterior    float64
topic_id                  object
user_id                   object
dtype: object
2018-04-29 17:38:10,658 - memory_profile6_log - INFO - 

2018-04-29 17:38:11,155 - memory_profile6_log - INFO - Len of fitted_models on main class: 253995
2018-04-29 17:38:11,157 - memory_profile6_log - INFO - 

2018-04-29 17:38:11,157 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-29 17:38:11,158 - memory_profile6_log - INFO - 

2018-04-29 17:38:11,158 - memory_profile6_log - INFO - Fitted models before concat:

2018-04-29 17:38:11,181 - memory_profile6_log - INFO -                                 user_id    topic_id  pt_posterior_x_Nt  smoothed_pt_posterior
0  107e02cd-54af-40fd-a602-e6105250ae8c  1046474724         951.062678             961.062678
1  107e02cd-54af-40fd-a602-e6105250ae8c  1060896301         673.030242             683.030242
2  107e02cd-54af-40fd-a602-e6105250ae8c   188400148          72.193555              82.193555
3  107e02cd-54af-40fd-a602-e6105250ae8c    22553543          35.980922              45.980922
4  107e02cd-54af-40fd-a602-e6105250ae8c    22596701          61.042850              71.042850
2018-04-29 17:38:11,183 - memory_profile6_log - INFO - 

2018-04-29 17:38:11,184 - memory_profile6_log - INFO - len of current fitted models: 253995
2018-04-29 17:38:11,184 - memory_profile6_log - INFO - 

2018-04-29 17:38:11,186 - memory_profile6_log - INFO - len of history fitted models: 5000
2018-04-29 17:38:11,186 - memory_profile6_log - INFO - 

2018-04-29 17:38:11,219 - memory_profile6_log - INFO - len of fitted models after concat: 258995
2018-04-29 17:38:11,220 - memory_profile6_log - INFO - 

2018-04-29 17:38:11,221 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-29 17:38:11,223 - memory_profile6_log - INFO - 

2018-04-29 17:38:11,371 - memory_profile6_log - INFO - Fitted models after concat:

2018-04-29 17:38:11,390 - memory_profile6_log - INFO -                                 user_id    topic_id  pt_posterior_x_Nt
0  107e02cd-54af-40fd-a602-e6105250ae8c  1046474724         951.062678
1  107e02cd-54af-40fd-a602-e6105250ae8c  1060896301         673.030242
2  107e02cd-54af-40fd-a602-e6105250ae8c   188400148          72.193555
3  107e02cd-54af-40fd-a602-e6105250ae8c    22553543          35.980922
4  107e02cd-54af-40fd-a602-e6105250ae8c    22596701          61.042850
2018-04-29 17:38:11,391 - memory_profile6_log - INFO - 

2018-04-29 17:38:11,407 - memory_profile6_log - INFO - Fitted models after concat-simplify:

2018-04-29 17:38:11,428 - memory_profile6_log - INFO -                                 user_id    topic_id  pt_posterior_x_Nt  smoothed_pt_posterior
0  107e02cd-54af-40fd-a602-e6105250ae8c  1046474724         951.062678             961.062678
1  107e02cd-54af-40fd-a602-e6105250ae8c  1060896301         673.030242             683.030242
2  107e02cd-54af-40fd-a602-e6105250ae8c   188400148          72.193555              82.193555
3  107e02cd-54af-40fd-a602-e6105250ae8c    22553543          35.980922              45.980922
4  107e02cd-54af-40fd-a602-e6105250ae8c    22596701          61.042850              71.042850
2018-04-29 17:38:11,430 - memory_profile6_log - INFO - 

2018-04-29 17:38:11,431 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 253995
2018-04-29 17:38:11,433 - memory_profile6_log - INFO - 

2018-04-29 17:38:11,503 - memory_profile6_log - INFO -                                 user_id    topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci
0  107e02cd-54af-40fd-a602-e6105250ae8c  1046474724         951.062678             961.062678   0.000090
1  107e02cd-54af-40fd-a602-e6105250ae8c  1060896301         673.030242             683.030242   0.000065
2  107e02cd-54af-40fd-a602-e6105250ae8c   188400148          72.193555              82.193555   0.036200
3  107e02cd-54af-40fd-a602-e6105250ae8c    22553543          35.980922              45.980922   0.107319
4  107e02cd-54af-40fd-a602-e6105250ae8c    22596701          61.042850              71.042850   0.040442
5  107e02cd-54af-40fd-a602-e6105250ae8c    22601470          16.743893              26.743893   0.057919
6  107e02cd-54af-40fd-a602-e6105250ae8c    22617325         569.663823             579.663823   0.002035
7  107e02cd-54af-40fd-a602-e6105250ae8c    22661796          78.768995              88.768995   0.019008
8  107e02cd-54af-40fd-a602-e6105250ae8c    27312625         287.778448             297.778448   0.003279
9  107e02cd-54af-40fd-a602-e6105250ae8c    27313228        1290.552835            1300.552835   0.000136
2018-04-29 17:38:11,506 - memory_profile6_log - INFO - 

2018-04-29 17:38:52,265 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-29 17:38:52,295 - memory_profile6_log - INFO - Total train time: 42.916s
2018-04-29 17:38:52,296 - memory_profile6_log - INFO - memory left before cleaning: 76.200 percent memory...
2018-04-29 17:38:52,298 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-29 17:38:52,299 - memory_profile6_log - INFO - deleting df_dut...
2018-04-29 17:38:52,299 - memory_profile6_log - INFO - deleting df_dt...
2018-04-29 17:38:52,301 - memory_profile6_log - INFO - deleting df_input...
2018-04-29 17:38:52,309 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-29 17:38:52,311 - memory_profile6_log - INFO - deleting df_current...
2018-04-29 17:38:52,312 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-29 17:38:52,322 - memory_profile6_log - INFO - deleting model_fit...
2018-04-29 17:38:52,323 - memory_profile6_log - INFO - deleting result...
2018-04-29 17:38:52,342 - memory_profile6_log - INFO - deleting model_transform...
2018-04-29 17:38:52,344 - memory_profile6_log - INFO - memory left after cleaning: 76.000 percent memory...
2018-04-29 17:38:52,345 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-29 17:38:52,346 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-29 17:38:52,526 - memory_profile6_log - INFO - deleting BR...
2018-04-29 17:38:52,532 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 17:38:52,532 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 17:38:52,533 - memory_profile6_log - INFO - ================================================

2018-04-29 17:38:52,533 - memory_profile6_log - INFO -    113    418.8 MiB    418.8 MiB   @profile

2018-04-29 17:38:52,536 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-29 17:38:52,536 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-29 17:38:52,538 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-29 17:38:52,538 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-29 17:38:52,538 - memory_profile6_log - INFO -    118                                      saveto="datastore", fitted_models_hist=None):

2018-04-29 17:38:52,539 - memory_profile6_log - INFO -    119                                 """

2018-04-29 17:38:52,539 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-29 17:38:52,539 - memory_profile6_log - INFO -    121                                 """

2018-04-29 17:38:52,540 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-29 17:38:52,543 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-29 17:38:52,545 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-29 17:38:52,546 - memory_profile6_log - INFO -    125    418.8 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-29 17:38:52,546 - memory_profile6_log - INFO -    126    426.8 MiB      8.0 MiB       df_dut = df_input.copy(deep=True)

2018-04-29 17:38:52,546 - memory_profile6_log - INFO -    127    426.8 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 17:38:52,546 - memory_profile6_log - INFO -    128                             

2018-04-29 17:38:52,548 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-29 17:38:52,548 - memory_profile6_log - INFO -    130    434.0 MiB      7.3 MiB       df_dt = df_current.copy(deep=True)

2018-04-29 17:38:52,549 - memory_profile6_log - INFO -    131    434.0 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 17:38:52,549 - memory_profile6_log - INFO -    132                             

2018-04-29 17:38:52,555 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-29 17:38:52,555 - memory_profile6_log - INFO -    134    434.0 MiB      0.0 MiB       t0 = time.time()

2018-04-29 17:38:52,559 - memory_profile6_log - INFO -    135    434.0 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-29 17:38:52,559 - memory_profile6_log - INFO -    136    434.0 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-29 17:38:52,559 - memory_profile6_log - INFO -    137    434.0 MiB      0.0 MiB       logger.info("apply on: %d total history...)", len(df_hist))

2018-04-29 17:38:52,561 - memory_profile6_log - INFO -    138                             

2018-04-29 17:38:52,561 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-29 17:38:52,561 - memory_profile6_log - INFO -    140    434.0 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-29 17:38:52,562 - memory_profile6_log - INFO -    141                             

2018-04-29 17:38:52,562 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-29 17:38:52,562 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-29 17:38:52,562 - memory_profile6_log - INFO -    144    436.6 MiB      2.5 MiB       NB = BR.processX(df_dut)

2018-04-29 17:38:52,562 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-29 17:38:52,565 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-29 17:38:52,566 - memory_profile6_log - INFO -    147    446.4 MiB      9.8 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-29 17:38:52,568 - memory_profile6_log - INFO -    148                                 """

2018-04-29 17:38:52,569 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-29 17:38:52,569 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-29 17:38:52,569 - memory_profile6_log - INFO -    151                                 """

2018-04-29 17:38:52,569 - memory_profile6_log - INFO -    152    446.4 MiB      0.0 MiB       fitby_sigmant = True

2018-04-29 17:38:52,569 - memory_profile6_log - INFO -    153    446.4 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-29 17:38:52,571 - memory_profile6_log - INFO -    154    446.4 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-29 17:38:52,571 - memory_profile6_log - INFO -    155    456.1 MiB      9.7 MiB                            'is_general']]

2018-04-29 17:38:52,571 - memory_profile6_log - INFO -    156                                 # get sigma_Nt from fitted_models_hist

2018-04-29 17:38:52,572 - memory_profile6_log - INFO -    157    456.4 MiB      0.3 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-04-29 17:38:52,572 - memory_profile6_log - INFO -    158    456.4 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-04-29 17:38:52,572 - memory_profile6_log - INFO -    159    456.5 MiB      0.1 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-04-29 17:38:52,572 - memory_profile6_log - INFO -    160    456.5 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-04-29 17:38:52,578 - memory_profile6_log - INFO -    161                             

2018-04-29 17:38:52,578 - memory_profile6_log - INFO -    162                                 # begin fit

2018-04-29 17:38:52,579 - memory_profile6_log - INFO -    163    456.5 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-29 17:38:52,581 - memory_profile6_log - INFO -    164    456.5 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-29 17:38:52,581 - memory_profile6_log - INFO -    165    485.4 MiB     28.9 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-04-29 17:38:52,582 - memory_profile6_log - INFO -    166    485.4 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-29 17:38:52,584 - memory_profile6_log - INFO -    167    485.4 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-29 17:38:52,584 - memory_profile6_log - INFO -    168                             

2018-04-29 17:38:52,584 - memory_profile6_log - INFO -    169                                 # ~~ and Transform ~~

2018-04-29 17:38:52,585 - memory_profile6_log - INFO -    170                                 #   handling current news interest == current date

2018-04-29 17:38:52,585 - memory_profile6_log - INFO -    171    485.4 MiB      0.0 MiB       if df_dt.empty:

2018-04-29 17:38:52,585 - memory_profile6_log - INFO -    172                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-29 17:38:52,585 - memory_profile6_log - INFO -    173                                     return None

2018-04-29 17:38:52,585 - memory_profile6_log - INFO -    174    485.8 MiB      0.3 MiB       NB = BR.processX(df_dt)

2018-04-29 17:38:52,586 - memory_profile6_log - INFO -    175    496.4 MiB     10.6 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-29 17:38:52,586 - memory_profile6_log - INFO -    176                             

2018-04-29 17:38:52,586 - memory_profile6_log - INFO -    177    496.4 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-29 17:38:52,591 - memory_profile6_log - INFO -    178    495.5 MiB     -0.9 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-29 17:38:52,591 - memory_profile6_log - INFO -    179                             

2018-04-29 17:38:52,592 - memory_profile6_log - INFO -    180    495.5 MiB      0.0 MiB       print "model_fit dtypes:\n", model_fit.dtypes

2018-04-29 17:38:52,592 - memory_profile6_log - INFO -    181    495.5 MiB      0.0 MiB       print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-04-29 17:38:52,594 - memory_profile6_log - INFO -    182    495.5 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-29 17:38:52,594 - memory_profile6_log - INFO -    183    495.5 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-29 17:38:52,595 - memory_profile6_log - INFO -    184    495.5 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-04-29 17:38:52,595 - memory_profile6_log - INFO -    185    495.5 MiB      0.0 MiB                                                                                           "topic_id", "user_id"]],

2018-04-29 17:38:52,595 - memory_profile6_log - INFO -    186    513.9 MiB     18.4 MiB                                                     verbose=False)

2018-04-29 17:38:52,595 - memory_profile6_log - INFO -    187                                 # ~~~ filter is general and specific topic ~~~

2018-04-29 17:38:52,595 - memory_profile6_log - INFO -    188                                 # the idea is just we need to rerank every topic according

2018-04-29 17:38:52,596 - memory_profile6_log - INFO -    189                                 # user_id and and is_general by p0_posterior

2018-04-29 17:38:52,596 - memory_profile6_log - INFO -    190    513.9 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-29 17:38:52,596 - memory_profile6_log - INFO -    191    515.8 MiB      1.9 MiB                                     'is_general']].groupby(['topic_id',

2018-04-29 17:38:52,598 - memory_profile6_log - INFO -    192    513.9 MiB     -1.9 MiB                                                             'is_general']

2018-04-29 17:38:52,598 - memory_profile6_log - INFO -    193                                                                                      ).size().to_frame().reset_index()

2018-04-29 17:38:52,602 - memory_profile6_log - INFO -    194                             

2018-04-29 17:38:52,604 - memory_profile6_log - INFO -    195                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-29 17:38:52,605 - memory_profile6_log - INFO -    196                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-29 17:38:52,605 - memory_profile6_log - INFO -    197    513.9 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-29 17:38:52,605 - memory_profile6_log - INFO -    198                             

2018-04-29 17:38:52,607 - memory_profile6_log - INFO -    199                                 # ~ start by provide rank for each topic type ~

2018-04-29 17:38:52,608 - memory_profile6_log - INFO -    200    536.2 MiB     22.3 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-29 17:38:52,608 - memory_profile6_log - INFO -    201    539.8 MiB      3.6 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-29 17:38:52,609 - memory_profile6_log - INFO -    202                             

2018-04-29 17:38:52,609 - memory_profile6_log - INFO -    203                                 # ~ set threshold to filter output

2018-04-29 17:38:52,609 - memory_profile6_log - INFO -    204    539.8 MiB      0.0 MiB       if threshold > 0:

2018-04-29 17:38:52,611 - memory_profile6_log - INFO -    205    539.8 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-29 17:38:52,611 - memory_profile6_log - INFO -    206    539.8 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-29 17:38:52,614 - memory_profile6_log - INFO -    207    539.1 MiB     -0.7 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-29 17:38:52,615 - memory_profile6_log - INFO -    208                             

2018-04-29 17:38:52,618 - memory_profile6_log - INFO -    209    539.1 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 17:38:52,618 - memory_profile6_log - INFO -    210    539.1 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-29 17:38:52,618 - memory_profile6_log - INFO -    211                             

2018-04-29 17:38:52,618 - memory_profile6_log - INFO -    212    539.1 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 17:38:52,618 - memory_profile6_log - INFO -    213                             

2018-04-29 17:38:52,619 - memory_profile6_log - INFO -    214    539.1 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-29 17:38:52,619 - memory_profile6_log - INFO -    215    539.1 MiB      0.0 MiB       del df_dut

2018-04-29 17:38:52,621 - memory_profile6_log - INFO -    216    539.1 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-29 17:38:52,621 - memory_profile6_log - INFO -    217    539.1 MiB      0.0 MiB       del df_dt

2018-04-29 17:38:52,621 - memory_profile6_log - INFO -    218    539.1 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-29 17:38:52,621 - memory_profile6_log - INFO -    219    539.1 MiB      0.0 MiB       del df_input

2018-04-29 17:38:52,621 - memory_profile6_log - INFO -    220    539.1 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-29 17:38:52,625 - memory_profile6_log - INFO -    221    530.3 MiB     -8.8 MiB       del df_input_X

2018-04-29 17:38:52,627 - memory_profile6_log - INFO -    222    530.3 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-29 17:38:52,627 - memory_profile6_log - INFO -    223    530.3 MiB      0.0 MiB       del df_current

2018-04-29 17:38:52,628 - memory_profile6_log - INFO -    224    530.3 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-29 17:38:52,628 - memory_profile6_log - INFO -    225    530.3 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-29 17:38:52,628 - memory_profile6_log - INFO -    226    530.3 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-29 17:38:52,628 - memory_profile6_log - INFO -    227    507.0 MiB    -23.3 MiB       del model_fit

2018-04-29 17:38:52,628 - memory_profile6_log - INFO -    228    507.0 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-29 17:38:52,630 - memory_profile6_log - INFO -    229    507.0 MiB      0.0 MiB       del result

2018-04-29 17:38:52,630 - memory_profile6_log - INFO -    230    507.0 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-29 17:38:52,631 - memory_profile6_log - INFO -    231                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 17:38:52,631 - memory_profile6_log - INFO -    232                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 17:38:52,631 - memory_profile6_log - INFO -    233                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 17:38:52,634 - memory_profile6_log - INFO -    234    507.0 MiB      0.0 MiB       if savetrain:

2018-04-29 17:38:52,635 - memory_profile6_log - INFO -    235    512.5 MiB      5.4 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-29 17:38:52,637 - memory_profile6_log - INFO -    236    512.5 MiB      0.0 MiB           del model_transform

2018-04-29 17:38:52,638 - memory_profile6_log - INFO -    237    512.5 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-29 17:38:52,638 - memory_profile6_log - INFO -    238    512.5 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 17:38:52,638 - memory_profile6_log - INFO -    239                             

2018-04-29 17:38:52,638 - memory_profile6_log - INFO -    240    512.5 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-29 17:38:52,638 - memory_profile6_log - INFO -    241                                     # ~ Place your code to save the training model here ~

2018-04-29 17:38:52,640 - memory_profile6_log - INFO -    242    512.5 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-29 17:38:52,641 - memory_profile6_log - INFO -    243    512.5 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-29 17:38:52,641 - memory_profile6_log - INFO -    244    512.5 MiB      0.0 MiB               if multproc:

2018-04-29 17:38:52,642 - memory_profile6_log - INFO -    245                                             # ~ save transform models ~

2018-04-29 17:38:52,642 - memory_profile6_log - INFO -    246    490.3 MiB    -22.1 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-29 17:38:52,642 - memory_profile6_log - INFO -    247                             

2018-04-29 17:38:52,644 - memory_profile6_log - INFO -    248                                             """

2018-04-29 17:38:52,644 - memory_profile6_log - INFO -    249                                             # ~ save fitted models ~

2018-04-29 17:38:52,644 - memory_profile6_log - INFO -    250                                             logger.info("Saving fitted_models as history...")

2018-04-29 17:38:52,648 - memory_profile6_log - INFO -    251                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 17:38:52,648 - memory_profile6_log - INFO -    252                                             fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-29 17:38:52,648 - memory_profile6_log - INFO -    253                             

2018-04-29 17:38:52,651 - memory_profile6_log - INFO -    254                                             X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-29 17:38:52,651 - memory_profile6_log - INFO -    255                                             logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-29 17:38:52,651 - memory_profile6_log - INFO -    256                                             for ix in range(len(X_split)):

2018-04-29 17:38:52,653 - memory_profile6_log - INFO -    257                                                 logger.info("processing batch-%d", ix)

2018-04-29 17:38:52,653 - memory_profile6_log - INFO -    258                                                 mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-29 17:38:52,654 - memory_profile6_log - INFO -    259                             

2018-04-29 17:38:52,654 - memory_profile6_log - INFO -    260                                             del X_split

2018-04-29 17:38:52,654 - memory_profile6_log - INFO -    261                                             logger.info("deleting X_split...")

2018-04-29 17:38:52,654 - memory_profile6_log - INFO -    262                                             del save_sigma_nt

2018-04-29 17:38:52,655 - memory_profile6_log - INFO -    263                                             logger.info("deleting save_sigma_nt...")

2018-04-29 17:38:52,658 - memory_profile6_log - INFO -    264                                             """

2018-04-29 17:38:52,661 - memory_profile6_log - INFO -    265                             

2018-04-29 17:38:52,661 - memory_profile6_log - INFO -    266    490.3 MiB      0.0 MiB                   del BR

2018-04-29 17:38:52,661 - memory_profile6_log - INFO -    267    490.3 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-29 17:38:52,661 - memory_profile6_log - INFO -    268                             

2018-04-29 17:38:52,663 - memory_profile6_log - INFO -    269                                     elif str(saveto).lower() == "elastic":

2018-04-29 17:38:52,663 - memory_profile6_log - INFO -    270                                         logger.info("Using ElasticSearch as storage...")

2018-04-29 17:38:52,664 - memory_profile6_log - INFO -    271                                         mh.saveElasticS(model_transformsv)

2018-04-29 17:38:52,664 - memory_profile6_log - INFO -    272                             

2018-04-29 17:38:52,664 - memory_profile6_log - INFO -    273                                     # need save sigma_nt for daily train

2018-04-29 17:38:52,664 - memory_profile6_log - INFO -    274                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-29 17:38:52,667 - memory_profile6_log - INFO -    275                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-29 17:38:52,670 - memory_profile6_log - INFO -    276    490.3 MiB      0.0 MiB           if start_date and end_date:

2018-04-29 17:38:52,671 - memory_profile6_log - INFO -    277                                         if not fitby_sigmant:

2018-04-29 17:38:52,671 - memory_profile6_log - INFO -    278                                             logging.info("Saving sigma Nt...")

2018-04-29 17:38:52,671 - memory_profile6_log - INFO -    279                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 17:38:52,673 - memory_profile6_log - INFO -    280                                             save_sigma_nt['start_date'] = start_date

2018-04-29 17:38:52,676 - memory_profile6_log - INFO -    281                                             save_sigma_nt['end_date'] = end_date

2018-04-29 17:38:52,676 - memory_profile6_log - INFO -    282                                             print save_sigma_nt.head(5)

2018-04-29 17:38:52,677 - memory_profile6_log - INFO -    283                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-29 17:38:52,677 - memory_profile6_log - INFO -    284                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-29 17:38:52,677 - memory_profile6_log - INFO -    285    490.3 MiB      0.0 MiB       return

2018-04-29 17:38:52,677 - memory_profile6_log - INFO - 


2018-04-29 17:38:52,677 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-29 18:28:23,822 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 18:28:23,825 - memory_profile6_log - INFO - date_generated: 
2018-04-29 18:28:23,825 - memory_profile6_log - INFO -  
2018-04-29 18:28:23,825 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 18:28:23,826 - memory_profile6_log - INFO - 

2018-04-29 18:28:23,826 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 18:28:23,828 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 18:28:23,828 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 18:28:23,953 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 18:28:23,957 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 18:29:34,921 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 18:29:34,921 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 18:29:34,963 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 18:29:34,963 - memory_profile6_log - INFO - Appending history data...
2018-04-29 18:29:34,964 - memory_profile6_log - INFO - processing batch-0
2018-04-29 18:29:34,966 - memory_profile6_log - INFO - creating list history data...
2018-04-29 18:29:35,007 - memory_profile6_log - INFO - call history data...
2018-04-29 18:30:07,163 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 18:30:07,826 - memory_profile6_log - INFO - processing batch-1
2018-04-29 18:30:07,828 - memory_profile6_log - INFO - creating list history data...
2018-04-29 18:30:07,836 - memory_profile6_log - INFO - call history data...
2018-04-29 18:30:37,776 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 18:30:38,444 - memory_profile6_log - INFO - processing batch-2
2018-04-29 18:30:38,447 - memory_profile6_log - INFO - creating list history data...
2018-04-29 18:30:38,454 - memory_profile6_log - INFO - call history data...
2018-04-29 18:31:07,184 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 18:31:07,842 - memory_profile6_log - INFO - processing batch-3
2018-04-29 18:31:07,842 - memory_profile6_log - INFO - creating list history data...
2018-04-29 18:31:07,851 - memory_profile6_log - INFO - call history data...
2018-04-29 18:31:35,867 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 18:31:36,525 - memory_profile6_log - INFO - processing batch-4
2018-04-29 18:31:36,526 - memory_profile6_log - INFO - creating list history data...
2018-04-29 18:31:36,536 - memory_profile6_log - INFO - call history data...
2018-04-29 18:32:04,362 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 18:32:05,019 - memory_profile6_log - INFO - Appending training data...
2018-04-29 18:32:05,020 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 18:32:05,022 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 18:32:05,023 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 18:32:05,023 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 18:32:05,025 - memory_profile6_log - INFO - ================================================

2018-04-29 18:32:05,026 - memory_profile6_log - INFO -    311     87.0 MiB     87.0 MiB   @profile

2018-04-29 18:32:05,026 - memory_profile6_log - INFO -    312                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 18:32:05,029 - memory_profile6_log - INFO -    313     87.0 MiB      0.0 MiB       bq_client = client

2018-04-29 18:32:05,030 - memory_profile6_log - INFO -    314     87.0 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 18:32:05,030 - memory_profile6_log - INFO -    315                             

2018-04-29 18:32:05,032 - memory_profile6_log - INFO -    316     87.0 MiB      0.0 MiB       datalist = []

2018-04-29 18:32:05,032 - memory_profile6_log - INFO -    317     87.0 MiB      0.0 MiB       datalist_hist = []

2018-04-29 18:32:05,033 - memory_profile6_log - INFO -    318                             

2018-04-29 18:32:05,035 - memory_profile6_log - INFO -    319     87.0 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 18:32:05,036 - memory_profile6_log - INFO -    320    352.0 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 18:32:05,036 - memory_profile6_log - INFO -    321    340.1 MiB    253.0 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 18:32:05,038 - memory_profile6_log - INFO -    322    340.1 MiB      0.0 MiB           if tframe is not None:

2018-04-29 18:32:05,040 - memory_profile6_log - INFO -    323    340.1 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 18:32:05,042 - memory_profile6_log - INFO -    324    347.7 MiB      7.6 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 18:32:05,042 - memory_profile6_log - INFO -    325    347.7 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 18:32:05,043 - memory_profile6_log - INFO -    326    347.7 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 18:32:05,045 - memory_profile6_log - INFO -    327    352.0 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 18:32:05,046 - memory_profile6_log - INFO -    328                                                 # ~ loading history

2018-04-29 18:32:05,046 - memory_profile6_log - INFO -    329                                                 """

2018-04-29 18:32:05,048 - memory_profile6_log - INFO -    330                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 18:32:05,051 - memory_profile6_log - INFO -    331                                                 """

2018-04-29 18:32:05,052 - memory_profile6_log - INFO -    332    351.9 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 18:32:05,052 - memory_profile6_log - INFO -    333                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 18:32:05,055 - memory_profile6_log - INFO -    334    351.9 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 18:32:05,055 - memory_profile6_log - INFO -    335    351.9 MiB      0.6 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 18:32:05,056 - memory_profile6_log - INFO -    336                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 18:32:05,058 - memory_profile6_log - INFO -    337                             

2018-04-29 18:32:05,061 - memory_profile6_log - INFO -    338    351.9 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 18:32:05,062 - memory_profile6_log - INFO -    339    352.0 MiB      3.0 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 18:32:05,063 - memory_profile6_log - INFO -    340                             

2018-04-29 18:32:05,065 - memory_profile6_log - INFO -    341                                                 # me = os.getpid()

2018-04-29 18:32:05,065 - memory_profile6_log - INFO -    342                                                 # kill_proc_tree(me)

2018-04-29 18:32:05,066 - memory_profile6_log - INFO -    343                             

2018-04-29 18:32:05,066 - memory_profile6_log - INFO -    344    352.0 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 18:32:05,069 - memory_profile6_log - INFO -    345    352.0 MiB      0.0 MiB                       for m in h_frame:

2018-04-29 18:32:05,072 - memory_profile6_log - INFO -    346    352.0 MiB      0.0 MiB                           if m is not None:

2018-04-29 18:32:05,075 - memory_profile6_log - INFO -    347    352.0 MiB      0.0 MiB                               if len(m) > 0:

2018-04-29 18:32:05,075 - memory_profile6_log - INFO -    348    352.0 MiB      0.8 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 18:32:05,076 - memory_profile6_log - INFO -    349    352.0 MiB      0.0 MiB                       del h_frame

2018-04-29 18:32:05,078 - memory_profile6_log - INFO -    350    352.0 MiB      0.0 MiB                       del lhistory

2018-04-29 18:32:05,078 - memory_profile6_log - INFO -    351                             

2018-04-29 18:32:05,079 - memory_profile6_log - INFO -    352    352.0 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 18:32:05,082 - memory_profile6_log - INFO -    353    352.0 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 18:32:05,085 - memory_profile6_log - INFO -    354                                     else: 

2018-04-29 18:32:05,085 - memory_profile6_log - INFO -    355                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 18:32:05,088 - memory_profile6_log - INFO -    356    352.0 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 18:32:05,088 - memory_profile6_log - INFO -    357    352.0 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 18:32:05,091 - memory_profile6_log - INFO -    358                             

2018-04-29 18:32:05,092 - memory_profile6_log - INFO -    359    352.0 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 18:32:05,095 - memory_profile6_log - INFO - 


2018-04-29 18:32:06,213 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 18:32:06,282 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0   0.000646        1844.325967        32            1854.325967  10960288  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
1   0.000646        1844.325967        16            1854.325967  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
2   0.000646        1844.325967        16            1854.325967  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
3   0.000646        1844.325967        16            1854.325967  10960288  1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...
4   0.000646        1844.325967        32            1854.325967  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
5        NaN      333823.000000       256          333833.000000  10959239  161129f951435-0d660a18dbcd87-173a7640-100200-1...
6   0.000646        1844.325967        32            1854.325967  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
0   0.000646        1844.325967         8            1854.325967  10960288  16169731ce618-00742c5e307396-5b4d6a36-38400-16...
1   0.000646        1844.325967        16            1854.325967  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
2   0.000646        1844.325967        16            1854.325967  10960288  1626fd160d9114-055d801ae6b39d-66265f05-38400-1...
3   0.000646        1844.325967         8            1854.325967  10960288  161e219cd891c8-0e882943705df4-15290f5f-38400-1...
4   0.000646        1844.325967         8            1854.325967  10960288  1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...
5   0.000646        1844.325967        16            1854.325967  10960288  161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...
0   0.000646        1844.325967         4            1854.325967  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
1   0.000646        1844.325967         4            1854.325967  10960288  1612d88e55ab8-09af9658f30a46-65117b23-38400-16...
2   0.000646        1844.325967         8            1854.325967  10960288  16149f0a1611-00a034775-63446032-38400-16149f0a...
3   0.000646        1844.325967         8            1854.325967  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
4   0.000646        1844.325967         4            1854.325967  10960288  16132f1045f30b-0613b72b33445e-5d640103-38400-1...
5   0.000646        1844.325967         4            1854.325967  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
6   0.000646        1844.325967         4            1854.325967  10960288  16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...
2018-04-29 18:32:06,286 - memory_profile6_log - INFO - 

2018-04-29 18:32:06,295 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 18:32:06,362 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 18:32:06,375 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 18:32:40,907 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 18:32:40,910 - memory_profile6_log - INFO - date_generated: 
2018-04-29 18:32:40,910 - memory_profile6_log - INFO -  
2018-04-29 18:32:40,911 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 18:32:40,911 - memory_profile6_log - INFO - 

2018-04-29 18:32:40,911 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 18:32:40,913 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 18:32:40,913 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 18:32:41,040 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 18:32:41,045 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 18:33:51,947 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 18:33:51,950 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 18:33:51,993 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 18:33:51,994 - memory_profile6_log - INFO - Appending history data...
2018-04-29 18:33:51,996 - memory_profile6_log - INFO - processing batch-0
2018-04-29 18:33:51,997 - memory_profile6_log - INFO - creating list history data...
2018-04-29 18:33:52,039 - memory_profile6_log - INFO - call history data...
2018-04-29 18:34:21,138 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 18:34:21,793 - memory_profile6_log - INFO - processing batch-1
2018-04-29 18:34:21,795 - memory_profile6_log - INFO - creating list history data...
2018-04-29 18:34:21,802 - memory_profile6_log - INFO - call history data...
2018-04-29 18:34:49,115 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 18:34:49,828 - memory_profile6_log - INFO - processing batch-2
2018-04-29 18:34:49,829 - memory_profile6_log - INFO - creating list history data...
2018-04-29 18:34:49,836 - memory_profile6_log - INFO - call history data...
2018-04-29 18:35:17,161 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 18:35:17,829 - memory_profile6_log - INFO - processing batch-3
2018-04-29 18:35:17,831 - memory_profile6_log - INFO - creating list history data...
2018-04-29 18:35:17,838 - memory_profile6_log - INFO - call history data...
2018-04-29 18:35:45,269 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 18:35:45,961 - memory_profile6_log - INFO - processing batch-4
2018-04-29 18:35:45,963 - memory_profile6_log - INFO - creating list history data...
2018-04-29 18:35:45,970 - memory_profile6_log - INFO - call history data...
2018-04-29 18:36:13,907 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 18:36:14,581 - memory_profile6_log - INFO - Appending training data...
2018-04-29 18:36:14,582 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 18:36:14,585 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 18:36:14,586 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 18:36:14,588 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 18:36:14,588 - memory_profile6_log - INFO - ================================================

2018-04-29 18:36:14,588 - memory_profile6_log - INFO -    311     86.6 MiB     86.6 MiB   @profile

2018-04-29 18:36:14,589 - memory_profile6_log - INFO -    312                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 18:36:14,592 - memory_profile6_log - INFO -    313     86.6 MiB      0.0 MiB       bq_client = client

2018-04-29 18:36:14,592 - memory_profile6_log - INFO -    314     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 18:36:14,594 - memory_profile6_log - INFO -    315                             

2018-04-29 18:36:14,594 - memory_profile6_log - INFO -    316     86.6 MiB      0.0 MiB       datalist = []

2018-04-29 18:36:14,595 - memory_profile6_log - INFO -    317     86.6 MiB      0.0 MiB       datalist_hist = []

2018-04-29 18:36:14,595 - memory_profile6_log - INFO -    318                             

2018-04-29 18:36:14,596 - memory_profile6_log - INFO -    319     86.6 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 18:36:14,596 - memory_profile6_log - INFO -    320    350.3 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 18:36:14,598 - memory_profile6_log - INFO -    321    340.5 MiB    253.9 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 18:36:14,598 - memory_profile6_log - INFO -    322    340.5 MiB      0.0 MiB           if tframe is not None:

2018-04-29 18:36:14,599 - memory_profile6_log - INFO -    323    340.5 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 18:36:14,599 - memory_profile6_log - INFO -    324    347.0 MiB      6.5 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 18:36:14,602 - memory_profile6_log - INFO -    325    347.0 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 18:36:14,604 - memory_profile6_log - INFO -    326    347.0 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 18:36:14,605 - memory_profile6_log - INFO -    327    350.3 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 18:36:14,605 - memory_profile6_log - INFO -    328                                                 # ~ loading history

2018-04-29 18:36:14,605 - memory_profile6_log - INFO -    329                                                 """

2018-04-29 18:36:14,607 - memory_profile6_log - INFO -    330                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 18:36:14,608 - memory_profile6_log - INFO -    331                                                 """

2018-04-29 18:36:14,608 - memory_profile6_log - INFO -    332    350.2 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 18:36:14,608 - memory_profile6_log - INFO -    333                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 18:36:14,609 - memory_profile6_log - INFO -    334    350.2 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 18:36:14,609 - memory_profile6_log - INFO -    335    350.2 MiB      0.6 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 18:36:14,611 - memory_profile6_log - INFO -    336                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 18:36:14,614 - memory_profile6_log - INFO -    337                             

2018-04-29 18:36:14,615 - memory_profile6_log - INFO -    338    350.2 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 18:36:14,615 - memory_profile6_log - INFO -    339    350.3 MiB      1.9 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 18:36:14,617 - memory_profile6_log - INFO -    340                             

2018-04-29 18:36:14,617 - memory_profile6_log - INFO -    341                                                 # me = os.getpid()

2018-04-29 18:36:14,618 - memory_profile6_log - INFO -    342                                                 # kill_proc_tree(me)

2018-04-29 18:36:14,618 - memory_profile6_log - INFO -    343                             

2018-04-29 18:36:14,619 - memory_profile6_log - INFO -    344    350.3 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 18:36:14,619 - memory_profile6_log - INFO -    345    350.3 MiB      0.0 MiB                       for m in h_frame:

2018-04-29 18:36:14,621 - memory_profile6_log - INFO -    346    350.3 MiB      0.0 MiB                           if m is not None:

2018-04-29 18:36:14,621 - memory_profile6_log - INFO -    347    350.3 MiB      0.0 MiB                               if len(m) > 0:

2018-04-29 18:36:14,621 - memory_profile6_log - INFO -    348    350.3 MiB      0.7 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 18:36:14,625 - memory_profile6_log - INFO -    349    350.3 MiB      0.0 MiB                       del h_frame

2018-04-29 18:36:14,625 - memory_profile6_log - INFO -    350    350.3 MiB      0.0 MiB                       del lhistory

2018-04-29 18:36:14,625 - memory_profile6_log - INFO -    351                             

2018-04-29 18:36:14,627 - memory_profile6_log - INFO -    352    350.3 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 18:36:14,628 - memory_profile6_log - INFO -    353    350.3 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 18:36:14,628 - memory_profile6_log - INFO -    354                                     else: 

2018-04-29 18:36:14,628 - memory_profile6_log - INFO -    355                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 18:36:14,630 - memory_profile6_log - INFO -    356    350.3 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 18:36:14,631 - memory_profile6_log - INFO -    357    350.3 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 18:36:14,631 - memory_profile6_log - INFO -    358                             

2018-04-29 18:36:14,632 - memory_profile6_log - INFO -    359    350.3 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 18:36:14,635 - memory_profile6_log - INFO - 


2018-04-29 18:36:15,734 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 18:36:15,805 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0   0.000646        1844.325967        16            1854.325967  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
1   0.000646        1844.325967        16            1854.325967  10960288  1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...
2   0.000646        1844.325967        32            1854.325967  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
3        NaN      333823.000000       256          333833.000000  10959239  161129f951435-0d660a18dbcd87-173a7640-100200-1...
4   0.000646        1844.325967        32            1854.325967  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
5   0.000646        1844.325967        32            1854.325967  10960288  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
6   0.000646        1844.325967        16            1854.325967  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
0   0.000646        1844.325967         8            1854.325967  10960288  1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...
1   0.000646        1844.325967         8            1854.325967  10960288  161e219cd891c8-0e882943705df4-15290f5f-38400-1...
2   0.000646        1844.325967        16            1854.325967  10960288  1626fd160d9114-055d801ae6b39d-66265f05-38400-1...
3   0.000646        1844.325967        16            1854.325967  10960288  161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...
4   0.000646        1844.325967         8            1854.325967  10960288  16169731ce618-00742c5e307396-5b4d6a36-38400-16...
5   0.000646        1844.325967        16            1854.325967  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
0   0.000646        1844.325967         4            1854.325967  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
1   0.000646        1844.325967         4            1854.325967  10960288  16132f1045f30b-0613b72b33445e-5d640103-38400-1...
2   0.000646        1844.325967         8            1854.325967  10960288  16149f0a1611-00a034775-63446032-38400-16149f0a...
3   0.000646        1844.325967         4            1854.325967  10960288  16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...
4   0.000646        1844.325967         4            1854.325967  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
5   0.000646        1844.325967         8            1854.325967  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
6   0.000646        1844.325967         4            1854.325967  10960288  1612d88e55ab8-09af9658f30a46-65117b23-38400-16...
2018-04-29 18:36:15,806 - memory_profile6_log - INFO - 

2018-04-29 18:36:15,816 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 18:36:15,887 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 18:36:15,898 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-04-29 18:36:57,322 - memory_profile6_log - INFO - size of df: 36.25 MB
2018-04-29 18:36:57,325 - memory_profile6_log - INFO - getting total: 148182 training data(current date interest)
2018-04-29 18:36:57,381 - memory_profile6_log - INFO - size of current_frame: 37.38 MB
2018-04-29 18:36:57,381 - memory_profile6_log - INFO - loading time of: 402177 total genuine-current interest data ~ take 256.363s
2018-04-29 18:36:57,388 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 18:36:57,388 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 18:36:57,390 - memory_profile6_log - INFO - ================================================

2018-04-29 18:36:57,391 - memory_profile6_log - INFO -    361     86.5 MiB     86.5 MiB   @profile

2018-04-29 18:36:57,392 - memory_profile6_log - INFO -    362                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 18:36:57,394 - memory_profile6_log - INFO -    363     86.6 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 18:36:57,394 - memory_profile6_log - INFO -    364     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 18:36:57,394 - memory_profile6_log - INFO -    365                             

2018-04-29 18:36:57,395 - memory_profile6_log - INFO -    366                                 # ~~~ Begin collecting data ~~~

2018-04-29 18:36:57,397 - memory_profile6_log - INFO -    367     86.6 MiB      0.0 MiB       t0 = time.time()

2018-04-29 18:36:57,397 - memory_profile6_log - INFO -    368    350.3 MiB    263.6 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 18:36:57,398 - memory_profile6_log - INFO -    369    350.3 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 18:36:57,398 - memory_profile6_log - INFO -    370                                     logger.info("Training cannot be empty..")

2018-04-29 18:36:57,398 - memory_profile6_log - INFO -    371                                     return False

2018-04-29 18:36:57,398 - memory_profile6_log - INFO -    372    348.5 MiB     -1.8 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 18:36:57,400 - memory_profile6_log - INFO -    373    348.6 MiB      0.1 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 18:36:57,403 - memory_profile6_log - INFO -    374    348.6 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 18:36:57,404 - memory_profile6_log - INFO -    375                             

2018-04-29 18:36:57,404 - memory_profile6_log - INFO -    376    354.4 MiB      5.8 MiB       big_frame = pd.concat(datalist)

2018-04-29 18:36:57,404 - memory_profile6_log - INFO -    377    354.4 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 18:36:57,404 - memory_profile6_log - INFO -    378    348.6 MiB     -5.8 MiB       del datalist

2018-04-29 18:36:57,405 - memory_profile6_log - INFO -    379                             

2018-04-29 18:36:57,405 - memory_profile6_log - INFO -    380    348.6 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 18:36:57,407 - memory_profile6_log - INFO -    381                             

2018-04-29 18:36:57,407 - memory_profile6_log - INFO -    382                                 # ~ get current news interest ~

2018-04-29 18:36:57,407 - memory_profile6_log - INFO -    383    348.6 MiB      0.0 MiB       if not cd:

2018-04-29 18:36:57,407 - memory_profile6_log - INFO -    384    348.6 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-04-29 18:36:57,407 - memory_profile6_log - INFO -    385    355.2 MiB      6.6 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 18:36:57,408 - memory_profile6_log - INFO -    386    355.2 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 18:36:57,408 - memory_profile6_log - INFO -    387                                 else:

2018-04-29 18:36:57,410 - memory_profile6_log - INFO -    388                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 18:36:57,410 - memory_profile6_log - INFO -    389                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 18:36:57,410 - memory_profile6_log - INFO -    390                             

2018-04-29 18:36:57,411 - memory_profile6_log - INFO -    391                                     # safe handling of query parameter

2018-04-29 18:36:57,411 - memory_profile6_log - INFO -    392                                     query_params = [

2018-04-29 18:36:57,414 - memory_profile6_log - INFO -    393                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 18:36:57,415 - memory_profile6_log - INFO -    394                                     ]

2018-04-29 18:36:57,417 - memory_profile6_log - INFO -    395                             

2018-04-29 18:36:57,417 - memory_profile6_log - INFO -    396                                     job_config.query_parameters = query_params

2018-04-29 18:36:57,417 - memory_profile6_log - INFO -    397                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 18:36:57,417 - memory_profile6_log - INFO -    398                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 18:36:57,418 - memory_profile6_log - INFO -    399                             

2018-04-29 18:36:57,420 - memory_profile6_log - INFO -    400    356.3 MiB      1.1 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 18:36:57,421 - memory_profile6_log - INFO -    401    356.3 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 18:36:57,421 - memory_profile6_log - INFO -    402    356.4 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 18:36:57,421 - memory_profile6_log - INFO -    403    356.4 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 18:36:57,421 - memory_profile6_log - INFO -    404    356.4 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 18:36:57,423 - memory_profile6_log - INFO -    405    356.4 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 18:36:57,423 - memory_profile6_log - INFO -    406                             

2018-04-29 18:36:57,426 - memory_profile6_log - INFO -    407    356.4 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 18:36:57,427 - memory_profile6_log - INFO - 


2018-04-29 18:36:57,430 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 18:36:57,463 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 18:36:57,463 - memory_profile6_log - INFO - transform on: 148182 total current data(D(t))
2018-04-29 18:36:57,464 - memory_profile6_log - INFO - apply on: 5000 total history...)
2018-04-29 18:36:57,684 - memory_profile6_log - INFO - len of uniques_fit_hist:5000
2018-04-29 18:36:57,690 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:3753
2018-04-29 18:36:58,499 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 18:36:58,500 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 18:36:58,651 - memory_profile6_log - INFO - model_fit dtypes:

2018-04-29 18:36:58,655 - memory_profile6_log - INFO - date              datetime64[ns, UTC]
user_id                        object
topic_id                       object
num_x                           int64
num_y                           int64
is_general                       bool
date_all_click                  int64
Ntotal                          int64
sigma_Nt                        int64
joinprob_ci                   float64
p_cat_ci                      float64
posterior                     float64
dtype: object
2018-04-29 18:36:58,657 - memory_profile6_log - INFO - 

2018-04-29 18:36:58,658 - memory_profile6_log - INFO - fitted_models_hist dtypes:

2018-04-29 18:36:58,664 - memory_profile6_log - INFO - p0_cat_ci                float64
pt_posterior_x_Nt        float64
sigma_Nt                   int64
smoothed_pt_posterior    float64
topic_id                  object
user_id                   object
dtype: object
2018-04-29 18:36:58,665 - memory_profile6_log - INFO - 

2018-04-29 18:36:59,122 - memory_profile6_log - INFO - Len of fitted_models on main class: 253995
2018-04-29 18:36:59,124 - memory_profile6_log - INFO - 

2018-04-29 18:36:59,125 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-29 18:36:59,125 - memory_profile6_log - INFO - 

2018-04-29 18:36:59,127 - memory_profile6_log - INFO - Fitted models before concat:

2018-04-29 18:36:59,148 - memory_profile6_log - INFO -                                 user_id    topic_id  pt_posterior_x_Nt  smoothed_pt_posterior
0  107e02cd-54af-40fd-a602-e6105250ae8c  1046474724         951.062678             961.062678
1  107e02cd-54af-40fd-a602-e6105250ae8c  1060896301         673.030242             683.030242
2  107e02cd-54af-40fd-a602-e6105250ae8c   188400148          72.193555              82.193555
3  107e02cd-54af-40fd-a602-e6105250ae8c    22553543          35.980922              45.980922
4  107e02cd-54af-40fd-a602-e6105250ae8c    22596701          61.042850              71.042850
2018-04-29 18:36:59,148 - memory_profile6_log - INFO - 

2018-04-29 18:36:59,150 - memory_profile6_log - INFO - len of current fitted models: 253995
2018-04-29 18:36:59,151 - memory_profile6_log - INFO - 

2018-04-29 18:36:59,151 - memory_profile6_log - INFO - len of history fitted models: 5000
2018-04-29 18:36:59,153 - memory_profile6_log - INFO - 

2018-04-29 18:36:59,187 - memory_profile6_log - INFO - len of fitted models after concat: 258995
2018-04-29 18:36:59,188 - memory_profile6_log - INFO - 

2018-04-29 18:36:59,190 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-29 18:36:59,190 - memory_profile6_log - INFO - 

2018-04-29 18:36:59,361 - memory_profile6_log - INFO - Fitted models after concat:

2018-04-29 18:36:59,380 - memory_profile6_log - INFO -                                 user_id    topic_id  pt_posterior_x_Nt
0  107e02cd-54af-40fd-a602-e6105250ae8c  1046474724         951.062678
1  107e02cd-54af-40fd-a602-e6105250ae8c  1060896301         673.030242
2  107e02cd-54af-40fd-a602-e6105250ae8c   188400148          72.193555
3  107e02cd-54af-40fd-a602-e6105250ae8c    22553543          35.980922
4  107e02cd-54af-40fd-a602-e6105250ae8c    22596701          61.042850
2018-04-29 18:36:59,381 - memory_profile6_log - INFO - 

2018-04-29 18:36:59,400 - memory_profile6_log - INFO - Fitted models after concat-simplify:

2018-04-29 18:36:59,421 - memory_profile6_log - INFO -                                 user_id    topic_id  pt_posterior_x_Nt  smoothed_pt_posterior
0  107e02cd-54af-40fd-a602-e6105250ae8c  1046474724         951.062678             961.062678
1  107e02cd-54af-40fd-a602-e6105250ae8c  1060896301         673.030242             683.030242
2  107e02cd-54af-40fd-a602-e6105250ae8c   188400148          72.193555              82.193555
3  107e02cd-54af-40fd-a602-e6105250ae8c    22553543          35.980922              45.980922
4  107e02cd-54af-40fd-a602-e6105250ae8c    22596701          61.042850              71.042850
2018-04-29 18:36:59,424 - memory_profile6_log - INFO - 

2018-04-29 18:36:59,424 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 253995
2018-04-29 18:36:59,427 - memory_profile6_log - INFO - 

2018-04-29 18:36:59,516 - memory_profile6_log - INFO -                                 user_id    topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci
0  107e02cd-54af-40fd-a602-e6105250ae8c  1046474724         951.062678             961.062678        NaN
1  107e02cd-54af-40fd-a602-e6105250ae8c  1060896301         673.030242             683.030242        NaN
2  107e02cd-54af-40fd-a602-e6105250ae8c   188400148          72.193555              82.193555   0.006656
3  107e02cd-54af-40fd-a602-e6105250ae8c    22553543          35.980922              45.980922   0.121529
4  107e02cd-54af-40fd-a602-e6105250ae8c    22596701          61.042850              71.042850   0.022806
5  107e02cd-54af-40fd-a602-e6105250ae8c    22601470          16.743893              26.743893   0.054982
6  107e02cd-54af-40fd-a602-e6105250ae8c    22617325         569.663823             579.663823   0.000579
7  107e02cd-54af-40fd-a602-e6105250ae8c    22661796          78.768995              88.768995   0.022598
8  107e02cd-54af-40fd-a602-e6105250ae8c    27312625         287.778448             297.778448   0.001685
9  107e02cd-54af-40fd-a602-e6105250ae8c    27313228        1290.552835            1300.552835   0.000500
2018-04-29 18:36:59,517 - memory_profile6_log - INFO - 

2018-04-29 18:37:41,017 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-29 18:37:41,046 - memory_profile6_log - INFO - Total train time: 43.584s
2018-04-29 18:37:41,048 - memory_profile6_log - INFO - memory left before cleaning: 76.700 percent memory...
2018-04-29 18:37:41,049 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-29 18:37:41,051 - memory_profile6_log - INFO - deleting df_dut...
2018-04-29 18:37:41,052 - memory_profile6_log - INFO - deleting df_dt...
2018-04-29 18:37:41,052 - memory_profile6_log - INFO - deleting df_input...
2018-04-29 18:37:41,059 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-29 18:37:41,059 - memory_profile6_log - INFO - deleting df_current...
2018-04-29 18:37:41,061 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-29 18:37:41,072 - memory_profile6_log - INFO - deleting model_fit...
2018-04-29 18:37:41,072 - memory_profile6_log - INFO - deleting result...
2018-04-29 18:37:41,088 - memory_profile6_log - INFO - deleting model_transform...
2018-04-29 18:37:41,088 - memory_profile6_log - INFO - memory left after cleaning: 76.500 percent memory...
2018-04-29 18:37:41,091 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-29 18:37:41,092 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-29 18:37:41,263 - memory_profile6_log - INFO - deleting BR...
2018-04-29 18:37:41,267 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 18:37:41,269 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 18:37:41,269 - memory_profile6_log - INFO - ================================================

2018-04-29 18:37:41,269 - memory_profile6_log - INFO -    113    356.3 MiB    356.3 MiB   @profile

2018-04-29 18:37:41,269 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-29 18:37:41,270 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-29 18:37:41,270 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-29 18:37:41,273 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-29 18:37:41,275 - memory_profile6_log - INFO -    118                                      saveto="datastore", fitted_models_hist=None):

2018-04-29 18:37:41,276 - memory_profile6_log - INFO -    119                                 """

2018-04-29 18:37:41,276 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-29 18:37:41,276 - memory_profile6_log - INFO -    121                                 """

2018-04-29 18:37:41,276 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-29 18:37:41,278 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-29 18:37:41,278 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-29 18:37:41,279 - memory_profile6_log - INFO -    125    356.3 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-29 18:37:41,279 - memory_profile6_log - INFO -    126    364.1 MiB      7.9 MiB       df_dut = df_input.copy(deep=True)

2018-04-29 18:37:41,279 - memory_profile6_log - INFO -    127    364.1 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 18:37:41,279 - memory_profile6_log - INFO -    128                             

2018-04-29 18:37:41,280 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-29 18:37:41,280 - memory_profile6_log - INFO -    130    368.8 MiB      4.7 MiB       df_dt = df_current.copy(deep=True)

2018-04-29 18:37:41,280 - memory_profile6_log - INFO -    131    368.8 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 18:37:41,285 - memory_profile6_log - INFO -    132                             

2018-04-29 18:37:41,285 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-29 18:37:41,286 - memory_profile6_log - INFO -    134    368.8 MiB      0.0 MiB       t0 = time.time()

2018-04-29 18:37:41,286 - memory_profile6_log - INFO -    135    368.8 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-29 18:37:41,286 - memory_profile6_log - INFO -    136    368.8 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-29 18:37:41,288 - memory_profile6_log - INFO -    137    368.8 MiB      0.0 MiB       logger.info("apply on: %d total history...)", len(df_hist))

2018-04-29 18:37:41,288 - memory_profile6_log - INFO -    138                             

2018-04-29 18:37:41,289 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-29 18:37:41,289 - memory_profile6_log - INFO -    140    368.8 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-29 18:37:41,289 - memory_profile6_log - INFO -    141                             

2018-04-29 18:37:41,289 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-29 18:37:41,289 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-29 18:37:41,289 - memory_profile6_log - INFO -    144    371.5 MiB      2.8 MiB       NB = BR.processX(df_dut)

2018-04-29 18:37:41,290 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-29 18:37:41,290 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-29 18:37:41,290 - memory_profile6_log - INFO -    147    381.4 MiB      9.8 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-29 18:37:41,292 - memory_profile6_log - INFO -    148                                 """

2018-04-29 18:37:41,292 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-29 18:37:41,292 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-29 18:37:41,292 - memory_profile6_log - INFO -    151                                 """

2018-04-29 18:37:41,292 - memory_profile6_log - INFO -    152    381.4 MiB      0.0 MiB       fitby_sigmant = True

2018-04-29 18:37:41,296 - memory_profile6_log - INFO -    153    381.4 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-29 18:37:41,296 - memory_profile6_log - INFO -    154    381.4 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-29 18:37:41,298 - memory_profile6_log - INFO -    155    391.1 MiB      9.7 MiB                            'is_general']]

2018-04-29 18:37:41,299 - memory_profile6_log - INFO -    156                                 # get sigma_Nt from fitted_models_hist

2018-04-29 18:37:41,299 - memory_profile6_log - INFO -    157    391.3 MiB      0.2 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-04-29 18:37:41,299 - memory_profile6_log - INFO -    158    391.3 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-04-29 18:37:41,301 - memory_profile6_log - INFO -    159    391.4 MiB      0.2 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-04-29 18:37:41,301 - memory_profile6_log - INFO -    160    391.4 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-04-29 18:37:41,301 - memory_profile6_log - INFO -    161                             

2018-04-29 18:37:41,302 - memory_profile6_log - INFO -    162                                 # begin fit

2018-04-29 18:37:41,302 - memory_profile6_log - INFO -    163    391.4 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-29 18:37:41,302 - memory_profile6_log - INFO -    164    391.4 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-29 18:37:41,302 - memory_profile6_log - INFO -    165    419.4 MiB     27.9 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-04-29 18:37:41,302 - memory_profile6_log - INFO -    166    419.4 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-29 18:37:41,303 - memory_profile6_log - INFO -    167    419.4 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-29 18:37:41,303 - memory_profile6_log - INFO -    168                             

2018-04-29 18:37:41,303 - memory_profile6_log - INFO -    169                                 # ~~ and Transform ~~

2018-04-29 18:37:41,305 - memory_profile6_log - INFO -    170                                 #   handling current news interest == current date

2018-04-29 18:37:41,305 - memory_profile6_log - INFO -    171    419.4 MiB      0.0 MiB       if df_dt.empty:

2018-04-29 18:37:41,305 - memory_profile6_log - INFO -    172                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-29 18:37:41,309 - memory_profile6_log - INFO -    173                                     return None

2018-04-29 18:37:41,309 - memory_profile6_log - INFO -    174    420.0 MiB      0.6 MiB       NB = BR.processX(df_dt)

2018-04-29 18:37:41,311 - memory_profile6_log - INFO -    175    426.8 MiB      6.8 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-29 18:37:41,311 - memory_profile6_log - INFO -    176                             

2018-04-29 18:37:41,311 - memory_profile6_log - INFO -    177    426.8 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-29 18:37:41,312 - memory_profile6_log - INFO -    178    422.8 MiB     -4.0 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-29 18:37:41,312 - memory_profile6_log - INFO -    179                             

2018-04-29 18:37:41,312 - memory_profile6_log - INFO -    180    422.8 MiB      0.0 MiB       print "model_fit dtypes:\n", model_fit.dtypes

2018-04-29 18:37:41,312 - memory_profile6_log - INFO -    181    422.8 MiB      0.0 MiB       print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-04-29 18:37:41,312 - memory_profile6_log - INFO -    182    422.8 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-29 18:37:41,313 - memory_profile6_log - INFO -    183    422.8 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-29 18:37:41,313 - memory_profile6_log - INFO -    184    422.8 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-04-29 18:37:41,313 - memory_profile6_log - INFO -    185    422.8 MiB      0.0 MiB                                                                                           "topic_id", "user_id"]],

2018-04-29 18:37:41,315 - memory_profile6_log - INFO -    186    441.1 MiB     18.3 MiB                                                     verbose=False)

2018-04-29 18:37:41,315 - memory_profile6_log - INFO -    187                                 # ~~~ filter is general and specific topic ~~~

2018-04-29 18:37:41,315 - memory_profile6_log - INFO -    188                                 # the idea is just we need to rerank every topic according

2018-04-29 18:37:41,315 - memory_profile6_log - INFO -    189                                 # user_id and and is_general by p0_posterior

2018-04-29 18:37:41,315 - memory_profile6_log - INFO -    190    441.1 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-29 18:37:41,315 - memory_profile6_log - INFO -    191    443.0 MiB      1.9 MiB                                     'is_general']].groupby(['topic_id',

2018-04-29 18:37:41,316 - memory_profile6_log - INFO -    192    441.1 MiB     -1.9 MiB                                                             'is_general']

2018-04-29 18:37:41,316 - memory_profile6_log - INFO -    193                                                                                      ).size().to_frame().reset_index()

2018-04-29 18:37:41,318 - memory_profile6_log - INFO -    194                             

2018-04-29 18:37:41,321 - memory_profile6_log - INFO -    195                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-29 18:37:41,322 - memory_profile6_log - INFO -    196                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-29 18:37:41,322 - memory_profile6_log - INFO -    197    441.1 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-29 18:37:41,323 - memory_profile6_log - INFO -    198                             

2018-04-29 18:37:41,323 - memory_profile6_log - INFO -    199                                 # ~ start by provide rank for each topic type ~

2018-04-29 18:37:41,325 - memory_profile6_log - INFO -    200    463.3 MiB     22.1 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-29 18:37:41,325 - memory_profile6_log - INFO -    201    465.7 MiB      2.5 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-29 18:37:41,325 - memory_profile6_log - INFO -    202                             

2018-04-29 18:37:41,325 - memory_profile6_log - INFO -    203                                 # ~ set threshold to filter output

2018-04-29 18:37:41,326 - memory_profile6_log - INFO -    204    465.7 MiB      0.0 MiB       if threshold > 0:

2018-04-29 18:37:41,326 - memory_profile6_log - INFO -    205    465.7 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-29 18:37:41,326 - memory_profile6_log - INFO -    206    465.7 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-29 18:37:41,328 - memory_profile6_log - INFO -    207    464.2 MiB     -1.6 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-29 18:37:41,328 - memory_profile6_log - INFO -    208                             

2018-04-29 18:37:41,328 - memory_profile6_log - INFO -    209    464.2 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 18:37:41,328 - memory_profile6_log - INFO -    210    464.2 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-29 18:37:41,328 - memory_profile6_log - INFO -    211                             

2018-04-29 18:37:41,332 - memory_profile6_log - INFO -    212    464.2 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 18:37:41,332 - memory_profile6_log - INFO -    213                             

2018-04-29 18:37:41,334 - memory_profile6_log - INFO -    214    464.2 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-29 18:37:41,334 - memory_profile6_log - INFO -    215    464.2 MiB      0.0 MiB       del df_dut

2018-04-29 18:37:41,335 - memory_profile6_log - INFO -    216    464.2 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-29 18:37:41,335 - memory_profile6_log - INFO -    217    464.2 MiB      0.0 MiB       del df_dt

2018-04-29 18:37:41,335 - memory_profile6_log - INFO -    218    464.2 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-29 18:37:41,335 - memory_profile6_log - INFO -    219    464.2 MiB      0.0 MiB       del df_input

2018-04-29 18:37:41,335 - memory_profile6_log - INFO -    220    464.2 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-29 18:37:41,336 - memory_profile6_log - INFO -    221    458.5 MiB     -5.7 MiB       del df_input_X

2018-04-29 18:37:41,336 - memory_profile6_log - INFO -    222    458.5 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-29 18:37:41,336 - memory_profile6_log - INFO -    223    458.5 MiB      0.0 MiB       del df_current

2018-04-29 18:37:41,338 - memory_profile6_log - INFO -    224    458.5 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-29 18:37:41,338 - memory_profile6_log - INFO -    225    458.5 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-29 18:37:41,338 - memory_profile6_log - INFO -    226    458.5 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-29 18:37:41,338 - memory_profile6_log - INFO -    227    435.0 MiB    -23.5 MiB       del model_fit

2018-04-29 18:37:41,338 - memory_profile6_log - INFO -    228    435.0 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-29 18:37:41,338 - memory_profile6_log - INFO -    229    435.0 MiB      0.0 MiB       del result

2018-04-29 18:37:41,339 - memory_profile6_log - INFO -    230    435.0 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-29 18:37:41,339 - memory_profile6_log - INFO -    231                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 18:37:41,341 - memory_profile6_log - INFO -    232                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 18:37:41,344 - memory_profile6_log - INFO -    233                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 18:37:41,345 - memory_profile6_log - INFO -    234    435.0 MiB      0.0 MiB       if savetrain:

2018-04-29 18:37:41,345 - memory_profile6_log - INFO -    235    440.2 MiB      5.2 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-29 18:37:41,345 - memory_profile6_log - INFO -    236    440.2 MiB      0.0 MiB           del model_transform

2018-04-29 18:37:41,346 - memory_profile6_log - INFO -    237    440.2 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-29 18:37:41,346 - memory_profile6_log - INFO -    238    440.2 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 18:37:41,346 - memory_profile6_log - INFO -    239                             

2018-04-29 18:37:41,348 - memory_profile6_log - INFO -    240    440.2 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-29 18:37:41,348 - memory_profile6_log - INFO -    241                                     # ~ Place your code to save the training model here ~

2018-04-29 18:37:41,348 - memory_profile6_log - INFO -    242    440.2 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-29 18:37:41,348 - memory_profile6_log - INFO -    243    440.2 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-29 18:37:41,348 - memory_profile6_log - INFO -    244    440.2 MiB      0.0 MiB               if multproc:

2018-04-29 18:37:41,349 - memory_profile6_log - INFO -    245                                             # ~ save transform models ~

2018-04-29 18:37:41,349 - memory_profile6_log - INFO -    246    419.1 MiB    -21.0 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-29 18:37:41,349 - memory_profile6_log - INFO -    247                             

2018-04-29 18:37:41,351 - memory_profile6_log - INFO -    248                                             """

2018-04-29 18:37:41,351 - memory_profile6_log - INFO -    249                                             # ~ save fitted models ~

2018-04-29 18:37:41,351 - memory_profile6_log - INFO -    250                                             logger.info("Saving fitted_models as history...")

2018-04-29 18:37:41,354 - memory_profile6_log - INFO -    251                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 18:37:41,355 - memory_profile6_log - INFO -    252                                             fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-29 18:37:41,355 - memory_profile6_log - INFO -    253                             

2018-04-29 18:37:41,357 - memory_profile6_log - INFO -    254                                             X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-29 18:37:41,358 - memory_profile6_log - INFO -    255                                             logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-29 18:37:41,358 - memory_profile6_log - INFO -    256                                             for ix in range(len(X_split)):

2018-04-29 18:37:41,359 - memory_profile6_log - INFO -    257                                                 logger.info("processing batch-%d", ix)

2018-04-29 18:37:41,361 - memory_profile6_log - INFO -    258                                                 mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-29 18:37:41,361 - memory_profile6_log - INFO -    259                             

2018-04-29 18:37:41,362 - memory_profile6_log - INFO -    260                                             del X_split

2018-04-29 18:37:41,365 - memory_profile6_log - INFO -    261                                             logger.info("deleting X_split...")

2018-04-29 18:37:41,365 - memory_profile6_log - INFO -    262                                             del save_sigma_nt

2018-04-29 18:37:41,367 - memory_profile6_log - INFO -    263                                             logger.info("deleting save_sigma_nt...")

2018-04-29 18:37:41,367 - memory_profile6_log - INFO -    264                                             """

2018-04-29 18:37:41,367 - memory_profile6_log - INFO -    265                             

2018-04-29 18:37:41,368 - memory_profile6_log - INFO -    266    419.1 MiB      0.0 MiB                   del BR

2018-04-29 18:37:41,368 - memory_profile6_log - INFO -    267    419.1 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-29 18:37:41,368 - memory_profile6_log - INFO -    268                             

2018-04-29 18:37:41,368 - memory_profile6_log - INFO -    269                                     elif str(saveto).lower() == "elastic":

2018-04-29 18:37:41,368 - memory_profile6_log - INFO -    270                                         logger.info("Using ElasticSearch as storage...")

2018-04-29 18:37:41,368 - memory_profile6_log - INFO -    271                                         mh.saveElasticS(model_transformsv)

2018-04-29 18:37:41,369 - memory_profile6_log - INFO -    272                             

2018-04-29 18:37:41,369 - memory_profile6_log - INFO -    273                                     # need save sigma_nt for daily train

2018-04-29 18:37:41,369 - memory_profile6_log - INFO -    274                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-29 18:37:41,371 - memory_profile6_log - INFO -    275                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-29 18:37:41,371 - memory_profile6_log - INFO -    276    419.1 MiB      0.0 MiB           if start_date and end_date:

2018-04-29 18:37:41,371 - memory_profile6_log - INFO -    277                                         if not fitby_sigmant:

2018-04-29 18:37:41,371 - memory_profile6_log - INFO -    278                                             logging.info("Saving sigma Nt...")

2018-04-29 18:37:41,371 - memory_profile6_log - INFO -    279                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 18:37:41,371 - memory_profile6_log - INFO -    280                                             save_sigma_nt['start_date'] = start_date

2018-04-29 18:37:41,375 - memory_profile6_log - INFO -    281                                             save_sigma_nt['end_date'] = end_date

2018-04-29 18:37:41,375 - memory_profile6_log - INFO -    282                                             print save_sigma_nt.head(5)

2018-04-29 18:37:41,377 - memory_profile6_log - INFO -    283                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-29 18:37:41,377 - memory_profile6_log - INFO -    284                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-29 18:37:41,378 - memory_profile6_log - INFO -    285    419.1 MiB      0.0 MiB       return

2018-04-29 18:37:41,378 - memory_profile6_log - INFO - 


2018-04-29 18:37:41,378 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-29 19:12:24,809 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 19:12:24,812 - memory_profile6_log - INFO - date_generated: 
2018-04-29 19:12:24,812 - memory_profile6_log - INFO -  
2018-04-29 19:12:24,812 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 9, 0, 0)]
2018-04-29 19:12:24,813 - memory_profile6_log - INFO - 

2018-04-29 19:12:24,813 - memory_profile6_log - INFO - using current date: 2018-04-10
2018-04-29 19:12:24,815 - memory_profile6_log - INFO - using start date: 2018-04-09 00:00:00
2018-04-29 19:12:24,815 - memory_profile6_log - INFO - using end date: 2018-04-09
2018-04-29 19:12:24,950 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 19:12:24,953 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-09
2018-04-29 19:14:22,890 - memory_profile6_log - INFO - size of df: 102.25 MB
2018-04-29 19:14:22,891 - memory_profile6_log - INFO - getting total: 403532 training data(genuine interest) for date: 2018-04-09
2018-04-29 19:14:22,947 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 19:14:22,947 - memory_profile6_log - INFO - Appending history data...
2018-04-29 19:14:22,948 - memory_profile6_log - INFO - processing batch-0
2018-04-29 19:14:22,950 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:14:22,990 - memory_profile6_log - INFO - call history data...
2018-04-29 19:14:55,359 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 19:14:56,073 - memory_profile6_log - INFO - processing batch-1
2018-04-29 19:14:56,075 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:14:56,084 - memory_profile6_log - INFO - call history data...
2018-04-29 19:15:29,852 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 19:15:30,546 - memory_profile6_log - INFO - processing batch-2
2018-04-29 19:15:30,549 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:15:30,556 - memory_profile6_log - INFO - call history data...
2018-04-29 19:16:00,101 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 19:16:00,851 - memory_profile6_log - INFO - processing batch-3
2018-04-29 19:16:00,852 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:16:00,861 - memory_profile6_log - INFO - call history data...
2018-04-29 19:16:30,316 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 19:16:31,029 - memory_profile6_log - INFO - processing batch-4
2018-04-29 19:16:31,030 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:16:31,039 - memory_profile6_log - INFO - call history data...
2018-04-29 19:16:59,282 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 19:16:59,940 - memory_profile6_log - INFO - Appending training data...
2018-04-29 19:16:59,940 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 19:16:59,944 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 19:16:59,944 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 19:16:59,946 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 19:16:59,947 - memory_profile6_log - INFO - ================================================

2018-04-29 19:16:59,947 - memory_profile6_log - INFO -    311     86.6 MiB     86.6 MiB   @profile

2018-04-29 19:16:59,948 - memory_profile6_log - INFO -    312                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 19:16:59,950 - memory_profile6_log - INFO -    313     86.6 MiB      0.0 MiB       bq_client = client

2018-04-29 19:16:59,951 - memory_profile6_log - INFO -    314     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 19:16:59,953 - memory_profile6_log - INFO -    315                             

2018-04-29 19:16:59,953 - memory_profile6_log - INFO -    316     86.6 MiB      0.0 MiB       datalist = []

2018-04-29 19:16:59,954 - memory_profile6_log - INFO -    317     86.6 MiB      0.0 MiB       datalist_hist = []

2018-04-29 19:16:59,956 - memory_profile6_log - INFO -    318                             

2018-04-29 19:16:59,957 - memory_profile6_log - INFO -    319     86.6 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 19:16:59,957 - memory_profile6_log - INFO -    320    407.3 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 19:16:59,959 - memory_profile6_log - INFO -    321    392.5 MiB    305.9 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 19:16:59,960 - memory_profile6_log - INFO -    322    392.5 MiB      0.0 MiB           if tframe is not None:

2018-04-29 19:16:59,961 - memory_profile6_log - INFO -    323    392.5 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 19:16:59,963 - memory_profile6_log - INFO -    324    405.6 MiB     13.1 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 19:16:59,963 - memory_profile6_log - INFO -    325    405.6 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 19:16:59,964 - memory_profile6_log - INFO -    326    405.6 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 19:16:59,966 - memory_profile6_log - INFO -    327    408.1 MiB     -0.8 MiB                   for ix in range(len(X_split)):

2018-04-29 19:16:59,967 - memory_profile6_log - INFO -    328                                                 # ~ loading history

2018-04-29 19:16:59,967 - memory_profile6_log - INFO -    329                                                 """

2018-04-29 19:16:59,969 - memory_profile6_log - INFO -    330                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 19:16:59,969 - memory_profile6_log - INFO -    331                                                 """

2018-04-29 19:16:59,970 - memory_profile6_log - INFO -    332    408.1 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 19:16:59,973 - memory_profile6_log - INFO -    333                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 19:16:59,973 - memory_profile6_log - INFO -    334    408.1 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 19:16:59,976 - memory_profile6_log - INFO -    335    408.1 MiB      0.5 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 19:16:59,976 - memory_profile6_log - INFO -    336                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 19:16:59,977 - memory_profile6_log - INFO -    337                             

2018-04-29 19:16:59,979 - memory_profile6_log - INFO -    338    408.1 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 19:16:59,979 - memory_profile6_log - INFO -    339    407.9 MiB      0.8 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 19:16:59,980 - memory_profile6_log - INFO -    340                             

2018-04-29 19:16:59,980 - memory_profile6_log - INFO -    341                                                 # me = os.getpid()

2018-04-29 19:16:59,983 - memory_profile6_log - INFO -    342                                                 # kill_proc_tree(me)

2018-04-29 19:16:59,983 - memory_profile6_log - INFO -    343                             

2018-04-29 19:16:59,984 - memory_profile6_log - INFO -    344    407.9 MiB     -0.7 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 19:16:59,986 - memory_profile6_log - INFO -    345    408.1 MiB   -129.0 MiB                       for m in h_frame:

2018-04-29 19:16:59,986 - memory_profile6_log - INFO -    346    408.1 MiB   -128.4 MiB                           if m is not None:

2018-04-29 19:16:59,986 - memory_profile6_log - INFO -    347    408.1 MiB   -128.4 MiB                               if len(m) > 0:

2018-04-29 19:16:59,987 - memory_profile6_log - INFO -    348    408.1 MiB   -127.9 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 19:16:59,989 - memory_profile6_log - INFO -    349    408.1 MiB     -0.8 MiB                       del h_frame

2018-04-29 19:16:59,990 - memory_profile6_log - INFO -    350    408.1 MiB     -0.8 MiB                       del lhistory

2018-04-29 19:16:59,990 - memory_profile6_log - INFO -    351                             

2018-04-29 19:16:59,993 - memory_profile6_log - INFO -    352    407.3 MiB     -0.8 MiB                   logger.info("Appending training data...")

2018-04-29 19:16:59,993 - memory_profile6_log - INFO -    353    407.3 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 19:16:59,994 - memory_profile6_log - INFO -    354                                     else: 

2018-04-29 19:16:59,996 - memory_profile6_log - INFO -    355                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 19:16:59,996 - memory_profile6_log - INFO -    356    407.3 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 19:16:59,996 - memory_profile6_log - INFO -    357    407.3 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 19:16:59,997 - memory_profile6_log - INFO -    358                             

2018-04-29 19:16:59,999 - memory_profile6_log - INFO -    359    407.3 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 19:17:00,000 - memory_profile6_log - INFO - 


2018-04-29 19:17:01,226 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 19:17:01,305 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0   0.000023       32511.100478        38           32521.100478  10959239  162198fdaba19d-0a7184d8c8672-592a1d75-38400-16...
1   0.000023        9899.213287        26            9909.213287  10959239  1626b3bae3f39d-0af051a01ecd42-33697b04-fa000-1...
2   0.000023       12089.039256       132           12099.039256  10959239  161ea30efe66-02cbbff5d443c5-97a5d17-38400-161e...
3   0.000023       12299.553500       113           12309.553500  10959239  16159190170d8-04cae6409daab9-24403d3b-38400-16...
4   0.000023        3297.010706       968            3307.010706  10959239  1610ca7cec566b-0ee3a59b262c89-4323461-100200-1...
5   0.000023       22520.710227        48           22530.710227  10959239  162496a89202ff-0684c1912b6e5b-7a6a1535-c0000-1...
6   0.000023        7479.405594       117            7489.405594  10959239  1628496d76732-018452219f5435-586b6e3c-38400-16...
0   0.001543         454.567622       129             464.567622  10960288  16298f93916aed-0539950d283a53-3f3c5501-100200-...
1   0.001543        1832.475728        32            1842.475728  10960288  1627f9280c59-0dde331730a581-1452633d-38400-162...
2   0.000023        4030.241263      1852            4040.241263  10959239  16199bdf440a-0d2220577b1b0e-247b140f-2c880-161...
3   0.000023       34552.322540       292           34562.322540  10959239  162aaa233289-0000c74b29fe93-17347840-100200-16...
4   0.000023        6702.354859      1129            6712.354859  10959239  1610cb3a8c1348-0c7cec1541e7ac-4323461-100200-1...
5   0.000646        1844.325967        32            1854.325967  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
0   0.001543         122.165049        60             132.165049  10960288  162234fe6afa0-0ef93e11855267-50683974-ff000-16...
1   0.001543         190.387089       154             200.387089  10960288  1610cb6edd9629-070a17912465ef-32637402-13c680-...
2   0.001543         318.691431        23             328.691431  10960288  1622c6334e1a0-0fa1a987448a9f8-77313729-4a640-1...
3   0.001543         385.170058       157             395.170058  10960288  16270ed081e195-0acf605a4cedf8-42584954-38400-1...
4   0.001543        1610.180397        59            1620.180397  10960288  161f36169fce02-03becb8d4c95b1-b353461-15f900-1...
5   0.001543         135.738943        54             145.738943  10960288  1613102d53112-0a021dd0e-6a1e1337-38400-1613102...
6   0.001543         203.069540       134             213.069540  10960288  1612d61b91d59-05f2c354a451f3-3d254822-64140-16...
2018-04-29 19:17:01,306 - memory_profile6_log - INFO - 

2018-04-29 19:17:01,316 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 19:17:01,420 - memory_profile6_log - INFO - size of big_frame: 102.25 MB
2018-04-29 19:17:01,440 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-04-29 19:18:39,648 - memory_profile6_log - INFO - size of df: 96.11 MB
2018-04-29 19:18:39,650 - memory_profile6_log - INFO - getting total: 393007 training data(current date interest)
2018-04-29 19:18:39,779 - memory_profile6_log - INFO - size of current_frame: 99.10 MB
2018-04-29 19:18:39,779 - memory_profile6_log - INFO - loading time of: 796539 total genuine-current interest data ~ take 374.852s
2018-04-29 19:18:39,786 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 19:18:39,786 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 19:18:39,788 - memory_profile6_log - INFO - ================================================

2018-04-29 19:18:39,789 - memory_profile6_log - INFO -    361     86.5 MiB     86.5 MiB   @profile

2018-04-29 19:18:39,790 - memory_profile6_log - INFO -    362                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 19:18:39,792 - memory_profile6_log - INFO -    363     86.6 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 19:18:39,792 - memory_profile6_log - INFO -    364     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 19:18:39,793 - memory_profile6_log - INFO -    365                             

2018-04-29 19:18:39,793 - memory_profile6_log - INFO -    366                                 # ~~~ Begin collecting data ~~~

2018-04-29 19:18:39,795 - memory_profile6_log - INFO -    367     86.6 MiB      0.0 MiB       t0 = time.time()

2018-04-29 19:18:39,795 - memory_profile6_log - INFO -    368    404.5 MiB    317.9 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 19:18:39,796 - memory_profile6_log - INFO -    369    404.5 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 19:18:39,798 - memory_profile6_log - INFO -    370                                     logger.info("Training cannot be empty..")

2018-04-29 19:18:39,798 - memory_profile6_log - INFO -    371                                     return False

2018-04-29 19:18:39,798 - memory_profile6_log - INFO -    372    399.3 MiB     -5.2 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 19:18:39,801 - memory_profile6_log - INFO -    373    399.5 MiB      0.2 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 19:18:39,801 - memory_profile6_log - INFO -    374    399.5 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 19:18:39,802 - memory_profile6_log - INFO -    375                             

2018-04-29 19:18:39,802 - memory_profile6_log - INFO -    376    409.3 MiB      9.7 MiB       big_frame = pd.concat(datalist)

2018-04-29 19:18:39,803 - memory_profile6_log - INFO -    377    409.3 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 19:18:39,805 - memory_profile6_log - INFO -    378    400.0 MiB     -9.2 MiB       del datalist

2018-04-29 19:18:39,805 - memory_profile6_log - INFO -    379                             

2018-04-29 19:18:39,806 - memory_profile6_log - INFO -    380    400.0 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 19:18:39,808 - memory_profile6_log - INFO -    381                             

2018-04-29 19:18:39,808 - memory_profile6_log - INFO -    382                                 # ~ get current news interest ~

2018-04-29 19:18:39,808 - memory_profile6_log - INFO -    383    400.0 MiB      0.0 MiB       if not cd:

2018-04-29 19:18:39,811 - memory_profile6_log - INFO -    384    400.0 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-04-29 19:18:39,815 - memory_profile6_log - INFO -    385    477.6 MiB     77.6 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 19:18:39,815 - memory_profile6_log - INFO -    386    477.6 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 19:18:39,815 - memory_profile6_log - INFO -    387                                 else:

2018-04-29 19:18:39,816 - memory_profile6_log - INFO -    388                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 19:18:39,816 - memory_profile6_log - INFO -    389                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 19:18:39,816 - memory_profile6_log - INFO -    390                             

2018-04-29 19:18:39,818 - memory_profile6_log - INFO -    391                                     # safe handling of query parameter

2018-04-29 19:18:39,818 - memory_profile6_log - INFO -    392                                     query_params = [

2018-04-29 19:18:39,819 - memory_profile6_log - INFO -    393                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 19:18:39,819 - memory_profile6_log - INFO -    394                                     ]

2018-04-29 19:18:39,822 - memory_profile6_log - INFO -    395                             

2018-04-29 19:18:39,823 - memory_profile6_log - INFO -    396                                     job_config.query_parameters = query_params

2018-04-29 19:18:39,825 - memory_profile6_log - INFO -    397                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 19:18:39,825 - memory_profile6_log - INFO -    398                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 19:18:39,826 - memory_profile6_log - INFO -    399                             

2018-04-29 19:18:39,826 - memory_profile6_log - INFO -    400    480.6 MiB      3.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 19:18:39,828 - memory_profile6_log - INFO -    401    480.6 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 19:18:39,828 - memory_profile6_log - INFO -    402    480.6 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 19:18:39,828 - memory_profile6_log - INFO -    403    480.6 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 19:18:39,831 - memory_profile6_log - INFO -    404    480.6 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 19:18:39,832 - memory_profile6_log - INFO -    405    480.6 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 19:18:39,834 - memory_profile6_log - INFO -    406                             

2018-04-29 19:18:39,835 - memory_profile6_log - INFO -    407    480.6 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 19:18:39,835 - memory_profile6_log - INFO - 


2018-04-29 19:18:39,839 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 19:18:39,885 - memory_profile6_log - INFO - train on: 403532 total genuine interest data(D(u, t))
2018-04-29 19:18:39,887 - memory_profile6_log - INFO - transform on: 393007 total current data(D(t))
2018-04-29 19:18:39,890 - memory_profile6_log - INFO - apply on: 5000 total history...)
2018-04-29 19:18:40,227 - memory_profile6_log - INFO - len of uniques_fit_hist:5000
2018-04-29 19:18:40,234 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:4481
2018-04-29 19:18:41,533 - memory_profile6_log - INFO - Len of model_fit: 403532
2018-04-29 19:18:41,536 - memory_profile6_log - INFO - Len of df_dut: 403532
2018-04-29 19:18:41,894 - memory_profile6_log - INFO - model_fit dtypes:

2018-04-29 19:18:41,900 - memory_profile6_log - INFO - date              datetime64[ns, UTC]
user_id                        object
topic_id                       object
num_x                           int64
num_y                           int64
is_general                       bool
date_all_click                  int64
Ntotal                          int64
sigma_Nt                        int64
joinprob_ci                   float64
p_cat_ci                      float64
posterior                     float64
dtype: object
2018-04-29 19:18:41,901 - memory_profile6_log - INFO - 

2018-04-29 19:18:41,903 - memory_profile6_log - INFO - fitted_models_hist dtypes:

2018-04-29 19:18:41,910 - memory_profile6_log - INFO - p0_cat_ci                float64
pt_posterior_x_Nt        float64
sigma_Nt                   int64
smoothed_pt_posterior    float64
topic_id                  object
user_id                   object
dtype: object
2018-04-29 19:18:41,911 - memory_profile6_log - INFO - 

2018-04-29 19:18:42,661 - memory_profile6_log - INFO - Len of fitted_models on main class: 403532
2018-04-29 19:18:42,663 - memory_profile6_log - INFO - 

2018-04-29 19:18:42,664 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-29 19:18:42,664 - memory_profile6_log - INFO - 

2018-04-29 19:18:42,667 - memory_profile6_log - INFO - Fitted models before concat:

2018-04-29 19:18:42,688 - memory_profile6_log - INFO -                                              user_id    topic_id  pt_posterior_x_Nt  smoothed_pt_posterior
0  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1064047503       13168.255814           13178.255814
1  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1065124711         209.639023             219.639023
2  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1443761312         829.040996             839.040996
3  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...   152104836         133.420123             143.420123
4  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...    22553543          28.208788              38.208788
2018-04-29 19:18:42,690 - memory_profile6_log - INFO - 

2018-04-29 19:18:42,691 - memory_profile6_log - INFO - len of current fitted models: 403532
2018-04-29 19:18:42,693 - memory_profile6_log - INFO - 

2018-04-29 19:18:42,694 - memory_profile6_log - INFO - len of history fitted models: 5000
2018-04-29 19:18:42,694 - memory_profile6_log - INFO - 

2018-04-29 19:18:42,743 - memory_profile6_log - INFO - len of fitted models after concat: 408532
2018-04-29 19:18:42,744 - memory_profile6_log - INFO - 

2018-04-29 19:18:42,746 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-29 19:18:42,747 - memory_profile6_log - INFO - 

2018-04-29 19:18:42,984 - memory_profile6_log - INFO - Fitted models after concat:

2018-04-29 19:18:43,006 - memory_profile6_log - INFO -                                              user_id    topic_id  pt_posterior_x_Nt
0  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1064047503       13168.255814
1  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1065124711         209.639023
2  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1443761312         829.040996
3  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...   152104836         133.420123
4  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...    22553543          28.208788
2018-04-29 19:18:43,006 - memory_profile6_log - INFO - 

2018-04-29 19:18:43,028 - memory_profile6_log - INFO - Fitted models after concat-simplify:

2018-04-29 19:18:43,051 - memory_profile6_log - INFO -                                              user_id    topic_id  pt_posterior_x_Nt  smoothed_pt_posterior
0  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1064047503       13168.255814           13178.255814
1  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1065124711         209.639023             219.639023
2  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1443761312         829.040996             839.040996
3  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...   152104836         133.420123             143.420123
4  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...    22553543          28.208788              38.208788
2018-04-29 19:18:43,052 - memory_profile6_log - INFO - 

2018-04-29 19:18:43,053 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 403532
2018-04-29 19:18:43,055 - memory_profile6_log - INFO - 

2018-04-29 19:18:43,147 - memory_profile6_log - INFO -                                              user_id           topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci
0  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...         1064047503       13168.255814           13178.255814   0.002257
1  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...         1065124711         209.639023             219.639023   0.000569
2  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...         1443761312         829.040996             839.040996   0.000754
3  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...          152104836         133.420123             143.420123   0.004796
4  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...           22553543          28.208788              38.208788   0.109945
5  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...           22662160         103.478618             113.478618   0.012649
6  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...           27311712         773.545082             783.545082   0.003327
7  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  27431110790312902        7167.531646            7177.531646   0.000012
8  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  27431110790312909        1959.290657            1969.290657   0.000069
9  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  27431110790313245        4225.634328            4235.634328   0.000208
2018-04-29 19:18:43,148 - memory_profile6_log - INFO - 

2018-04-29 19:19:39,857 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-29 19:19:39,905 - memory_profile6_log - INFO - Total train time: 60.020s
2018-04-29 19:19:39,907 - memory_profile6_log - INFO - memory left before cleaning: 79.800 percent memory...
2018-04-29 19:19:39,907 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-29 19:19:39,911 - memory_profile6_log - INFO - deleting df_dut...
2018-04-29 19:19:39,911 - memory_profile6_log - INFO - deleting df_dt...
2018-04-29 19:19:39,913 - memory_profile6_log - INFO - deleting df_input...
2018-04-29 19:19:39,926 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-29 19:19:39,927 - memory_profile6_log - INFO - deleting df_current...
2018-04-29 19:19:39,927 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-29 19:19:39,947 - memory_profile6_log - INFO - deleting model_fit...
2018-04-29 19:19:39,948 - memory_profile6_log - INFO - deleting result...
2018-04-29 19:19:39,974 - memory_profile6_log - INFO - deleting model_transform...
2018-04-29 19:19:39,976 - memory_profile6_log - INFO - memory left after cleaning: 79.500 percent memory...
2018-04-29 19:19:39,977 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-29 19:19:39,979 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-29 19:19:40,148 - memory_profile6_log - INFO - deleting BR...
2018-04-29 19:19:40,157 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 19:19:40,157 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 19:19:40,157 - memory_profile6_log - INFO - ================================================

2018-04-29 19:19:40,160 - memory_profile6_log - INFO -    113    480.6 MiB    480.6 MiB   @profile

2018-04-29 19:19:40,161 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-29 19:19:40,161 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-29 19:19:40,161 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-29 19:19:40,163 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-29 19:19:40,163 - memory_profile6_log - INFO -    118                                      saveto="datastore", fitted_models_hist=None):

2018-04-29 19:19:40,167 - memory_profile6_log - INFO -    119                                 """

2018-04-29 19:19:40,167 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-29 19:19:40,168 - memory_profile6_log - INFO -    121                                 """

2018-04-29 19:19:40,168 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-29 19:19:40,168 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-29 19:19:40,170 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-29 19:19:40,170 - memory_profile6_log - INFO -    125    480.6 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-29 19:19:40,171 - memory_profile6_log - INFO -    126    493.1 MiB     12.4 MiB       df_dut = df_input.copy(deep=True)

2018-04-29 19:19:40,171 - memory_profile6_log - INFO -    127    493.1 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 19:19:40,174 - memory_profile6_log - INFO -    128                             

2018-04-29 19:19:40,174 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-29 19:19:40,174 - memory_profile6_log - INFO -    130    505.5 MiB     12.4 MiB       df_dt = df_current.copy(deep=True)

2018-04-29 19:19:40,174 - memory_profile6_log - INFO -    131    505.5 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 19:19:40,174 - memory_profile6_log - INFO -    132                             

2018-04-29 19:19:40,178 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-29 19:19:40,180 - memory_profile6_log - INFO -    134    505.5 MiB      0.0 MiB       t0 = time.time()

2018-04-29 19:19:40,183 - memory_profile6_log - INFO -    135    505.5 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-29 19:19:40,183 - memory_profile6_log - INFO -    136    505.5 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-29 19:19:40,184 - memory_profile6_log - INFO -    137    505.5 MiB      0.0 MiB       logger.info("apply on: %d total history...)", len(df_hist))

2018-04-29 19:19:40,184 - memory_profile6_log - INFO -    138                             

2018-04-29 19:19:40,184 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-29 19:19:40,184 - memory_profile6_log - INFO -    140    505.5 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-29 19:19:40,186 - memory_profile6_log - INFO -    141                             

2018-04-29 19:19:40,186 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-29 19:19:40,187 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-29 19:19:40,187 - memory_profile6_log - INFO -    144    506.7 MiB      1.2 MiB       NB = BR.processX(df_dut)

2018-04-29 19:19:40,187 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-29 19:19:40,190 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-29 19:19:40,191 - memory_profile6_log - INFO -    147    522.6 MiB     15.9 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-29 19:19:40,191 - memory_profile6_log - INFO -    148                                 """

2018-04-29 19:19:40,194 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-29 19:19:40,194 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-29 19:19:40,194 - memory_profile6_log - INFO -    151                                 """

2018-04-29 19:19:40,194 - memory_profile6_log - INFO -    152    522.6 MiB      0.0 MiB       fitby_sigmant = True

2018-04-29 19:19:40,194 - memory_profile6_log - INFO -    153    522.6 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-29 19:19:40,196 - memory_profile6_log - INFO -    154    522.6 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-29 19:19:40,196 - memory_profile6_log - INFO -    155    538.0 MiB     15.4 MiB                            'is_general']]

2018-04-29 19:19:40,196 - memory_profile6_log - INFO -    156                                 # get sigma_Nt from fitted_models_hist

2018-04-29 19:19:40,200 - memory_profile6_log - INFO -    157    538.3 MiB      0.3 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-04-29 19:19:40,200 - memory_profile6_log - INFO -    158    538.3 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-04-29 19:19:40,203 - memory_profile6_log - INFO -    159    538.5 MiB      0.3 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-04-29 19:19:40,203 - memory_profile6_log - INFO -    160    538.5 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-04-29 19:19:40,203 - memory_profile6_log - INFO -    161                             

2018-04-29 19:19:40,203 - memory_profile6_log - INFO -    162                                 # begin fit

2018-04-29 19:19:40,204 - memory_profile6_log - INFO -    163    538.5 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-29 19:19:40,207 - memory_profile6_log - INFO -    164    538.5 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-29 19:19:40,207 - memory_profile6_log - INFO -    165    579.2 MiB     40.7 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-04-29 19:19:40,209 - memory_profile6_log - INFO -    166    579.2 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-29 19:19:40,209 - memory_profile6_log - INFO -    167    579.2 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-29 19:19:40,209 - memory_profile6_log - INFO -    168                             

2018-04-29 19:19:40,213 - memory_profile6_log - INFO -    169                                 # ~~ and Transform ~~

2018-04-29 19:19:40,213 - memory_profile6_log - INFO -    170                                 #   handling current news interest == current date

2018-04-29 19:19:40,216 - memory_profile6_log - INFO -    171    579.2 MiB      0.0 MiB       if df_dt.empty:

2018-04-29 19:19:40,217 - memory_profile6_log - INFO -    172                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-29 19:19:40,217 - memory_profile6_log - INFO -    173                                     return None

2018-04-29 19:19:40,217 - memory_profile6_log - INFO -    174    579.3 MiB      0.2 MiB       NB = BR.processX(df_dt)

2018-04-29 19:19:40,217 - memory_profile6_log - INFO -    175    597.3 MiB     18.0 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-29 19:19:40,219 - memory_profile6_log - INFO -    176                             

2018-04-29 19:19:40,219 - memory_profile6_log - INFO -    177    597.3 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-29 19:19:40,220 - memory_profile6_log - INFO -    178    596.9 MiB     -0.4 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-29 19:19:40,220 - memory_profile6_log - INFO -    179                             

2018-04-29 19:19:40,220 - memory_profile6_log - INFO -    180    596.9 MiB      0.0 MiB       print "model_fit dtypes:\n", model_fit.dtypes

2018-04-29 19:19:40,221 - memory_profile6_log - INFO -    181    596.9 MiB      0.0 MiB       print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-04-29 19:19:40,226 - memory_profile6_log - INFO -    182    596.9 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-29 19:19:40,226 - memory_profile6_log - INFO -    183    596.9 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-29 19:19:40,229 - memory_profile6_log - INFO -    184    596.9 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-04-29 19:19:40,230 - memory_profile6_log - INFO -    185    596.9 MiB      0.0 MiB                                                                                           "topic_id", "user_id"]],

2018-04-29 19:19:40,230 - memory_profile6_log - INFO -    186    625.2 MiB     28.2 MiB                                                     verbose=False)

2018-04-29 19:19:40,232 - memory_profile6_log - INFO -    187                                 # ~~~ filter is general and specific topic ~~~

2018-04-29 19:19:40,233 - memory_profile6_log - INFO -    188                                 # the idea is just we need to rerank every topic according

2018-04-29 19:19:40,233 - memory_profile6_log - INFO -    189                                 # user_id and and is_general by p0_posterior

2018-04-29 19:19:40,233 - memory_profile6_log - INFO -    190    625.2 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-29 19:19:40,233 - memory_profile6_log - INFO -    191    628.3 MiB      3.1 MiB                                     'is_general']].groupby(['topic_id',

2018-04-29 19:19:40,233 - memory_profile6_log - INFO -    192    625.2 MiB     -3.1 MiB                                                             'is_general']

2018-04-29 19:19:40,236 - memory_profile6_log - INFO -    193                                                                                      ).size().to_frame().reset_index()

2018-04-29 19:19:40,236 - memory_profile6_log - INFO -    194                             

2018-04-29 19:19:40,240 - memory_profile6_log - INFO -    195                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-29 19:19:40,240 - memory_profile6_log - INFO -    196                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-29 19:19:40,240 - memory_profile6_log - INFO -    197    625.2 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-29 19:19:40,242 - memory_profile6_log - INFO -    198                             

2018-04-29 19:19:40,242 - memory_profile6_log - INFO -    199                                 # ~ start by provide rank for each topic type ~

2018-04-29 19:19:40,243 - memory_profile6_log - INFO -    200    645.8 MiB     20.6 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-29 19:19:40,243 - memory_profile6_log - INFO -    201    651.1 MiB      5.3 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-29 19:19:40,243 - memory_profile6_log - INFO -    202                             

2018-04-29 19:19:40,243 - memory_profile6_log - INFO -    203                                 # ~ set threshold to filter output

2018-04-29 19:19:40,243 - memory_profile6_log - INFO -    204    651.1 MiB      0.0 MiB       if threshold > 0:

2018-04-29 19:19:40,246 - memory_profile6_log - INFO -    205    651.1 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-29 19:19:40,246 - memory_profile6_log - INFO -    206    651.1 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-29 19:19:40,249 - memory_profile6_log - INFO -    207    648.0 MiB     -3.1 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-29 19:19:40,250 - memory_profile6_log - INFO -    208                             

2018-04-29 19:19:40,250 - memory_profile6_log - INFO -    209    648.0 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 19:19:40,250 - memory_profile6_log - INFO -    210    648.0 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-29 19:19:40,250 - memory_profile6_log - INFO -    211                             

2018-04-29 19:19:40,250 - memory_profile6_log - INFO -    212    648.0 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 19:19:40,252 - memory_profile6_log - INFO -    213                             

2018-04-29 19:19:40,252 - memory_profile6_log - INFO -    214    648.0 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-29 19:19:40,253 - memory_profile6_log - INFO -    215    648.0 MiB      0.0 MiB       del df_dut

2018-04-29 19:19:40,253 - memory_profile6_log - INFO -    216    648.0 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-29 19:19:40,253 - memory_profile6_log - INFO -    217    648.0 MiB      0.0 MiB       del df_dt

2018-04-29 19:19:40,253 - memory_profile6_log - INFO -    218    648.0 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-29 19:19:40,253 - memory_profile6_log - INFO -    219    648.0 MiB      0.0 MiB       del df_input

2018-04-29 19:19:40,255 - memory_profile6_log - INFO -    220    648.0 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-29 19:19:40,259 - memory_profile6_log - INFO -    221    633.0 MiB    -15.0 MiB       del df_input_X

2018-04-29 19:19:40,260 - memory_profile6_log - INFO -    222    633.0 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-29 19:19:40,260 - memory_profile6_log - INFO -    223    633.0 MiB      0.0 MiB       del df_current

2018-04-29 19:19:40,262 - memory_profile6_log - INFO -    224    633.0 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-29 19:19:40,262 - memory_profile6_log - INFO -    225    633.0 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-29 19:19:40,263 - memory_profile6_log - INFO -    226    633.0 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-29 19:19:40,263 - memory_profile6_log - INFO -    227    596.1 MiB    -37.0 MiB       del model_fit

2018-04-29 19:19:40,265 - memory_profile6_log - INFO -    228    596.1 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-29 19:19:40,265 - memory_profile6_log - INFO -    229    596.1 MiB      0.0 MiB       del result

2018-04-29 19:19:40,266 - memory_profile6_log - INFO -    230    596.1 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-29 19:19:40,266 - memory_profile6_log - INFO -    231                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 19:19:40,266 - memory_profile6_log - INFO -    232                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 19:19:40,279 - memory_profile6_log - INFO -    233                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 19:19:40,282 - memory_profile6_log - INFO -    234    596.1 MiB      0.0 MiB       if savetrain:

2018-04-29 19:19:40,282 - memory_profile6_log - INFO -    235    604.2 MiB      8.1 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-29 19:19:40,283 - memory_profile6_log - INFO -    236    604.2 MiB      0.0 MiB           del model_transform

2018-04-29 19:19:40,283 - memory_profile6_log - INFO -    237    604.2 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-29 19:19:40,285 - memory_profile6_log - INFO -    238    604.2 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 19:19:40,286 - memory_profile6_log - INFO -    239                             

2018-04-29 19:19:40,286 - memory_profile6_log - INFO -    240    604.2 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-29 19:19:40,286 - memory_profile6_log - INFO -    241                                     # ~ Place your code to save the training model here ~

2018-04-29 19:19:40,286 - memory_profile6_log - INFO -    242    604.2 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-29 19:19:40,288 - memory_profile6_log - INFO -    243    604.2 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-29 19:19:40,288 - memory_profile6_log - INFO -    244    604.2 MiB      0.0 MiB               if multproc:

2018-04-29 19:19:40,292 - memory_profile6_log - INFO -    245                                             # ~ save transform models ~

2018-04-29 19:19:40,292 - memory_profile6_log - INFO -    246    574.5 MiB    -29.6 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-29 19:19:40,293 - memory_profile6_log - INFO -    247                             

2018-04-29 19:19:40,293 - memory_profile6_log - INFO -    248                                             """

2018-04-29 19:19:40,295 - memory_profile6_log - INFO -    249                                             # ~ save fitted models ~

2018-04-29 19:19:40,295 - memory_profile6_log - INFO -    250                                             logger.info("Saving fitted_models as history...")

2018-04-29 19:19:40,295 - memory_profile6_log - INFO -    251                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 19:19:40,296 - memory_profile6_log - INFO -    252                                             fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-29 19:19:40,296 - memory_profile6_log - INFO -    253                             

2018-04-29 19:19:40,298 - memory_profile6_log - INFO -    254                                             X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-29 19:19:40,299 - memory_profile6_log - INFO -    255                                             logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-29 19:19:40,299 - memory_profile6_log - INFO -    256                                             for ix in range(len(X_split)):

2018-04-29 19:19:40,299 - memory_profile6_log - INFO -    257                                                 logger.info("processing batch-%d", ix)

2018-04-29 19:19:40,299 - memory_profile6_log - INFO -    258                                                 mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-29 19:19:40,299 - memory_profile6_log - INFO -    259                             

2018-04-29 19:19:40,301 - memory_profile6_log - INFO -    260                                             del X_split

2018-04-29 19:19:40,302 - memory_profile6_log - INFO -    261                                             logger.info("deleting X_split...")

2018-04-29 19:19:40,305 - memory_profile6_log - INFO -    262                                             del save_sigma_nt

2018-04-29 19:19:40,305 - memory_profile6_log - INFO -    263                                             logger.info("deleting save_sigma_nt...")

2018-04-29 19:19:40,306 - memory_profile6_log - INFO -    264                                             """

2018-04-29 19:19:40,306 - memory_profile6_log - INFO -    265                             

2018-04-29 19:19:40,306 - memory_profile6_log - INFO -    266    574.5 MiB      0.0 MiB                   del BR

2018-04-29 19:19:40,308 - memory_profile6_log - INFO -    267    574.5 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-29 19:19:40,308 - memory_profile6_log - INFO -    268                             

2018-04-29 19:19:40,309 - memory_profile6_log - INFO -    269                                     elif str(saveto).lower() == "elastic":

2018-04-29 19:19:40,309 - memory_profile6_log - INFO -    270                                         logger.info("Using ElasticSearch as storage...")

2018-04-29 19:19:40,309 - memory_profile6_log - INFO -    271                                         mh.saveElasticS(model_transformsv)

2018-04-29 19:19:40,309 - memory_profile6_log - INFO -    272                             

2018-04-29 19:19:40,309 - memory_profile6_log - INFO -    273                                     # need save sigma_nt for daily train

2018-04-29 19:19:40,309 - memory_profile6_log - INFO -    274                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-29 19:19:40,311 - memory_profile6_log - INFO -    275                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-29 19:19:40,311 - memory_profile6_log - INFO -    276    574.5 MiB      0.0 MiB           if start_date and end_date:

2018-04-29 19:19:40,312 - memory_profile6_log - INFO -    277                                         if not fitby_sigmant:

2018-04-29 19:19:40,313 - memory_profile6_log - INFO -    278                                             logging.info("Saving sigma Nt...")

2018-04-29 19:19:40,315 - memory_profile6_log - INFO -    279                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 19:19:40,315 - memory_profile6_log - INFO -    280                                             save_sigma_nt['start_date'] = start_date

2018-04-29 19:19:40,316 - memory_profile6_log - INFO -    281                                             save_sigma_nt['end_date'] = end_date

2018-04-29 19:19:40,316 - memory_profile6_log - INFO -    282                                             print save_sigma_nt.head(5)

2018-04-29 19:19:40,316 - memory_profile6_log - INFO -    283                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-29 19:19:40,318 - memory_profile6_log - INFO -    284                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-29 19:19:40,318 - memory_profile6_log - INFO -    285    574.5 MiB      0.0 MiB       return

2018-04-29 19:19:40,318 - memory_profile6_log - INFO - 


2018-04-29 19:19:40,319 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-29 19:24:57,645 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 19:24:57,648 - memory_profile6_log - INFO - date_generated: 
2018-04-29 19:24:57,648 - memory_profile6_log - INFO -  
2018-04-29 19:24:57,648 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 9, 0, 0)]
2018-04-29 19:24:57,648 - memory_profile6_log - INFO - 

2018-04-29 19:24:57,650 - memory_profile6_log - INFO - using current date: 2018-04-10
2018-04-29 19:24:57,650 - memory_profile6_log - INFO - using start date: 2018-04-09 00:00:00
2018-04-29 19:24:57,650 - memory_profile6_log - INFO - using end date: 2018-04-09
2018-04-29 19:24:57,767 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 19:24:57,772 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-09
2018-04-29 19:26:50,930 - memory_profile6_log - INFO - size of df: 102.25 MB
2018-04-29 19:26:50,931 - memory_profile6_log - INFO - getting total: 403532 training data(genuine interest) for date: 2018-04-09
2018-04-29 19:26:50,986 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 19:26:50,986 - memory_profile6_log - INFO - Appending history data...
2018-04-29 19:26:50,987 - memory_profile6_log - INFO - processing batch-0
2018-04-29 19:26:50,990 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:26:51,092 - memory_profile6_log - INFO - call history data...
2018-04-29 19:28:08,778 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 19:28:10,358 - memory_profile6_log - INFO - processing batch-1
2018-04-29 19:28:10,358 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:28:10,434 - memory_profile6_log - INFO - call history data...
2018-04-29 19:29:48,668 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 19:29:50,296 - memory_profile6_log - INFO - processing batch-2
2018-04-29 19:29:50,298 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:29:50,374 - memory_profile6_log - INFO - call history data...
2018-04-29 19:31:14,719 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 19:31:16,334 - memory_profile6_log - INFO - processing batch-3
2018-04-29 19:31:16,335 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:31:16,410 - memory_profile6_log - INFO - call history data...
2018-04-29 19:34:44,229 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 19:34:44,232 - memory_profile6_log - INFO - date_generated: 
2018-04-29 19:34:44,232 - memory_profile6_log - INFO -  
2018-04-29 19:34:44,232 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 9, 0, 0)]
2018-04-29 19:34:44,232 - memory_profile6_log - INFO - 

2018-04-29 19:34:44,233 - memory_profile6_log - INFO - using current date: 2018-04-10
2018-04-29 19:34:44,233 - memory_profile6_log - INFO - using start date: 2018-04-09 00:00:00
2018-04-29 19:34:44,233 - memory_profile6_log - INFO - using end date: 2018-04-09
2018-04-29 19:34:44,351 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 19:34:44,355 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-09
2018-04-29 19:36:35,736 - memory_profile6_log - INFO - size of df: 102.25 MB
2018-04-29 19:36:35,737 - memory_profile6_log - INFO - getting total: 403532 training data(genuine interest) for date: 2018-04-09
2018-04-29 19:36:35,799 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 19:36:35,801 - memory_profile6_log - INFO - Appending history data...
2018-04-29 19:36:35,802 - memory_profile6_log - INFO - processing batch-0
2018-04-29 19:36:35,802 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:36:35,928 - memory_profile6_log - INFO - call history data...
2018-04-29 19:37:47,816 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 19:37:49,484 - memory_profile6_log - INFO - processing batch-1
2018-04-29 19:37:49,486 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:37:49,559 - memory_profile6_log - INFO - call history data...
2018-04-29 19:39:00,859 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 19:39:02,565 - memory_profile6_log - INFO - processing batch-2
2018-04-29 19:39:02,566 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:39:02,638 - memory_profile6_log - INFO - call history data...
2018-04-29 19:40:56,267 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 19:40:56,272 - memory_profile6_log - INFO - date_generated: 
2018-04-29 19:40:56,272 - memory_profile6_log - INFO -  
2018-04-29 19:40:56,273 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 9, 0, 0)]
2018-04-29 19:40:56,273 - memory_profile6_log - INFO - 

2018-04-29 19:40:56,273 - memory_profile6_log - INFO - using current date: 2018-04-10
2018-04-29 19:40:56,275 - memory_profile6_log - INFO - using start date: 2018-04-09 00:00:00
2018-04-29 19:40:56,275 - memory_profile6_log - INFO - using end date: 2018-04-09
2018-04-29 19:40:56,395 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 19:40:56,400 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-09
2018-04-29 19:42:47,125 - memory_profile6_log - INFO - size of df: 102.25 MB
2018-04-29 19:42:47,128 - memory_profile6_log - INFO - getting total: 403532 training data(genuine interest) for date: 2018-04-09
2018-04-29 19:42:47,190 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 19:42:47,193 - memory_profile6_log - INFO - Appending history data...
2018-04-29 19:42:47,194 - memory_profile6_log - INFO - processing batch-0
2018-04-29 19:42:47,196 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:42:47,305 - memory_profile6_log - INFO - call history data...
2018-04-29 19:43:38,927 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 19:43:40,500 - memory_profile6_log - INFO - processing batch-1
2018-04-29 19:43:40,502 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:43:40,572 - memory_profile6_log - INFO - call history data...
2018-04-29 19:44:32,694 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 19:44:34,489 - memory_profile6_log - INFO - processing batch-2
2018-04-29 19:44:34,490 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:44:34,569 - memory_profile6_log - INFO - call history data...
2018-04-29 19:45:26,542 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 19:45:28,401 - memory_profile6_log - INFO - processing batch-3
2018-04-29 19:45:28,403 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:45:28,490 - memory_profile6_log - INFO - call history data...
2018-04-29 19:46:20,619 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 19:46:22,243 - memory_profile6_log - INFO - processing batch-4
2018-04-29 19:46:22,243 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:46:22,313 - memory_profile6_log - INFO - call history data...
2018-04-29 19:47:17,898 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 19:47:19,575 - memory_profile6_log - INFO - Appending training data...
2018-04-29 19:47:19,576 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 19:47:19,578 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 19:47:19,596 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 19:47:19,598 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 19:47:19,599 - memory_profile6_log - INFO - ================================================

2018-04-29 19:47:19,599 - memory_profile6_log - INFO -    311     86.8 MiB     86.8 MiB   @profile

2018-04-29 19:47:19,601 - memory_profile6_log - INFO -    312                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 19:47:19,604 - memory_profile6_log - INFO -    313     86.8 MiB      0.0 MiB       bq_client = client

2018-04-29 19:47:19,604 - memory_profile6_log - INFO -    314     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 19:47:19,605 - memory_profile6_log - INFO -    315                             

2018-04-29 19:47:19,605 - memory_profile6_log - INFO -    316     86.8 MiB      0.0 MiB       datalist = []

2018-04-29 19:47:19,607 - memory_profile6_log - INFO -    317     86.8 MiB      0.0 MiB       datalist_hist = []

2018-04-29 19:47:19,607 - memory_profile6_log - INFO -    318                             

2018-04-29 19:47:19,608 - memory_profile6_log - INFO -    319     86.8 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 19:47:19,611 - memory_profile6_log - INFO -    320    655.4 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 19:47:19,611 - memory_profile6_log - INFO -    321    393.1 MiB    306.4 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 19:47:19,612 - memory_profile6_log - INFO -    322    393.1 MiB      0.0 MiB           if tframe is not None:

2018-04-29 19:47:19,614 - memory_profile6_log - INFO -    323    393.1 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 19:47:19,614 - memory_profile6_log - INFO -    324    406.3 MiB     13.1 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 19:47:19,615 - memory_profile6_log - INFO -    325    406.3 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 19:47:19,615 - memory_profile6_log - INFO -    326    406.3 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 19:47:19,615 - memory_profile6_log - INFO -    327    655.4 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 19:47:19,617 - memory_profile6_log - INFO -    328                                                 # ~ loading history

2018-04-29 19:47:19,618 - memory_profile6_log - INFO -    329                                                 """

2018-04-29 19:47:19,619 - memory_profile6_log - INFO -    330                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 19:47:19,621 - memory_profile6_log - INFO -    331                                                 """

2018-04-29 19:47:19,621 - memory_profile6_log - INFO -    332    628.0 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 19:47:19,622 - memory_profile6_log - INFO -    333                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 19:47:19,624 - memory_profile6_log - INFO -    334    628.0 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 19:47:19,624 - memory_profile6_log - INFO -    335                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 19:47:19,625 - memory_profile6_log - INFO -    336    629.8 MiB      7.7 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 19:47:19,625 - memory_profile6_log - INFO -    337                             

2018-04-29 19:47:19,625 - memory_profile6_log - INFO -    338    629.8 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 19:47:19,627 - memory_profile6_log - INFO -    339    704.7 MiB    525.5 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 19:47:19,628 - memory_profile6_log - INFO -    340                             

2018-04-29 19:47:19,631 - memory_profile6_log - INFO -    341                                                 # me = os.getpid()

2018-04-29 19:47:19,631 - memory_profile6_log - INFO -    342                                                 # kill_proc_tree(me)

2018-04-29 19:47:19,632 - memory_profile6_log - INFO -    343                             

2018-04-29 19:47:19,632 - memory_profile6_log - INFO -    344    704.7 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 19:47:19,634 - memory_profile6_log - INFO -    345    706.9 MiB     -0.7 MiB                       for m in h_frame:

2018-04-29 19:47:19,634 - memory_profile6_log - INFO -    346    706.9 MiB     -0.7 MiB                           if m is not None:

2018-04-29 19:47:19,635 - memory_profile6_log - INFO -    347    706.9 MiB     -0.7 MiB                               if len(m) > 0:

2018-04-29 19:47:19,635 - memory_profile6_log - INFO -    348    706.9 MiB      8.8 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 19:47:19,637 - memory_profile6_log - INFO -    349    655.4 MiB   -292.5 MiB                       del h_frame

2018-04-29 19:47:19,638 - memory_profile6_log - INFO -    350    655.4 MiB     -1.1 MiB                       del lhistory

2018-04-29 19:47:19,638 - memory_profile6_log - INFO -    351                             

2018-04-29 19:47:19,638 - memory_profile6_log - INFO -    352    655.4 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 19:47:19,641 - memory_profile6_log - INFO -    353    655.4 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 19:47:19,642 - memory_profile6_log - INFO -    354                                     else: 

2018-04-29 19:47:19,645 - memory_profile6_log - INFO -    355                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 19:47:19,647 - memory_profile6_log - INFO -    356    655.4 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 19:47:19,648 - memory_profile6_log - INFO -    357    655.4 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 19:47:19,648 - memory_profile6_log - INFO -    358                             

2018-04-29 19:47:19,648 - memory_profile6_log - INFO -    359    655.4 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 19:47:19,650 - memory_profile6_log - INFO - 


2018-04-29 19:47:20,783 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 19:47:20,858 - memory_profile6_log - INFO -     p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0    0.001543         445.737339       185             455.737339  10960288  16137e7988e400-0e4f9d96053ce-2c5f3268-4a640-16...
1    0.001543         791.412368        48             801.412368  10960288  16131c8488cad-00d85aed867452-39626377-55188-16...
2    0.001543         467.043053       143             477.043053  10960288  1616ab2aea51bb-095e9f7c6f74b3-514c673d-38400-1...
3    0.000023       12089.039256       132           12099.039256  10959239  161ea30efe66-02cbbff5d443c5-97a5d17-38400-161e...
4    0.000088         910.931467        37             920.931467  11911567  1614c2d888521-0362a12a9300a8-57516539-38400-16...
5    0.001473         269.647011        66             279.647011  22291119  1621f5b8e9b1-0128731ade18b5-2e483333-38400-162...
6    0.001543         285.850187      1230             295.850187  10960288  1622eb63ec39-0961b0bad640fe-582d1346-8d272-162...
7    0.001543        1105.960925       149            1115.960925  10960288  161b32ec49815-09b38099f85a43-a4f5b5e-38400-161...
8    0.000088        6740.892857        10            6750.892857  11911567  161409a03c93e-0823b8d96d566d-69575974-38400-16...
9    0.001543         233.550828       102             243.550828  10960288  1613a0f880013c-008204f8136e34-435b4850-38400-1...
10   0.001543         381.996451       331             391.996451  10960288  16130047ada52-0ada44117-14d422a-29b80-16130047...
11   0.000088        2202.469943       101            2212.469943  11911567  161ae06c69f28-099979cacde8b8-75194b59-38400-16...
12   0.001543         138.215931       822             148.215931  10960288  1612e0dc6af1c-06e6aa483502b9-b2f5b2f-38400-161...
13   0.001543        1086.583319      1401            1096.583319  10960288  1616ac78a3e5b5-09b19c5de4820b-4c534c69-100200-...
14   0.000646        1844.325967        32            1854.325967  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
15   0.001543         502.346467       311             512.346467  10960288  16275ef9b5ad-0d25ed18f50a1c-3b670e20-4df28-162...
16   0.001543         860.466864       115             870.466864  10960288  162a8c642a71e4-0e9ff4c73ef7f1-3f3c5501-100200-...
17   0.000023        9899.213287        26            9909.213287  10959239  1626b3bae3f39d-0af051a01ecd42-33697b04-fa000-1...
18   0.001543         454.567622       129             464.567622  10960288  16298f93916aed-0539950d283a53-3f3c5501-100200-...
19   0.004470         100.216367        61             110.216367  22291119  16199f44e7753-064fe64d8c6899-73261514-3f480-16...
2018-04-29 19:47:20,858 - memory_profile6_log - INFO - 

2018-04-29 19:47:20,990 - memory_profile6_log - INFO - size of big_frame_hist: 111.10 MB
2018-04-29 19:47:21,095 - memory_profile6_log - INFO - size of big_frame: 102.25 MB
2018-04-29 19:47:21,117 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-04-29 19:48:28,717 - memory_profile6_log - INFO - size of df: 96.11 MB
2018-04-29 19:48:28,719 - memory_profile6_log - INFO - getting total: 393007 training data(current date interest)
2018-04-29 19:48:28,855 - memory_profile6_log - INFO - size of current_frame: 99.10 MB
2018-04-29 19:48:28,857 - memory_profile6_log - INFO - loading time of: 796539 total genuine-current interest data ~ take 452.480s
2018-04-29 19:48:28,885 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 19:48:28,887 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 19:48:28,888 - memory_profile6_log - INFO - ================================================

2018-04-29 19:48:28,888 - memory_profile6_log - INFO -    361     86.6 MiB     86.6 MiB   @profile

2018-04-29 19:48:28,890 - memory_profile6_log - INFO -    362                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 19:48:28,891 - memory_profile6_log - INFO -    363     86.8 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 19:48:28,891 - memory_profile6_log - INFO -    364     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 19:48:28,892 - memory_profile6_log - INFO -    365                             

2018-04-29 19:48:28,892 - memory_profile6_log - INFO -    366                                 # ~~~ Begin collecting data ~~~

2018-04-29 19:48:28,894 - memory_profile6_log - INFO -    367     86.8 MiB      0.0 MiB       t0 = time.time()

2018-04-29 19:48:28,894 - memory_profile6_log - INFO -    368    645.6 MiB    558.8 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 19:48:28,897 - memory_profile6_log - INFO -    369    645.6 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 19:48:28,897 - memory_profile6_log - INFO -    370                                     logger.info("Training cannot be empty..")

2018-04-29 19:48:28,898 - memory_profile6_log - INFO -    371                                     return False

2018-04-29 19:48:28,900 - memory_profile6_log - INFO -    372    669.7 MiB     24.1 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 19:48:28,900 - memory_profile6_log - INFO -    373    669.8 MiB      0.1 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 19:48:28,901 - memory_profile6_log - INFO -    374    669.8 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 19:48:28,901 - memory_profile6_log - INFO -    375                             

2018-04-29 19:48:28,901 - memory_profile6_log - INFO -    376    679.1 MiB      9.3 MiB       big_frame = pd.concat(datalist)

2018-04-29 19:48:28,903 - memory_profile6_log - INFO -    377    679.1 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 19:48:28,903 - memory_profile6_log - INFO -    378    669.9 MiB     -9.2 MiB       del datalist

2018-04-29 19:48:28,904 - memory_profile6_log - INFO -    379                             

2018-04-29 19:48:28,907 - memory_profile6_log - INFO -    380    669.9 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 19:48:28,907 - memory_profile6_log - INFO -    381                             

2018-04-29 19:48:28,910 - memory_profile6_log - INFO -    382                                 # ~ get current news interest ~

2018-04-29 19:48:28,911 - memory_profile6_log - INFO -    383    669.9 MiB      0.0 MiB       if not cd:

2018-04-29 19:48:28,911 - memory_profile6_log - INFO -    384    669.9 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-04-29 19:48:28,911 - memory_profile6_log - INFO -    385    655.8 MiB    -14.0 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 19:48:28,911 - memory_profile6_log - INFO -    386    655.8 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 19:48:28,914 - memory_profile6_log - INFO -    387                                 else:

2018-04-29 19:48:28,914 - memory_profile6_log - INFO -    388                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 19:48:28,914 - memory_profile6_log - INFO -    389                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 19:48:28,915 - memory_profile6_log - INFO -    390                             

2018-04-29 19:48:28,915 - memory_profile6_log - INFO -    391                                     # safe handling of query parameter

2018-04-29 19:48:28,918 - memory_profile6_log - INFO -    392                                     query_params = [

2018-04-29 19:48:28,920 - memory_profile6_log - INFO -    393                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 19:48:28,924 - memory_profile6_log - INFO -    394                                     ]

2018-04-29 19:48:28,924 - memory_profile6_log - INFO -    395                             

2018-04-29 19:48:28,924 - memory_profile6_log - INFO -    396                                     job_config.query_parameters = query_params

2018-04-29 19:48:28,924 - memory_profile6_log - INFO -    397                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 19:48:28,926 - memory_profile6_log - INFO -    398                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 19:48:28,926 - memory_profile6_log - INFO -    399                             

2018-04-29 19:48:28,928 - memory_profile6_log - INFO -    400    658.9 MiB      3.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 19:48:28,928 - memory_profile6_log - INFO -    401    658.9 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 19:48:28,930 - memory_profile6_log - INFO -    402    658.9 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 19:48:28,931 - memory_profile6_log - INFO -    403    658.9 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 19:48:28,931 - memory_profile6_log - INFO -    404    658.9 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 19:48:28,933 - memory_profile6_log - INFO -    405    658.9 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 19:48:28,933 - memory_profile6_log - INFO -    406                             

2018-04-29 19:48:28,934 - memory_profile6_log - INFO -    407    658.9 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 19:48:28,934 - memory_profile6_log - INFO - 


2018-04-29 19:48:28,937 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 19:48:28,982 - memory_profile6_log - INFO - train on: 403532 total genuine interest data(D(u, t))
2018-04-29 19:48:28,983 - memory_profile6_log - INFO - transform on: 393007 total current data(D(t))
2018-04-29 19:48:28,984 - memory_profile6_log - INFO - apply on: 403532 total history...)
2018-04-29 19:48:29,326 - memory_profile6_log - INFO - len of uniques_fit_hist:403532
2018-04-29 19:48:29,556 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:50399
2018-04-29 19:48:30,897 - memory_profile6_log - INFO - Len of model_fit: 403532
2018-04-29 19:48:30,898 - memory_profile6_log - INFO - Len of df_dut: 403532
2018-04-29 19:48:31,259 - memory_profile6_log - INFO - model_fit dtypes:

2018-04-29 19:48:31,263 - memory_profile6_log - INFO - date              datetime64[ns, UTC]
user_id                        object
topic_id                       object
num_x                           int64
num_y                           int64
is_general                       bool
date_all_click                  int64
Ntotal                          int64
sigma_Nt                        int64
joinprob_ci                   float64
p_cat_ci                      float64
posterior                     float64
dtype: object
2018-04-29 19:48:31,265 - memory_profile6_log - INFO - 

2018-04-29 19:48:31,266 - memory_profile6_log - INFO - fitted_models_hist dtypes:

2018-04-29 19:48:31,269 - memory_profile6_log - INFO - p0_cat_ci                float64
pt_posterior_x_Nt        float64
sigma_Nt                   int64
smoothed_pt_posterior    float64
topic_id                  object
user_id                   object
dtype: object
2018-04-29 19:48:31,270 - memory_profile6_log - INFO - 

2018-04-29 19:48:32,096 - memory_profile6_log - INFO - Len of fitted_models on main class: 403532
2018-04-29 19:48:32,098 - memory_profile6_log - INFO - 

2018-04-29 19:48:32,098 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-29 19:48:32,099 - memory_profile6_log - INFO - 

2018-04-29 19:48:32,101 - memory_profile6_log - INFO - Fitted models before concat:

2018-04-29 19:48:32,128 - memory_profile6_log - INFO -                                              user_id    topic_id  pt_posterior_x_Nt  smoothed_pt_posterior
0  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1064047503        4802.540356            4812.540356
1  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1065124711          76.456585              86.456585
2  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1443761312         302.356128             312.356128
3  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...   152104836          48.659104              58.659104
4  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...    22553543          10.287911              20.287911
2018-04-29 19:48:32,130 - memory_profile6_log - INFO - 

2018-04-29 19:48:32,131 - memory_profile6_log - INFO - len of current fitted models: 403532
2018-04-29 19:48:32,131 - memory_profile6_log - INFO - 

2018-04-29 19:48:32,132 - memory_profile6_log - INFO - len of history fitted models: 403532
2018-04-29 19:48:32,134 - memory_profile6_log - INFO - 

2018-04-29 19:48:32,207 - memory_profile6_log - INFO - len of fitted models after concat: 807064
2018-04-29 19:48:32,207 - memory_profile6_log - INFO - 

2018-04-29 19:48:32,210 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-29 19:48:32,210 - memory_profile6_log - INFO - 

2018-04-29 19:48:32,733 - memory_profile6_log - INFO - Fitted models after concat:

2018-04-29 19:48:32,753 - memory_profile6_log - INFO -                                              user_id    topic_id  pt_posterior_x_Nt
0  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1064047503       12362.094619
1  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1065124711         196.804912
2  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1443761312         778.287070
3  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...   152104836         125.252137
4  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...    22553543          28.114433
2018-04-29 19:48:32,753 - memory_profile6_log - INFO - 

2018-04-29 19:48:32,775 - memory_profile6_log - INFO - Fitted models after concat-simplify:

2018-04-29 19:48:32,799 - memory_profile6_log - INFO -                                              user_id    topic_id  pt_posterior_x_Nt  smoothed_pt_posterior
0  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1064047503       12362.094619           12372.094619
1  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1065124711         196.804912             206.804912
2  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1443761312         778.287070             788.287070
3  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...   152104836         125.252137             135.252137
4  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...    22553543          28.114433              38.114433
2018-04-29 19:48:32,799 - memory_profile6_log - INFO - 

2018-04-29 19:48:32,802 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 403532
2018-04-29 19:48:32,802 - memory_profile6_log - INFO - 

2018-04-29 19:48:32,900 - memory_profile6_log - INFO -                                              user_id           topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci
0  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...         1064047503       12362.094619           12372.094619   0.002257
1  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...         1065124711         196.804912             206.804912   0.000569
2  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...         1443761312         778.287070             788.287070   0.000754
3  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...          152104836         125.252137             135.252137   0.004796
4  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...           22553543          28.114433              38.114433   0.109945
5  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...           22662160          97.143653             107.143653   0.012649
6  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...           27311712         772.295711             782.295711   0.003327
7  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  27431110790312902        6728.735046            6738.735046   0.000012
8  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  27431110790312909        1839.342798            1849.342798   0.000069
9  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  27431110790313245        3966.940811            3976.940811   0.000208
2018-04-29 19:48:32,901 - memory_profile6_log - INFO - 

2018-04-29 19:49:29,867 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-29 19:49:29,911 - memory_profile6_log - INFO - Total train time: 60.930s
2018-04-29 19:49:29,914 - memory_profile6_log - INFO - memory left before cleaning: 80.600 percent memory...
2018-04-29 19:49:29,914 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-29 19:49:29,915 - memory_profile6_log - INFO - deleting df_dut...
2018-04-29 19:49:29,915 - memory_profile6_log - INFO - deleting df_dt...
2018-04-29 19:49:29,917 - memory_profile6_log - INFO - deleting df_input...
2018-04-29 19:49:29,931 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-29 19:49:29,933 - memory_profile6_log - INFO - deleting df_current...
2018-04-29 19:49:29,934 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-29 19:49:29,953 - memory_profile6_log - INFO - deleting model_fit...
2018-04-29 19:49:29,956 - memory_profile6_log - INFO - deleting result...
2018-04-29 19:49:29,986 - memory_profile6_log - INFO - deleting model_transform...
2018-04-29 19:49:29,986 - memory_profile6_log - INFO - memory left after cleaning: 80.300 percent memory...
2018-04-29 19:49:29,989 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-29 19:49:29,990 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-29 19:49:30,174 - memory_profile6_log - INFO - deleting BR...
2018-04-29 19:49:30,183 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 19:49:30,183 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 19:49:30,183 - memory_profile6_log - INFO - ================================================

2018-04-29 19:49:30,184 - memory_profile6_log - INFO -    113    650.1 MiB    650.1 MiB   @profile

2018-04-29 19:49:30,184 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-29 19:49:30,186 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-29 19:49:30,186 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-29 19:49:30,186 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-29 19:49:30,187 - memory_profile6_log - INFO -    118                                      saveto="datastore", fitted_models_hist=None):

2018-04-29 19:49:30,187 - memory_profile6_log - INFO -    119                                 """

2018-04-29 19:49:30,188 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-29 19:49:30,191 - memory_profile6_log - INFO -    121                                 """

2018-04-29 19:49:30,191 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-29 19:49:30,193 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-29 19:49:30,194 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-29 19:49:30,194 - memory_profile6_log - INFO -    125    650.1 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-29 19:49:30,194 - memory_profile6_log - INFO -    126    662.8 MiB     12.6 MiB       df_dut = df_input.copy(deep=True)

2018-04-29 19:49:30,194 - memory_profile6_log - INFO -    127    662.8 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 19:49:30,194 - memory_profile6_log - INFO -    128                             

2018-04-29 19:49:30,196 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-29 19:49:30,197 - memory_profile6_log - INFO -    130    675.1 MiB     12.4 MiB       df_dt = df_current.copy(deep=True)

2018-04-29 19:49:30,197 - memory_profile6_log - INFO -    131    675.1 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 19:49:30,198 - memory_profile6_log - INFO -    132                             

2018-04-29 19:49:30,198 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-29 19:49:30,200 - memory_profile6_log - INFO -    134    675.1 MiB      0.0 MiB       t0 = time.time()

2018-04-29 19:49:30,200 - memory_profile6_log - INFO -    135    675.1 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-29 19:49:30,200 - memory_profile6_log - INFO -    136    675.1 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-29 19:49:30,203 - memory_profile6_log - INFO -    137    675.1 MiB      0.0 MiB       logger.info("apply on: %d total history...)", len(df_hist))

2018-04-29 19:49:30,204 - memory_profile6_log - INFO -    138                             

2018-04-29 19:49:30,209 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-29 19:49:30,210 - memory_profile6_log - INFO -    140    675.1 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-29 19:49:30,210 - memory_profile6_log - INFO -    141                             

2018-04-29 19:49:30,210 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-29 19:49:30,210 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-29 19:49:30,210 - memory_profile6_log - INFO -    144    676.9 MiB      1.8 MiB       NB = BR.processX(df_dut)

2018-04-29 19:49:30,211 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-29 19:49:30,211 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-29 19:49:30,217 - memory_profile6_log - INFO -    147    692.4 MiB     15.5 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-29 19:49:30,217 - memory_profile6_log - INFO -    148                                 """

2018-04-29 19:49:30,219 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-29 19:49:30,219 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-29 19:49:30,220 - memory_profile6_log - INFO -    151                                 """

2018-04-29 19:49:30,220 - memory_profile6_log - INFO -    152    692.4 MiB      0.0 MiB       fitby_sigmant = True

2018-04-29 19:49:30,220 - memory_profile6_log - INFO -    153    692.4 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-29 19:49:30,223 - memory_profile6_log - INFO -    154    692.4 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-29 19:49:30,223 - memory_profile6_log - INFO -    155    707.8 MiB     15.4 MiB                            'is_general']]

2018-04-29 19:49:30,223 - memory_profile6_log - INFO -    156                                 # get sigma_Nt from fitted_models_hist

2018-04-29 19:49:30,223 - memory_profile6_log - INFO -    157    714.0 MiB      6.2 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-04-29 19:49:30,224 - memory_profile6_log - INFO -    158    714.0 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-04-29 19:49:30,224 - memory_profile6_log - INFO -    159    710.6 MiB     -3.4 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-04-29 19:49:30,227 - memory_profile6_log - INFO -    160    710.6 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-04-29 19:49:30,229 - memory_profile6_log - INFO -    161                             

2018-04-29 19:49:30,230 - memory_profile6_log - INFO -    162                                 # begin fit

2018-04-29 19:49:30,230 - memory_profile6_log - INFO -    163    710.6 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-29 19:49:30,232 - memory_profile6_log - INFO -    164    710.6 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-29 19:49:30,232 - memory_profile6_log - INFO -    165    753.1 MiB     42.5 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-04-29 19:49:30,232 - memory_profile6_log - INFO -    166    753.1 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-29 19:49:30,233 - memory_profile6_log - INFO -    167    753.1 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-29 19:49:30,233 - memory_profile6_log - INFO -    168                             

2018-04-29 19:49:30,233 - memory_profile6_log - INFO -    169                                 # ~~ and Transform ~~

2018-04-29 19:49:30,233 - memory_profile6_log - INFO -    170                                 #   handling current news interest == current date

2018-04-29 19:49:30,233 - memory_profile6_log - INFO -    171    753.1 MiB      0.0 MiB       if df_dt.empty:

2018-04-29 19:49:30,234 - memory_profile6_log - INFO -    172                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-29 19:49:30,234 - memory_profile6_log - INFO -    173                                     return None

2018-04-29 19:49:30,234 - memory_profile6_log - INFO -    174    753.2 MiB      0.1 MiB       NB = BR.processX(df_dt)

2018-04-29 19:49:30,236 - memory_profile6_log - INFO -    175    771.2 MiB     18.0 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-29 19:49:30,236 - memory_profile6_log - INFO -    176                             

2018-04-29 19:49:30,236 - memory_profile6_log - INFO -    177    771.2 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-29 19:49:30,240 - memory_profile6_log - INFO -    178    770.8 MiB     -0.4 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-29 19:49:30,242 - memory_profile6_log - INFO -    179                             

2018-04-29 19:49:30,243 - memory_profile6_log - INFO -    180    770.8 MiB      0.0 MiB       print "model_fit dtypes:\n", model_fit.dtypes

2018-04-29 19:49:30,243 - memory_profile6_log - INFO -    181    770.8 MiB      0.0 MiB       print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-04-29 19:49:30,243 - memory_profile6_log - INFO -    182    770.8 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-29 19:49:30,243 - memory_profile6_log - INFO -    183    770.8 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-29 19:49:30,243 - memory_profile6_log - INFO -    184    770.8 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-04-29 19:49:30,243 - memory_profile6_log - INFO -    185    780.0 MiB      9.2 MiB                                                                                           "topic_id", "user_id"]],

2018-04-29 19:49:30,244 - memory_profile6_log - INFO -    186    801.3 MiB     21.3 MiB                                                     verbose=False)

2018-04-29 19:49:30,244 - memory_profile6_log - INFO -    187                                 # ~~~ filter is general and specific topic ~~~

2018-04-29 19:49:30,246 - memory_profile6_log - INFO -    188                                 # the idea is just we need to rerank every topic according

2018-04-29 19:49:30,247 - memory_profile6_log - INFO -    189                                 # user_id and and is_general by p0_posterior

2018-04-29 19:49:30,247 - memory_profile6_log - INFO -    190    801.3 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-29 19:49:30,247 - memory_profile6_log - INFO -    191    804.4 MiB      3.1 MiB                                     'is_general']].groupby(['topic_id',

2018-04-29 19:49:30,252 - memory_profile6_log - INFO -    192    801.4 MiB     -3.0 MiB                                                             'is_general']

2018-04-29 19:49:30,253 - memory_profile6_log - INFO -    193                                                                                      ).size().to_frame().reset_index()

2018-04-29 19:49:30,253 - memory_profile6_log - INFO -    194                             

2018-04-29 19:49:30,253 - memory_profile6_log - INFO -    195                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-29 19:49:30,253 - memory_profile6_log - INFO -    196                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-29 19:49:30,256 - memory_profile6_log - INFO -    197    801.4 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-29 19:49:30,256 - memory_profile6_log - INFO -    198                             

2018-04-29 19:49:30,256 - memory_profile6_log - INFO -    199                                 # ~ start by provide rank for each topic type ~

2018-04-29 19:49:30,256 - memory_profile6_log - INFO -    200    808.2 MiB      6.7 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-29 19:49:30,257 - memory_profile6_log - INFO -    201    815.9 MiB      7.8 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-29 19:49:30,257 - memory_profile6_log - INFO -    202                             

2018-04-29 19:49:30,259 - memory_profile6_log - INFO -    203                                 # ~ set threshold to filter output

2018-04-29 19:49:30,259 - memory_profile6_log - INFO -    204    815.9 MiB      0.0 MiB       if threshold > 0:

2018-04-29 19:49:30,259 - memory_profile6_log - INFO -    205    815.9 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-29 19:49:30,259 - memory_profile6_log - INFO -    206    815.9 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-29 19:49:30,259 - memory_profile6_log - INFO -    207    812.8 MiB     -3.1 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-29 19:49:30,263 - memory_profile6_log - INFO -    208                             

2018-04-29 19:49:30,265 - memory_profile6_log - INFO -    209    812.8 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 19:49:30,266 - memory_profile6_log - INFO -    210    812.8 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-29 19:49:30,266 - memory_profile6_log - INFO -    211                             

2018-04-29 19:49:30,266 - memory_profile6_log - INFO -    212    812.8 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 19:49:30,266 - memory_profile6_log - INFO -    213                             

2018-04-29 19:49:30,267 - memory_profile6_log - INFO -    214    812.8 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-29 19:49:30,267 - memory_profile6_log - INFO -    215    812.8 MiB      0.0 MiB       del df_dut

2018-04-29 19:49:30,267 - memory_profile6_log - INFO -    216    812.8 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-29 19:49:30,269 - memory_profile6_log - INFO -    217    812.8 MiB      0.0 MiB       del df_dt

2018-04-29 19:49:30,269 - memory_profile6_log - INFO -    218    812.8 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-29 19:49:30,269 - memory_profile6_log - INFO -    219    812.8 MiB      0.0 MiB       del df_input

2018-04-29 19:49:30,269 - memory_profile6_log - INFO -    220    812.8 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-29 19:49:30,269 - memory_profile6_log - INFO -    221    797.8 MiB    -15.0 MiB       del df_input_X

2018-04-29 19:49:30,270 - memory_profile6_log - INFO -    222    797.8 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-29 19:49:30,275 - memory_profile6_log - INFO -    223    797.8 MiB      0.0 MiB       del df_current

2018-04-29 19:49:30,275 - memory_profile6_log - INFO -    224    797.8 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-29 19:49:30,276 - memory_profile6_log - INFO -    225    797.8 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-29 19:49:30,276 - memory_profile6_log - INFO -    226    797.8 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-29 19:49:30,276 - memory_profile6_log - INFO -    227    760.9 MiB    -37.0 MiB       del model_fit

2018-04-29 19:49:30,276 - memory_profile6_log - INFO -    228    760.9 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-29 19:49:30,278 - memory_profile6_log - INFO -    229    760.9 MiB      0.0 MiB       del result

2018-04-29 19:49:30,278 - memory_profile6_log - INFO -    230    760.9 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-29 19:49:30,278 - memory_profile6_log - INFO -    231                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 19:49:30,279 - memory_profile6_log - INFO -    232                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 19:49:30,280 - memory_profile6_log - INFO -    233                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 19:49:30,282 - memory_profile6_log - INFO -    234    760.9 MiB      0.0 MiB       if savetrain:

2018-04-29 19:49:30,282 - memory_profile6_log - INFO -    235    769.0 MiB      8.1 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-29 19:49:30,282 - memory_profile6_log - INFO -    236    769.0 MiB      0.0 MiB           del model_transform

2018-04-29 19:49:30,282 - memory_profile6_log - INFO -    237    769.0 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-29 19:49:30,288 - memory_profile6_log - INFO -    238    769.0 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 19:49:30,288 - memory_profile6_log - INFO -    239                             

2018-04-29 19:49:30,289 - memory_profile6_log - INFO -    240    769.0 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-29 19:49:30,290 - memory_profile6_log - INFO -    241                                     # ~ Place your code to save the training model here ~

2018-04-29 19:49:30,290 - memory_profile6_log - INFO -    242    769.0 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-29 19:49:30,292 - memory_profile6_log - INFO -    243    769.0 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-29 19:49:30,292 - memory_profile6_log - INFO -    244    769.0 MiB      0.0 MiB               if multproc:

2018-04-29 19:49:30,292 - memory_profile6_log - INFO -    245                                             # ~ save transform models ~

2018-04-29 19:49:30,292 - memory_profile6_log - INFO -    246    740.8 MiB    -28.1 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-29 19:49:30,292 - memory_profile6_log - INFO -    247                             

2018-04-29 19:49:30,292 - memory_profile6_log - INFO -    248                                             """

2018-04-29 19:49:30,293 - memory_profile6_log - INFO -    249                                             # ~ save fitted models ~

2018-04-29 19:49:30,293 - memory_profile6_log - INFO -    250                                             logger.info("Saving fitted_models as history...")

2018-04-29 19:49:30,295 - memory_profile6_log - INFO -    251                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 19:49:30,295 - memory_profile6_log - INFO -    252                                             fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-29 19:49:30,295 - memory_profile6_log - INFO -    253                             

2018-04-29 19:49:30,299 - memory_profile6_log - INFO -    254                                             X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-29 19:49:30,299 - memory_profile6_log - INFO -    255                                             logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-29 19:49:30,299 - memory_profile6_log - INFO -    256                                             for ix in range(len(X_split)):

2018-04-29 19:49:30,301 - memory_profile6_log - INFO -    257                                                 logger.info("processing batch-%d", ix)

2018-04-29 19:49:30,301 - memory_profile6_log - INFO -    258                                                 mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-29 19:49:30,301 - memory_profile6_log - INFO -    259                             

2018-04-29 19:49:30,302 - memory_profile6_log - INFO -    260                                             del X_split

2018-04-29 19:49:30,302 - memory_profile6_log - INFO -    261                                             logger.info("deleting X_split...")

2018-04-29 19:49:30,302 - memory_profile6_log - INFO -    262                                             del save_sigma_nt

2018-04-29 19:49:30,302 - memory_profile6_log - INFO -    263                                             logger.info("deleting save_sigma_nt...")

2018-04-29 19:49:30,302 - memory_profile6_log - INFO -    264                                             """

2018-04-29 19:49:30,303 - memory_profile6_log - INFO -    265                             

2018-04-29 19:49:30,305 - memory_profile6_log - INFO -    266    740.8 MiB      0.0 MiB                   del BR

2018-04-29 19:49:30,305 - memory_profile6_log - INFO -    267    740.8 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-29 19:49:30,306 - memory_profile6_log - INFO -    268                             

2018-04-29 19:49:30,306 - memory_profile6_log - INFO -    269                                     elif str(saveto).lower() == "elastic":

2018-04-29 19:49:30,308 - memory_profile6_log - INFO -    270                                         logger.info("Using ElasticSearch as storage...")

2018-04-29 19:49:30,312 - memory_profile6_log - INFO -    271                                         mh.saveElasticS(model_transformsv)

2018-04-29 19:49:30,312 - memory_profile6_log - INFO -    272                             

2018-04-29 19:49:30,313 - memory_profile6_log - INFO -    273                                     # need save sigma_nt for daily train

2018-04-29 19:49:30,315 - memory_profile6_log - INFO -    274                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-29 19:49:30,315 - memory_profile6_log - INFO -    275                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-29 19:49:30,315 - memory_profile6_log - INFO -    276    740.8 MiB      0.0 MiB           if start_date and end_date:

2018-04-29 19:49:30,315 - memory_profile6_log - INFO -    277                                         if not fitby_sigmant:

2018-04-29 19:49:30,316 - memory_profile6_log - INFO -    278                                             logging.info("Saving sigma Nt...")

2018-04-29 19:49:30,316 - memory_profile6_log - INFO -    279                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 19:49:30,318 - memory_profile6_log - INFO -    280                                             save_sigma_nt['start_date'] = start_date

2018-04-29 19:49:30,318 - memory_profile6_log - INFO -    281                                             save_sigma_nt['end_date'] = end_date

2018-04-29 19:49:30,319 - memory_profile6_log - INFO -    282                                             print save_sigma_nt.head(5)

2018-04-29 19:49:30,319 - memory_profile6_log - INFO -    283                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-29 19:49:30,322 - memory_profile6_log - INFO -    284                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-29 19:49:30,322 - memory_profile6_log - INFO -    285    740.8 MiB      0.0 MiB       return

2018-04-29 19:49:30,323 - memory_profile6_log - INFO - 


2018-04-29 19:49:30,325 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-30 08:24:55,375 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-30 08:24:55,446 - memory_profile6_log - INFO - date_generated: 
2018-04-30 08:24:55,446 - memory_profile6_log - INFO -  
2018-04-30 08:24:55,446 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 10, 0, 0)]
2018-04-30 08:24:55,447 - memory_profile6_log - INFO - 

2018-04-30 08:24:55,447 - memory_profile6_log - INFO - using current date: 2018-04-11
2018-04-30 08:24:55,447 - memory_profile6_log - INFO - using start date: 2018-04-10 00:00:00
2018-04-30 08:24:55,447 - memory_profile6_log - INFO - using end date: 2018-04-10
2018-04-30 08:24:55,601 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-30 08:24:55,605 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-10
2018-04-30 08:26:58,683 - memory_profile6_log - INFO - size of df: 108.51 MB
2018-04-30 08:26:58,684 - memory_profile6_log - INFO - getting total: 427635 training data(genuine interest) for date: 2018-04-10
2018-04-30 08:26:58,743 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-30 08:26:58,746 - memory_profile6_log - INFO - Appending history data...
2018-04-30 08:26:58,746 - memory_profile6_log - INFO - processing batch-0
2018-04-30 08:26:58,747 - memory_profile6_log - INFO - creating list history data...
2018-04-30 08:26:58,865 - memory_profile6_log - INFO - call history data...
2018-04-30 08:27:48,453 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 08:27:50,319 - memory_profile6_log - INFO - processing batch-1
2018-04-30 08:27:50,321 - memory_profile6_log - INFO - creating list history data...
2018-04-30 08:27:50,400 - memory_profile6_log - INFO - call history data...
2018-04-30 08:28:43,073 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 08:28:44,871 - memory_profile6_log - INFO - processing batch-2
2018-04-30 08:28:44,874 - memory_profile6_log - INFO - creating list history data...
2018-04-30 08:28:44,953 - memory_profile6_log - INFO - call history data...
2018-04-30 08:29:40,082 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 08:29:41,931 - memory_profile6_log - INFO - processing batch-3
2018-04-30 08:29:41,933 - memory_profile6_log - INFO - creating list history data...
2018-04-30 08:29:42,012 - memory_profile6_log - INFO - call history data...
2018-04-30 08:30:34,167 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 08:30:36,066 - memory_profile6_log - INFO - processing batch-4
2018-04-30 08:30:36,068 - memory_profile6_log - INFO - creating list history data...
2018-04-30 08:30:36,151 - memory_profile6_log - INFO - call history data...
2018-04-30 08:31:29,246 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 08:31:30,976 - memory_profile6_log - INFO - Appending training data...
2018-04-30 08:31:30,977 - memory_profile6_log - INFO - len datalist: 1
2018-04-30 08:31:30,979 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-30 08:31:30,993 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 08:31:30,996 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 08:31:30,997 - memory_profile6_log - INFO - ================================================

2018-04-30 08:31:31,000 - memory_profile6_log - INFO -    312     86.7 MiB     86.7 MiB   @profile

2018-04-30 08:31:31,000 - memory_profile6_log - INFO -    313                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-30 08:31:31,002 - memory_profile6_log - INFO -    314     86.7 MiB      0.0 MiB       bq_client = client

2018-04-30 08:31:31,003 - memory_profile6_log - INFO -    315     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 08:31:31,005 - memory_profile6_log - INFO -    316                             

2018-04-30 08:31:31,007 - memory_profile6_log - INFO -    317     86.7 MiB      0.0 MiB       datalist = []

2018-04-30 08:31:31,009 - memory_profile6_log - INFO -    318     86.7 MiB      0.0 MiB       datalist_hist = []

2018-04-30 08:31:31,009 - memory_profile6_log - INFO -    319                             

2018-04-30 08:31:31,010 - memory_profile6_log - INFO -    320     86.7 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-30 08:31:31,012 - memory_profile6_log - INFO -    321    689.3 MiB      0.0 MiB       for ndate in date_generated:

2018-04-30 08:31:31,012 - memory_profile6_log - INFO -    322    393.7 MiB    306.9 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-30 08:31:31,013 - memory_profile6_log - INFO -    323    393.7 MiB      0.0 MiB           if tframe is not None:

2018-04-30 08:31:31,013 - memory_profile6_log - INFO -    324    393.7 MiB      0.0 MiB               if not tframe.empty:

2018-04-30 08:31:31,013 - memory_profile6_log - INFO -    325    407.6 MiB     13.9 MiB                   X_split = np.array_split(tframe, 5)

2018-04-30 08:31:31,015 - memory_profile6_log - INFO -    326    407.6 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-30 08:31:31,017 - memory_profile6_log - INFO -    327    407.6 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-30 08:31:31,019 - memory_profile6_log - INFO -    328    689.3 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-30 08:31:31,019 - memory_profile6_log - INFO -    329                                                 # ~ loading history

2018-04-30 08:31:31,020 - memory_profile6_log - INFO -    330                                                 """

2018-04-30 08:31:31,022 - memory_profile6_log - INFO -    331                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-30 08:31:31,023 - memory_profile6_log - INFO -    332                                                 """

2018-04-30 08:31:31,023 - memory_profile6_log - INFO -    333    663.4 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-30 08:31:31,023 - memory_profile6_log - INFO -    334                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-30 08:31:31,025 - memory_profile6_log - INFO -    335    663.4 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-30 08:31:31,025 - memory_profile6_log - INFO -    336                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-30 08:31:31,026 - memory_profile6_log - INFO -    337    663.8 MiB      4.7 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-30 08:31:31,026 - memory_profile6_log - INFO -    338                             

2018-04-30 08:31:31,029 - memory_profile6_log - INFO -    339    663.8 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-30 08:31:31,032 - memory_profile6_log - INFO -    340    741.3 MiB    559.7 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-30 08:31:31,032 - memory_profile6_log - INFO -    341                             

2018-04-30 08:31:31,033 - memory_profile6_log - INFO -    342                                                 # me = os.getpid()

2018-04-30 08:31:31,035 - memory_profile6_log - INFO -    343                                                 # kill_proc_tree(me)

2018-04-30 08:31:31,036 - memory_profile6_log - INFO -    344                             

2018-04-30 08:31:31,036 - memory_profile6_log - INFO -    345    741.3 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-30 08:31:31,039 - memory_profile6_log - INFO -    346    743.5 MiB     -1.3 MiB                       for m in h_frame:

2018-04-30 08:31:31,040 - memory_profile6_log - INFO -    347    743.4 MiB     -1.3 MiB                           if m is not None:

2018-04-30 08:31:31,042 - memory_profile6_log - INFO -    348    743.4 MiB     -1.3 MiB                               if len(m) > 0:

2018-04-30 08:31:31,043 - memory_profile6_log - INFO -    349    743.5 MiB      8.2 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-30 08:31:31,045 - memory_profile6_log - INFO -    350    689.3 MiB   -290.9 MiB                       del h_frame

2018-04-30 08:31:31,046 - memory_profile6_log - INFO -    351    689.3 MiB     -1.4 MiB                       del lhistory

2018-04-30 08:31:31,049 - memory_profile6_log - INFO -    352                             

2018-04-30 08:31:31,051 - memory_profile6_log - INFO -    353    689.3 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-30 08:31:31,053 - memory_profile6_log - INFO -    354    689.3 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-30 08:31:31,055 - memory_profile6_log - INFO -    355                                     else: 

2018-04-30 08:31:31,055 - memory_profile6_log - INFO -    356                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-30 08:31:31,058 - memory_profile6_log - INFO -    357    689.3 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-30 08:31:31,058 - memory_profile6_log - INFO -    358    689.3 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-30 08:31:31,062 - memory_profile6_log - INFO -    359                             

2018-04-30 08:31:31,063 - memory_profile6_log - INFO -    360    689.3 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-30 08:31:31,065 - memory_profile6_log - INFO - 


2018-04-30 08:31:32,299 - memory_profile6_log - INFO - big_frame_hist:

2018-04-30 08:31:32,374 - memory_profile6_log - INFO -     p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0    0.001543        1903.744479        20            1913.744479  10960288  162a84a4871114-02e5edb56e5e0e-4446062d-15f900-...
1    0.001543         278.821163       215             288.821163  10960288  1612d4a7d4244-03b9fb7b2b4f32-770c003f-38400-16...
2    0.001543        1105.960925       149            1115.960925  10960288  161b32ec49815-09b38099f85a43-a4f5b5e-38400-161...
3    0.001543         571.176014       683             581.176014  10960288  16134c6d4922f-0e70501ee6f181-5e686126-38400-16...
4    0.001543        1244.110653       532            1254.110653  10960288  162772b177485-0845b768cd4ee68-767f210c-34080-1...
5    0.001543         291.268320       941             301.268320  10960288  16132603fed40-03fd04d7460f14-29056814-38400-16...
6    0.000088         919.410641       322             929.410641  11911567  161abbd40942a2-09e23706b8fe74-393d5f0e-15f900-...
7    0.001543         171.249190       363             181.249190  10960288  161369100c721-0027b84fe2cab8-292a5039-38400-16...
8    0.001543         533.742995      5176             543.742995  10960288  16221e5284648a-03aee4c728eb91-b353461-100200-1...
9    0.001543        1903.744479        70            1913.744479  10960288  162af59848b6e-0079e2e24-25594112-38400-162af59...
10   0.000023       13649.808773      1459           13659.808773  10959239  1610ca028e379-098a172ba6d096-4323461-100200-16...
11   0.001543         729.768717       120             739.768717  10960288  16227bb8fa51ac-0892979e6d8e14-282b503d-38400-1...
12   0.001543         367.856158      3694             377.856158  10960288  161ebc44254171-014528993a9fb2-b353461-ff000-16...
13   0.001543         204.534531       121             214.534531  10960288  1612dae1ed239-05acfbe26-18762e3c-2c880-1612dae...
14   0.001543         781.023376        39             791.023376  10960288  16137a258dbc1-0fac0c829c1047-50670302-38400-16...
15   0.001543          39.252463       194              49.252463  10960288  1612d3f021a6f-05fd792fe3dbcc-5672072d-38400-16...
16   0.001543         322.474550       392             332.474550  10960288  161ad4d785798-019de802542e46-282b503d-38400-16...
17   0.001543        1618.846766       156            1628.846766  10960288  16285bc572a839-041f74c2037c478-3f636c4c-fa000-...
18   0.001543         572.554731       133             582.554731  10960288  161302b767e72-07d8e2e5043058-503343a-8d272-161...
19   0.000088         753.521504       136             763.521504  11911567  16130322f1e4b-09f36b8aa-7637c0a-2c880-16130322...
2018-04-30 08:31:32,375 - memory_profile6_log - INFO - 

2018-04-30 08:31:32,509 - memory_profile6_log - INFO - size of big_frame_hist: 117.89 MB
2018-04-30 08:31:32,622 - memory_profile6_log - INFO - size of big_frame: 108.51 MB
2018-04-30 08:31:32,641 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-04-30 08:31:52,854 - memory_profile6_log - INFO - size of df: 11.88 MB
2018-04-30 08:31:52,855 - memory_profile6_log - INFO - getting total: 48430 training data(current date interest)
2018-04-30 08:31:52,888 - memory_profile6_log - INFO - size of current_frame: 12.25 MB
2018-04-30 08:31:52,890 - memory_profile6_log - INFO - loading time of: 476065 total genuine-current interest data ~ take 417.311s
2018-04-30 08:31:52,921 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 08:31:52,921 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 08:31:52,921 - memory_profile6_log - INFO - ================================================

2018-04-30 08:31:52,921 - memory_profile6_log - INFO -    362     86.6 MiB     86.6 MiB   @profile

2018-04-30 08:31:52,923 - memory_profile6_log - INFO -    363                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-30 08:31:52,923 - memory_profile6_log - INFO -    364     86.7 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-30 08:31:52,924 - memory_profile6_log - INFO -    365     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 08:31:52,926 - memory_profile6_log - INFO -    366                             

2018-04-30 08:31:52,927 - memory_profile6_log - INFO -    367                                 # ~~~ Begin collecting data ~~~

2018-04-30 08:31:52,927 - memory_profile6_log - INFO -    368     86.7 MiB      0.0 MiB       t0 = time.time()

2018-04-30 08:31:52,927 - memory_profile6_log - INFO -    369    679.5 MiB    592.7 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-30 08:31:52,927 - memory_profile6_log - INFO -    370    679.5 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-30 08:31:52,927 - memory_profile6_log - INFO -    371                                     logger.info("Training cannot be empty..")

2018-04-30 08:31:52,931 - memory_profile6_log - INFO -    372                                     return False

2018-04-30 08:31:52,931 - memory_profile6_log - INFO -    373    705.6 MiB     26.2 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-30 08:31:52,933 - memory_profile6_log - INFO -    374    705.8 MiB      0.1 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-30 08:31:52,933 - memory_profile6_log - INFO -    375    705.8 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-30 08:31:52,934 - memory_profile6_log - INFO -    376                             

2018-04-30 08:31:52,934 - memory_profile6_log - INFO -    377    716.0 MiB     10.2 MiB       big_frame = pd.concat(datalist)

2018-04-30 08:31:52,934 - memory_profile6_log - INFO -    378    716.0 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-30 08:31:52,934 - memory_profile6_log - INFO -    379    706.2 MiB     -9.8 MiB       del datalist

2018-04-30 08:31:52,936 - memory_profile6_log - INFO -    380                             

2018-04-30 08:31:52,936 - memory_profile6_log - INFO -    381    706.2 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 08:31:52,937 - memory_profile6_log - INFO -    382                             

2018-04-30 08:31:52,937 - memory_profile6_log - INFO -    383                                 # ~ get current news interest ~

2018-04-30 08:31:52,937 - memory_profile6_log - INFO -    384    706.2 MiB      0.0 MiB       if not cd:

2018-04-30 08:31:52,937 - memory_profile6_log - INFO -    385    706.2 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-04-30 08:31:52,938 - memory_profile6_log - INFO -    386    718.0 MiB     11.8 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-30 08:31:52,938 - memory_profile6_log - INFO -    387    718.0 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-30 08:31:52,940 - memory_profile6_log - INFO -    388                                 else:

2018-04-30 08:31:52,940 - memory_profile6_log - INFO -    389                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-30 08:31:52,944 - memory_profile6_log - INFO -    390                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-30 08:31:52,944 - memory_profile6_log - INFO -    391                             

2018-04-30 08:31:52,946 - memory_profile6_log - INFO -    392                                     # safe handling of query parameter

2018-04-30 08:31:52,946 - memory_profile6_log - INFO -    393                                     query_params = [

2018-04-30 08:31:52,946 - memory_profile6_log - INFO -    394                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-30 08:31:52,947 - memory_profile6_log - INFO -    395                                     ]

2018-04-30 08:31:52,947 - memory_profile6_log - INFO -    396                             

2018-04-30 08:31:52,948 - memory_profile6_log - INFO -    397                                     job_config.query_parameters = query_params

2018-04-30 08:31:52,948 - memory_profile6_log - INFO -    398                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-30 08:31:52,950 - memory_profile6_log - INFO -    399                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-30 08:31:52,950 - memory_profile6_log - INFO -    400                             

2018-04-30 08:31:52,950 - memory_profile6_log - INFO -    401    718.0 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-30 08:31:52,950 - memory_profile6_log - INFO -    402    718.0 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-30 08:31:52,951 - memory_profile6_log - INFO -    403    718.3 MiB      0.3 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-30 08:31:52,951 - memory_profile6_log - INFO -    404    718.3 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-30 08:31:52,957 - memory_profile6_log - INFO -    405    718.3 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-30 08:31:52,957 - memory_profile6_log - INFO -    406    718.3 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-30 08:31:52,957 - memory_profile6_log - INFO -    407                             

2018-04-30 08:31:52,959 - memory_profile6_log - INFO -    408    718.3 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-30 08:31:52,960 - memory_profile6_log - INFO - 


2018-04-30 08:31:52,966 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-30 08:31:53,003 - memory_profile6_log - INFO - train on: 427635 total genuine interest data(D(u, t))
2018-04-30 08:31:53,003 - memory_profile6_log - INFO - transform on: 48430 total current data(D(t))
2018-04-30 08:31:53,005 - memory_profile6_log - INFO - apply on: 427635 total history...)
2018-04-30 08:31:53,457 - memory_profile6_log - INFO - len of uniques_fit_hist:427635
2018-04-30 08:31:53,816 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:52258
2018-04-30 08:31:55,426 - memory_profile6_log - INFO - Len of model_fit: 427635
2018-04-30 08:31:55,427 - memory_profile6_log - INFO - Len of df_dut: 427635
2018-04-30 08:31:55,431 - memory_profile6_log - INFO - 0    1591
1    1591
2    1591
3    1591
4    1591
5    1591
6    1591
7    1591
8    1591
9    1591
Name: sigma_Nt, dtype: int64
2018-04-30 08:31:55,433 - memory_profile6_log - INFO - 

2018-04-30 08:31:55,556 - memory_profile6_log - INFO - model_fit dtypes:

2018-04-30 08:31:55,569 - memory_profile6_log - INFO - date              datetime64[ns, UTC]
user_id                        object
topic_id                       object
num_x                           int64
num_y                           int64
is_general                       bool
date_all_click                  int64
Ntotal                          int64
sigma_Nt                        int64
joinprob_ci                   float64
p_cat_ci                      float64
posterior                     float64
dtype: object
2018-04-30 08:31:55,571 - memory_profile6_log - INFO - 

2018-04-30 08:31:55,572 - memory_profile6_log - INFO - fitted_models_hist dtypes:

2018-04-30 08:31:55,576 - memory_profile6_log - INFO - p0_cat_ci                float64
pt_posterior_x_Nt        float64
sigma_Nt                   int64
smoothed_pt_posterior    float64
topic_id                  object
user_id                   object
dtype: object
2018-04-30 08:31:55,578 - memory_profile6_log - INFO - 

2018-04-30 08:31:56,358 - memory_profile6_log - INFO - Len of fitted_models on main class: 427635
2018-04-30 08:31:56,359 - memory_profile6_log - INFO - 

2018-04-30 08:31:56,361 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-30 08:31:56,361 - memory_profile6_log - INFO - 

2018-04-30 08:31:56,364 - memory_profile6_log - INFO - Fitted models before concat:

2018-04-30 08:31:56,392 - memory_profile6_log - INFO -                                              user_id   topic_id  pt_posterior_x_Nt  smoothed_pt_posterior
0               0ba81918-23e5-4d7c-b2ce-f943cc893b46   22601470           0.861691              10.861691
1               0ba81918-23e5-4d7c-b2ce-f943cc893b46   27431099           0.994540              10.994540
2               0ba81918-23e5-4d7c-b2ce-f943cc893b46   39301645           2.958966              12.958966
3               0ba81918-23e5-4d7c-b2ce-f943cc893b46   40347025          22.187022              32.187022
4  161035a29015-0041351a-5d670102-38400-161035a29...  132166743           1.004834              11.004834
2018-04-30 08:31:56,394 - memory_profile6_log - INFO - 

2018-04-30 08:31:56,395 - memory_profile6_log - INFO - len of current fitted models: 427635
2018-04-30 08:31:56,400 - memory_profile6_log - INFO - 

2018-04-30 08:31:56,401 - memory_profile6_log - INFO - len of history fitted models: 427635
2018-04-30 08:31:56,401 - memory_profile6_log - INFO - 

2018-04-30 08:31:56,484 - memory_profile6_log - INFO - len of fitted models after concat: 855270
2018-04-30 08:31:56,484 - memory_profile6_log - INFO - 

2018-04-30 08:31:56,486 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-30 08:31:56,486 - memory_profile6_log - INFO - 

2018-04-30 08:31:57,095 - memory_profile6_log - INFO - Fitted models after concat:

2018-04-30 08:31:57,117 - memory_profile6_log - INFO -                                              user_id   topic_id  pt_posterior_x_Nt
0               0ba81918-23e5-4d7c-b2ce-f943cc893b46   22601470          93.677269
1               0ba81918-23e5-4d7c-b2ce-f943cc893b46   27431099          96.341213
2               0ba81918-23e5-4d7c-b2ce-f943cc893b46   39301645         126.807034
3               0ba81918-23e5-4d7c-b2ce-f943cc893b46   40347025          45.853179
4  161035a29015-0041351a-5d670102-38400-161035a29...  132166743           2.044801
2018-04-30 08:31:57,117 - memory_profile6_log - INFO - 

2018-04-30 08:31:57,134 - memory_profile6_log - INFO - Fitted models after concat-simplify:

2018-04-30 08:31:57,157 - memory_profile6_log - INFO -                                              user_id   topic_id  pt_posterior_x_Nt  smoothed_pt_posterior
0               0ba81918-23e5-4d7c-b2ce-f943cc893b46   22601470          93.677269             103.677269
1               0ba81918-23e5-4d7c-b2ce-f943cc893b46   27431099          96.341213             106.341213
2               0ba81918-23e5-4d7c-b2ce-f943cc893b46   39301645         126.807034             136.807034
3               0ba81918-23e5-4d7c-b2ce-f943cc893b46   40347025          45.853179              55.853179
4  161035a29015-0041351a-5d670102-38400-161035a29...  132166743           2.044801              12.044801
2018-04-30 08:31:57,158 - memory_profile6_log - INFO - 

2018-04-30 08:31:57,161 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 427635
2018-04-30 08:31:57,161 - memory_profile6_log - INFO - 

2018-04-30 08:31:57,266 - memory_profile6_log - INFO -                                              user_id           topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci
0               0ba81918-23e5-4d7c-b2ce-f943cc893b46           22601470          93.677269             103.677269   0.030239
1               0ba81918-23e5-4d7c-b2ce-f943cc893b46           27431099          96.341213             106.341213   0.024702
2               0ba81918-23e5-4d7c-b2ce-f943cc893b46           39301645         126.807034             136.807034   0.012648
3               0ba81918-23e5-4d7c-b2ce-f943cc893b46           40347025          45.853179              55.853179   0.000460
4  161035a29015-0041351a-5d670102-38400-161035a29...          132166743           2.044801              12.044801   0.000223
5  161035a29015-0041351a-5d670102-38400-161035a29...           22601470          97.110842             107.110842   0.030239
6  161035a29015-0041351a-5d670102-38400-161035a29...           27431099          99.890801             109.890801   0.024702
7  161035a29015-0041351a-5d670102-38400-161035a29...  27431110790313993           2.113336              12.113336        NaN
8  161035a29015-0041351a-5d670102-38400-161035a29...           47091877         166.285815             176.285815   0.001158
9  1610c8e6b6466b-05a6ac1ba8c1b8-4323461-100200-1...         1019587357       31335.850426           31345.850426        NaN
2018-04-30 08:31:57,266 - memory_profile6_log - INFO - 

2018-04-30 08:33:03,900 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-30 08:33:03,941 - memory_profile6_log - INFO - Total train time: 70.938s
2018-04-30 08:33:03,944 - memory_profile6_log - INFO - memory left before cleaning: 80.300 percent memory...
2018-04-30 08:33:03,944 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-30 08:33:03,948 - memory_profile6_log - INFO - deleting df_dut...
2018-04-30 08:33:03,950 - memory_profile6_log - INFO - deleting df_dt...
2018-04-30 08:33:03,950 - memory_profile6_log - INFO - deleting df_input...
2018-04-30 08:33:03,953 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-30 08:33:03,953 - memory_profile6_log - INFO - deleting df_current...
2018-04-30 08:33:03,956 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-30 08:33:03,974 - memory_profile6_log - INFO - deleting model_fit...
2018-04-30 08:33:03,976 - memory_profile6_log - INFO - deleting result...
2018-04-30 08:33:04,000 - memory_profile6_log - INFO - deleting model_transform...
2018-04-30 08:33:04,000 - memory_profile6_log - INFO - memory left after cleaning: 80.000 percent memory...
2018-04-30 08:33:04,003 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-30 08:33:04,003 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-30 08:33:04,174 - memory_profile6_log - INFO - deleting BR...
2018-04-30 08:33:04,180 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 08:33:04,181 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 08:33:04,183 - memory_profile6_log - INFO - ================================================

2018-04-30 08:33:04,184 - memory_profile6_log - INFO -    113    661.6 MiB    661.6 MiB   @profile

2018-04-30 08:33:04,184 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-30 08:33:04,184 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-30 08:33:04,184 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-30 08:33:04,186 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-30 08:33:04,186 - memory_profile6_log - INFO -    118                                      saveto="datastore", fitted_models_hist=None):

2018-04-30 08:33:04,187 - memory_profile6_log - INFO -    119                                 """

2018-04-30 08:33:04,187 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-30 08:33:04,187 - memory_profile6_log - INFO -    121                                 """

2018-04-30 08:33:04,187 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-30 08:33:04,187 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-30 08:33:04,187 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-30 08:33:04,188 - memory_profile6_log - INFO -    125    661.6 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-30 08:33:04,188 - memory_profile6_log - INFO -    126    674.8 MiB     13.2 MiB       df_dut = df_input.copy(deep=True)

2018-04-30 08:33:04,190 - memory_profile6_log - INFO -    127    674.8 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 08:33:04,190 - memory_profile6_log - INFO -    128                             

2018-04-30 08:33:04,190 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-30 08:33:04,190 - memory_profile6_log - INFO -    130    675.7 MiB      0.9 MiB       df_dt = df_current.copy(deep=True)

2018-04-30 08:33:04,190 - memory_profile6_log - INFO -    131    676.8 MiB      1.1 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 08:33:04,194 - memory_profile6_log - INFO -    132                             

2018-04-30 08:33:04,196 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-30 08:33:04,196 - memory_profile6_log - INFO -    134    676.8 MiB      0.0 MiB       t0 = time.time()

2018-04-30 08:33:04,196 - memory_profile6_log - INFO -    135    676.8 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-30 08:33:04,197 - memory_profile6_log - INFO -    136    676.8 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-30 08:33:04,197 - memory_profile6_log - INFO -    137    676.8 MiB      0.0 MiB       logger.info("apply on: %d total history...)", len(df_hist))

2018-04-30 08:33:04,197 - memory_profile6_log - INFO -    138                             

2018-04-30 08:33:04,197 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-30 08:33:04,197 - memory_profile6_log - INFO -    140    676.8 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-30 08:33:04,198 - memory_profile6_log - INFO -    141                             

2018-04-30 08:33:04,198 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-30 08:33:04,200 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-30 08:33:04,200 - memory_profile6_log - INFO -    144    678.0 MiB      1.2 MiB       NB = BR.processX(df_dut)

2018-04-30 08:33:04,200 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-30 08:33:04,200 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-30 08:33:04,200 - memory_profile6_log - INFO -    147    694.5 MiB     16.5 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-30 08:33:04,200 - memory_profile6_log - INFO -    148                                 """

2018-04-30 08:33:04,201 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-30 08:33:04,201 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-30 08:33:04,201 - memory_profile6_log - INFO -    151                                 """

2018-04-30 08:33:04,203 - memory_profile6_log - INFO -    152    694.5 MiB      0.0 MiB       fitby_sigmant = True

2018-04-30 08:33:04,203 - memory_profile6_log - INFO -    153    694.5 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-30 08:33:04,203 - memory_profile6_log - INFO -    154    694.5 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-30 08:33:04,207 - memory_profile6_log - INFO -    155    710.8 MiB     16.3 MiB                            'is_general']]

2018-04-30 08:33:04,207 - memory_profile6_log - INFO -    156                                 # get sigma_Nt from fitted_models_hist

2018-04-30 08:33:04,207 - memory_profile6_log - INFO -    157    717.3 MiB      6.5 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-04-30 08:33:04,209 - memory_profile6_log - INFO -    158    717.3 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-04-30 08:33:04,209 - memory_profile6_log - INFO -    159    713.9 MiB     -3.4 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-04-30 08:33:04,209 - memory_profile6_log - INFO -    160    713.9 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-04-30 08:33:04,210 - memory_profile6_log - INFO -    161                             

2018-04-30 08:33:04,210 - memory_profile6_log - INFO -    162                                 # begin fit

2018-04-30 08:33:04,210 - memory_profile6_log - INFO -    163    713.9 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-30 08:33:04,210 - memory_profile6_log - INFO -    164    713.9 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-30 08:33:04,210 - memory_profile6_log - INFO -    165    761.0 MiB     47.1 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-04-30 08:33:04,211 - memory_profile6_log - INFO -    166    761.0 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-30 08:33:04,211 - memory_profile6_log - INFO -    167    761.0 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-30 08:33:04,211 - memory_profile6_log - INFO -    168    761.0 MiB      0.0 MiB       print model_fit['sigma_Nt'].head(10)

2018-04-30 08:33:04,213 - memory_profile6_log - INFO -    169                             

2018-04-30 08:33:04,213 - memory_profile6_log - INFO -    170                                 # ~~ and Transform ~~

2018-04-30 08:33:04,213 - memory_profile6_log - INFO -    171                                 #   handling current news interest == current date

2018-04-30 08:33:04,213 - memory_profile6_log - INFO -    172    761.0 MiB      0.0 MiB       if df_dt.empty:

2018-04-30 08:33:04,213 - memory_profile6_log - INFO -    173                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-30 08:33:04,213 - memory_profile6_log - INFO -    174                                     return None

2018-04-30 08:33:04,214 - memory_profile6_log - INFO -    175    762.8 MiB      1.8 MiB       NB = BR.processX(df_dt)

2018-04-30 08:33:04,214 - memory_profile6_log - INFO -    176    762.8 MiB      0.0 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-30 08:33:04,214 - memory_profile6_log - INFO -    177                             

2018-04-30 08:33:04,220 - memory_profile6_log - INFO -    178    762.8 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-30 08:33:04,220 - memory_profile6_log - INFO -    179    748.0 MiB    -14.8 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-30 08:33:04,220 - memory_profile6_log - INFO -    180                             

2018-04-30 08:33:04,220 - memory_profile6_log - INFO -    181    748.0 MiB      0.0 MiB       print "model_fit dtypes:\n", model_fit.dtypes

2018-04-30 08:33:04,220 - memory_profile6_log - INFO -    182    748.0 MiB      0.0 MiB       print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-04-30 08:33:04,221 - memory_profile6_log - INFO -    183    748.0 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-30 08:33:04,221 - memory_profile6_log - INFO -    184    748.0 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-30 08:33:04,223 - memory_profile6_log - INFO -    185    748.0 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-04-30 08:33:04,223 - memory_profile6_log - INFO -    186    757.8 MiB      9.8 MiB                                                                                           "topic_id", "user_id"]],

2018-04-30 08:33:04,223 - memory_profile6_log - INFO -    187    803.1 MiB     45.4 MiB                                                     verbose=False)

2018-04-30 08:33:04,223 - memory_profile6_log - INFO -    188                                 # ~~~ filter is general and specific topic ~~~

2018-04-30 08:33:04,223 - memory_profile6_log - INFO -    189                                 # the idea is just we need to rerank every topic according

2018-04-30 08:33:04,224 - memory_profile6_log - INFO -    190                                 # user_id and and is_general by p0_posterior

2018-04-30 08:33:04,224 - memory_profile6_log - INFO -    191    803.1 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-30 08:33:04,224 - memory_profile6_log - INFO -    192    806.4 MiB      3.3 MiB                                     'is_general']].groupby(['topic_id',

2018-04-30 08:33:04,226 - memory_profile6_log - INFO -    193    803.2 MiB     -3.2 MiB                                                             'is_general']

2018-04-30 08:33:04,226 - memory_profile6_log - INFO -    194                                                                                      ).size().to_frame().reset_index()

2018-04-30 08:33:04,226 - memory_profile6_log - INFO -    195                             

2018-04-30 08:33:04,226 - memory_profile6_log - INFO -    196                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-30 08:33:04,226 - memory_profile6_log - INFO -    197                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-30 08:33:04,226 - memory_profile6_log - INFO -    198    803.2 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-30 08:33:04,230 - memory_profile6_log - INFO -    199                             

2018-04-30 08:33:04,232 - memory_profile6_log - INFO -    200                                 # ~ start by provide rank for each topic type ~

2018-04-30 08:33:04,232 - memory_profile6_log - INFO -    201    812.7 MiB      9.5 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-30 08:33:04,233 - memory_profile6_log - INFO -    202    820.8 MiB      8.1 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-30 08:33:04,233 - memory_profile6_log - INFO -    203                             

2018-04-30 08:33:04,233 - memory_profile6_log - INFO -    204                                 # ~ set threshold to filter output

2018-04-30 08:33:04,233 - memory_profile6_log - INFO -    205    820.8 MiB      0.0 MiB       if threshold > 0:

2018-04-30 08:33:04,233 - memory_profile6_log - INFO -    206    820.8 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-30 08:33:04,233 - memory_profile6_log - INFO -    207    820.8 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-30 08:33:04,234 - memory_profile6_log - INFO -    208    813.7 MiB     -7.1 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-30 08:33:04,234 - memory_profile6_log - INFO -    209                             

2018-04-30 08:33:04,234 - memory_profile6_log - INFO -    210    813.7 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-30 08:33:04,236 - memory_profile6_log - INFO -    211    813.7 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-30 08:33:04,236 - memory_profile6_log - INFO -    212                             

2018-04-30 08:33:04,236 - memory_profile6_log - INFO -    213    813.7 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-30 08:33:04,236 - memory_profile6_log - INFO -    214                             

2018-04-30 08:33:04,236 - memory_profile6_log - INFO -    215    813.7 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-30 08:33:04,236 - memory_profile6_log - INFO -    216    813.7 MiB      0.0 MiB       del df_dut

2018-04-30 08:33:04,237 - memory_profile6_log - INFO -    217    813.7 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-30 08:33:04,237 - memory_profile6_log - INFO -    218    813.7 MiB      0.0 MiB       del df_dt

2018-04-30 08:33:04,239 - memory_profile6_log - INFO -    219    813.7 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-30 08:33:04,239 - memory_profile6_log - INFO -    220    813.7 MiB      0.0 MiB       del df_input

2018-04-30 08:33:04,239 - memory_profile6_log - INFO -    221    813.7 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-30 08:33:04,243 - memory_profile6_log - INFO -    222    813.7 MiB      0.0 MiB       del df_input_X

2018-04-30 08:33:04,243 - memory_profile6_log - INFO -    223    813.7 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-30 08:33:04,243 - memory_profile6_log - INFO -    224    813.7 MiB      0.0 MiB       del df_current

2018-04-30 08:33:04,244 - memory_profile6_log - INFO -    225    813.7 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-30 08:33:04,244 - memory_profile6_log - INFO -    226    813.7 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-30 08:33:04,244 - memory_profile6_log - INFO -    227    813.7 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-30 08:33:04,246 - memory_profile6_log - INFO -    228    777.8 MiB    -35.9 MiB       del model_fit

2018-04-30 08:33:04,246 - memory_profile6_log - INFO -    229    777.8 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-30 08:33:04,246 - memory_profile6_log - INFO -    230    777.8 MiB      0.0 MiB       del result

2018-04-30 08:33:04,246 - memory_profile6_log - INFO -    231    777.8 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-30 08:33:04,247 - memory_profile6_log - INFO -    232                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 08:33:04,247 - memory_profile6_log - INFO -    233                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 08:33:04,249 - memory_profile6_log - INFO -    234                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 08:33:04,249 - memory_profile6_log - INFO -    235    777.8 MiB      0.0 MiB       if savetrain:

2018-04-30 08:33:04,249 - memory_profile6_log - INFO -    236    784.9 MiB      7.1 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-30 08:33:04,250 - memory_profile6_log - INFO -    237    784.9 MiB      0.0 MiB           del model_transform

2018-04-30 08:33:04,250 - memory_profile6_log - INFO -    238    784.9 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-30 08:33:04,250 - memory_profile6_log - INFO -    239    784.9 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-30 08:33:04,250 - memory_profile6_log - INFO -    240                             

2018-04-30 08:33:04,250 - memory_profile6_log - INFO -    241    784.9 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-30 08:33:04,256 - memory_profile6_log - INFO -    242                                     # ~ Place your code to save the training model here ~

2018-04-30 08:33:04,256 - memory_profile6_log - INFO -    243    784.9 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-30 08:33:04,256 - memory_profile6_log - INFO -    244    784.9 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-30 08:33:04,256 - memory_profile6_log - INFO -    245    784.9 MiB      0.0 MiB               if multproc:

2018-04-30 08:33:04,257 - memory_profile6_log - INFO -    246                                             # ~ save transform models ~

2018-04-30 08:33:04,257 - memory_profile6_log - INFO -    247    750.3 MiB    -34.5 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-30 08:33:04,259 - memory_profile6_log - INFO -    248                             

2018-04-30 08:33:04,259 - memory_profile6_log - INFO -    249                                             """

2018-04-30 08:33:04,259 - memory_profile6_log - INFO -    250                                             # ~ save fitted models ~

2018-04-30 08:33:04,259 - memory_profile6_log - INFO -    251                                             logger.info("Saving fitted_models as history...")

2018-04-30 08:33:04,259 - memory_profile6_log - INFO -    252                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-30 08:33:04,260 - memory_profile6_log - INFO -    253                                             fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-30 08:33:04,260 - memory_profile6_log - INFO -    254                             

2018-04-30 08:33:04,262 - memory_profile6_log - INFO -    255                                             X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-30 08:33:04,262 - memory_profile6_log - INFO -    256                                             logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-30 08:33:04,262 - memory_profile6_log - INFO -    257                                             for ix in range(len(X_split)):

2018-04-30 08:33:04,263 - memory_profile6_log - INFO -    258                                                 logger.info("processing batch-%d", ix)

2018-04-30 08:33:04,263 - memory_profile6_log - INFO -    259                                                 mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-30 08:33:04,263 - memory_profile6_log - INFO -    260                             

2018-04-30 08:33:04,266 - memory_profile6_log - INFO -    261                                             del X_split

2018-04-30 08:33:04,266 - memory_profile6_log - INFO -    262                                             logger.info("deleting X_split...")

2018-04-30 08:33:04,267 - memory_profile6_log - INFO -    263                                             del save_sigma_nt

2018-04-30 08:33:04,267 - memory_profile6_log - INFO -    264                                             logger.info("deleting save_sigma_nt...")

2018-04-30 08:33:04,269 - memory_profile6_log - INFO -    265                                             """

2018-04-30 08:33:04,269 - memory_profile6_log - INFO -    266                             

2018-04-30 08:33:04,269 - memory_profile6_log - INFO -    267    750.3 MiB      0.0 MiB                   del BR

2018-04-30 08:33:04,269 - memory_profile6_log - INFO -    268    750.3 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-30 08:33:04,270 - memory_profile6_log - INFO -    269                             

2018-04-30 08:33:04,270 - memory_profile6_log - INFO -    270                                     elif str(saveto).lower() == "elastic":

2018-04-30 08:33:04,272 - memory_profile6_log - INFO -    271                                         logger.info("Using ElasticSearch as storage...")

2018-04-30 08:33:04,272 - memory_profile6_log - INFO -    272                                         mh.saveElasticS(model_transformsv)

2018-04-30 08:33:04,272 - memory_profile6_log - INFO -    273                             

2018-04-30 08:33:04,273 - memory_profile6_log - INFO -    274                                     # need save sigma_nt for daily train

2018-04-30 08:33:04,273 - memory_profile6_log - INFO -    275                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-30 08:33:04,273 - memory_profile6_log - INFO -    276                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-30 08:33:04,273 - memory_profile6_log - INFO -    277    750.3 MiB      0.0 MiB           if start_date and end_date:

2018-04-30 08:33:04,273 - memory_profile6_log - INFO -    278                                         if not fitby_sigmant:

2018-04-30 08:33:04,275 - memory_profile6_log - INFO -    279                                             logging.info("Saving sigma Nt...")

2018-04-30 08:33:04,275 - memory_profile6_log - INFO -    280                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-30 08:33:04,279 - memory_profile6_log - INFO -    281                                             save_sigma_nt['start_date'] = start_date

2018-04-30 08:33:04,280 - memory_profile6_log - INFO -    282                                             save_sigma_nt['end_date'] = end_date

2018-04-30 08:33:04,280 - memory_profile6_log - INFO -    283                                             print save_sigma_nt.head(5)

2018-04-30 08:33:04,280 - memory_profile6_log - INFO -    284                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-30 08:33:04,282 - memory_profile6_log - INFO -    285                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-30 08:33:04,282 - memory_profile6_log - INFO -    286    750.3 MiB      0.0 MiB       return

2018-04-30 08:33:04,282 - memory_profile6_log - INFO - 


2018-04-30 08:33:04,282 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-30 08:35:25,161 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-30 08:35:25,164 - memory_profile6_log - INFO - date_generated: 
2018-04-30 08:35:25,165 - memory_profile6_log - INFO -  
2018-04-30 08:35:25,165 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 10, 0, 0)]
2018-04-30 08:35:25,167 - memory_profile6_log - INFO - 

2018-04-30 08:35:25,167 - memory_profile6_log - INFO - using current date: 2018-04-11
2018-04-30 08:35:25,167 - memory_profile6_log - INFO - using start date: 2018-04-10 00:00:00
2018-04-30 08:35:25,167 - memory_profile6_log - INFO - using end date: 2018-04-10
2018-04-30 08:35:25,326 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-30 08:35:25,331 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-10
2018-04-30 08:37:26,792 - memory_profile6_log - INFO - size of df: 108.51 MB
2018-04-30 08:37:26,795 - memory_profile6_log - INFO - getting total: 427635 training data(genuine interest) for date: 2018-04-10
2018-04-30 08:37:26,858 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-30 08:37:26,859 - memory_profile6_log - INFO - Appending history data...
2018-04-30 08:37:26,861 - memory_profile6_log - INFO - processing batch-0
2018-04-30 08:37:26,861 - memory_profile6_log - INFO - creating list history data...
2018-04-30 08:37:26,967 - memory_profile6_log - INFO - call history data...
2018-04-30 08:38:15,101 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 08:38:16,849 - memory_profile6_log - INFO - processing batch-1
2018-04-30 08:38:16,851 - memory_profile6_log - INFO - creating list history data...
2018-04-30 08:38:16,938 - memory_profile6_log - INFO - call history data...
2018-04-30 08:39:05,635 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 08:39:07,341 - memory_profile6_log - INFO - processing batch-2
2018-04-30 08:39:07,342 - memory_profile6_log - INFO - creating list history data...
2018-04-30 08:39:07,418 - memory_profile6_log - INFO - call history data...
2018-04-30 08:39:56,260 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 08:39:57,921 - memory_profile6_log - INFO - processing batch-3
2018-04-30 08:39:57,923 - memory_profile6_log - INFO - creating list history data...
2018-04-30 08:39:57,999 - memory_profile6_log - INFO - call history data...
2018-04-30 08:40:44,917 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 08:40:46,599 - memory_profile6_log - INFO - processing batch-4
2018-04-30 08:40:46,601 - memory_profile6_log - INFO - creating list history data...
2018-04-30 08:40:46,674 - memory_profile6_log - INFO - call history data...
2018-04-30 08:41:34,743 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 08:41:36,424 - memory_profile6_log - INFO - Appending training data...
2018-04-30 08:41:36,428 - memory_profile6_log - INFO - len datalist: 1
2018-04-30 08:41:36,434 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-30 08:41:36,447 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 08:41:36,448 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 08:41:36,450 - memory_profile6_log - INFO - ================================================

2018-04-30 08:41:36,450 - memory_profile6_log - INFO -    312     86.9 MiB     86.9 MiB   @profile

2018-04-30 08:41:36,451 - memory_profile6_log - INFO -    313                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-30 08:41:36,453 - memory_profile6_log - INFO -    314     86.9 MiB      0.0 MiB       bq_client = client

2018-04-30 08:41:36,453 - memory_profile6_log - INFO -    315     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 08:41:36,454 - memory_profile6_log - INFO -    316                             

2018-04-30 08:41:36,456 - memory_profile6_log - INFO -    317     86.9 MiB      0.0 MiB       datalist = []

2018-04-30 08:41:36,460 - memory_profile6_log - INFO -    318     86.9 MiB      0.0 MiB       datalist_hist = []

2018-04-30 08:41:36,460 - memory_profile6_log - INFO -    319                             

2018-04-30 08:41:36,461 - memory_profile6_log - INFO -    320     86.9 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-30 08:41:36,463 - memory_profile6_log - INFO -    321    689.4 MiB      0.0 MiB       for ndate in date_generated:

2018-04-30 08:41:36,463 - memory_profile6_log - INFO -    322    394.2 MiB    307.3 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-30 08:41:36,463 - memory_profile6_log - INFO -    323    394.2 MiB      0.0 MiB           if tframe is not None:

2018-04-30 08:41:36,466 - memory_profile6_log - INFO -    324    394.2 MiB      0.0 MiB               if not tframe.empty:

2018-04-30 08:41:36,466 - memory_profile6_log - INFO -    325    408.2 MiB     13.9 MiB                   X_split = np.array_split(tframe, 5)

2018-04-30 08:41:36,467 - memory_profile6_log - INFO -    326    408.2 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-30 08:41:36,470 - memory_profile6_log - INFO -    327    408.2 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-30 08:41:36,470 - memory_profile6_log - INFO -    328    689.4 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-30 08:41:36,471 - memory_profile6_log - INFO -    329                                                 # ~ loading history

2018-04-30 08:41:36,473 - memory_profile6_log - INFO -    330                                                 """

2018-04-30 08:41:36,474 - memory_profile6_log - INFO -    331                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-30 08:41:36,476 - memory_profile6_log - INFO -    332                                                 """

2018-04-30 08:41:36,476 - memory_profile6_log - INFO -    333    662.3 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-30 08:41:36,480 - memory_profile6_log - INFO -    334                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-30 08:41:36,483 - memory_profile6_log - INFO -    335    662.3 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-30 08:41:36,484 - memory_profile6_log - INFO -    336                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-30 08:41:36,486 - memory_profile6_log - INFO -    337    663.6 MiB      8.2 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-30 08:41:36,486 - memory_profile6_log - INFO -    338                             

2018-04-30 08:41:36,489 - memory_profile6_log - INFO -    339    663.6 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-30 08:41:36,490 - memory_profile6_log - INFO -    340    738.1 MiB    569.3 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-30 08:41:36,492 - memory_profile6_log - INFO -    341                             

2018-04-30 08:41:36,493 - memory_profile6_log - INFO -    342                                                 # me = os.getpid()

2018-04-30 08:41:36,493 - memory_profile6_log - INFO -    343                                                 # kill_proc_tree(me)

2018-04-30 08:41:36,494 - memory_profile6_log - INFO -    344                             

2018-04-30 08:41:36,496 - memory_profile6_log - INFO -    345    738.1 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-30 08:41:36,496 - memory_profile6_log - INFO -    346    740.4 MiB     -1.5 MiB                       for m in h_frame:

2018-04-30 08:41:36,497 - memory_profile6_log - INFO -    347    740.3 MiB     -1.5 MiB                           if m is not None:

2018-04-30 08:41:36,500 - memory_profile6_log - INFO -    348    740.3 MiB     -1.5 MiB                               if len(m) > 0:

2018-04-30 08:41:36,503 - memory_profile6_log - INFO -    349    740.4 MiB      9.1 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-30 08:41:36,503 - memory_profile6_log - INFO -    350    689.4 MiB   -305.0 MiB                       del h_frame

2018-04-30 08:41:36,505 - memory_profile6_log - INFO -    351    689.4 MiB     -1.8 MiB                       del lhistory

2018-04-30 08:41:36,506 - memory_profile6_log - INFO -    352                             

2018-04-30 08:41:36,507 - memory_profile6_log - INFO -    353    689.4 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-30 08:41:36,509 - memory_profile6_log - INFO -    354    689.4 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-30 08:41:36,509 - memory_profile6_log - INFO -    355                                     else: 

2018-04-30 08:41:36,512 - memory_profile6_log - INFO -    356                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-30 08:41:36,520 - memory_profile6_log - INFO -    357    689.4 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-30 08:41:36,523 - memory_profile6_log - INFO -    358    689.4 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-30 08:41:36,526 - memory_profile6_log - INFO -    359                             

2018-04-30 08:41:36,529 - memory_profile6_log - INFO -    360    689.4 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-30 08:41:36,529 - memory_profile6_log - INFO - 


2018-04-30 08:41:37,680 - memory_profile6_log - INFO - big_frame_hist:

2018-04-30 08:41:37,757 - memory_profile6_log - INFO -     p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0    0.000088         753.521504       136             763.521504  11911567  16130322f1e4b-09f36b8aa-7637c0a-2c880-16130322...
1    0.001543         294.721949       239             304.721949  10960288  161b0f100321ca-0fddb5a9a35cb2-1696810-38400-16...
2    0.001543          80.461290      1254              90.461290  10960288  1618908e8309f-04ea856d64df54-38636771-38400-16...
3    0.001543         313.801082       896             323.801082  10960288               e5ca5fa0-a122-4ca7-928e-006d6c4b5f81
4    0.001543         598.880775        88             608.880775  10960288  1618cc98bb237-00c1cad8a48273-23476f04-38400-16...
5    0.001543         304.863129       491             314.863129  10960288  1612d39e121d6-01ecaef-5f696329-38400-1612d39e1...
6    0.001543         281.539677       142             291.539677  10960288  16137aebce0109-0db2471d3699e9-83c521d-38400-16...
7    0.001543          99.614537       172             109.614537  10960288  1612f34a4fc2f2-0972560ace4316-1f711f53-38400-1...
8    0.001543         315.639635       558             325.639635  10960288  162636dc4b188-0c81f05c159894-7765020d-29b80-16...
9    0.001543         427.847526       403             437.847526  10960288  1610c95b8c427-04bd536859bb0c8-173a7640-100200-...
10   0.001543         340.942614       156             350.942614  10960288  1616e8bba3b92-081e9fb60240fd-6363259-38400-161...
11   0.001543        3769.488351       134            3779.488351  10960288  1621973201418c-0a1b25a09e2d6-454c062c-100200-1...
12   0.001543         296.389397       466             306.389397  10960288  1613c1d9d8ae9-0024af6a20f0d2-3d080e08-38400-16...
13   0.001543         729.768717       120             739.768717  10960288  16227bb8fa51ac-0892979e6d8e14-282b503d-38400-1...
14   0.001543         150.791642       101             160.791642  10960288  161310d5c9bee-09af35adf2f191-58596970-38400-16...
15   0.001543         356.952090       224             366.952090  10960288  16175b8e997a6-099b5ce5d2b74c-2b2b573d-38400-16...
16   0.001543         287.579000       816             297.579000  10960288  162376d0c5f173-00f8115f3e5d06-4e3b1c76-38400-1...
17   0.001543         283.201658       121             293.201658  10960288  161c8452477bc-0adf679599e183-71237f69-38400-16...
18   0.001543         697.752396       263             707.752396  10960288  1612e1acdf51-0e89f425e-39254002-38400-1612e1ac...
19   0.001543         387.202267        59             397.202267  10960288  1616120954a78-0767f36191742a-3b626275-29b80-16...
2018-04-30 08:41:37,759 - memory_profile6_log - INFO - 

2018-04-30 08:41:37,901 - memory_profile6_log - INFO - size of big_frame_hist: 117.89 MB
2018-04-30 08:41:38,023 - memory_profile6_log - INFO - size of big_frame: 108.51 MB
2018-04-30 08:41:38,042 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-04-30 08:41:49,868 - memory_profile6_log - INFO - size of df: 11.88 MB
2018-04-30 08:41:49,869 - memory_profile6_log - INFO - getting total: 48430 training data(current date interest)
2018-04-30 08:41:49,898 - memory_profile6_log - INFO - size of current_frame: 12.25 MB
2018-04-30 08:41:49,900 - memory_profile6_log - INFO - loading time of: 476065 total genuine-current interest data ~ take 384.599s
2018-04-30 08:41:49,931 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 08:41:49,933 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 08:41:49,934 - memory_profile6_log - INFO - ================================================

2018-04-30 08:41:49,934 - memory_profile6_log - INFO -    362     86.8 MiB     86.8 MiB   @profile

2018-04-30 08:41:49,936 - memory_profile6_log - INFO -    363                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-30 08:41:49,937 - memory_profile6_log - INFO -    364     86.9 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-30 08:41:49,937 - memory_profile6_log - INFO -    365     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 08:41:49,937 - memory_profile6_log - INFO -    366                             

2018-04-30 08:41:49,938 - memory_profile6_log - INFO -    367                                 # ~~~ Begin collecting data ~~~

2018-04-30 08:41:49,940 - memory_profile6_log - INFO -    368     86.9 MiB      0.0 MiB       t0 = time.time()

2018-04-30 08:41:49,941 - memory_profile6_log - INFO -    369    679.3 MiB    592.4 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-30 08:41:49,944 - memory_profile6_log - INFO -    370    679.3 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-30 08:41:49,947 - memory_profile6_log - INFO -    371                                     logger.info("Training cannot be empty..")

2018-04-30 08:41:49,947 - memory_profile6_log - INFO -    372                                     return False

2018-04-30 08:41:49,948 - memory_profile6_log - INFO -    373    705.9 MiB     26.6 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-30 08:41:49,948 - memory_profile6_log - INFO -    374    706.1 MiB      0.2 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-30 08:41:49,948 - memory_profile6_log - INFO -    375    706.1 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-30 08:41:49,950 - memory_profile6_log - INFO -    376                             

2018-04-30 08:41:49,950 - memory_profile6_log - INFO -    377    716.0 MiB      9.9 MiB       big_frame = pd.concat(datalist)

2018-04-30 08:41:49,950 - memory_profile6_log - INFO -    378    716.0 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-30 08:41:49,953 - memory_profile6_log - INFO -    379    706.2 MiB     -9.8 MiB       del datalist

2018-04-30 08:41:49,957 - memory_profile6_log - INFO -    380                             

2018-04-30 08:41:49,957 - memory_profile6_log - INFO -    381    706.2 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 08:41:49,959 - memory_profile6_log - INFO -    382                             

2018-04-30 08:41:49,960 - memory_profile6_log - INFO -    383                                 # ~ get current news interest ~

2018-04-30 08:41:49,961 - memory_profile6_log - INFO -    384    706.2 MiB      0.0 MiB       if not cd:

2018-04-30 08:41:49,961 - memory_profile6_log - INFO -    385    706.2 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-04-30 08:41:49,963 - memory_profile6_log - INFO -    386    717.8 MiB     11.6 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-30 08:41:49,963 - memory_profile6_log - INFO -    387    717.8 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-30 08:41:49,963 - memory_profile6_log - INFO -    388                                 else:

2018-04-30 08:41:49,963 - memory_profile6_log - INFO -    389                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-30 08:41:49,967 - memory_profile6_log - INFO -    390                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-30 08:41:49,969 - memory_profile6_log - INFO -    391                             

2018-04-30 08:41:49,970 - memory_profile6_log - INFO -    392                                     # safe handling of query parameter

2018-04-30 08:41:49,970 - memory_profile6_log - INFO -    393                                     query_params = [

2018-04-30 08:41:49,971 - memory_profile6_log - INFO -    394                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-30 08:41:49,971 - memory_profile6_log - INFO -    395                                     ]

2018-04-30 08:41:49,973 - memory_profile6_log - INFO -    396                             

2018-04-30 08:41:49,973 - memory_profile6_log - INFO -    397                                     job_config.query_parameters = query_params

2018-04-30 08:41:49,973 - memory_profile6_log - INFO -    398                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-30 08:41:49,973 - memory_profile6_log - INFO -    399                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-30 08:41:49,974 - memory_profile6_log - INFO -    400                             

2018-04-30 08:41:49,974 - memory_profile6_log - INFO -    401    717.8 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-30 08:41:49,974 - memory_profile6_log - INFO -    402    717.8 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-30 08:41:49,976 - memory_profile6_log - INFO -    403    718.3 MiB      0.5 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-30 08:41:49,980 - memory_profile6_log - INFO -    404    718.3 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-30 08:41:49,983 - memory_profile6_log - INFO -    405    718.3 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-30 08:41:49,983 - memory_profile6_log - INFO -    406    718.3 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-30 08:41:49,983 - memory_profile6_log - INFO -    407                             

2018-04-30 08:41:49,984 - memory_profile6_log - INFO -    408    718.3 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-30 08:41:49,986 - memory_profile6_log - INFO - 


2018-04-30 08:41:49,990 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-30 08:41:50,023 - memory_profile6_log - INFO - train on: 427635 total genuine interest data(D(u, t))
2018-04-30 08:41:50,023 - memory_profile6_log - INFO - transform on: 48430 total current data(D(t))
2018-04-30 08:41:50,026 - memory_profile6_log - INFO - apply on: 427635 total history...)
2018-04-30 08:41:50,414 - memory_profile6_log - INFO - len of uniques_fit_hist:427635
2018-04-30 08:41:50,670 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:52258
2018-04-30 08:41:52,082 - memory_profile6_log - INFO - Len of model_fit: 427635
2018-04-30 08:41:52,085 - memory_profile6_log - INFO - Len of df_dut: 427635
2018-04-30 08:41:52,128 - memory_profile6_log - INFO -    sigma_Nt  date_all_click
0      1591          603487
1      1591          603487
2      1591          603487
3      1591          603487
4      1591          603487
2018-04-30 08:41:52,128 - memory_profile6_log - INFO - 

2018-04-30 08:41:52,217 - memory_profile6_log - INFO - model_fit dtypes:

2018-04-30 08:41:52,223 - memory_profile6_log - INFO - date              datetime64[ns, UTC]
user_id                        object
topic_id                       object
num_x                           int64
num_y                           int64
is_general                       bool
date_all_click                  int64
Ntotal                          int64
sigma_Nt                        int64
joinprob_ci                   float64
p_cat_ci                      float64
posterior                     float64
dtype: object
2018-04-30 08:41:52,223 - memory_profile6_log - INFO - 

2018-04-30 08:41:52,224 - memory_profile6_log - INFO - fitted_models_hist dtypes:

2018-04-30 08:41:52,230 - memory_profile6_log - INFO - p0_cat_ci                float64
pt_posterior_x_Nt        float64
sigma_Nt                   int64
smoothed_pt_posterior    float64
topic_id                  object
user_id                   object
dtype: object
2018-04-30 08:41:52,233 - memory_profile6_log - INFO - 

2018-04-30 08:41:52,868 - memory_profile6_log - INFO - Len of fitted_models on main class: 427635
2018-04-30 08:41:52,868 - memory_profile6_log - INFO - 

2018-04-30 08:41:52,869 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-30 08:41:52,871 - memory_profile6_log - INFO - 

2018-04-30 08:41:52,874 - memory_profile6_log - INFO - Fitted models before concat:

2018-04-30 08:41:52,895 - memory_profile6_log - INFO -                                              user_id   topic_id  pt_posterior_x_Nt  smoothed_pt_posterior
0               0ba81918-23e5-4d7c-b2ce-f943cc893b46   22601470           0.861691              10.861691
1               0ba81918-23e5-4d7c-b2ce-f943cc893b46   27431099           0.994540              10.994540
2               0ba81918-23e5-4d7c-b2ce-f943cc893b46   39301645           2.958966              12.958966
3               0ba81918-23e5-4d7c-b2ce-f943cc893b46   40347025          22.187022              32.187022
4  161035a29015-0041351a-5d670102-38400-161035a29...  132166743           1.004834              11.004834
2018-04-30 08:41:52,898 - memory_profile6_log - INFO - 

2018-04-30 08:41:52,898 - memory_profile6_log - INFO - len of current fitted models: 427635
2018-04-30 08:41:52,901 - memory_profile6_log - INFO - 

2018-04-30 08:41:52,903 - memory_profile6_log - INFO - len of history fitted models: 427635
2018-04-30 08:41:52,903 - memory_profile6_log - INFO - 

2018-04-30 08:41:52,976 - memory_profile6_log - INFO - len of fitted models after concat: 855270
2018-04-30 08:41:52,977 - memory_profile6_log - INFO - 

2018-04-30 08:41:52,979 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-30 08:41:52,980 - memory_profile6_log - INFO - 

2018-04-30 08:41:53,543 - memory_profile6_log - INFO - Fitted models after concat:

2018-04-30 08:41:53,563 - memory_profile6_log - INFO -                                              user_id   topic_id  pt_posterior_x_Nt
0               0ba81918-23e5-4d7c-b2ce-f943cc893b46   22601470          93.677269
1               0ba81918-23e5-4d7c-b2ce-f943cc893b46   27431099          96.341213
2               0ba81918-23e5-4d7c-b2ce-f943cc893b46   39301645         126.807034
3               0ba81918-23e5-4d7c-b2ce-f943cc893b46   40347025          45.853179
4  161035a29015-0041351a-5d670102-38400-161035a29...  132166743           2.044801
2018-04-30 08:41:53,565 - memory_profile6_log - INFO - 

2018-04-30 08:41:53,586 - memory_profile6_log - INFO - Fitted models after concat-simplify:

2018-04-30 08:41:53,611 - memory_profile6_log - INFO -                                              user_id   topic_id  pt_posterior_x_Nt  smoothed_pt_posterior
0               0ba81918-23e5-4d7c-b2ce-f943cc893b46   22601470          93.677269             103.677269
1               0ba81918-23e5-4d7c-b2ce-f943cc893b46   27431099          96.341213             106.341213
2               0ba81918-23e5-4d7c-b2ce-f943cc893b46   39301645         126.807034             136.807034
3               0ba81918-23e5-4d7c-b2ce-f943cc893b46   40347025          45.853179              55.853179
4  161035a29015-0041351a-5d670102-38400-161035a29...  132166743           2.044801              12.044801
2018-04-30 08:41:53,614 - memory_profile6_log - INFO - 

2018-04-30 08:41:53,615 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 427635
2018-04-30 08:41:53,615 - memory_profile6_log - INFO - 

2018-04-30 08:41:53,717 - memory_profile6_log - INFO -                                              user_id           topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci
0               0ba81918-23e5-4d7c-b2ce-f943cc893b46           22601470          93.677269             103.677269   0.030239
1               0ba81918-23e5-4d7c-b2ce-f943cc893b46           27431099          96.341213             106.341213   0.024702
2               0ba81918-23e5-4d7c-b2ce-f943cc893b46           39301645         126.807034             136.807034   0.012648
3               0ba81918-23e5-4d7c-b2ce-f943cc893b46           40347025          45.853179              55.853179   0.000460
4  161035a29015-0041351a-5d670102-38400-161035a29...          132166743           2.044801              12.044801   0.000223
5  161035a29015-0041351a-5d670102-38400-161035a29...           22601470          97.110842             107.110842   0.030239
6  161035a29015-0041351a-5d670102-38400-161035a29...           27431099          99.890801             109.890801   0.024702
7  161035a29015-0041351a-5d670102-38400-161035a29...  27431110790313993           2.113336              12.113336        NaN
8  161035a29015-0041351a-5d670102-38400-161035a29...           47091877         166.285815             176.285815   0.001158
9  1610c8e6b6466b-05a6ac1ba8c1b8-4323461-100200-1...         1019587357       31335.850426           31345.850426        NaN
2018-04-30 08:41:53,720 - memory_profile6_log - INFO - 

2018-04-30 08:42:55,099 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-30 08:42:55,145 - memory_profile6_log - INFO - Total train time: 65.123s
2018-04-30 08:42:55,148 - memory_profile6_log - INFO - memory left before cleaning: 77.900 percent memory...
2018-04-30 08:42:55,148 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-30 08:42:55,150 - memory_profile6_log - INFO - deleting df_dut...
2018-04-30 08:42:55,151 - memory_profile6_log - INFO - deleting df_dt...
2018-04-30 08:42:55,153 - memory_profile6_log - INFO - deleting df_input...
2018-04-30 08:42:55,154 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-30 08:42:55,157 - memory_profile6_log - INFO - deleting df_current...
2018-04-30 08:42:55,157 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-30 08:42:55,160 - memory_profile6_log - INFO - deleting model_fit...
2018-04-30 08:42:55,161 - memory_profile6_log - INFO - deleting result...
2018-04-30 08:42:55,184 - memory_profile6_log - INFO - deleting model_transform...
2018-04-30 08:42:55,184 - memory_profile6_log - INFO - memory left after cleaning: 77.900 percent memory...
2018-04-30 08:42:55,187 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-30 08:42:55,187 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-30 08:42:55,358 - memory_profile6_log - INFO - deleting BR...
2018-04-30 08:42:55,365 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 08:42:55,365 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 08:42:55,367 - memory_profile6_log - INFO - ================================================

2018-04-30 08:42:55,368 - memory_profile6_log - INFO -    113    662.4 MiB    662.4 MiB   @profile

2018-04-30 08:42:55,368 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-30 08:42:55,368 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-30 08:42:55,368 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-30 08:42:55,368 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-30 08:42:55,369 - memory_profile6_log - INFO -    118                                      saveto="datastore", fitted_models_hist=None):

2018-04-30 08:42:55,369 - memory_profile6_log - INFO -    119                                 """

2018-04-30 08:42:55,369 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-30 08:42:55,371 - memory_profile6_log - INFO -    121                                 """

2018-04-30 08:42:55,371 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-30 08:42:55,371 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-30 08:42:55,371 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-30 08:42:55,371 - memory_profile6_log - INFO -    125    662.4 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-30 08:42:55,374 - memory_profile6_log - INFO -    126    675.8 MiB     13.5 MiB       df_dut = df_input.copy(deep=True)

2018-04-30 08:42:55,374 - memory_profile6_log - INFO -    127    675.8 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 08:42:55,374 - memory_profile6_log - INFO -    128                             

2018-04-30 08:42:55,378 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-30 08:42:55,378 - memory_profile6_log - INFO -    130    676.8 MiB      1.0 MiB       df_dt = df_current.copy(deep=True)

2018-04-30 08:42:55,378 - memory_profile6_log - INFO -    131    677.7 MiB      0.8 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 08:42:55,380 - memory_profile6_log - INFO -    132                             

2018-04-30 08:42:55,380 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-30 08:42:55,381 - memory_profile6_log - INFO -    134    677.7 MiB      0.0 MiB       t0 = time.time()

2018-04-30 08:42:55,381 - memory_profile6_log - INFO -    135    677.7 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-30 08:42:55,381 - memory_profile6_log - INFO -    136    677.7 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-30 08:42:55,382 - memory_profile6_log - INFO -    137    677.7 MiB      0.0 MiB       logger.info("apply on: %d total history...)", len(df_hist))

2018-04-30 08:42:55,382 - memory_profile6_log - INFO -    138                             

2018-04-30 08:42:55,382 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-30 08:42:55,384 - memory_profile6_log - INFO -    140    677.7 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-30 08:42:55,384 - memory_profile6_log - INFO -    141                             

2018-04-30 08:42:55,384 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-30 08:42:55,384 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-30 08:42:55,384 - memory_profile6_log - INFO -    144    678.8 MiB      1.1 MiB       NB = BR.processX(df_dut)

2018-04-30 08:42:55,384 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-30 08:42:55,385 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-30 08:42:55,385 - memory_profile6_log - INFO -    147    695.2 MiB     16.4 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-30 08:42:55,387 - memory_profile6_log - INFO -    148                                 """

2018-04-30 08:42:55,390 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-30 08:42:55,391 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-30 08:42:55,392 - memory_profile6_log - INFO -    151                                 """

2018-04-30 08:42:55,392 - memory_profile6_log - INFO -    152    695.2 MiB      0.0 MiB       fitby_sigmant = True

2018-04-30 08:42:55,394 - memory_profile6_log - INFO -    153    695.2 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-30 08:42:55,394 - memory_profile6_log - INFO -    154    695.2 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-30 08:42:55,394 - memory_profile6_log - INFO -    155    711.6 MiB     16.3 MiB                            'is_general']]

2018-04-30 08:42:55,394 - memory_profile6_log - INFO -    156                                 # get sigma_Nt from fitted_models_hist

2018-04-30 08:42:55,394 - memory_profile6_log - INFO -    157    718.1 MiB      6.5 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-04-30 08:42:55,394 - memory_profile6_log - INFO -    158    718.1 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-04-30 08:42:55,395 - memory_profile6_log - INFO -    159    714.6 MiB     -3.5 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-04-30 08:42:55,395 - memory_profile6_log - INFO -    160    714.6 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-04-30 08:42:55,397 - memory_profile6_log - INFO -    161                             

2018-04-30 08:42:55,397 - memory_profile6_log - INFO -    162                                 # begin fit

2018-04-30 08:42:55,398 - memory_profile6_log - INFO -    163    714.6 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-30 08:42:55,401 - memory_profile6_log - INFO -    164    714.6 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-30 08:42:55,403 - memory_profile6_log - INFO -    165    761.8 MiB     47.3 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-04-30 08:42:55,404 - memory_profile6_log - INFO -    166    761.8 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-30 08:42:55,404 - memory_profile6_log - INFO -    167    761.8 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-30 08:42:55,404 - memory_profile6_log - INFO -    168    768.3 MiB      6.5 MiB       print model_fit[['sigma_Nt','date_all_click']].head(5)

2018-04-30 08:42:55,404 - memory_profile6_log - INFO -    169                             

2018-04-30 08:42:55,404 - memory_profile6_log - INFO -    170                                 # ~~ and Transform ~~

2018-04-30 08:42:55,405 - memory_profile6_log - INFO -    171                                 #   handling current news interest == current date

2018-04-30 08:42:55,407 - memory_profile6_log - INFO -    172    768.3 MiB      0.0 MiB       if df_dt.empty:

2018-04-30 08:42:55,407 - memory_profile6_log - INFO -    173                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-30 08:42:55,407 - memory_profile6_log - INFO -    174                                     return None

2018-04-30 08:42:55,410 - memory_profile6_log - INFO -    175    764.8 MiB     -3.5 MiB       NB = BR.processX(df_dt)

2018-04-30 08:42:55,414 - memory_profile6_log - INFO -    176    764.8 MiB      0.0 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-30 08:42:55,414 - memory_profile6_log - INFO -    177                             

2018-04-30 08:42:55,417 - memory_profile6_log - INFO -    178    764.8 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-30 08:42:55,417 - memory_profile6_log - INFO -    179    749.3 MiB    -15.5 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-30 08:42:55,417 - memory_profile6_log - INFO -    180                             

2018-04-30 08:42:55,418 - memory_profile6_log - INFO -    181    749.3 MiB      0.0 MiB       print "model_fit dtypes:\n", model_fit.dtypes

2018-04-30 08:42:55,418 - memory_profile6_log - INFO -    182    749.3 MiB      0.0 MiB       print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-04-30 08:42:55,418 - memory_profile6_log - INFO -    183    749.3 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-30 08:42:55,418 - memory_profile6_log - INFO -    184    749.3 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-30 08:42:55,420 - memory_profile6_log - INFO -    185    749.3 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-04-30 08:42:55,420 - memory_profile6_log - INFO -    186    759.1 MiB      9.8 MiB                                                                                           "topic_id", "user_id"]],

2018-04-30 08:42:55,421 - memory_profile6_log - INFO -    187    784.1 MiB     25.0 MiB                                                     verbose=False)

2018-04-30 08:42:55,421 - memory_profile6_log - INFO -    188                                 # ~~~ filter is general and specific topic ~~~

2018-04-30 08:42:55,421 - memory_profile6_log - INFO -    189                                 # the idea is just we need to rerank every topic according

2018-04-30 08:42:55,426 - memory_profile6_log - INFO -    190                                 # user_id and and is_general by p0_posterior

2018-04-30 08:42:55,426 - memory_profile6_log - INFO -    191    784.1 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-30 08:42:55,427 - memory_profile6_log - INFO -    192    787.4 MiB      3.3 MiB                                     'is_general']].groupby(['topic_id',

2018-04-30 08:42:55,427 - memory_profile6_log - INFO -    193    784.1 MiB     -3.2 MiB                                                             'is_general']

2018-04-30 08:42:55,427 - memory_profile6_log - INFO -    194                                                                                      ).size().to_frame().reset_index()

2018-04-30 08:42:55,428 - memory_profile6_log - INFO -    195                             

2018-04-30 08:42:55,430 - memory_profile6_log - INFO -    196                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-30 08:42:55,430 - memory_profile6_log - INFO -    197                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-30 08:42:55,430 - memory_profile6_log - INFO -    198    784.1 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-30 08:42:55,431 - memory_profile6_log - INFO -    199                             

2018-04-30 08:42:55,431 - memory_profile6_log - INFO -    200                                 # ~ start by provide rank for each topic type ~

2018-04-30 08:42:55,431 - memory_profile6_log - INFO -    201    810.2 MiB     26.1 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-30 08:42:55,433 - memory_profile6_log - INFO -    202    818.4 MiB      8.2 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-30 08:42:55,433 - memory_profile6_log - INFO -    203                             

2018-04-30 08:42:55,433 - memory_profile6_log - INFO -    204                                 # ~ set threshold to filter output

2018-04-30 08:42:55,437 - memory_profile6_log - INFO -    205    818.4 MiB      0.0 MiB       if threshold > 0:

2018-04-30 08:42:55,437 - memory_profile6_log - INFO -    206    818.4 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-30 08:42:55,438 - memory_profile6_log - INFO -    207    818.4 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-30 08:42:55,440 - memory_profile6_log - INFO -    208    811.2 MiB     -7.2 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-30 08:42:55,440 - memory_profile6_log - INFO -    209                             

2018-04-30 08:42:55,440 - memory_profile6_log - INFO -    210    811.2 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-30 08:42:55,441 - memory_profile6_log - INFO -    211    811.2 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-30 08:42:55,443 - memory_profile6_log - INFO -    212                             

2018-04-30 08:42:55,443 - memory_profile6_log - INFO -    213    811.2 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-30 08:42:55,444 - memory_profile6_log - INFO -    214                             

2018-04-30 08:42:55,444 - memory_profile6_log - INFO -    215    811.2 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-30 08:42:55,447 - memory_profile6_log - INFO -    216    811.2 MiB      0.0 MiB       del df_dut

2018-04-30 08:42:55,447 - memory_profile6_log - INFO -    217    811.2 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-30 08:42:55,448 - memory_profile6_log - INFO -    218    811.2 MiB      0.0 MiB       del df_dt

2018-04-30 08:42:55,450 - memory_profile6_log - INFO -    219    811.2 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-30 08:42:55,450 - memory_profile6_log - INFO -    220    811.2 MiB      0.0 MiB       del df_input

2018-04-30 08:42:55,450 - memory_profile6_log - INFO -    221    811.2 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-30 08:42:55,450 - memory_profile6_log - INFO -    222    811.2 MiB      0.0 MiB       del df_input_X

2018-04-30 08:42:55,450 - memory_profile6_log - INFO -    223    811.2 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-30 08:42:55,451 - memory_profile6_log - INFO -    224    811.2 MiB      0.0 MiB       del df_current

2018-04-30 08:42:55,451 - memory_profile6_log - INFO -    225    811.2 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-30 08:42:55,451 - memory_profile6_log - INFO -    226    811.2 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-30 08:42:55,453 - memory_profile6_log - INFO -    227    811.2 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-30 08:42:55,453 - memory_profile6_log - INFO -    228    811.2 MiB      0.0 MiB       del model_fit

2018-04-30 08:42:55,453 - memory_profile6_log - INFO -    229    811.2 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-30 08:42:55,453 - memory_profile6_log - INFO -    230    811.2 MiB      0.0 MiB       del result

2018-04-30 08:42:55,460 - memory_profile6_log - INFO -    231    811.2 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-30 08:42:55,461 - memory_profile6_log - INFO -    232                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 08:42:55,463 - memory_profile6_log - INFO -    233                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 08:42:55,463 - memory_profile6_log - INFO -    234                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 08:42:55,463 - memory_profile6_log - INFO -    235    811.2 MiB      0.0 MiB       if savetrain:

2018-04-30 08:42:55,464 - memory_profile6_log - INFO -    236    818.3 MiB      7.1 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-30 08:42:55,466 - memory_profile6_log - INFO -    237    818.3 MiB      0.0 MiB           del model_transform

2018-04-30 08:42:55,466 - memory_profile6_log - INFO -    238    818.3 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-30 08:42:55,467 - memory_profile6_log - INFO -    239    818.3 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-30 08:42:55,467 - memory_profile6_log - INFO -    240                             

2018-04-30 08:42:55,467 - memory_profile6_log - INFO -    241    818.3 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-30 08:42:55,467 - memory_profile6_log - INFO -    242                                     # ~ Place your code to save the training model here ~

2018-04-30 08:42:55,467 - memory_profile6_log - INFO -    243    818.3 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-30 08:42:55,469 - memory_profile6_log - INFO -    244    818.3 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-30 08:42:55,469 - memory_profile6_log - INFO -    245    818.3 MiB      0.0 MiB               if multproc:

2018-04-30 08:42:55,470 - memory_profile6_log - INFO -    246                                             # ~ save transform models ~

2018-04-30 08:42:55,473 - memory_profile6_log - INFO -    247    783.4 MiB    -34.9 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-30 08:42:55,473 - memory_profile6_log - INFO -    248                             

2018-04-30 08:42:55,476 - memory_profile6_log - INFO -    249                                             """

2018-04-30 08:42:55,476 - memory_profile6_log - INFO -    250                                             # ~ save fitted models ~

2018-04-30 08:42:55,476 - memory_profile6_log - INFO -    251                                             logger.info("Saving fitted_models as history...")

2018-04-30 08:42:55,476 - memory_profile6_log - INFO -    252                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-30 08:42:55,476 - memory_profile6_log - INFO -    253                                             fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-30 08:42:55,477 - memory_profile6_log - INFO -    254                             

2018-04-30 08:42:55,477 - memory_profile6_log - INFO -    255                                             X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-30 08:42:55,477 - memory_profile6_log - INFO -    256                                             logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-30 08:42:55,479 - memory_profile6_log - INFO -    257                                             for ix in range(len(X_split)):

2018-04-30 08:42:55,479 - memory_profile6_log - INFO -    258                                                 logger.info("processing batch-%d", ix)

2018-04-30 08:42:55,480 - memory_profile6_log - INFO -    259                                                 mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-30 08:42:55,480 - memory_profile6_log - INFO -    260                             

2018-04-30 08:42:55,480 - memory_profile6_log - INFO -    261                                             del X_split

2018-04-30 08:42:55,483 - memory_profile6_log - INFO -    262                                             logger.info("deleting X_split...")

2018-04-30 08:42:55,486 - memory_profile6_log - INFO -    263                                             del save_sigma_nt

2018-04-30 08:42:55,486 - memory_profile6_log - INFO -    264                                             logger.info("deleting save_sigma_nt...")

2018-04-30 08:42:55,486 - memory_profile6_log - INFO -    265                                             """

2018-04-30 08:42:55,487 - memory_profile6_log - INFO -    266                             

2018-04-30 08:42:55,489 - memory_profile6_log - INFO -    267    783.4 MiB      0.0 MiB                   del BR

2018-04-30 08:42:55,489 - memory_profile6_log - INFO -    268    783.4 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-30 08:42:55,490 - memory_profile6_log - INFO -    269                             

2018-04-30 08:42:55,490 - memory_profile6_log - INFO -    270                                     elif str(saveto).lower() == "elastic":

2018-04-30 08:42:55,490 - memory_profile6_log - INFO -    271                                         logger.info("Using ElasticSearch as storage...")

2018-04-30 08:42:55,490 - memory_profile6_log - INFO -    272                                         mh.saveElasticS(model_transformsv)

2018-04-30 08:42:55,492 - memory_profile6_log - INFO -    273                             

2018-04-30 08:42:55,492 - memory_profile6_log - INFO -    274                                     # need save sigma_nt for daily train

2018-04-30 08:42:55,492 - memory_profile6_log - INFO -    275                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-30 08:42:55,494 - memory_profile6_log - INFO -    276                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-30 08:42:55,496 - memory_profile6_log - INFO -    277    783.4 MiB      0.0 MiB           if start_date and end_date:

2018-04-30 08:42:55,496 - memory_profile6_log - INFO -    278                                         if not fitby_sigmant:

2018-04-30 08:42:55,497 - memory_profile6_log - INFO -    279                                             logging.info("Saving sigma Nt...")

2018-04-30 08:42:55,497 - memory_profile6_log - INFO -    280                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-30 08:42:55,499 - memory_profile6_log - INFO -    281                                             save_sigma_nt['start_date'] = start_date

2018-04-30 08:42:55,500 - memory_profile6_log - INFO -    282                                             save_sigma_nt['end_date'] = end_date

2018-04-30 08:42:55,500 - memory_profile6_log - INFO -    283                                             print save_sigma_nt.head(5)

2018-04-30 08:42:55,500 - memory_profile6_log - INFO -    284                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-30 08:42:55,500 - memory_profile6_log - INFO -    285                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-30 08:42:55,500 - memory_profile6_log - INFO -    286    783.4 MiB      0.0 MiB       return

2018-04-30 08:42:55,502 - memory_profile6_log - INFO - 


2018-04-30 08:42:55,502 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-30 08:45:09,517 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-30 08:45:09,519 - memory_profile6_log - INFO - date_generated: 
2018-04-30 08:45:09,520 - memory_profile6_log - INFO -  
2018-04-30 08:45:09,520 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 10, 0, 0)]
2018-04-30 08:45:09,520 - memory_profile6_log - INFO - 

2018-04-30 08:45:09,520 - memory_profile6_log - INFO - using current date: 2018-04-11
2018-04-30 08:45:09,522 - memory_profile6_log - INFO - using start date: 2018-04-10 00:00:00
2018-04-30 08:45:09,522 - memory_profile6_log - INFO - using end date: 2018-04-10
2018-04-30 08:45:09,655 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-30 08:45:09,663 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-10
2018-04-30 08:47:08,490 - memory_profile6_log - INFO - size of df: 108.51 MB
2018-04-30 08:47:08,493 - memory_profile6_log - INFO - getting total: 427635 training data(genuine interest) for date: 2018-04-10
2018-04-30 08:47:08,573 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-30 08:47:08,575 - memory_profile6_log - INFO - Appending history data...
2018-04-30 08:47:08,576 - memory_profile6_log - INFO - processing batch-0
2018-04-30 08:47:08,578 - memory_profile6_log - INFO - creating list history data...
2018-04-30 08:47:08,638 - memory_profile6_log - INFO - call history data...
2018-04-30 08:47:28,299 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 08:47:29,005 - memory_profile6_log - INFO - processing batch-1
2018-04-30 08:47:29,006 - memory_profile6_log - INFO - creating list history data...
2018-04-30 08:47:29,015 - memory_profile6_log - INFO - call history data...
2018-04-30 08:47:48,831 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 08:47:49,539 - memory_profile6_log - INFO - processing batch-2
2018-04-30 08:47:49,542 - memory_profile6_log - INFO - creating list history data...
2018-04-30 08:47:49,549 - memory_profile6_log - INFO - call history data...
2018-04-30 08:48:08,898 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 08:48:09,588 - memory_profile6_log - INFO - processing batch-3
2018-04-30 08:48:09,589 - memory_profile6_log - INFO - creating list history data...
2018-04-30 08:48:09,598 - memory_profile6_log - INFO - call history data...
2018-04-30 08:48:28,594 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 08:48:29,275 - memory_profile6_log - INFO - processing batch-4
2018-04-30 08:48:29,278 - memory_profile6_log - INFO - creating list history data...
2018-04-30 08:48:29,286 - memory_profile6_log - INFO - call history data...
2018-04-30 08:48:47,681 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 08:48:48,411 - memory_profile6_log - INFO - Appending training data...
2018-04-30 08:48:48,413 - memory_profile6_log - INFO - len datalist: 1
2018-04-30 08:48:48,417 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-30 08:48:48,418 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 08:48:48,420 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 08:48:48,421 - memory_profile6_log - INFO - ================================================

2018-04-30 08:48:48,424 - memory_profile6_log - INFO -    312     86.9 MiB     86.9 MiB   @profile

2018-04-30 08:48:48,424 - memory_profile6_log - INFO -    313                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-30 08:48:48,427 - memory_profile6_log - INFO -    314     86.9 MiB      0.0 MiB       bq_client = client

2018-04-30 08:48:48,427 - memory_profile6_log - INFO -    315     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 08:48:48,428 - memory_profile6_log - INFO -    316                             

2018-04-30 08:48:48,430 - memory_profile6_log - INFO -    317     86.9 MiB      0.0 MiB       datalist = []

2018-04-30 08:48:48,430 - memory_profile6_log - INFO -    318     86.9 MiB      0.0 MiB       datalist_hist = []

2018-04-30 08:48:48,430 - memory_profile6_log - INFO -    319                             

2018-04-30 08:48:48,434 - memory_profile6_log - INFO -    320     86.9 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-30 08:48:48,434 - memory_profile6_log - INFO -    321    410.6 MiB      0.0 MiB       for ndate in date_generated:

2018-04-30 08:48:48,437 - memory_profile6_log - INFO -    322    393.7 MiB    306.8 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-30 08:48:48,437 - memory_profile6_log - INFO -    323    393.7 MiB      0.0 MiB           if tframe is not None:

2018-04-30 08:48:48,438 - memory_profile6_log - INFO -    324    393.7 MiB      0.0 MiB               if not tframe.empty:

2018-04-30 08:48:48,438 - memory_profile6_log - INFO -    325    407.6 MiB     13.9 MiB                   X_split = np.array_split(tframe, 5)

2018-04-30 08:48:48,440 - memory_profile6_log - INFO -    326    407.6 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-30 08:48:48,441 - memory_profile6_log - INFO -    327    407.6 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-30 08:48:48,443 - memory_profile6_log - INFO -    328    410.6 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-30 08:48:48,446 - memory_profile6_log - INFO -    329                                                 # ~ loading history

2018-04-30 08:48:48,447 - memory_profile6_log - INFO -    330                                                 """

2018-04-30 08:48:48,448 - memory_profile6_log - INFO -    331                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-30 08:48:48,450 - memory_profile6_log - INFO -    332                                                 """

2018-04-30 08:48:48,450 - memory_profile6_log - INFO -    333    410.5 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-30 08:48:48,451 - memory_profile6_log - INFO -    334                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-30 08:48:48,453 - memory_profile6_log - INFO -    335    410.5 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-30 08:48:48,456 - memory_profile6_log - INFO -    336    410.5 MiB      0.5 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-30 08:48:48,457 - memory_profile6_log - INFO -    337                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-30 08:48:48,459 - memory_profile6_log - INFO -    338                             

2018-04-30 08:48:48,460 - memory_profile6_log - INFO -    339    410.5 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-30 08:48:48,460 - memory_profile6_log - INFO -    340    410.6 MiB      1.7 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-30 08:48:48,461 - memory_profile6_log - INFO -    341                             

2018-04-30 08:48:48,461 - memory_profile6_log - INFO -    342                                                 # me = os.getpid()

2018-04-30 08:48:48,463 - memory_profile6_log - INFO -    343                                                 # kill_proc_tree(me)

2018-04-30 08:48:48,466 - memory_profile6_log - INFO -    344                             

2018-04-30 08:48:48,466 - memory_profile6_log - INFO -    345    410.6 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-30 08:48:48,467 - memory_profile6_log - INFO -    346    410.6 MiB     -0.1 MiB                       for m in h_frame:

2018-04-30 08:48:48,469 - memory_profile6_log - INFO -    347    410.6 MiB     -0.1 MiB                           if m is not None:

2018-04-30 08:48:48,470 - memory_profile6_log - INFO -    348    410.6 MiB     -0.1 MiB                               if len(m) > 0:

2018-04-30 08:48:48,470 - memory_profile6_log - INFO -    349    410.6 MiB      0.8 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-30 08:48:48,470 - memory_profile6_log - INFO -    350    410.6 MiB     -0.0 MiB                       del h_frame

2018-04-30 08:48:48,471 - memory_profile6_log - INFO -    351    410.6 MiB      0.0 MiB                       del lhistory

2018-04-30 08:48:48,473 - memory_profile6_log - INFO -    352                             

2018-04-30 08:48:48,476 - memory_profile6_log - INFO -    353    410.6 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-30 08:48:48,477 - memory_profile6_log - INFO -    354    410.6 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-30 08:48:48,479 - memory_profile6_log - INFO -    355                                     else: 

2018-04-30 08:48:48,480 - memory_profile6_log - INFO -    356                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-30 08:48:48,482 - memory_profile6_log - INFO -    357    410.6 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-30 08:48:48,483 - memory_profile6_log - INFO -    358    410.6 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-30 08:48:48,483 - memory_profile6_log - INFO -    359                             

2018-04-30 08:48:48,483 - memory_profile6_log - INFO -    360    410.6 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-30 08:48:48,486 - memory_profile6_log - INFO - 


2018-04-30 08:48:49,638 - memory_profile6_log - INFO - big_frame_hist:

2018-04-30 08:48:49,710 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0   0.000023       74088.805804       112           74098.805804  10959239  1610d2a81e0215-06c836f37305ee-5768397b-100200-...
1   0.001543         537.007220       164             547.007220  10960288  1610c983f92247-0cb59d76ae1759-4323461-100200-1...
2   0.000023       13649.808773      1459           13659.808773  10959239  1610ca028e379-098a172ba6d096-4323461-100200-16...
3   0.001543         196.515559       310             206.515559  10960288  1613079731c1c-0eac7775a89e19-7765060f-38400-16...
4   0.001543         301.439596       351             311.439596  10960288  16137464995157-06708db64e6ab2-710d033f-49a10-1...
5   0.000023       10388.414989       944           10398.414989  10959239  1610c945d23697-026cb4a68def18-4323461-100200-1...
6   0.000023        9319.839208      1376            9329.839208  10959239  1617d812dcf543-04791235802297-d35346d-100200-1...
0   0.001543         781.023376        39             791.023376  10960288  16137a258dbc1-0fac0c829c1047-50670302-38400-16...
1   0.001543         150.048826       203             160.048826  10960288  161325123001ff-092fa493526f0a-232b513b-38400-1...
2   0.001543         227.312774        67             237.312774  10960288  1625886f62c7d-034bef9733379d-a4f0b16-38400-162...
3   0.001543         242.144412       199             252.144412  10960288  16215b3e11829-0c256381f35151-6c37203c-2c880-16...
4   0.001543         185.731169       164             195.731169  10960288  1612ff604ba15c-0af75aba2777c8-33626173-49a10-1...
5   0.001543         287.579000       816             297.579000  10960288  162376d0c5f173-00f8115f3e5d06-4e3b1c76-38400-1...
0   0.001543          82.771499       184              92.771499  10960288  16130803e4048e-03a3b88e7bcf61-3a626275-43113-1...
1   0.001543          63.991411       238              73.991411  10960288  161935e58109ba-0985dfa4c7c3f6-3e3d5100-100200-...
2   0.001543         150.791642       101             160.791642  10960288  161310d5c9bee-09af35adf2f191-58596970-38400-16...
3   0.001543         395.623238        76             405.623238  10960288  161317c95f721c-04c3aa4e14f42b-685c2a14-2c600-1...
4   0.001543         292.883766        52             302.883766  10960288  161354380993c-04ad78f34c8d01-39626377-38400-16...
5   0.001543         101.327614       174             111.327614  10960288  161364f0c3f28-07895ed553fd55-e0744-38400-16136...
6   0.001543         725.235992        21             735.235992  10960288  1621ce544d12f6-0b2c797f4f2328-50683974-100200-...
2018-04-30 08:48:49,713 - memory_profile6_log - INFO - 

2018-04-30 08:48:49,723 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-30 08:48:49,844 - memory_profile6_log - INFO - size of big_frame: 108.51 MB
2018-04-30 08:48:49,865 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-04-30 08:49:01,533 - memory_profile6_log - INFO - size of df: 11.88 MB
2018-04-30 08:49:01,535 - memory_profile6_log - INFO - getting total: 48430 training data(current date interest)
2018-04-30 08:49:01,559 - memory_profile6_log - INFO - size of current_frame: 12.25 MB
2018-04-30 08:49:01,562 - memory_profile6_log - INFO - loading time of: 476065 total genuine-current interest data ~ take 231.928s
2018-04-30 08:49:01,568 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 08:49:01,569 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 08:49:01,572 - memory_profile6_log - INFO - ================================================

2018-04-30 08:49:01,572 - memory_profile6_log - INFO -    362     86.7 MiB     86.7 MiB   @profile

2018-04-30 08:49:01,575 - memory_profile6_log - INFO -    363                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-30 08:49:01,575 - memory_profile6_log - INFO -    364     86.9 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-30 08:49:01,578 - memory_profile6_log - INFO -    365     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 08:49:01,579 - memory_profile6_log - INFO -    366                             

2018-04-30 08:49:01,581 - memory_profile6_log - INFO -    367                                 # ~~~ Begin collecting data ~~~

2018-04-30 08:49:01,582 - memory_profile6_log - INFO -    368     86.9 MiB      0.0 MiB       t0 = time.time()

2018-04-30 08:49:01,582 - memory_profile6_log - INFO -    369    407.9 MiB    321.0 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-30 08:49:01,584 - memory_profile6_log - INFO -    370    407.9 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-30 08:49:01,585 - memory_profile6_log - INFO -    371                                     logger.info("Training cannot be empty..")

2018-04-30 08:49:01,586 - memory_profile6_log - INFO -    372                                     return False

2018-04-30 08:49:01,586 - memory_profile6_log - INFO -    373    401.9 MiB     -6.0 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-30 08:49:01,588 - memory_profile6_log - INFO -    374    402.0 MiB      0.2 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-30 08:49:01,588 - memory_profile6_log - INFO -    375    402.0 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-30 08:49:01,591 - memory_profile6_log - INFO -    376                             

2018-04-30 08:49:01,592 - memory_profile6_log - INFO -    377    411.8 MiB      9.8 MiB       big_frame = pd.concat(datalist)

2018-04-30 08:49:01,592 - memory_profile6_log - INFO -    378    411.8 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-30 08:49:01,595 - memory_profile6_log - INFO -    379    402.0 MiB     -9.8 MiB       del datalist

2018-04-30 08:49:01,595 - memory_profile6_log - INFO -    380                             

2018-04-30 08:49:01,595 - memory_profile6_log - INFO -    381    402.0 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 08:49:01,596 - memory_profile6_log - INFO -    382                             

2018-04-30 08:49:01,596 - memory_profile6_log - INFO -    383                                 # ~ get current news interest ~

2018-04-30 08:49:01,598 - memory_profile6_log - INFO -    384    402.0 MiB      0.0 MiB       if not cd:

2018-04-30 08:49:01,598 - memory_profile6_log - INFO -    385    402.0 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-04-30 08:49:01,598 - memory_profile6_log - INFO -    386    416.8 MiB     14.7 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-30 08:49:01,598 - memory_profile6_log - INFO -    387    416.8 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-30 08:49:01,598 - memory_profile6_log - INFO -    388                                 else:

2018-04-30 08:49:01,602 - memory_profile6_log - INFO -    389                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-30 08:49:01,605 - memory_profile6_log - INFO -    390                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-30 08:49:01,605 - memory_profile6_log - INFO -    391                             

2018-04-30 08:49:01,605 - memory_profile6_log - INFO -    392                                     # safe handling of query parameter

2018-04-30 08:49:01,607 - memory_profile6_log - INFO -    393                                     query_params = [

2018-04-30 08:49:01,607 - memory_profile6_log - INFO -    394                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-30 08:49:01,608 - memory_profile6_log - INFO -    395                                     ]

2018-04-30 08:49:01,608 - memory_profile6_log - INFO -    396                             

2018-04-30 08:49:01,608 - memory_profile6_log - INFO -    397                                     job_config.query_parameters = query_params

2018-04-30 08:49:01,609 - memory_profile6_log - INFO -    398                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-30 08:49:01,615 - memory_profile6_log - INFO -    399                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-30 08:49:01,621 - memory_profile6_log - INFO -    400                             

2018-04-30 08:49:01,621 - memory_profile6_log - INFO -    401    416.8 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-30 08:49:01,625 - memory_profile6_log - INFO -    402    416.8 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-30 08:49:01,630 - memory_profile6_log - INFO -    403    416.9 MiB      0.2 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-30 08:49:01,631 - memory_profile6_log - INFO -    404    416.9 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-30 08:49:01,631 - memory_profile6_log - INFO -    405    416.9 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-30 08:49:01,631 - memory_profile6_log - INFO -    406    416.9 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-30 08:49:01,631 - memory_profile6_log - INFO -    407                             

2018-04-30 08:49:01,632 - memory_profile6_log - INFO -    408    416.9 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-30 08:49:01,632 - memory_profile6_log - INFO - 


2018-04-30 08:49:01,638 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-30 08:49:01,667 - memory_profile6_log - INFO - train on: 427635 total genuine interest data(D(u, t))
2018-04-30 08:49:01,668 - memory_profile6_log - INFO - transform on: 48430 total current data(D(t))
2018-04-30 08:49:01,671 - memory_profile6_log - INFO - apply on: 5000 total history...)
2018-04-30 08:49:02,039 - memory_profile6_log - INFO - len of uniques_fit_hist:5000
2018-04-30 08:49:02,048 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:4636
2018-04-30 08:49:03,483 - memory_profile6_log - INFO - Len of model_fit: 427635
2018-04-30 08:49:03,484 - memory_profile6_log - INFO - Len of df_dut: 427635
2018-04-30 08:49:03,727 - memory_profile6_log - INFO -        sigma_Nt  date_all_click
24396      1389          603487
24397      1389          603487
24398      1389          603487
24399      1389          603487
24400      1389          603487
2018-04-30 08:49:03,730 - memory_profile6_log - INFO - 

2018-04-30 08:49:03,846 - memory_profile6_log - INFO - model_fit dtypes:

2018-04-30 08:49:03,858 - memory_profile6_log - INFO - date              datetime64[ns, UTC]
user_id                        object
topic_id                       object
num_x                           int64
num_y                           int64
is_general                       bool
date_all_click                  int64
Ntotal                          int64
sigma_Nt                        int64
joinprob_ci                   float64
p_cat_ci                      float64
posterior                     float64
dtype: object
2018-04-30 08:49:03,861 - memory_profile6_log - INFO - 

2018-04-30 08:49:03,861 - memory_profile6_log - INFO - fitted_models_hist dtypes:

2018-04-30 08:49:03,868 - memory_profile6_log - INFO - p0_cat_ci                float64
pt_posterior_x_Nt        float64
sigma_Nt                   int64
smoothed_pt_posterior    float64
topic_id                  object
user_id                   object
dtype: object
2018-04-30 08:49:03,869 - memory_profile6_log - INFO - 

2018-04-30 08:49:04,494 - memory_profile6_log - INFO - Len of fitted_models on main class: 427635
2018-04-30 08:49:04,496 - memory_profile6_log - INFO - 

2018-04-30 08:49:04,496 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-30 08:49:04,499 - memory_profile6_log - INFO - 

2018-04-30 08:49:04,500 - memory_profile6_log - INFO - Fitted models before concat:

2018-04-30 08:49:04,526 - memory_profile6_log - INFO -                                              user_id   topic_id  pt_posterior_x_Nt  smoothed_pt_posterior
0               0ba81918-23e5-4d7c-b2ce-f943cc893b46   22601470          13.787056              23.787056
1               0ba81918-23e5-4d7c-b2ce-f943cc893b46   27431099          15.912643              25.912643
2               0ba81918-23e5-4d7c-b2ce-f943cc893b46   39301645          47.343453              57.343453
3               0ba81918-23e5-4d7c-b2ce-f943cc893b46   40347025         354.992353             364.992353
4  161035a29015-0041351a-5d670102-38400-161035a29...  132166743          29.743075              39.743075
2018-04-30 08:49:04,526 - memory_profile6_log - INFO - 

2018-04-30 08:49:04,529 - memory_profile6_log - INFO - len of current fitted models: 427635
2018-04-30 08:49:04,529 - memory_profile6_log - INFO - 

2018-04-30 08:49:04,530 - memory_profile6_log - INFO - len of history fitted models: 5000
2018-04-30 08:49:04,532 - memory_profile6_log - INFO - 

2018-04-30 08:49:04,588 - memory_profile6_log - INFO - len of fitted models after concat: 432635
2018-04-30 08:49:04,589 - memory_profile6_log - INFO - 

2018-04-30 08:49:04,591 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-30 08:49:04,592 - memory_profile6_log - INFO - 

2018-04-30 08:49:04,871 - memory_profile6_log - INFO - Fitted models after concat:

2018-04-30 08:49:04,891 - memory_profile6_log - INFO -                                              user_id   topic_id  pt_posterior_x_Nt
0               0ba81918-23e5-4d7c-b2ce-f943cc893b46   22601470          13.787056
1               0ba81918-23e5-4d7c-b2ce-f943cc893b46   27431099          15.912643
2               0ba81918-23e5-4d7c-b2ce-f943cc893b46   39301645          47.343453
3               0ba81918-23e5-4d7c-b2ce-f943cc893b46   40347025         354.992353
4  161035a29015-0041351a-5d670102-38400-161035a29...  132166743          29.743075
2018-04-30 08:49:04,892 - memory_profile6_log - INFO - 

2018-04-30 08:49:04,917 - memory_profile6_log - INFO - Fitted models after concat-simplify:

2018-04-30 08:49:04,940 - memory_profile6_log - INFO -                                              user_id   topic_id  pt_posterior_x_Nt  smoothed_pt_posterior
0               0ba81918-23e5-4d7c-b2ce-f943cc893b46   22601470          13.787056              23.787056
1               0ba81918-23e5-4d7c-b2ce-f943cc893b46   27431099          15.912643              25.912643
2               0ba81918-23e5-4d7c-b2ce-f943cc893b46   39301645          47.343453              57.343453
3               0ba81918-23e5-4d7c-b2ce-f943cc893b46   40347025         354.992353             364.992353
4  161035a29015-0041351a-5d670102-38400-161035a29...  132166743          29.743075              39.743075
2018-04-30 08:49:04,943 - memory_profile6_log - INFO - 

2018-04-30 08:49:04,944 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 427635
2018-04-30 08:49:04,944 - memory_profile6_log - INFO - 

2018-04-30 08:49:05,043 - memory_profile6_log - INFO -                                              user_id           topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci
0               0ba81918-23e5-4d7c-b2ce-f943cc893b46           22601470          13.787056              23.787056   0.030239
1               0ba81918-23e5-4d7c-b2ce-f943cc893b46           27431099          15.912643              25.912643   0.024702
2               0ba81918-23e5-4d7c-b2ce-f943cc893b46           39301645          47.343453              57.343453   0.012648
3               0ba81918-23e5-4d7c-b2ce-f943cc893b46           40347025         354.992353             364.992353   0.000460
4  161035a29015-0041351a-5d670102-38400-161035a29...          132166743          29.743075              39.743075   0.000223
5  161035a29015-0041351a-5d670102-38400-161035a29...           22601470          13.787056              23.787056   0.030239
6  161035a29015-0041351a-5d670102-38400-161035a29...           27431099          15.912643              25.912643   0.024702
7  161035a29015-0041351a-5d670102-38400-161035a29...  27431110790313993          30.739965              40.739965        NaN
8  161035a29015-0041351a-5d670102-38400-161035a29...           47091877         339.610017             349.610017   0.001158
9  1610c8e6b6466b-05a6ac1ba8c1b8-4323461-100200-1...         1019587357        2295.209178            2305.209178        NaN
2018-04-30 08:49:05,046 - memory_profile6_log - INFO - 

2018-04-30 08:50:08,332 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-30 08:50:08,382 - memory_profile6_log - INFO - Total train time: 66.715s
2018-04-30 08:50:08,384 - memory_profile6_log - INFO - memory left before cleaning: 79.500 percent memory...
2018-04-30 08:50:08,385 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-30 08:50:08,387 - memory_profile6_log - INFO - deleting df_dut...
2018-04-30 08:50:08,388 - memory_profile6_log - INFO - deleting df_dt...
2018-04-30 08:50:08,388 - memory_profile6_log - INFO - deleting df_input...
2018-04-30 08:50:08,392 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-30 08:50:08,394 - memory_profile6_log - INFO - deleting df_current...
2018-04-30 08:50:08,395 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-30 08:50:08,398 - memory_profile6_log - INFO - deleting model_fit...
2018-04-30 08:50:08,400 - memory_profile6_log - INFO - deleting result...
2018-04-30 08:50:08,430 - memory_profile6_log - INFO - deleting model_transform...
2018-04-30 08:50:08,433 - memory_profile6_log - INFO - memory left after cleaning: 79.500 percent memory...
2018-04-30 08:50:08,434 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-30 08:50:08,436 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-30 08:50:08,617 - memory_profile6_log - INFO - Saving fitted_models as history...
2018-04-30 08:50:08,750 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 10
2018-04-30 08:50:08,750 - memory_profile6_log - INFO - processing batch-0
2018-04-30 08:50:08,944 - memory_profile6_log - INFO - processing batch-1
2018-04-30 08:50:09,144 - memory_profile6_log - INFO - processing batch-2
2018-04-30 08:50:09,335 - memory_profile6_log - INFO - processing batch-3
2018-04-30 08:50:09,513 - memory_profile6_log - INFO - processing batch-4
2018-04-30 08:50:09,697 - memory_profile6_log - INFO - processing batch-5
2018-04-30 08:50:09,885 - memory_profile6_log - INFO - processing batch-6
2018-04-30 08:50:10,085 - memory_profile6_log - INFO - processing batch-7
2018-04-30 08:50:10,269 - memory_profile6_log - INFO - processing batch-8
2018-04-30 08:50:10,460 - memory_profile6_log - INFO - processing batch-9
2018-04-30 08:50:10,651 - memory_profile6_log - INFO - deleting X_split...
2018-04-30 08:50:10,653 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-04-30 08:50:10,654 - memory_profile6_log - INFO - deleting BR...
2018-04-30 08:50:10,664 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 08:50:10,665 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 08:50:10,665 - memory_profile6_log - INFO - ================================================

2018-04-30 08:50:10,667 - memory_profile6_log - INFO -    113    416.9 MiB    416.9 MiB   @profile

2018-04-30 08:50:10,667 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-30 08:50:10,670 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-30 08:50:10,670 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-30 08:50:10,671 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-30 08:50:10,671 - memory_profile6_log - INFO -    118                                      saveto="datastore", fitted_models_hist=None):

2018-04-30 08:50:10,671 - memory_profile6_log - INFO -    119                                 """

2018-04-30 08:50:10,671 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-30 08:50:10,671 - memory_profile6_log - INFO -    121                                 """

2018-04-30 08:50:10,671 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-30 08:50:10,673 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-30 08:50:10,673 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-30 08:50:10,673 - memory_profile6_log - INFO -    125    416.9 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-30 08:50:10,677 - memory_profile6_log - INFO -    126    430.0 MiB     13.1 MiB       df_dut = df_input.copy(deep=True)

2018-04-30 08:50:10,677 - memory_profile6_log - INFO -    127    430.0 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 08:50:10,678 - memory_profile6_log - INFO -    128                             

2018-04-30 08:50:10,680 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-30 08:50:10,680 - memory_profile6_log - INFO -    130    430.1 MiB      0.1 MiB       df_dt = df_current.copy(deep=True)

2018-04-30 08:50:10,680 - memory_profile6_log - INFO -    131    430.5 MiB      0.4 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 08:50:10,680 - memory_profile6_log - INFO -    132                             

2018-04-30 08:50:10,680 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-30 08:50:10,681 - memory_profile6_log - INFO -    134    430.5 MiB      0.0 MiB       t0 = time.time()

2018-04-30 08:50:10,681 - memory_profile6_log - INFO -    135    430.5 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-30 08:50:10,683 - memory_profile6_log - INFO -    136    430.5 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-30 08:50:10,683 - memory_profile6_log - INFO -    137    430.5 MiB      0.0 MiB       logger.info("apply on: %d total history...)", len(df_hist))

2018-04-30 08:50:10,683 - memory_profile6_log - INFO -    138                             

2018-04-30 08:50:10,684 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-30 08:50:10,684 - memory_profile6_log - INFO -    140    430.5 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-30 08:50:10,684 - memory_profile6_log - INFO -    141                             

2018-04-30 08:50:10,684 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-30 08:50:10,684 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-30 08:50:10,690 - memory_profile6_log - INFO -    144    431.3 MiB      0.8 MiB       NB = BR.processX(df_dut)

2018-04-30 08:50:10,690 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-30 08:50:10,691 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-30 08:50:10,693 - memory_profile6_log - INFO -    147    447.8 MiB     16.5 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-30 08:50:10,694 - memory_profile6_log - INFO -    148                                 """

2018-04-30 08:50:10,694 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-30 08:50:10,694 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-30 08:50:10,694 - memory_profile6_log - INFO -    151                                 """

2018-04-30 08:50:10,696 - memory_profile6_log - INFO -    152    447.8 MiB      0.0 MiB       fitby_sigmant = True

2018-04-30 08:50:10,697 - memory_profile6_log - INFO -    153    447.8 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-30 08:50:10,697 - memory_profile6_log - INFO -    154    447.8 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-30 08:50:10,701 - memory_profile6_log - INFO -    155    464.1 MiB     16.3 MiB                            'is_general']]

2018-04-30 08:50:10,703 - memory_profile6_log - INFO -    156                                 # get sigma_Nt from fitted_models_hist

2018-04-30 08:50:10,703 - memory_profile6_log - INFO -    157    464.2 MiB      0.1 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-04-30 08:50:10,703 - memory_profile6_log - INFO -    158    464.2 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-04-30 08:50:10,704 - memory_profile6_log - INFO -    159    464.3 MiB      0.1 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-04-30 08:50:10,706 - memory_profile6_log - INFO -    160    464.3 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-04-30 08:50:10,707 - memory_profile6_log - INFO -    161                             

2018-04-30 08:50:10,707 - memory_profile6_log - INFO -    162                                 # begin fit

2018-04-30 08:50:10,707 - memory_profile6_log - INFO -    163    464.3 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-30 08:50:10,707 - memory_profile6_log - INFO -    164    464.3 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-30 08:50:10,709 - memory_profile6_log - INFO -    165    505.7 MiB     41.3 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-04-30 08:50:10,710 - memory_profile6_log - INFO -    166    505.7 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-30 08:50:10,713 - memory_profile6_log - INFO -    167    505.7 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-30 08:50:10,717 - memory_profile6_log - INFO -    168    512.2 MiB      6.5 MiB       print model_fit[['sigma_Nt','date_all_click']].loc[model_fit['user_id']=="1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157"].head(5)

2018-04-30 08:50:10,717 - memory_profile6_log - INFO -    169                             

2018-04-30 08:50:10,717 - memory_profile6_log - INFO -    170                                 # ~~ and Transform ~~

2018-04-30 08:50:10,719 - memory_profile6_log - INFO -    171                                 #   handling current news interest == current date

2018-04-30 08:50:10,719 - memory_profile6_log - INFO -    172    512.2 MiB      0.0 MiB       if df_dt.empty:

2018-04-30 08:50:10,720 - memory_profile6_log - INFO -    173                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-30 08:50:10,720 - memory_profile6_log - INFO -    174                                     return None

2018-04-30 08:50:10,721 - memory_profile6_log - INFO -    175    506.0 MiB     -6.2 MiB       NB = BR.processX(df_dt)

2018-04-30 08:50:10,723 - memory_profile6_log - INFO -    176    506.0 MiB      0.0 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-30 08:50:10,724 - memory_profile6_log - INFO -    177                             

2018-04-30 08:50:10,726 - memory_profile6_log - INFO -    178    506.0 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-30 08:50:10,726 - memory_profile6_log - INFO -    179    489.7 MiB    -16.3 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-30 08:50:10,727 - memory_profile6_log - INFO -    180                             

2018-04-30 08:50:10,729 - memory_profile6_log - INFO -    181    489.7 MiB      0.0 MiB       print "model_fit dtypes:\n", model_fit.dtypes

2018-04-30 08:50:10,729 - memory_profile6_log - INFO -    182    489.7 MiB      0.0 MiB       print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-04-30 08:50:10,729 - memory_profile6_log - INFO -    183    489.7 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-30 08:50:10,730 - memory_profile6_log - INFO -    184    489.7 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-30 08:50:10,730 - memory_profile6_log - INFO -    185    489.7 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-04-30 08:50:10,730 - memory_profile6_log - INFO -    186    489.7 MiB      0.0 MiB                                                                                           "topic_id", "user_id"]],

2018-04-30 08:50:10,730 - memory_profile6_log - INFO -    187    524.3 MiB     34.6 MiB                                                     verbose=False)

2018-04-30 08:50:10,730 - memory_profile6_log - INFO -    188                                 # ~~~ filter is general and specific topic ~~~

2018-04-30 08:50:10,732 - memory_profile6_log - INFO -    189                                 # the idea is just we need to rerank every topic according

2018-04-30 08:50:10,732 - memory_profile6_log - INFO -    190                                 # user_id and and is_general by p0_posterior

2018-04-30 08:50:10,732 - memory_profile6_log - INFO -    191    524.3 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-30 08:50:10,733 - memory_profile6_log - INFO -    192    527.6 MiB      3.3 MiB                                     'is_general']].groupby(['topic_id',

2018-04-30 08:50:10,733 - memory_profile6_log - INFO -    193    524.3 MiB     -3.2 MiB                                                             'is_general']

2018-04-30 08:50:10,736 - memory_profile6_log - INFO -    194                                                                                      ).size().to_frame().reset_index()

2018-04-30 08:50:10,737 - memory_profile6_log - INFO -    195                             

2018-04-30 08:50:10,740 - memory_profile6_log - INFO -    196                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-30 08:50:10,740 - memory_profile6_log - INFO -    197                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-30 08:50:10,740 - memory_profile6_log - INFO -    198    524.3 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-30 08:50:10,740 - memory_profile6_log - INFO -    199                             

2018-04-30 08:50:10,742 - memory_profile6_log - INFO -    200                                 # ~ start by provide rank for each topic type ~

2018-04-30 08:50:10,742 - memory_profile6_log - INFO -    201    556.9 MiB     32.6 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-30 08:50:10,743 - memory_profile6_log - INFO -    202    561.8 MiB      4.9 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-30 08:50:10,743 - memory_profile6_log - INFO -    203                             

2018-04-30 08:50:10,743 - memory_profile6_log - INFO -    204                                 # ~ set threshold to filter output

2018-04-30 08:50:10,744 - memory_profile6_log - INFO -    205    561.8 MiB      0.0 MiB       if threshold > 0:

2018-04-30 08:50:10,744 - memory_profile6_log - INFO -    206    561.8 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-30 08:50:10,744 - memory_profile6_log - INFO -    207    561.8 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-30 08:50:10,747 - memory_profile6_log - INFO -    208    554.7 MiB     -7.1 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-30 08:50:10,750 - memory_profile6_log - INFO -    209                             

2018-04-30 08:50:10,750 - memory_profile6_log - INFO -    210    554.7 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-30 08:50:10,750 - memory_profile6_log - INFO -    211    554.7 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-30 08:50:10,752 - memory_profile6_log - INFO -    212                             

2018-04-30 08:50:10,752 - memory_profile6_log - INFO -    213    554.7 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-30 08:50:10,753 - memory_profile6_log - INFO -    214                             

2018-04-30 08:50:10,753 - memory_profile6_log - INFO -    215    554.7 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-30 08:50:10,753 - memory_profile6_log - INFO -    216    554.7 MiB      0.0 MiB       del df_dut

2018-04-30 08:50:10,753 - memory_profile6_log - INFO -    217    554.7 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-30 08:50:10,755 - memory_profile6_log - INFO -    218    554.7 MiB      0.0 MiB       del df_dt

2018-04-30 08:50:10,755 - memory_profile6_log - INFO -    219    554.7 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-30 08:50:10,755 - memory_profile6_log - INFO -    220    554.7 MiB      0.0 MiB       del df_input

2018-04-30 08:50:10,756 - memory_profile6_log - INFO -    221    554.7 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-30 08:50:10,756 - memory_profile6_log - INFO -    222    554.7 MiB      0.0 MiB       del df_input_X

2018-04-30 08:50:10,756 - memory_profile6_log - INFO -    223    554.7 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-30 08:50:10,760 - memory_profile6_log - INFO -    224    554.7 MiB      0.0 MiB       del df_current

2018-04-30 08:50:10,760 - memory_profile6_log - INFO -    225    554.7 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-30 08:50:10,763 - memory_profile6_log - INFO -    226    554.2 MiB     -0.4 MiB       del map_topic_isgeneral

2018-04-30 08:50:10,763 - memory_profile6_log - INFO -    227    554.2 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-30 08:50:10,763 - memory_profile6_log - INFO -    228    554.2 MiB      0.0 MiB       del model_fit

2018-04-30 08:50:10,765 - memory_profile6_log - INFO -    229    554.2 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-30 08:50:10,765 - memory_profile6_log - INFO -    230    554.2 MiB      0.0 MiB       del result

2018-04-30 08:50:10,766 - memory_profile6_log - INFO -    231    554.2 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-30 08:50:10,766 - memory_profile6_log - INFO -    232                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 08:50:10,766 - memory_profile6_log - INFO -    233                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 08:50:10,766 - memory_profile6_log - INFO -    234                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 08:50:10,766 - memory_profile6_log - INFO -    235    554.2 MiB      0.0 MiB       if savetrain:

2018-04-30 08:50:10,767 - memory_profile6_log - INFO -    236    561.3 MiB      7.1 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-30 08:50:10,767 - memory_profile6_log - INFO -    237    561.3 MiB      0.0 MiB           del model_transform

2018-04-30 08:50:10,767 - memory_profile6_log - INFO -    238    561.3 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-30 08:50:10,772 - memory_profile6_log - INFO -    239    561.3 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-30 08:50:10,773 - memory_profile6_log - INFO -    240                             

2018-04-30 08:50:10,773 - memory_profile6_log - INFO -    241    561.3 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-30 08:50:10,775 - memory_profile6_log - INFO -    242                                     # ~ Place your code to save the training model here ~

2018-04-30 08:50:10,775 - memory_profile6_log - INFO -    243    561.3 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-30 08:50:10,776 - memory_profile6_log - INFO -    244    561.3 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-30 08:50:10,776 - memory_profile6_log - INFO -    245    561.3 MiB      0.0 MiB               if multproc:

2018-04-30 08:50:10,776 - memory_profile6_log - INFO -    246                                             # ~ save transform models ~

2018-04-30 08:50:10,776 - memory_profile6_log - INFO -    247    526.4 MiB    -34.9 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-30 08:50:10,778 - memory_profile6_log - INFO -    248                             

2018-04-30 08:50:10,778 - memory_profile6_log - INFO -    249                                             

2018-04-30 08:50:10,779 - memory_profile6_log - INFO -    250                                             # ~ save fitted models ~

2018-04-30 08:50:10,779 - memory_profile6_log - INFO -    251    526.4 MiB      0.0 MiB                   logger.info("Saving fitted_models as history...")

2018-04-30 08:50:10,779 - memory_profile6_log - INFO -    252    526.4 MiB      0.0 MiB                   save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-30 08:50:10,779 - memory_profile6_log - INFO -    253    550.1 MiB     23.7 MiB                   fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-30 08:50:10,779 - memory_profile6_log - INFO -    254                             

2018-04-30 08:50:10,779 - memory_profile6_log - INFO -    255    561.6 MiB     11.5 MiB                   X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-30 08:50:10,783 - memory_profile6_log - INFO -    256    561.6 MiB      0.0 MiB                   logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-30 08:50:10,783 - memory_profile6_log - INFO -    257    562.3 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-30 08:50:10,786 - memory_profile6_log - INFO -    258    562.3 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-30 08:50:10,788 - memory_profile6_log - INFO -    259    562.3 MiB      0.8 MiB                       mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-30 08:50:10,788 - memory_profile6_log - INFO -    260                             

2018-04-30 08:50:10,788 - memory_profile6_log - INFO -    261    557.1 MiB     -5.2 MiB                   del X_split

2018-04-30 08:50:10,789 - memory_profile6_log - INFO -    262    557.1 MiB      0.0 MiB                   logger.info("deleting X_split...")

2018-04-30 08:50:10,789 - memory_profile6_log - INFO -    263    555.8 MiB     -1.3 MiB                   del save_sigma_nt

2018-04-30 08:50:10,789 - memory_profile6_log - INFO -    264    555.8 MiB      0.0 MiB                   logger.info("deleting save_sigma_nt...")

2018-04-30 08:50:10,789 - memory_profile6_log - INFO -    265                                             

2018-04-30 08:50:10,789 - memory_profile6_log - INFO -    266                             

2018-04-30 08:50:10,790 - memory_profile6_log - INFO -    267    555.8 MiB      0.0 MiB                   del BR

2018-04-30 08:50:10,790 - memory_profile6_log - INFO -    268    555.8 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-30 08:50:10,792 - memory_profile6_log - INFO -    269                             

2018-04-30 08:50:10,796 - memory_profile6_log - INFO -    270                                     elif str(saveto).lower() == "elastic":

2018-04-30 08:50:10,796 - memory_profile6_log - INFO -    271                                         logger.info("Using ElasticSearch as storage...")

2018-04-30 08:50:10,798 - memory_profile6_log - INFO -    272                                         mh.saveElasticS(model_transformsv)

2018-04-30 08:50:10,799 - memory_profile6_log - INFO -    273                             

2018-04-30 08:50:10,799 - memory_profile6_log - INFO -    274                                     # need save sigma_nt for daily train

2018-04-30 08:50:10,801 - memory_profile6_log - INFO -    275                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-30 08:50:10,802 - memory_profile6_log - INFO -    276                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-30 08:50:10,802 - memory_profile6_log - INFO -    277    555.8 MiB      0.0 MiB           if start_date and end_date:

2018-04-30 08:50:10,803 - memory_profile6_log - INFO -    278                                         if not fitby_sigmant:

2018-04-30 08:50:10,806 - memory_profile6_log - INFO -    279                                             logging.info("Saving sigma Nt...")

2018-04-30 08:50:10,808 - memory_profile6_log - INFO -    280                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-30 08:50:10,809 - memory_profile6_log - INFO -    281                                             save_sigma_nt['start_date'] = start_date

2018-04-30 08:50:10,809 - memory_profile6_log - INFO -    282                                             save_sigma_nt['end_date'] = end_date

2018-04-30 08:50:10,812 - memory_profile6_log - INFO -    283                                             print save_sigma_nt.head(5)

2018-04-30 08:50:10,812 - memory_profile6_log - INFO -    284                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-30 08:50:10,816 - memory_profile6_log - INFO -    285                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-30 08:50:10,819 - memory_profile6_log - INFO -    286    555.8 MiB      0.0 MiB       return

2018-04-30 08:50:10,821 - memory_profile6_log - INFO - 


2018-04-30 08:50:10,821 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-30 08:56:39,414 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-30 08:56:39,417 - memory_profile6_log - INFO - date_generated: 
2018-04-30 08:56:39,418 - memory_profile6_log - INFO -  
2018-04-30 08:56:39,420 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 10, 0, 0)]
2018-04-30 08:56:39,420 - memory_profile6_log - INFO - 

2018-04-30 08:56:39,421 - memory_profile6_log - INFO - using current date: 2018-04-11
2018-04-30 08:56:39,421 - memory_profile6_log - INFO - using start date: 2018-04-10 00:00:00
2018-04-30 08:56:39,421 - memory_profile6_log - INFO - using end date: 2018-04-10
2018-04-30 08:56:39,565 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-30 08:56:39,569 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-10
2018-04-30 08:58:38,049 - memory_profile6_log - INFO - size of df: 108.51 MB
2018-04-30 08:58:38,051 - memory_profile6_log - INFO - getting total: 427635 training data(genuine interest) for date: 2018-04-10
2018-04-30 08:58:38,111 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-30 08:58:38,112 - memory_profile6_log - INFO - Appending history data...
2018-04-30 08:58:38,114 - memory_profile6_log - INFO - processing batch-0
2018-04-30 08:58:38,115 - memory_profile6_log - INFO - creating list history data...
2018-04-30 08:58:38,154 - memory_profile6_log - INFO - call history data...
2018-04-30 08:58:57,476 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 08:58:58,207 - memory_profile6_log - INFO - processing batch-1
2018-04-30 08:58:58,210 - memory_profile6_log - INFO - creating list history data...
2018-04-30 08:58:58,217 - memory_profile6_log - INFO - call history data...
2018-04-30 08:59:17,677 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 08:59:18,414 - memory_profile6_log - INFO - processing batch-2
2018-04-30 08:59:18,415 - memory_profile6_log - INFO - creating list history data...
2018-04-30 08:59:18,424 - memory_profile6_log - INFO - call history data...
2018-04-30 08:59:37,437 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 08:59:38,184 - memory_profile6_log - INFO - processing batch-3
2018-04-30 08:59:38,187 - memory_profile6_log - INFO - creating list history data...
2018-04-30 08:59:38,194 - memory_profile6_log - INFO - call history data...
2018-04-30 08:59:56,861 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 08:59:57,546 - memory_profile6_log - INFO - processing batch-4
2018-04-30 08:59:57,548 - memory_profile6_log - INFO - creating list history data...
2018-04-30 08:59:57,556 - memory_profile6_log - INFO - call history data...
2018-04-30 09:00:16,825 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 09:00:17,522 - memory_profile6_log - INFO - Appending training data...
2018-04-30 09:00:17,523 - memory_profile6_log - INFO - len datalist: 1
2018-04-30 09:00:17,526 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-30 09:00:17,528 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 09:00:17,529 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 09:00:17,529 - memory_profile6_log - INFO - ================================================

2018-04-30 09:00:17,529 - memory_profile6_log - INFO -    312     87.0 MiB     87.0 MiB   @profile

2018-04-30 09:00:17,530 - memory_profile6_log - INFO -    313                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-30 09:00:17,533 - memory_profile6_log - INFO -    314     87.0 MiB      0.0 MiB       bq_client = client

2018-04-30 09:00:17,533 - memory_profile6_log - INFO -    315     87.0 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 09:00:17,536 - memory_profile6_log - INFO -    316                             

2018-04-30 09:00:17,536 - memory_profile6_log - INFO -    317     87.0 MiB      0.0 MiB       datalist = []

2018-04-30 09:00:17,538 - memory_profile6_log - INFO -    318     87.0 MiB      0.0 MiB       datalist_hist = []

2018-04-30 09:00:17,539 - memory_profile6_log - INFO -    319                             

2018-04-30 09:00:17,539 - memory_profile6_log - INFO -    320     87.0 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-30 09:00:17,540 - memory_profile6_log - INFO -    321    410.8 MiB      0.0 MiB       for ndate in date_generated:

2018-04-30 09:00:17,543 - memory_profile6_log - INFO -    322    393.5 MiB    306.5 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-30 09:00:17,543 - memory_profile6_log - INFO -    323    393.5 MiB      0.0 MiB           if tframe is not None:

2018-04-30 09:00:17,546 - memory_profile6_log - INFO -    324    393.5 MiB      0.0 MiB               if not tframe.empty:

2018-04-30 09:00:17,548 - memory_profile6_log - INFO -    325    407.4 MiB     13.9 MiB                   X_split = np.array_split(tframe, 5)

2018-04-30 09:00:17,549 - memory_profile6_log - INFO -    326    407.4 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-30 09:00:17,549 - memory_profile6_log - INFO -    327    407.4 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-30 09:00:17,549 - memory_profile6_log - INFO -    328    410.8 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-30 09:00:17,551 - memory_profile6_log - INFO -    329                                                 # ~ loading history

2018-04-30 09:00:17,553 - memory_profile6_log - INFO -    330                                                 """

2018-04-30 09:00:17,555 - memory_profile6_log - INFO -    331                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-30 09:00:17,555 - memory_profile6_log - INFO -    332                                                 """

2018-04-30 09:00:17,556 - memory_profile6_log - INFO -    333    410.6 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-30 09:00:17,558 - memory_profile6_log - INFO -    334                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-30 09:00:17,559 - memory_profile6_log - INFO -    335    410.6 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-30 09:00:17,559 - memory_profile6_log - INFO -    336    410.6 MiB      0.6 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-30 09:00:17,559 - memory_profile6_log - INFO -    337                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-30 09:00:17,561 - memory_profile6_log - INFO -    338                             

2018-04-30 09:00:17,562 - memory_profile6_log - INFO -    339    410.6 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-30 09:00:17,562 - memory_profile6_log - INFO -    340    410.7 MiB      2.0 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-30 09:00:17,565 - memory_profile6_log - INFO -    341                             

2018-04-30 09:00:17,566 - memory_profile6_log - INFO -    342                                                 # me = os.getpid()

2018-04-30 09:00:17,568 - memory_profile6_log - INFO -    343                                                 # kill_proc_tree(me)

2018-04-30 09:00:17,569 - memory_profile6_log - INFO -    344                             

2018-04-30 09:00:17,569 - memory_profile6_log - INFO -    345    410.7 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-30 09:00:17,571 - memory_profile6_log - INFO -    346    410.8 MiB     -0.1 MiB                       for m in h_frame:

2018-04-30 09:00:17,572 - memory_profile6_log - INFO -    347    410.8 MiB     -0.1 MiB                           if m is not None:

2018-04-30 09:00:17,572 - memory_profile6_log - INFO -    348    410.8 MiB     -0.1 MiB                               if len(m) > 0:

2018-04-30 09:00:17,575 - memory_profile6_log - INFO -    349    410.8 MiB      0.8 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-30 09:00:17,575 - memory_profile6_log - INFO -    350    410.8 MiB     -0.3 MiB                       del h_frame

2018-04-30 09:00:17,576 - memory_profile6_log - INFO -    351    410.8 MiB      0.0 MiB                       del lhistory

2018-04-30 09:00:17,578 - memory_profile6_log - INFO -    352                             

2018-04-30 09:00:17,578 - memory_profile6_log - INFO -    353    410.8 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-30 09:00:17,578 - memory_profile6_log - INFO -    354    410.8 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-30 09:00:17,579 - memory_profile6_log - INFO -    355                                     else: 

2018-04-30 09:00:17,581 - memory_profile6_log - INFO -    356                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-30 09:00:17,581 - memory_profile6_log - INFO -    357    410.8 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-30 09:00:17,582 - memory_profile6_log - INFO -    358    410.8 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-30 09:00:17,582 - memory_profile6_log - INFO -    359                             

2018-04-30 09:00:17,582 - memory_profile6_log - INFO -    360    410.8 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-30 09:00:17,585 - memory_profile6_log - INFO - 


2018-04-30 09:00:18,719 - memory_profile6_log - INFO - big_frame_hist:

2018-04-30 09:00:18,789 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0   0.001543         537.007220       164             547.007220  10960288  1610c983f92247-0cb59d76ae1759-4323461-100200-1...
1   0.001543         196.515559       310             206.515559  10960288  1613079731c1c-0eac7775a89e19-7765060f-38400-16...
2   0.000023        9319.839208      1376            9329.839208  10959239  1617d812dcf543-04791235802297-d35346d-100200-1...
3   0.000023       13649.808773      1459           13659.808773  10959239  1610ca028e379-098a172ba6d096-4323461-100200-16...
4   0.001543         301.439596       351             311.439596  10960288  16137464995157-06708db64e6ab2-710d033f-49a10-1...
5   0.000023       74088.805804       112           74098.805804  10959239  1610d2a81e0215-06c836f37305ee-5768397b-100200-...
6   0.000023       10388.414989       944           10398.414989  10959239  1610c945d23697-026cb4a68def18-4323461-100200-1...
0   0.001543         242.144412       199             252.144412  10960288  16215b3e11829-0c256381f35151-6c37203c-2c880-16...
1   0.001543         150.048826       203             160.048826  10960288  161325123001ff-092fa493526f0a-232b513b-38400-1...
2   0.001543         287.579000       816             297.579000  10960288  162376d0c5f173-00f8115f3e5d06-4e3b1c76-38400-1...
3   0.001543         781.023376        39             791.023376  10960288  16137a258dbc1-0fac0c829c1047-50670302-38400-16...
4   0.001543         227.312774        67             237.312774  10960288  1625886f62c7d-034bef9733379d-a4f0b16-38400-162...
5   0.001543         185.731169       164             195.731169  10960288  1612ff604ba15c-0af75aba2777c8-33626173-49a10-1...
0   0.001543         725.235992        21             735.235992  10960288  1621ce544d12f6-0b2c797f4f2328-50683974-100200-...
1   0.001543          63.991411       238              73.991411  10960288  161935e58109ba-0985dfa4c7c3f6-3e3d5100-100200-...
2   0.001543          82.771499       184              92.771499  10960288  16130803e4048e-03a3b88e7bcf61-3a626275-43113-1...
3   0.001543         150.791642       101             160.791642  10960288  161310d5c9bee-09af35adf2f191-58596970-38400-16...
4   0.001543         292.883766        52             302.883766  10960288  161354380993c-04ad78f34c8d01-39626377-38400-16...
5   0.001543         395.623238        76             405.623238  10960288  161317c95f721c-04c3aa4e14f42b-685c2a14-2c600-1...
6   0.001543         101.327614       174             111.327614  10960288  161364f0c3f28-07895ed553fd55-e0744-38400-16136...
2018-04-30 09:00:18,792 - memory_profile6_log - INFO - 

2018-04-30 09:00:18,803 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-30 09:00:18,927 - memory_profile6_log - INFO - size of big_frame: 108.51 MB
2018-04-30 09:00:18,947 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-04-30 09:00:30,983 - memory_profile6_log - INFO - size of df: 11.88 MB
2018-04-30 09:00:30,986 - memory_profile6_log - INFO - getting total: 48430 training data(current date interest)
2018-04-30 09:00:31,013 - memory_profile6_log - INFO - size of current_frame: 12.25 MB
2018-04-30 09:00:31,015 - memory_profile6_log - INFO - loading time of: 476065 total genuine-current interest data ~ take 231.471s
2018-04-30 09:00:31,023 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 09:00:31,026 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 09:00:31,026 - memory_profile6_log - INFO - ================================================

2018-04-30 09:00:31,029 - memory_profile6_log - INFO -    362     86.9 MiB     86.9 MiB   @profile

2018-04-30 09:00:31,029 - memory_profile6_log - INFO -    363                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-30 09:00:31,029 - memory_profile6_log - INFO -    364     87.0 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-30 09:00:31,030 - memory_profile6_log - INFO -    365     87.0 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 09:00:31,032 - memory_profile6_log - INFO -    366                             

2018-04-30 09:00:31,032 - memory_profile6_log - INFO -    367                                 # ~~~ Begin collecting data ~~~

2018-04-30 09:00:31,032 - memory_profile6_log - INFO -    368     87.0 MiB      0.0 MiB       t0 = time.time()

2018-04-30 09:00:31,036 - memory_profile6_log - INFO -    369    410.8 MiB    323.8 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-30 09:00:31,038 - memory_profile6_log - INFO -    370    410.8 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-30 09:00:31,039 - memory_profile6_log - INFO -    371                                     logger.info("Training cannot be empty..")

2018-04-30 09:00:31,039 - memory_profile6_log - INFO -    372                                     return False

2018-04-30 09:00:31,039 - memory_profile6_log - INFO -    373    404.7 MiB     -6.1 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-30 09:00:31,040 - memory_profile6_log - INFO -    374    404.8 MiB      0.1 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-30 09:00:31,040 - memory_profile6_log - INFO -    375    404.8 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-30 09:00:31,042 - memory_profile6_log - INFO -    376                             

2018-04-30 09:00:31,042 - memory_profile6_log - INFO -    377    415.0 MiB     10.2 MiB       big_frame = pd.concat(datalist)

2018-04-30 09:00:31,042 - memory_profile6_log - INFO -    378    415.0 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-30 09:00:31,046 - memory_profile6_log - INFO -    379    405.2 MiB     -9.8 MiB       del datalist

2018-04-30 09:00:31,049 - memory_profile6_log - INFO -    380                             

2018-04-30 09:00:31,049 - memory_profile6_log - INFO -    381    405.2 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 09:00:31,052 - memory_profile6_log - INFO -    382                             

2018-04-30 09:00:31,052 - memory_profile6_log - INFO -    383                                 # ~ get current news interest ~

2018-04-30 09:00:31,053 - memory_profile6_log - INFO -    384    405.2 MiB      0.0 MiB       if not cd:

2018-04-30 09:00:31,055 - memory_profile6_log - INFO -    385    405.2 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-04-30 09:00:31,055 - memory_profile6_log - INFO -    386    416.8 MiB     11.6 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-30 09:00:31,058 - memory_profile6_log - INFO -    387    416.8 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-30 09:00:31,059 - memory_profile6_log - INFO -    388                                 else:

2018-04-30 09:00:31,059 - memory_profile6_log - INFO -    389                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-30 09:00:31,061 - memory_profile6_log - INFO -    390                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-30 09:00:31,061 - memory_profile6_log - INFO -    391                             

2018-04-30 09:00:31,062 - memory_profile6_log - INFO -    392                                     # safe handling of query parameter

2018-04-30 09:00:31,062 - memory_profile6_log - INFO -    393                                     query_params = [

2018-04-30 09:00:31,062 - memory_profile6_log - INFO -    394                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-30 09:00:31,062 - memory_profile6_log - INFO -    395                                     ]

2018-04-30 09:00:31,063 - memory_profile6_log - INFO -    396                             

2018-04-30 09:00:31,063 - memory_profile6_log - INFO -    397                                     job_config.query_parameters = query_params

2018-04-30 09:00:31,063 - memory_profile6_log - INFO -    398                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-30 09:00:31,065 - memory_profile6_log - INFO -    399                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-30 09:00:31,065 - memory_profile6_log - INFO -    400                             

2018-04-30 09:00:31,069 - memory_profile6_log - INFO -    401    416.8 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-30 09:00:31,071 - memory_profile6_log - INFO -    402    416.8 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-30 09:00:31,072 - memory_profile6_log - INFO -    403    416.8 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-30 09:00:31,072 - memory_profile6_log - INFO -    404    416.8 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-30 09:00:31,072 - memory_profile6_log - INFO -    405    416.8 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-30 09:00:31,072 - memory_profile6_log - INFO -    406    416.8 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-30 09:00:31,073 - memory_profile6_log - INFO -    407                             

2018-04-30 09:00:31,073 - memory_profile6_log - INFO -    408    416.8 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-30 09:00:31,075 - memory_profile6_log - INFO - 


2018-04-30 09:00:31,078 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-30 09:00:31,111 - memory_profile6_log - INFO - train on: 427635 total genuine interest data(D(u, t))
2018-04-30 09:00:31,112 - memory_profile6_log - INFO - transform on: 48430 total current data(D(t))
2018-04-30 09:00:31,114 - memory_profile6_log - INFO - apply on: 5000 total history...)
2018-04-30 09:00:31,467 - memory_profile6_log - INFO - len of uniques_fit_hist:5000
2018-04-30 09:00:31,473 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:4636
2018-04-30 09:00:32,881 - memory_profile6_log - INFO - Len of model_fit: 427635
2018-04-30 09:00:32,882 - memory_profile6_log - INFO - Len of df_dut: 427635
2018-04-30 09:00:33,131 - memory_profile6_log - INFO -        num_x  num_x  sigma_Nt  date_all_click
24396      1      1      1389          603487
24397      1      1      1389          603487
24398      2      2      1389          603487
24399      1      1      1389          603487
24400      9      9      1389          603487
2018-04-30 09:00:33,131 - memory_profile6_log - INFO - 

2018-04-30 09:00:33,213 - memory_profile6_log - INFO - model_fit dtypes:

2018-04-30 09:00:33,220 - memory_profile6_log - INFO - date              datetime64[ns, UTC]
user_id                        object
topic_id                       object
num_x                           int64
num_y                           int64
is_general                       bool
date_all_click                  int64
Ntotal                          int64
sigma_Nt                        int64
joinprob_ci                   float64
p_cat_ci                      float64
posterior                     float64
dtype: object
2018-04-30 09:00:33,220 - memory_profile6_log - INFO - 

2018-04-30 09:00:33,221 - memory_profile6_log - INFO - fitted_models_hist dtypes:

2018-04-30 09:00:33,230 - memory_profile6_log - INFO - p0_cat_ci                float64
pt_posterior_x_Nt        float64
sigma_Nt                   int64
smoothed_pt_posterior    float64
topic_id                  object
user_id                   object
dtype: object
2018-04-30 09:00:33,230 - memory_profile6_log - INFO - 

2018-04-30 09:00:33,858 - memory_profile6_log - INFO - Len of fitted_models on main class: 427635
2018-04-30 09:00:33,859 - memory_profile6_log - INFO - 

2018-04-30 09:00:33,861 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-30 09:00:33,862 - memory_profile6_log - INFO - 

2018-04-30 09:00:33,864 - memory_profile6_log - INFO - Fitted models before concat:

2018-04-30 09:00:33,885 - memory_profile6_log - INFO -                                              user_id   topic_id  pt_posterior_x_Nt  smoothed_pt_posterior
0               0ba81918-23e5-4d7c-b2ce-f943cc893b46   22601470          13.787056              23.787056
1               0ba81918-23e5-4d7c-b2ce-f943cc893b46   27431099          15.912643              25.912643
2               0ba81918-23e5-4d7c-b2ce-f943cc893b46   39301645          47.343453              57.343453
3               0ba81918-23e5-4d7c-b2ce-f943cc893b46   40347025         354.992353             364.992353
4  161035a29015-0041351a-5d670102-38400-161035a29...  132166743          29.743075              39.743075
2018-04-30 09:00:33,887 - memory_profile6_log - INFO - 

2018-04-30 09:00:33,888 - memory_profile6_log - INFO - len of current fitted models: 427635
2018-04-30 09:00:33,888 - memory_profile6_log - INFO - 

2018-04-30 09:00:33,890 - memory_profile6_log - INFO - len of history fitted models: 5000
2018-04-30 09:00:33,891 - memory_profile6_log - INFO - 

2018-04-30 09:00:33,947 - memory_profile6_log - INFO - len of fitted models after concat: 432635
2018-04-30 09:00:33,948 - memory_profile6_log - INFO - 

2018-04-30 09:00:33,948 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-30 09:00:33,950 - memory_profile6_log - INFO - 

2018-04-30 09:00:34,214 - memory_profile6_log - INFO - Fitted models after concat:

2018-04-30 09:00:34,234 - memory_profile6_log - INFO -                                              user_id   topic_id  pt_posterior_x_Nt
0               0ba81918-23e5-4d7c-b2ce-f943cc893b46   22601470          13.787056
1               0ba81918-23e5-4d7c-b2ce-f943cc893b46   27431099          15.912643
2               0ba81918-23e5-4d7c-b2ce-f943cc893b46   39301645          47.343453
3               0ba81918-23e5-4d7c-b2ce-f943cc893b46   40347025         354.992353
4  161035a29015-0041351a-5d670102-38400-161035a29...  132166743          29.743075
2018-04-30 09:00:34,236 - memory_profile6_log - INFO - 

2018-04-30 09:00:34,257 - memory_profile6_log - INFO - Fitted models after concat-simplify:

2018-04-30 09:00:34,280 - memory_profile6_log - INFO -                                              user_id   topic_id  pt_posterior_x_Nt  smoothed_pt_posterior
0               0ba81918-23e5-4d7c-b2ce-f943cc893b46   22601470          13.787056              23.787056
1               0ba81918-23e5-4d7c-b2ce-f943cc893b46   27431099          15.912643              25.912643
2               0ba81918-23e5-4d7c-b2ce-f943cc893b46   39301645          47.343453              57.343453
3               0ba81918-23e5-4d7c-b2ce-f943cc893b46   40347025         354.992353             364.992353
4  161035a29015-0041351a-5d670102-38400-161035a29...  132166743          29.743075              39.743075
2018-04-30 09:00:34,282 - memory_profile6_log - INFO - 

2018-04-30 09:00:34,283 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 427635
2018-04-30 09:00:34,285 - memory_profile6_log - INFO - 

2018-04-30 09:00:34,382 - memory_profile6_log - INFO -                                              user_id           topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci
0               0ba81918-23e5-4d7c-b2ce-f943cc893b46           22601470          13.787056              23.787056   0.030239
1               0ba81918-23e5-4d7c-b2ce-f943cc893b46           27431099          15.912643              25.912643   0.024702
2               0ba81918-23e5-4d7c-b2ce-f943cc893b46           39301645          47.343453              57.343453   0.012648
3               0ba81918-23e5-4d7c-b2ce-f943cc893b46           40347025         354.992353             364.992353   0.000460
4  161035a29015-0041351a-5d670102-38400-161035a29...          132166743          29.743075              39.743075   0.000223
5  161035a29015-0041351a-5d670102-38400-161035a29...           22601470          13.787056              23.787056   0.030239
6  161035a29015-0041351a-5d670102-38400-161035a29...           27431099          15.912643              25.912643   0.024702
7  161035a29015-0041351a-5d670102-38400-161035a29...  27431110790313993          30.739965              40.739965        NaN
8  161035a29015-0041351a-5d670102-38400-161035a29...           47091877         339.610017             349.610017   0.001158
9  1610c8e6b6466b-05a6ac1ba8c1b8-4323461-100200-1...         1019587357        2295.209178            2305.209178        NaN
2018-04-30 09:00:34,384 - memory_profile6_log - INFO - 

2018-04-30 09:01:34,608 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-30 09:01:34,653 - memory_profile6_log - INFO - Total train time: 63.542s
2018-04-30 09:01:34,654 - memory_profile6_log - INFO - memory left before cleaning: 80.500 percent memory...
2018-04-30 09:01:34,655 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-30 09:01:34,657 - memory_profile6_log - INFO - deleting df_dut...
2018-04-30 09:01:34,658 - memory_profile6_log - INFO - deleting df_dt...
2018-04-30 09:01:34,660 - memory_profile6_log - INFO - deleting df_input...
2018-04-30 09:01:34,663 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-30 09:01:34,664 - memory_profile6_log - INFO - deleting df_current...
2018-04-30 09:01:34,670 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-30 09:01:34,671 - memory_profile6_log - INFO - deleting model_fit...
2018-04-30 09:01:34,671 - memory_profile6_log - INFO - deleting result...
2018-04-30 09:01:34,694 - memory_profile6_log - INFO - deleting model_transform...
2018-04-30 09:01:34,696 - memory_profile6_log - INFO - memory left after cleaning: 80.500 percent memory...
2018-04-30 09:01:34,697 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-30 09:01:34,698 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-30 09:01:34,871 - memory_profile6_log - INFO - Saving fitted_models as history...
2018-04-30 09:01:34,994 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 10
2018-04-30 09:01:34,996 - memory_profile6_log - INFO - processing batch-0
2018-04-30 09:01:35,191 - memory_profile6_log - INFO - processing batch-1
2018-04-30 09:01:35,387 - memory_profile6_log - INFO - processing batch-2
2018-04-30 09:01:35,592 - memory_profile6_log - INFO - processing batch-3
2018-04-30 09:01:35,783 - memory_profile6_log - INFO - processing batch-4
2018-04-30 09:01:35,976 - memory_profile6_log - INFO - processing batch-5
2018-04-30 09:01:36,161 - memory_profile6_log - INFO - processing batch-6
2018-04-30 09:01:36,344 - memory_profile6_log - INFO - processing batch-7
2018-04-30 09:01:36,543 - memory_profile6_log - INFO - processing batch-8
2018-04-30 09:01:36,732 - memory_profile6_log - INFO - processing batch-9
2018-04-30 09:01:36,921 - memory_profile6_log - INFO - deleting X_split...
2018-04-30 09:01:36,923 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-04-30 09:01:36,928 - memory_profile6_log - INFO - deleting BR...
2018-04-30 09:01:36,944 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 09:01:36,946 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 09:01:36,946 - memory_profile6_log - INFO - ================================================

2018-04-30 09:01:36,947 - memory_profile6_log - INFO -    113    416.8 MiB    416.8 MiB   @profile

2018-04-30 09:01:36,947 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-30 09:01:36,948 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-30 09:01:36,948 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-30 09:01:36,950 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-30 09:01:36,950 - memory_profile6_log - INFO -    118                                      saveto="datastore", fitted_models_hist=None):

2018-04-30 09:01:36,950 - memory_profile6_log - INFO -    119                                 """

2018-04-30 09:01:36,950 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-30 09:01:36,950 - memory_profile6_log - INFO -    121                                 """

2018-04-30 09:01:36,951 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-30 09:01:36,951 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-30 09:01:36,953 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-30 09:01:36,953 - memory_profile6_log - INFO -    125    416.8 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-30 09:01:36,953 - memory_profile6_log - INFO -    126    429.9 MiB     13.1 MiB       df_dut = df_input.copy(deep=True)

2018-04-30 09:01:36,959 - memory_profile6_log - INFO -    127    429.9 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 09:01:36,960 - memory_profile6_log - INFO -    128                             

2018-04-30 09:01:36,960 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-30 09:01:36,960 - memory_profile6_log - INFO -    130    429.9 MiB      0.0 MiB       df_dt = df_current.copy(deep=True)

2018-04-30 09:01:36,961 - memory_profile6_log - INFO -    131    429.9 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 09:01:36,963 - memory_profile6_log - INFO -    132                             

2018-04-30 09:01:36,963 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-30 09:01:36,964 - memory_profile6_log - INFO -    134    429.9 MiB      0.0 MiB       t0 = time.time()

2018-04-30 09:01:36,964 - memory_profile6_log - INFO -    135    429.9 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-30 09:01:36,964 - memory_profile6_log - INFO -    136    429.9 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-30 09:01:36,966 - memory_profile6_log - INFO -    137    429.9 MiB      0.0 MiB       logger.info("apply on: %d total history...)", len(df_hist))

2018-04-30 09:01:36,966 - memory_profile6_log - INFO -    138                             

2018-04-30 09:01:36,966 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-30 09:01:36,967 - memory_profile6_log - INFO -    140    429.9 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-30 09:01:36,967 - memory_profile6_log - INFO -    141                             

2018-04-30 09:01:36,967 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-30 09:01:36,970 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-30 09:01:36,970 - memory_profile6_log - INFO -    144    430.5 MiB      0.6 MiB       NB = BR.processX(df_dut)

2018-04-30 09:01:36,973 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-30 09:01:36,973 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-30 09:01:36,973 - memory_profile6_log - INFO -    147    446.9 MiB     16.4 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-30 09:01:36,974 - memory_profile6_log - INFO -    148                                 """

2018-04-30 09:01:36,974 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-30 09:01:36,974 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-30 09:01:36,976 - memory_profile6_log - INFO -    151                                 """

2018-04-30 09:01:36,976 - memory_profile6_log - INFO -    152    446.9 MiB      0.0 MiB       fitby_sigmant = True

2018-04-30 09:01:36,976 - memory_profile6_log - INFO -    153    446.9 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-30 09:01:36,976 - memory_profile6_log - INFO -    154    446.9 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-30 09:01:36,976 - memory_profile6_log - INFO -    155    463.2 MiB     16.3 MiB                            'is_general']]

2018-04-30 09:01:36,977 - memory_profile6_log - INFO -    156                                 # get sigma_Nt from fitted_models_hist

2018-04-30 09:01:36,977 - memory_profile6_log - INFO -    157    463.3 MiB      0.1 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-04-30 09:01:36,977 - memory_profile6_log - INFO -    158    463.3 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-04-30 09:01:36,983 - memory_profile6_log - INFO -    159    463.4 MiB      0.0 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-04-30 09:01:36,984 - memory_profile6_log - INFO -    160    463.4 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-04-30 09:01:36,984 - memory_profile6_log - INFO -    161                             

2018-04-30 09:01:36,986 - memory_profile6_log - INFO -    162                                 # begin fit

2018-04-30 09:01:36,986 - memory_profile6_log - INFO -    163    463.4 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-30 09:01:36,986 - memory_profile6_log - INFO -    164    463.4 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-30 09:01:36,990 - memory_profile6_log - INFO -    165    505.4 MiB     42.0 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-04-30 09:01:36,990 - memory_profile6_log - INFO -    166    505.4 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-30 09:01:36,990 - memory_profile6_log - INFO -    167    505.4 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-30 09:01:36,992 - memory_profile6_log - INFO -    168    518.5 MiB     13.1 MiB       print model_fit[['num_x', 'num_x', 'sigma_Nt', 'date_all_click']].loc[model_fit['user_id']=="1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157"].head(5)

2018-04-30 09:01:36,997 - memory_profile6_log - INFO -    169                             

2018-04-30 09:01:36,997 - memory_profile6_log - INFO -    170                                 # ~~ and Transform ~~

2018-04-30 09:01:37,000 - memory_profile6_log - INFO -    171                                 #   handling current news interest == current date

2018-04-30 09:01:37,000 - memory_profile6_log - INFO -    172    518.5 MiB      0.0 MiB       if df_dt.empty:

2018-04-30 09:01:37,002 - memory_profile6_log - INFO -    173                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-30 09:01:37,003 - memory_profile6_log - INFO -    174                                     return None

2018-04-30 09:01:37,005 - memory_profile6_log - INFO -    175    505.6 MiB    -12.9 MiB       NB = BR.processX(df_dt)

2018-04-30 09:01:37,007 - memory_profile6_log - INFO -    176    505.6 MiB      0.0 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-30 09:01:37,009 - memory_profile6_log - INFO -    177                             

2018-04-30 09:01:37,010 - memory_profile6_log - INFO -    178    505.6 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-30 09:01:37,010 - memory_profile6_log - INFO -    179    489.7 MiB    -15.8 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-30 09:01:37,012 - memory_profile6_log - INFO -    180                             

2018-04-30 09:01:37,013 - memory_profile6_log - INFO -    181    489.7 MiB      0.0 MiB       print "model_fit dtypes:\n", model_fit.dtypes

2018-04-30 09:01:37,023 - memory_profile6_log - INFO -    182    489.7 MiB      0.0 MiB       print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-04-30 09:01:37,023 - memory_profile6_log - INFO -    183    489.7 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-30 09:01:37,026 - memory_profile6_log - INFO -    184    489.7 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-30 09:01:37,026 - memory_profile6_log - INFO -    185    489.7 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-04-30 09:01:37,028 - memory_profile6_log - INFO -    186    489.8 MiB      0.0 MiB                                                                                           "topic_id", "user_id"]],

2018-04-30 09:01:37,029 - memory_profile6_log - INFO -    187    523.5 MiB     33.8 MiB                                                     verbose=False)

2018-04-30 09:01:37,029 - memory_profile6_log - INFO -    188                                 # ~~~ filter is general and specific topic ~~~

2018-04-30 09:01:37,032 - memory_profile6_log - INFO -    189                                 # the idea is just we need to rerank every topic according

2018-04-30 09:01:37,033 - memory_profile6_log - INFO -    190                                 # user_id and and is_general by p0_posterior

2018-04-30 09:01:37,036 - memory_profile6_log - INFO -    191    523.5 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-30 09:01:37,036 - memory_profile6_log - INFO -    192    526.8 MiB      3.3 MiB                                     'is_general']].groupby(['topic_id',

2018-04-30 09:01:37,038 - memory_profile6_log - INFO -    193    523.5 MiB     -3.2 MiB                                                             'is_general']

2018-04-30 09:01:37,039 - memory_profile6_log - INFO -    194                                                                                      ).size().to_frame().reset_index()

2018-04-30 09:01:37,040 - memory_profile6_log - INFO -    195                             

2018-04-30 09:01:37,040 - memory_profile6_log - INFO -    196                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-30 09:01:37,042 - memory_profile6_log - INFO -    197                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-30 09:01:37,043 - memory_profile6_log - INFO -    198    523.5 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-30 09:01:37,049 - memory_profile6_log - INFO -    199                             

2018-04-30 09:01:37,049 - memory_profile6_log - INFO -    200                                 # ~ start by provide rank for each topic type ~

2018-04-30 09:01:37,051 - memory_profile6_log - INFO -    201    557.0 MiB     33.4 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-30 09:01:37,051 - memory_profile6_log - INFO -    202    563.6 MiB      6.6 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-30 09:01:37,053 - memory_profile6_log - INFO -    203                             

2018-04-30 09:01:37,056 - memory_profile6_log - INFO -    204                                 # ~ set threshold to filter output

2018-04-30 09:01:37,058 - memory_profile6_log - INFO -    205    563.6 MiB      0.0 MiB       if threshold > 0:

2018-04-30 09:01:37,059 - memory_profile6_log - INFO -    206    563.6 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-30 09:01:37,059 - memory_profile6_log - INFO -    207    563.6 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-30 09:01:37,059 - memory_profile6_log - INFO -    208    556.3 MiB     -7.4 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-30 09:01:37,061 - memory_profile6_log - INFO -    209                             

2018-04-30 09:01:37,061 - memory_profile6_log - INFO -    210    556.3 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-30 09:01:37,062 - memory_profile6_log - INFO -    211    556.3 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-30 09:01:37,065 - memory_profile6_log - INFO -    212                             

2018-04-30 09:01:37,065 - memory_profile6_log - INFO -    213    556.3 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-30 09:01:37,065 - memory_profile6_log - INFO -    214                             

2018-04-30 09:01:37,065 - memory_profile6_log - INFO -    215    556.3 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-30 09:01:37,066 - memory_profile6_log - INFO -    216    556.3 MiB      0.0 MiB       del df_dut

2018-04-30 09:01:37,066 - memory_profile6_log - INFO -    217    556.3 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-30 09:01:37,068 - memory_profile6_log - INFO -    218    556.3 MiB      0.0 MiB       del df_dt

2018-04-30 09:01:37,068 - memory_profile6_log - INFO -    219    556.3 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-30 09:01:37,068 - memory_profile6_log - INFO -    220    556.3 MiB      0.0 MiB       del df_input

2018-04-30 09:01:37,069 - memory_profile6_log - INFO -    221    556.3 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-30 09:01:37,069 - memory_profile6_log - INFO -    222    555.2 MiB     -1.0 MiB       del df_input_X

2018-04-30 09:01:37,069 - memory_profile6_log - INFO -    223    555.2 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-30 09:01:37,072 - memory_profile6_log - INFO -    224    555.2 MiB      0.0 MiB       del df_current

2018-04-30 09:01:37,072 - memory_profile6_log - INFO -    225    555.2 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-30 09:01:37,072 - memory_profile6_log - INFO -    226    555.2 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-30 09:01:37,075 - memory_profile6_log - INFO -    227    555.2 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-30 09:01:37,078 - memory_profile6_log - INFO -    228    555.2 MiB      0.0 MiB       del model_fit

2018-04-30 09:01:37,078 - memory_profile6_log - INFO -    229    555.2 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-30 09:01:37,078 - memory_profile6_log - INFO -    230    555.2 MiB      0.0 MiB       del result

2018-04-30 09:01:37,078 - memory_profile6_log - INFO -    231    555.2 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-30 09:01:37,082 - memory_profile6_log - INFO -    232                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 09:01:37,082 - memory_profile6_log - INFO -    233                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 09:01:37,082 - memory_profile6_log - INFO -    234                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 09:01:37,082 - memory_profile6_log - INFO -    235    555.2 MiB      0.0 MiB       if savetrain:

2018-04-30 09:01:37,082 - memory_profile6_log - INFO -    236    562.6 MiB      7.4 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-30 09:01:37,086 - memory_profile6_log - INFO -    237    562.6 MiB      0.0 MiB           del model_transform

2018-04-30 09:01:37,086 - memory_profile6_log - INFO -    238    562.6 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-30 09:01:37,089 - memory_profile6_log - INFO -    239    562.6 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-30 09:01:37,089 - memory_profile6_log - INFO -    240                             

2018-04-30 09:01:37,091 - memory_profile6_log - INFO -    241    562.6 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-30 09:01:37,091 - memory_profile6_log - INFO -    242                                     # ~ Place your code to save the training model here ~

2018-04-30 09:01:37,092 - memory_profile6_log - INFO -    243    562.6 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-30 09:01:37,092 - memory_profile6_log - INFO -    244    562.6 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-30 09:01:37,092 - memory_profile6_log - INFO -    245    562.6 MiB      0.0 MiB               if multproc:

2018-04-30 09:01:37,092 - memory_profile6_log - INFO -    246                                             # ~ save transform models ~

2018-04-30 09:01:37,094 - memory_profile6_log - INFO -    247    527.9 MiB    -34.7 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-30 09:01:37,095 - memory_profile6_log - INFO -    248                             

2018-04-30 09:01:37,095 - memory_profile6_log - INFO -    249                                             

2018-04-30 09:01:37,099 - memory_profile6_log - INFO -    250                                             # ~ save fitted models ~

2018-04-30 09:01:37,101 - memory_profile6_log - INFO -    251    527.9 MiB      0.0 MiB                   logger.info("Saving fitted_models as history...")

2018-04-30 09:01:37,102 - memory_profile6_log - INFO -    252    527.9 MiB      0.0 MiB                   save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-30 09:01:37,102 - memory_profile6_log - INFO -    253    552.4 MiB     24.4 MiB                   fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-30 09:01:37,104 - memory_profile6_log - INFO -    254                             

2018-04-30 09:01:37,105 - memory_profile6_log - INFO -    255    564.9 MiB     12.6 MiB                   X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-30 09:01:37,107 - memory_profile6_log - INFO -    256    564.9 MiB      0.0 MiB                   logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-30 09:01:37,108 - memory_profile6_log - INFO -    257    565.9 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-30 09:01:37,108 - memory_profile6_log - INFO -    258    565.9 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-30 09:01:37,114 - memory_profile6_log - INFO -    259    565.9 MiB      1.0 MiB                       mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-30 09:01:37,114 - memory_profile6_log - INFO -    260                             

2018-04-30 09:01:37,119 - memory_profile6_log - INFO -    261    560.6 MiB     -5.3 MiB                   del X_split

2018-04-30 09:01:37,119 - memory_profile6_log - INFO -    262    560.6 MiB      0.0 MiB                   logger.info("deleting X_split...")

2018-04-30 09:01:37,121 - memory_profile6_log - INFO -    263    560.6 MiB      0.0 MiB                   del save_sigma_nt

2018-04-30 09:01:37,125 - memory_profile6_log - INFO -    264    560.6 MiB      0.0 MiB                   logger.info("deleting save_sigma_nt...")

2018-04-30 09:01:37,125 - memory_profile6_log - INFO -    265                                             

2018-04-30 09:01:37,128 - memory_profile6_log - INFO -    266                             

2018-04-30 09:01:37,128 - memory_profile6_log - INFO -    267    560.6 MiB      0.0 MiB                   del BR

2018-04-30 09:01:37,128 - memory_profile6_log - INFO -    268    560.6 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-30 09:01:37,130 - memory_profile6_log - INFO -    269                             

2018-04-30 09:01:37,131 - memory_profile6_log - INFO -    270                                     elif str(saveto).lower() == "elastic":

2018-04-30 09:01:37,131 - memory_profile6_log - INFO -    271                                         logger.info("Using ElasticSearch as storage...")

2018-04-30 09:01:37,132 - memory_profile6_log - INFO -    272                                         mh.saveElasticS(model_transformsv)

2018-04-30 09:01:37,134 - memory_profile6_log - INFO -    273                             

2018-04-30 09:01:37,135 - memory_profile6_log - INFO -    274                                     # need save sigma_nt for daily train

2018-04-30 09:01:37,140 - memory_profile6_log - INFO -    275                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-30 09:01:37,141 - memory_profile6_log - INFO -    276                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-30 09:01:37,141 - memory_profile6_log - INFO -    277    560.6 MiB      0.0 MiB           if start_date and end_date:

2018-04-30 09:01:37,141 - memory_profile6_log - INFO -    278                                         if not fitby_sigmant:

2018-04-30 09:01:37,142 - memory_profile6_log - INFO -    279                                             logging.info("Saving sigma Nt...")

2018-04-30 09:01:37,142 - memory_profile6_log - INFO -    280                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-30 09:01:37,142 - memory_profile6_log - INFO -    281                                             save_sigma_nt['start_date'] = start_date

2018-04-30 09:01:37,144 - memory_profile6_log - INFO -    282                                             save_sigma_nt['end_date'] = end_date

2018-04-30 09:01:37,144 - memory_profile6_log - INFO -    283                                             print save_sigma_nt.head(5)

2018-04-30 09:01:37,144 - memory_profile6_log - INFO -    284                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-30 09:01:37,148 - memory_profile6_log - INFO -    285                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-30 09:01:37,150 - memory_profile6_log - INFO -    286    560.6 MiB      0.0 MiB       return

2018-04-30 09:01:37,150 - memory_profile6_log - INFO - 


2018-04-30 09:01:37,151 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-30 09:15:37,121 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-30 09:15:37,125 - memory_profile6_log - INFO - date_generated: 
2018-04-30 09:15:37,125 - memory_profile6_log - INFO -  
2018-04-30 09:15:37,127 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 10, 0, 0)]
2018-04-30 09:15:37,127 - memory_profile6_log - INFO - 

2018-04-30 09:15:37,127 - memory_profile6_log - INFO - using current date: 2018-04-11
2018-04-30 09:15:37,128 - memory_profile6_log - INFO - using start date: 2018-04-10 00:00:00
2018-04-30 09:15:37,128 - memory_profile6_log - INFO - using end date: 2018-04-10
2018-04-30 09:15:37,272 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-30 09:15:37,276 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-10
2018-04-30 09:17:37,125 - memory_profile6_log - INFO - size of df: 108.51 MB
2018-04-30 09:17:37,127 - memory_profile6_log - INFO - getting total: 427635 training data(genuine interest) for date: 2018-04-10
2018-04-30 09:17:37,184 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-30 09:17:37,184 - memory_profile6_log - INFO - Appending history data...
2018-04-30 09:17:37,186 - memory_profile6_log - INFO - processing batch-0
2018-04-30 09:17:37,187 - memory_profile6_log - INFO - creating list history data...
2018-04-30 09:17:37,230 - memory_profile6_log - INFO - call history data...
2018-04-30 09:17:57,621 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 09:17:58,299 - memory_profile6_log - INFO - processing batch-1
2018-04-30 09:17:58,302 - memory_profile6_log - INFO - creating list history data...
2018-04-30 09:17:58,309 - memory_profile6_log - INFO - call history data...
2018-04-30 09:18:17,104 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 09:18:17,852 - memory_profile6_log - INFO - processing batch-2
2018-04-30 09:18:17,854 - memory_profile6_log - INFO - creating list history data...
2018-04-30 09:18:17,861 - memory_profile6_log - INFO - call history data...
2018-04-30 09:18:36,513 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 09:18:37,210 - memory_profile6_log - INFO - processing batch-3
2018-04-30 09:18:37,210 - memory_profile6_log - INFO - creating list history data...
2018-04-30 09:18:37,220 - memory_profile6_log - INFO - call history data...
2018-04-30 09:18:57,545 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 09:18:58,220 - memory_profile6_log - INFO - processing batch-4
2018-04-30 09:18:58,220 - memory_profile6_log - INFO - creating list history data...
2018-04-30 09:18:58,232 - memory_profile6_log - INFO - call history data...
2018-04-30 09:19:17,098 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 09:19:17,990 - memory_profile6_log - INFO - Appending training data...
2018-04-30 09:19:17,990 - memory_profile6_log - INFO - len datalist: 1
2018-04-30 09:19:17,993 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-30 09:19:17,994 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 09:19:17,997 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 09:19:17,999 - memory_profile6_log - INFO - ================================================

2018-04-30 09:19:18,002 - memory_profile6_log - INFO -    312     86.6 MiB     86.6 MiB   @profile

2018-04-30 09:19:18,003 - memory_profile6_log - INFO -    313                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-30 09:19:18,007 - memory_profile6_log - INFO -    314     86.6 MiB      0.0 MiB       bq_client = client

2018-04-30 09:19:18,009 - memory_profile6_log - INFO -    315     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 09:19:18,009 - memory_profile6_log - INFO -    316                             

2018-04-30 09:19:18,016 - memory_profile6_log - INFO -    317     86.6 MiB      0.0 MiB       datalist = []

2018-04-30 09:19:18,019 - memory_profile6_log - INFO -    318     86.6 MiB      0.0 MiB       datalist_hist = []

2018-04-30 09:19:18,019 - memory_profile6_log - INFO -    319                             

2018-04-30 09:19:18,023 - memory_profile6_log - INFO -    320     86.6 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-30 09:19:18,023 - memory_profile6_log - INFO -    321    410.6 MiB      0.0 MiB       for ndate in date_generated:

2018-04-30 09:19:18,026 - memory_profile6_log - INFO -    322    392.7 MiB    306.0 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-30 09:19:18,028 - memory_profile6_log - INFO -    323    392.7 MiB      0.0 MiB           if tframe is not None:

2018-04-30 09:19:18,032 - memory_profile6_log - INFO -    324    392.7 MiB      0.0 MiB               if not tframe.empty:

2018-04-30 09:19:18,035 - memory_profile6_log - INFO -    325    406.6 MiB     13.9 MiB                   X_split = np.array_split(tframe, 5)

2018-04-30 09:19:18,039 - memory_profile6_log - INFO -    326    406.6 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-30 09:19:18,040 - memory_profile6_log - INFO -    327    406.6 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-30 09:19:18,053 - memory_profile6_log - INFO -    328    410.6 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-30 09:19:18,058 - memory_profile6_log - INFO -    329                                                 # ~ loading history

2018-04-30 09:19:18,059 - memory_profile6_log - INFO -    330                                                 """

2018-04-30 09:19:18,061 - memory_profile6_log - INFO -    331                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-30 09:19:18,069 - memory_profile6_log - INFO -    332                                                 """

2018-04-30 09:19:18,071 - memory_profile6_log - INFO -    333    410.4 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-30 09:19:18,072 - memory_profile6_log - INFO -    334                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-30 09:19:18,073 - memory_profile6_log - INFO -    335    410.4 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-30 09:19:18,108 - memory_profile6_log - INFO -    336    410.4 MiB      1.1 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-30 09:19:18,121 - memory_profile6_log - INFO -    337                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-30 09:19:18,122 - memory_profile6_log - INFO -    338                             

2018-04-30 09:19:18,138 - memory_profile6_log - INFO -    339    410.4 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-30 09:19:18,138 - memory_profile6_log - INFO -    340    410.4 MiB      1.8 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-30 09:19:18,154 - memory_profile6_log - INFO -    341                             

2018-04-30 09:19:18,155 - memory_profile6_log - INFO -    342                                                 # me = os.getpid()

2018-04-30 09:19:18,161 - memory_profile6_log - INFO -    343                                                 # kill_proc_tree(me)

2018-04-30 09:19:18,163 - memory_profile6_log - INFO -    344                             

2018-04-30 09:19:18,165 - memory_profile6_log - INFO -    345    410.4 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-30 09:19:18,167 - memory_profile6_log - INFO -    346    410.6 MiB     -0.2 MiB                       for m in h_frame:

2018-04-30 09:19:18,168 - memory_profile6_log - INFO -    347    410.6 MiB     -0.2 MiB                           if m is not None:

2018-04-30 09:19:18,171 - memory_profile6_log - INFO -    348    410.6 MiB     -0.2 MiB                               if len(m) > 0:

2018-04-30 09:19:18,171 - memory_profile6_log - INFO -    349    410.6 MiB      0.9 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-30 09:19:18,173 - memory_profile6_log - INFO -    350    410.6 MiB      0.0 MiB                       del h_frame

2018-04-30 09:19:18,173 - memory_profile6_log - INFO -    351    410.6 MiB      0.0 MiB                       del lhistory

2018-04-30 09:19:18,176 - memory_profile6_log - INFO -    352                             

2018-04-30 09:19:18,177 - memory_profile6_log - INFO -    353    410.6 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-30 09:19:18,177 - memory_profile6_log - INFO -    354    410.6 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-30 09:19:18,178 - memory_profile6_log - INFO -    355                                     else: 

2018-04-30 09:19:18,180 - memory_profile6_log - INFO -    356                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-30 09:19:18,180 - memory_profile6_log - INFO -    357    410.6 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-30 09:19:18,181 - memory_profile6_log - INFO -    358    410.6 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-30 09:19:18,183 - memory_profile6_log - INFO -    359                             

2018-04-30 09:19:18,184 - memory_profile6_log - INFO -    360    410.6 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-30 09:19:18,187 - memory_profile6_log - INFO - 


2018-04-30 09:19:19,335 - memory_profile6_log - INFO - big_frame_hist:

2018-04-30 09:19:19,414 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0   0.001543         301.439596       351             311.439596  10960288  16137464995157-06708db64e6ab2-710d033f-49a10-1...
1   0.001543         537.007220       164             547.007220  10960288  1610c983f92247-0cb59d76ae1759-4323461-100200-1...
2   0.000023       13649.808773      1459           13659.808773  10959239  1610ca028e379-098a172ba6d096-4323461-100200-16...
3   0.000023        9319.839208      1376            9329.839208  10959239  1617d812dcf543-04791235802297-d35346d-100200-1...
4   0.000023       74088.805804       112           74098.805804  10959239  1610d2a81e0215-06c836f37305ee-5768397b-100200-...
5   0.001543         196.515559       310             206.515559  10960288  1613079731c1c-0eac7775a89e19-7765060f-38400-16...
6   0.000023       10388.414989       944           10398.414989  10959239  1610c945d23697-026cb4a68def18-4323461-100200-1...
0   0.001543         287.579000       816             297.579000  10960288  162376d0c5f173-00f8115f3e5d06-4e3b1c76-38400-1...
1   0.001543         227.312774        67             237.312774  10960288  1625886f62c7d-034bef9733379d-a4f0b16-38400-162...
2   0.001543         150.048826       203             160.048826  10960288  161325123001ff-092fa493526f0a-232b513b-38400-1...
3   0.001543         185.731169       164             195.731169  10960288  1612ff604ba15c-0af75aba2777c8-33626173-49a10-1...
4   0.001543         781.023376        39             791.023376  10960288  16137a258dbc1-0fac0c829c1047-50670302-38400-16...
5   0.001543         242.144412       199             252.144412  10960288  16215b3e11829-0c256381f35151-6c37203c-2c880-16...
0   0.001543         292.883766        52             302.883766  10960288  161354380993c-04ad78f34c8d01-39626377-38400-16...
1   0.001543          82.771499       184              92.771499  10960288  16130803e4048e-03a3b88e7bcf61-3a626275-43113-1...
2   0.001543         150.791642       101             160.791642  10960288  161310d5c9bee-09af35adf2f191-58596970-38400-16...
3   0.001543          63.991411       238              73.991411  10960288  161935e58109ba-0985dfa4c7c3f6-3e3d5100-100200-...
4   0.001543         101.327614       174             111.327614  10960288  161364f0c3f28-07895ed553fd55-e0744-38400-16136...
5   0.001543         395.623238        76             405.623238  10960288  161317c95f721c-04c3aa4e14f42b-685c2a14-2c600-1...
6   0.001543         725.235992        21             735.235992  10960288  1621ce544d12f6-0b2c797f4f2328-50683974-100200-...
2018-04-30 09:19:19,415 - memory_profile6_log - INFO - 

2018-04-30 09:19:19,424 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-30 09:19:19,553 - memory_profile6_log - INFO - size of big_frame: 108.51 MB
2018-04-30 09:19:19,575 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-04-30 09:19:31,187 - memory_profile6_log - INFO - size of df: 11.88 MB
2018-04-30 09:19:31,187 - memory_profile6_log - INFO - getting total: 48430 training data(current date interest)
2018-04-30 09:19:31,223 - memory_profile6_log - INFO - size of current_frame: 12.25 MB
2018-04-30 09:19:31,224 - memory_profile6_log - INFO - loading time of: 476065 total genuine-current interest data ~ take 233.979s
2018-04-30 09:19:31,232 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 09:19:31,233 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 09:19:31,234 - memory_profile6_log - INFO - ================================================

2018-04-30 09:19:31,236 - memory_profile6_log - INFO -    362     86.5 MiB     86.5 MiB   @profile

2018-04-30 09:19:31,237 - memory_profile6_log - INFO -    363                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-30 09:19:31,239 - memory_profile6_log - INFO -    364     86.6 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-30 09:19:31,240 - memory_profile6_log - INFO -    365     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 09:19:31,240 - memory_profile6_log - INFO -    366                             

2018-04-30 09:19:31,240 - memory_profile6_log - INFO -    367                                 # ~~~ Begin collecting data ~~~

2018-04-30 09:19:31,240 - memory_profile6_log - INFO -    368     86.6 MiB      0.0 MiB       t0 = time.time()

2018-04-30 09:19:31,242 - memory_profile6_log - INFO -    369    406.3 MiB    319.7 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-30 09:19:31,242 - memory_profile6_log - INFO -    370    406.3 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-30 09:19:31,243 - memory_profile6_log - INFO -    371                                     logger.info("Training cannot be empty..")

2018-04-30 09:19:31,246 - memory_profile6_log - INFO -    372                                     return False

2018-04-30 09:19:31,247 - memory_profile6_log - INFO -    373    400.2 MiB     -6.1 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-30 09:19:31,249 - memory_profile6_log - INFO -    374    400.4 MiB      0.2 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-30 09:19:31,250 - memory_profile6_log - INFO -    375    400.4 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-30 09:19:31,250 - memory_profile6_log - INFO -    376                             

2018-04-30 09:19:31,250 - memory_profile6_log - INFO -    377    410.6 MiB     10.2 MiB       big_frame = pd.concat(datalist)

2018-04-30 09:19:31,250 - memory_profile6_log - INFO -    378    410.6 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-30 09:19:31,250 - memory_profile6_log - INFO -    379    400.8 MiB     -9.8 MiB       del datalist

2018-04-30 09:19:31,252 - memory_profile6_log - INFO -    380                             

2018-04-30 09:19:31,252 - memory_profile6_log - INFO -    381    400.8 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 09:19:31,253 - memory_profile6_log - INFO -    382                             

2018-04-30 09:19:31,253 - memory_profile6_log - INFO -    383                                 # ~ get current news interest ~

2018-04-30 09:19:31,253 - memory_profile6_log - INFO -    384    400.8 MiB      0.0 MiB       if not cd:

2018-04-30 09:19:31,253 - memory_profile6_log - INFO -    385    400.8 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-04-30 09:19:31,255 - memory_profile6_log - INFO -    386    415.6 MiB     14.8 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-30 09:19:31,255 - memory_profile6_log - INFO -    387    415.6 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-30 09:19:31,257 - memory_profile6_log - INFO -    388                                 else:

2018-04-30 09:19:31,259 - memory_profile6_log - INFO -    389                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-30 09:19:31,260 - memory_profile6_log - INFO -    390                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-30 09:19:31,260 - memory_profile6_log - INFO -    391                             

2018-04-30 09:19:31,262 - memory_profile6_log - INFO -    392                                     # safe handling of query parameter

2018-04-30 09:19:31,262 - memory_profile6_log - INFO -    393                                     query_params = [

2018-04-30 09:19:31,263 - memory_profile6_log - INFO -    394                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-30 09:19:31,263 - memory_profile6_log - INFO -    395                                     ]

2018-04-30 09:19:31,263 - memory_profile6_log - INFO -    396                             

2018-04-30 09:19:31,265 - memory_profile6_log - INFO -    397                                     job_config.query_parameters = query_params

2018-04-30 09:19:31,265 - memory_profile6_log - INFO -    398                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-30 09:19:31,266 - memory_profile6_log - INFO -    399                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-30 09:19:31,266 - memory_profile6_log - INFO -    400                             

2018-04-30 09:19:31,269 - memory_profile6_log - INFO -    401    415.6 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-30 09:19:31,270 - memory_profile6_log - INFO -    402    415.6 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-30 09:19:31,272 - memory_profile6_log - INFO -    403    415.6 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-30 09:19:31,273 - memory_profile6_log - INFO -    404    415.6 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-30 09:19:31,273 - memory_profile6_log - INFO -    405    415.6 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-30 09:19:31,273 - memory_profile6_log - INFO -    406    415.6 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-30 09:19:31,273 - memory_profile6_log - INFO -    407                             

2018-04-30 09:19:31,275 - memory_profile6_log - INFO -    408    415.6 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-30 09:19:31,275 - memory_profile6_log - INFO - 


2018-04-30 09:19:31,279 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-30 09:19:31,306 - memory_profile6_log - INFO - train on: 427635 total genuine interest data(D(u, t))
2018-04-30 09:19:31,308 - memory_profile6_log - INFO - transform on: 48430 total current data(D(t))
2018-04-30 09:19:31,309 - memory_profile6_log - INFO - apply on: 5000 total history...)
2018-04-30 09:19:31,661 - memory_profile6_log - INFO - len of uniques_fit_hist:5000
2018-04-30 09:19:31,671 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:4636
2018-04-30 09:19:32,855 - memory_profile6_log - INFO - Len of model_fit: 427635
2018-04-30 09:19:32,857 - memory_profile6_log - INFO - Len of df_dut: 427635
2018-04-30 09:19:32,937 - memory_profile6_log - INFO - model_fit dtypes:

2018-04-30 09:19:32,944 - memory_profile6_log - INFO - date              datetime64[ns, UTC]
user_id                        object
topic_id                       object
num_x                           int64
num_y                           int64
is_general                       bool
date_all_click                  int64
Ntotal                          int64
joinprob_ci                   float64
p_cat_ci                      float64
posterior                     float64
dtype: object
2018-04-30 09:19:32,946 - memory_profile6_log - INFO - 

2018-04-30 09:19:32,947 - memory_profile6_log - INFO - fitted_models_hist dtypes:

2018-04-30 09:19:32,953 - memory_profile6_log - INFO - p0_cat_ci                float64
pt_posterior_x_Nt        float64
sigma_Nt                   int64
smoothed_pt_posterior    float64
topic_id                  object
user_id                   object
dtype: object
2018-04-30 09:19:32,953 - memory_profile6_log - INFO - 

2018-04-30 09:19:33,546 - memory_profile6_log - INFO - Len of fitted_models on main class: 427635
2018-04-30 09:19:33,549 - memory_profile6_log - INFO - 

2018-04-30 09:19:33,549 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-30 09:19:33,551 - memory_profile6_log - INFO - 

2018-04-30 09:19:33,552 - memory_profile6_log - INFO - Fitted models before concat:

2018-04-30 09:19:33,578 - memory_profile6_log - INFO -                                              user_id   topic_id  pt_posterior_x_Nt  smoothed_pt_posterior
0               0ba81918-23e5-4d7c-b2ce-f943cc893b46   22601470          13.787056              23.787056
1               0ba81918-23e5-4d7c-b2ce-f943cc893b46   27431099          15.912643              25.912643
2               0ba81918-23e5-4d7c-b2ce-f943cc893b46   39301645          47.343453              57.343453
3               0ba81918-23e5-4d7c-b2ce-f943cc893b46   40347025         354.992353             364.992353
4  161035a29015-0041351a-5d670102-38400-161035a29...  132166743          29.743075              39.743075
2018-04-30 09:19:33,579 - memory_profile6_log - INFO - 

2018-04-30 09:19:33,581 - memory_profile6_log - INFO - len of current fitted models: 427635
2018-04-30 09:19:33,582 - memory_profile6_log - INFO - 

2018-04-30 09:19:33,584 - memory_profile6_log - INFO - len of history fitted models: 5000
2018-04-30 09:19:33,585 - memory_profile6_log - INFO - 

2018-04-30 09:19:33,641 - memory_profile6_log - INFO - len of fitted models after concat: 432635
2018-04-30 09:19:33,644 - memory_profile6_log - INFO - 

2018-04-30 09:19:33,644 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-30 09:19:33,645 - memory_profile6_log - INFO - 

2018-04-30 09:19:33,951 - memory_profile6_log - INFO - Fitted models after concat:

2018-04-30 09:19:33,973 - memory_profile6_log - INFO -                                              user_id   topic_id  pt_posterior_x_Nt
0               0ba81918-23e5-4d7c-b2ce-f943cc893b46   22601470          13.787056
1               0ba81918-23e5-4d7c-b2ce-f943cc893b46   27431099          15.912643
2               0ba81918-23e5-4d7c-b2ce-f943cc893b46   39301645          47.343453
3               0ba81918-23e5-4d7c-b2ce-f943cc893b46   40347025         354.992353
4  161035a29015-0041351a-5d670102-38400-161035a29...  132166743          29.743075
2018-04-30 09:19:33,974 - memory_profile6_log - INFO - 

2018-04-30 09:19:33,993 - memory_profile6_log - INFO - Fitted models after concat-simplify:

2018-04-30 09:19:34,016 - memory_profile6_log - INFO -                                              user_id   topic_id  pt_posterior_x_Nt  smoothed_pt_posterior
0               0ba81918-23e5-4d7c-b2ce-f943cc893b46   22601470          13.787056              23.787056
1               0ba81918-23e5-4d7c-b2ce-f943cc893b46   27431099          15.912643              25.912643
2               0ba81918-23e5-4d7c-b2ce-f943cc893b46   39301645          47.343453              57.343453
3               0ba81918-23e5-4d7c-b2ce-f943cc893b46   40347025         354.992353             364.992353
4  161035a29015-0041351a-5d670102-38400-161035a29...  132166743          29.743075              39.743075
2018-04-30 09:19:34,016 - memory_profile6_log - INFO - 

2018-04-30 09:19:34,017 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 427635
2018-04-30 09:19:34,019 - memory_profile6_log - INFO - 

2018-04-30 09:19:34,122 - memory_profile6_log - INFO -                                              user_id           topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci
0               0ba81918-23e5-4d7c-b2ce-f943cc893b46           22601470          13.787056              23.787056   0.030239
1               0ba81918-23e5-4d7c-b2ce-f943cc893b46           27431099          15.912643              25.912643   0.024702
2               0ba81918-23e5-4d7c-b2ce-f943cc893b46           39301645          47.343453              57.343453   0.012648
3               0ba81918-23e5-4d7c-b2ce-f943cc893b46           40347025         354.992353             364.992353   0.000460
4  161035a29015-0041351a-5d670102-38400-161035a29...          132166743          29.743075              39.743075   0.000223
5  161035a29015-0041351a-5d670102-38400-161035a29...           22601470          13.787056              23.787056   0.030239
6  161035a29015-0041351a-5d670102-38400-161035a29...           27431099          15.912643              25.912643   0.024702
7  161035a29015-0041351a-5d670102-38400-161035a29...  27431110790313993          30.739965              40.739965        NaN
8  161035a29015-0041351a-5d670102-38400-161035a29...           47091877         339.610017             349.610017   0.001158
9  1610c8e6b6466b-05a6ac1ba8c1b8-4323461-100200-1...         1019587357       43106.214286           43116.214286        NaN
2018-04-30 09:19:34,125 - memory_profile6_log - INFO - 

2018-04-30 09:20:34,568 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-30 09:20:34,607 - memory_profile6_log - INFO - Total train time: 63.300s
2018-04-30 09:20:34,608 - memory_profile6_log - INFO - memory left before cleaning: 82.100 percent memory...
2018-04-30 09:20:34,609 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-30 09:20:34,611 - memory_profile6_log - INFO - deleting df_dut...
2018-04-30 09:20:34,612 - memory_profile6_log - INFO - deleting df_dt...
2018-04-30 09:20:34,614 - memory_profile6_log - INFO - deleting df_input...
2018-04-30 09:20:34,617 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-30 09:20:34,618 - memory_profile6_log - INFO - deleting df_current...
2018-04-30 09:20:34,618 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-30 09:20:34,637 - memory_profile6_log - INFO - deleting model_fit...
2018-04-30 09:20:34,638 - memory_profile6_log - INFO - deleting result...
2018-04-30 09:20:34,665 - memory_profile6_log - INFO - deleting model_transform...
2018-04-30 09:20:34,667 - memory_profile6_log - INFO - memory left after cleaning: 81.900 percent memory...
2018-04-30 09:20:34,667 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-30 09:20:34,668 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-30 09:20:34,671 - memory_profile6_log - INFO - Saving main Transform model to Google DataStore...
2018-04-30 09:20:34,877 - memory_profile6_log - INFO - Saving fitted_models as history...
2018-04-30 09:20:35,006 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 10
2018-04-30 09:20:35,009 - memory_profile6_log - INFO - processing batch-0
2018-04-30 09:20:35,203 - memory_profile6_log - INFO - processing batch-1
2018-04-30 09:20:35,385 - memory_profile6_log - INFO - processing batch-2
2018-04-30 09:20:35,568 - memory_profile6_log - INFO - processing batch-3
2018-04-30 09:20:35,769 - memory_profile6_log - INFO - processing batch-4
2018-04-30 09:20:35,953 - memory_profile6_log - INFO - processing batch-5
2018-04-30 09:20:36,134 - memory_profile6_log - INFO - processing batch-6
2018-04-30 09:20:36,322 - memory_profile6_log - INFO - processing batch-7
2018-04-30 09:20:36,533 - memory_profile6_log - INFO - processing batch-8
2018-04-30 09:20:36,724 - memory_profile6_log - INFO - processing batch-9
2018-04-30 09:20:36,934 - memory_profile6_log - INFO - deleting X_split...
2018-04-30 09:20:36,936 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-04-30 09:20:36,938 - memory_profile6_log - INFO - deleting BR...
2018-04-30 09:20:36,953 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 09:20:36,953 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 09:20:36,954 - memory_profile6_log - INFO - ================================================

2018-04-30 09:20:36,954 - memory_profile6_log - INFO -    113    415.6 MiB    415.6 MiB   @profile

2018-04-30 09:20:36,956 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-30 09:20:36,956 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-30 09:20:36,956 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-30 09:20:36,957 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-30 09:20:36,957 - memory_profile6_log - INFO -    118                                      saveto="datastore", fitted_models_hist=None):

2018-04-30 09:20:36,957 - memory_profile6_log - INFO -    119                                 """

2018-04-30 09:20:36,957 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-30 09:20:36,957 - memory_profile6_log - INFO -    121                                 """

2018-04-30 09:20:36,959 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-30 09:20:36,961 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-30 09:20:36,963 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-30 09:20:36,963 - memory_profile6_log - INFO -    125    415.6 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-30 09:20:36,964 - memory_profile6_log - INFO -    126    428.7 MiB     13.1 MiB       df_dut = df_input.copy(deep=True)

2018-04-30 09:20:36,964 - memory_profile6_log - INFO -    127    428.7 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 09:20:36,964 - memory_profile6_log - INFO -    128                             

2018-04-30 09:20:36,966 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-30 09:20:36,966 - memory_profile6_log - INFO -    130    428.7 MiB      0.0 MiB       df_dt = df_current.copy(deep=True)

2018-04-30 09:20:36,966 - memory_profile6_log - INFO -    131    428.7 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 09:20:36,967 - memory_profile6_log - INFO -    132                             

2018-04-30 09:20:36,967 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-30 09:20:36,967 - memory_profile6_log - INFO -    134    428.7 MiB      0.0 MiB       t0 = time.time()

2018-04-30 09:20:36,967 - memory_profile6_log - INFO -    135    428.7 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-30 09:20:36,969 - memory_profile6_log - INFO -    136    428.7 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-30 09:20:36,969 - memory_profile6_log - INFO -    137    428.7 MiB      0.0 MiB       logger.info("apply on: %d total history...)", len(df_hist))

2018-04-30 09:20:36,970 - memory_profile6_log - INFO -    138                             

2018-04-30 09:20:36,970 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-30 09:20:36,973 - memory_profile6_log - INFO -    140    428.7 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-30 09:20:36,974 - memory_profile6_log - INFO -    141                             

2018-04-30 09:20:36,974 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-30 09:20:36,976 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-30 09:20:36,976 - memory_profile6_log - INFO -    144    430.2 MiB      1.6 MiB       NB = BR.processX(df_dut)

2018-04-30 09:20:36,976 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-30 09:20:36,976 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-30 09:20:36,977 - memory_profile6_log - INFO -    147    446.7 MiB     16.5 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-30 09:20:36,979 - memory_profile6_log - INFO -    148                                 """

2018-04-30 09:20:36,979 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-30 09:20:36,980 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-30 09:20:36,980 - memory_profile6_log - INFO -    151                                 """

2018-04-30 09:20:36,980 - memory_profile6_log - INFO -    152    446.7 MiB      0.0 MiB       fitby_sigmant = False

2018-04-30 09:20:36,982 - memory_profile6_log - INFO -    153    446.7 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-30 09:20:36,982 - memory_profile6_log - INFO -    154    446.7 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-30 09:20:36,982 - memory_profile6_log - INFO -    155    463.0 MiB     16.3 MiB                            'is_general']]

2018-04-30 09:20:36,986 - memory_profile6_log - INFO -    156                                 # get sigma_Nt from fitted_models_hist

2018-04-30 09:20:36,986 - memory_profile6_log - INFO -    157    463.1 MiB      0.1 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-04-30 09:20:36,987 - memory_profile6_log - INFO -    158    463.1 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-04-30 09:20:36,987 - memory_profile6_log - INFO -    159    463.1 MiB      0.1 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-04-30 09:20:36,989 - memory_profile6_log - INFO -    160    463.1 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-04-30 09:20:36,989 - memory_profile6_log - INFO -    161                             

2018-04-30 09:20:36,990 - memory_profile6_log - INFO -    162                                 # begin fit

2018-04-30 09:20:36,990 - memory_profile6_log - INFO -    163    463.1 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-30 09:20:36,990 - memory_profile6_log - INFO -    164    463.1 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-30 09:20:36,990 - memory_profile6_log - INFO -    165    502.2 MiB     39.1 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-04-30 09:20:36,990 - memory_profile6_log - INFO -    166    502.2 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-30 09:20:36,992 - memory_profile6_log - INFO -    167    502.2 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-30 09:20:36,992 - memory_profile6_log - INFO -    168                                 # print model_fit[['num_x', 'num_x', 'sigma_Nt', 'date_all_click']].loc[model_fit['user_id']=="1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157"].head(5)

2018-04-30 09:20:36,992 - memory_profile6_log - INFO -    169                             

2018-04-30 09:20:36,993 - memory_profile6_log - INFO -    170                                 # ~~ and Transform ~~

2018-04-30 09:20:36,993 - memory_profile6_log - INFO -    171                                 #   handling current news interest == current date

2018-04-30 09:20:36,993 - memory_profile6_log - INFO -    172    502.2 MiB      0.0 MiB       if df_dt.empty:

2018-04-30 09:20:36,994 - memory_profile6_log - INFO -    173                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-30 09:20:36,999 - memory_profile6_log - INFO -    174                                     return None

2018-04-30 09:20:37,000 - memory_profile6_log - INFO -    175    502.9 MiB      0.6 MiB       NB = BR.processX(df_dt)

2018-04-30 09:20:37,000 - memory_profile6_log - INFO -    176    503.0 MiB      0.1 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-30 09:20:37,000 - memory_profile6_log - INFO -    177                             

2018-04-30 09:20:37,000 - memory_profile6_log - INFO -    178    503.0 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-30 09:20:37,000 - memory_profile6_log - INFO -    179    487.9 MiB    -15.1 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-30 09:20:37,002 - memory_profile6_log - INFO -    180                             

2018-04-30 09:20:37,002 - memory_profile6_log - INFO -    181    487.9 MiB      0.0 MiB       print "model_fit dtypes:\n", model_fit.dtypes

2018-04-30 09:20:37,003 - memory_profile6_log - INFO -    182    487.9 MiB      0.0 MiB       print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-04-30 09:20:37,003 - memory_profile6_log - INFO -    183    487.9 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-30 09:20:37,003 - memory_profile6_log - INFO -    184    487.9 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-30 09:20:37,005 - memory_profile6_log - INFO -    185    487.9 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-04-30 09:20:37,005 - memory_profile6_log - INFO -    186    487.9 MiB      0.0 MiB                                                                                           "topic_id", "user_id"]],

2018-04-30 09:20:37,006 - memory_profile6_log - INFO -    187    520.3 MiB     32.4 MiB                                                     verbose=False)

2018-04-30 09:20:37,006 - memory_profile6_log - INFO -    188                                 # ~~~ filter is general and specific topic ~~~

2018-04-30 09:20:37,006 - memory_profile6_log - INFO -    189                                 # the idea is just we need to rerank every topic according

2018-04-30 09:20:37,006 - memory_profile6_log - INFO -    190                                 # user_id and and is_general by p0_posterior

2018-04-30 09:20:37,006 - memory_profile6_log - INFO -    191    520.3 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-30 09:20:37,007 - memory_profile6_log - INFO -    192    523.5 MiB      3.3 MiB                                     'is_general']].groupby(['topic_id',

2018-04-30 09:20:37,012 - memory_profile6_log - INFO -    193    520.3 MiB     -3.2 MiB                                                             'is_general']

2018-04-30 09:20:37,013 - memory_profile6_log - INFO -    194                                                                                      ).size().to_frame().reset_index()

2018-04-30 09:20:37,013 - memory_profile6_log - INFO -    195                             

2018-04-30 09:20:37,013 - memory_profile6_log - INFO -    196                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-30 09:20:37,015 - memory_profile6_log - INFO -    197                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-30 09:20:37,015 - memory_profile6_log - INFO -    198    520.3 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-30 09:20:37,016 - memory_profile6_log - INFO -    199                             

2018-04-30 09:20:37,016 - memory_profile6_log - INFO -    200                                 # ~ start by provide rank for each topic type ~

2018-04-30 09:20:37,016 - memory_profile6_log - INFO -    201    553.2 MiB     32.9 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-30 09:20:37,016 - memory_profile6_log - INFO -    202    559.8 MiB      6.6 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-30 09:20:37,016 - memory_profile6_log - INFO -    203                             

2018-04-30 09:20:37,017 - memory_profile6_log - INFO -    204                                 # ~ set threshold to filter output

2018-04-30 09:20:37,017 - memory_profile6_log - INFO -    205    559.8 MiB      0.0 MiB       if threshold > 0:

2018-04-30 09:20:37,019 - memory_profile6_log - INFO -    206    559.8 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-30 09:20:37,019 - memory_profile6_log - INFO -    207    559.8 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-30 09:20:37,019 - memory_profile6_log - INFO -    208    552.3 MiB     -7.6 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-30 09:20:37,019 - memory_profile6_log - INFO -    209                             

2018-04-30 09:20:37,020 - memory_profile6_log - INFO -    210    552.3 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-30 09:20:37,025 - memory_profile6_log - INFO -    211    552.3 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-30 09:20:37,025 - memory_profile6_log - INFO -    212                             

2018-04-30 09:20:37,026 - memory_profile6_log - INFO -    213    552.3 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-30 09:20:37,026 - memory_profile6_log - INFO -    214                             

2018-04-30 09:20:37,026 - memory_profile6_log - INFO -    215    552.3 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-30 09:20:37,026 - memory_profile6_log - INFO -    216    552.3 MiB      0.0 MiB       del df_dut

2018-04-30 09:20:37,028 - memory_profile6_log - INFO -    217    552.3 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-30 09:20:37,028 - memory_profile6_log - INFO -    218    552.3 MiB      0.0 MiB       del df_dt

2018-04-30 09:20:37,029 - memory_profile6_log - INFO -    219    552.3 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-30 09:20:37,029 - memory_profile6_log - INFO -    220    552.3 MiB      0.0 MiB       del df_input

2018-04-30 09:20:37,029 - memory_profile6_log - INFO -    221    552.3 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-30 09:20:37,030 - memory_profile6_log - INFO -    222    552.3 MiB      0.0 MiB       del df_input_X

2018-04-30 09:20:37,030 - memory_profile6_log - INFO -    223    552.3 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-30 09:20:37,032 - memory_profile6_log - INFO -    224    552.3 MiB      0.0 MiB       del df_current

2018-04-30 09:20:37,032 - memory_profile6_log - INFO -    225    552.3 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-30 09:20:37,032 - memory_profile6_log - INFO -    226    552.3 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-30 09:20:37,032 - memory_profile6_log - INFO -    227    552.3 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-30 09:20:37,032 - memory_profile6_log - INFO -    228    516.4 MiB    -35.9 MiB       del model_fit

2018-04-30 09:20:37,032 - memory_profile6_log - INFO -    229    516.4 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-30 09:20:37,038 - memory_profile6_log - INFO -    230    516.4 MiB      0.0 MiB       del result

2018-04-30 09:20:37,039 - memory_profile6_log - INFO -    231    516.4 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-30 09:20:37,040 - memory_profile6_log - INFO -    232                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 09:20:37,042 - memory_profile6_log - INFO -    233                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 09:20:37,042 - memory_profile6_log - INFO -    234                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 09:20:37,043 - memory_profile6_log - INFO -    235    516.4 MiB      0.0 MiB       if savetrain:

2018-04-30 09:20:37,045 - memory_profile6_log - INFO -    236    523.5 MiB      7.1 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-30 09:20:37,046 - memory_profile6_log - INFO -    237    523.5 MiB      0.0 MiB           del model_transform

2018-04-30 09:20:37,049 - memory_profile6_log - INFO -    238    523.5 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-30 09:20:37,049 - memory_profile6_log - INFO -    239    523.5 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-30 09:20:37,049 - memory_profile6_log - INFO -    240                             

2018-04-30 09:20:37,049 - memory_profile6_log - INFO -    241    523.5 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-30 09:20:37,049 - memory_profile6_log - INFO -    242                                     # ~ Place your code to save the training model here ~

2018-04-30 09:20:37,051 - memory_profile6_log - INFO -    243    523.5 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-30 09:20:37,052 - memory_profile6_log - INFO -    244    523.5 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-30 09:20:37,052 - memory_profile6_log - INFO -    245    523.5 MiB      0.0 MiB               if multproc:

2018-04-30 09:20:37,052 - memory_profile6_log - INFO -    246                                             # ~ save transform models ~

2018-04-30 09:20:37,052 - memory_profile6_log - INFO -    247    523.5 MiB      0.0 MiB                   logger.info("Saving main Transform model to Google DataStore...")

2018-04-30 09:20:37,053 - memory_profile6_log - INFO -    248    488.4 MiB    -35.1 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-30 09:20:37,053 - memory_profile6_log - INFO -    249                             

2018-04-30 09:20:37,055 - memory_profile6_log - INFO -    250                                             # ~ save fitted models ~

2018-04-30 09:20:37,055 - memory_profile6_log - INFO -    251    488.4 MiB      0.0 MiB                   logger.info("Saving fitted_models as history...")

2018-04-30 09:20:37,055 - memory_profile6_log - INFO -    252    488.4 MiB      0.0 MiB                   save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-30 09:20:37,055 - memory_profile6_log - INFO -    253    511.2 MiB     22.9 MiB                   fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-30 09:20:37,056 - memory_profile6_log - INFO -    254                             

2018-04-30 09:20:37,056 - memory_profile6_log - INFO -    255    523.7 MiB     12.5 MiB                   X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-30 09:20:37,056 - memory_profile6_log - INFO -    256    523.7 MiB      0.0 MiB                   logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-30 09:20:37,062 - memory_profile6_log - INFO -    257    524.6 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-30 09:20:37,062 - memory_profile6_log - INFO -    258    524.6 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-30 09:20:37,062 - memory_profile6_log - INFO -    259    524.6 MiB      0.9 MiB                       mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-30 09:20:37,063 - memory_profile6_log - INFO -    260                             

2018-04-30 09:20:37,063 - memory_profile6_log - INFO -    261    520.7 MiB     -3.9 MiB                   del X_split

2018-04-30 09:20:37,063 - memory_profile6_log - INFO -    262    520.7 MiB      0.0 MiB                   logger.info("deleting X_split...")

2018-04-30 09:20:37,065 - memory_profile6_log - INFO -    263    520.7 MiB      0.0 MiB                   del save_sigma_nt

2018-04-30 09:20:37,065 - memory_profile6_log - INFO -    264    520.7 MiB      0.0 MiB                   logger.info("deleting save_sigma_nt...")

2018-04-30 09:20:37,065 - memory_profile6_log - INFO -    265                             

2018-04-30 09:20:37,065 - memory_profile6_log - INFO -    266    520.7 MiB      0.0 MiB                   del BR

2018-04-30 09:20:37,065 - memory_profile6_log - INFO -    267    520.7 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-30 09:20:37,066 - memory_profile6_log - INFO -    268                             

2018-04-30 09:20:37,066 - memory_profile6_log - INFO -    269                                     elif str(saveto).lower() == "elastic":

2018-04-30 09:20:37,068 - memory_profile6_log - INFO -    270                                         logger.info("Using ElasticSearch as storage...")

2018-04-30 09:20:37,068 - memory_profile6_log - INFO -    271                                         logging.info("Saving main Transform model to Elasticsearch...")

2018-04-30 09:20:37,069 - memory_profile6_log - INFO -    272                                         mh.saveElasticS(model_transformsv)

2018-04-30 09:20:37,069 - memory_profile6_log - INFO -    273                             

2018-04-30 09:20:37,073 - memory_profile6_log - INFO -    274                                     # need save sigma_nt for daily train

2018-04-30 09:20:37,073 - memory_profile6_log - INFO -    275                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-30 09:20:37,073 - memory_profile6_log - INFO -    276                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-30 09:20:37,075 - memory_profile6_log - INFO -    277    520.7 MiB      0.0 MiB           if start_date and end_date:

2018-04-30 09:20:37,075 - memory_profile6_log - INFO -    278                                         if not fitby_sigmant:

2018-04-30 09:20:37,075 - memory_profile6_log - INFO -    279                                             logging.info("Saving sigma Nt...")

2018-04-30 09:20:37,075 - memory_profile6_log - INFO -    280                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-30 09:20:37,075 - memory_profile6_log - INFO -    281                                             save_sigma_nt['start_date'] = start_date

2018-04-30 09:20:37,076 - memory_profile6_log - INFO -    282                                             save_sigma_nt['end_date'] = end_date

2018-04-30 09:20:37,076 - memory_profile6_log - INFO -    283                                             print save_sigma_nt.head(5)

2018-04-30 09:20:37,078 - memory_profile6_log - INFO -    284                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-30 09:20:37,078 - memory_profile6_log - INFO -    285                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-30 09:20:37,078 - memory_profile6_log - INFO -    286    520.7 MiB      0.0 MiB       return

2018-04-30 09:20:37,079 - memory_profile6_log - INFO - 


2018-04-30 09:20:37,079 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-30 11:24:50,528 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-30 11:24:50,530 - memory_profile6_log - INFO - date_generated: 
2018-04-30 11:24:50,532 - memory_profile6_log - INFO -  
2018-04-30 11:24:50,532 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 10, 0, 0)]
2018-04-30 11:24:50,532 - memory_profile6_log - INFO - 

2018-04-30 11:24:50,532 - memory_profile6_log - INFO - using current date: 2018-04-11
2018-04-30 11:24:50,532 - memory_profile6_log - INFO - using start date: 2018-04-10 00:00:00
2018-04-30 11:24:50,532 - memory_profile6_log - INFO - using end date: 2018-04-10
2018-04-30 11:24:50,658 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-30 11:24:50,663 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-10
2018-04-30 11:26:49,127 - memory_profile6_log - INFO - size of df: 108.51 MB
2018-04-30 11:26:49,128 - memory_profile6_log - INFO - getting total: 427635 training data(genuine interest) for date: 2018-04-10
2018-04-30 11:26:49,190 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-30 11:26:49,190 - memory_profile6_log - INFO - Appending history data...
2018-04-30 11:26:49,191 - memory_profile6_log - INFO - processing batch-0
2018-04-30 11:26:49,194 - memory_profile6_log - INFO - creating list history data...
2018-04-30 11:26:49,298 - memory_profile6_log - INFO - call history data...
2018-04-30 11:27:39,530 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 11:27:41,164 - memory_profile6_log - INFO - processing batch-1
2018-04-30 11:27:41,167 - memory_profile6_log - INFO - creating list history data...
2018-04-30 11:27:41,240 - memory_profile6_log - INFO - call history data...
2018-04-30 11:28:31,165 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 11:28:32,872 - memory_profile6_log - INFO - processing batch-2
2018-04-30 11:28:32,875 - memory_profile6_log - INFO - creating list history data...
2018-04-30 11:28:32,954 - memory_profile6_log - INFO - call history data...
2018-04-30 11:29:25,144 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 11:29:26,818 - memory_profile6_log - INFO - processing batch-3
2018-04-30 11:29:26,819 - memory_profile6_log - INFO - creating list history data...
2018-04-30 11:29:26,895 - memory_profile6_log - INFO - call history data...
2018-04-30 11:30:18,407 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 11:30:20,084 - memory_profile6_log - INFO - processing batch-4
2018-04-30 11:30:20,085 - memory_profile6_log - INFO - creating list history data...
2018-04-30 11:30:20,155 - memory_profile6_log - INFO - call history data...
2018-04-30 11:31:10,647 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 11:31:12,417 - memory_profile6_log - INFO - Appending training data...
2018-04-30 11:31:12,418 - memory_profile6_log - INFO - len datalist: 1
2018-04-30 11:31:12,421 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-30 11:31:12,434 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 11:31:12,436 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 11:31:12,437 - memory_profile6_log - INFO - ================================================

2018-04-30 11:31:12,438 - memory_profile6_log - INFO -    313     87.0 MiB     87.0 MiB   @profile

2018-04-30 11:31:12,440 - memory_profile6_log - INFO -    314                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-30 11:31:12,440 - memory_profile6_log - INFO -    315     87.0 MiB      0.0 MiB       bq_client = client

2018-04-30 11:31:12,443 - memory_profile6_log - INFO -    316     87.0 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 11:31:12,444 - memory_profile6_log - INFO -    317                             

2018-04-30 11:31:12,444 - memory_profile6_log - INFO -    318     87.0 MiB      0.0 MiB       datalist = []

2018-04-30 11:31:12,446 - memory_profile6_log - INFO -    319     87.0 MiB      0.0 MiB       datalist_hist = []

2018-04-30 11:31:12,446 - memory_profile6_log - INFO -    320                             

2018-04-30 11:31:12,447 - memory_profile6_log - INFO -    321     87.0 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-30 11:31:12,450 - memory_profile6_log - INFO -    322    688.4 MiB      0.0 MiB       for ndate in date_generated:

2018-04-30 11:31:12,451 - memory_profile6_log - INFO -    323    392.9 MiB    305.9 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-30 11:31:12,451 - memory_profile6_log - INFO -    324    392.9 MiB      0.0 MiB           if tframe is not None:

2018-04-30 11:31:12,453 - memory_profile6_log - INFO -    325    392.9 MiB      0.0 MiB               if not tframe.empty:

2018-04-30 11:31:12,453 - memory_profile6_log - INFO -    326    406.4 MiB     13.5 MiB                   X_split = np.array_split(tframe, 5)

2018-04-30 11:31:12,454 - memory_profile6_log - INFO -    327    406.4 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-30 11:31:12,454 - memory_profile6_log - INFO -    328    406.4 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-30 11:31:12,456 - memory_profile6_log - INFO -    329    688.4 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-30 11:31:12,457 - memory_profile6_log - INFO -    330                                                 # ~ loading history

2018-04-30 11:31:12,457 - memory_profile6_log - INFO -    331                                                 """

2018-04-30 11:31:12,460 - memory_profile6_log - INFO -    332                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-30 11:31:12,463 - memory_profile6_log - INFO -    333                                                 """

2018-04-30 11:31:12,463 - memory_profile6_log - INFO -    334    661.9 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-30 11:31:12,463 - memory_profile6_log - INFO -    335                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-30 11:31:12,464 - memory_profile6_log - INFO -    336    661.9 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-30 11:31:12,466 - memory_profile6_log - INFO -    337                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-30 11:31:12,466 - memory_profile6_log - INFO -    338    662.2 MiB      5.5 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-30 11:31:12,467 - memory_profile6_log - INFO -    339                             

2018-04-30 11:31:12,467 - memory_profile6_log - INFO -    340    662.2 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-30 11:31:12,467 - memory_profile6_log - INFO -    341    739.6 MiB    576.6 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-30 11:31:12,470 - memory_profile6_log - INFO -    342                             

2018-04-30 11:31:12,471 - memory_profile6_log - INFO -    343                                                 # me = os.getpid()

2018-04-30 11:31:12,473 - memory_profile6_log - INFO -    344                                                 # kill_proc_tree(me)

2018-04-30 11:31:12,473 - memory_profile6_log - INFO -    345                             

2018-04-30 11:31:12,473 - memory_profile6_log - INFO -    346    739.6 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-30 11:31:12,474 - memory_profile6_log - INFO -    347    741.8 MiB     -1.8 MiB                       for m in h_frame:

2018-04-30 11:31:12,476 - memory_profile6_log - INFO -    348    741.8 MiB     -1.8 MiB                           if m is not None:

2018-04-30 11:31:12,476 - memory_profile6_log - INFO -    349    741.8 MiB     -1.8 MiB                               if len(m) > 0:

2018-04-30 11:31:12,476 - memory_profile6_log - INFO -    350    741.8 MiB      8.1 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-30 11:31:12,477 - memory_profile6_log - INFO -    351    688.4 MiB   -293.0 MiB                       del h_frame

2018-04-30 11:31:12,479 - memory_profile6_log - INFO -    352    688.4 MiB    -17.0 MiB                       del lhistory

2018-04-30 11:31:12,482 - memory_profile6_log - INFO -    353                             

2018-04-30 11:31:12,483 - memory_profile6_log - INFO -    354    688.4 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-30 11:31:12,484 - memory_profile6_log - INFO -    355    688.4 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-30 11:31:12,486 - memory_profile6_log - INFO -    356                                     else: 

2018-04-30 11:31:12,486 - memory_profile6_log - INFO -    357                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-30 11:31:12,486 - memory_profile6_log - INFO -    358    688.4 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-30 11:31:12,489 - memory_profile6_log - INFO -    359    688.4 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-30 11:31:12,490 - memory_profile6_log - INFO -    360                             

2018-04-30 11:31:12,490 - memory_profile6_log - INFO -    361    688.4 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-30 11:31:12,493 - memory_profile6_log - INFO - 


2018-04-30 11:31:13,828 - memory_profile6_log - INFO - size of big_frame_hist: 117.89 MB
2018-04-30 11:31:13,937 - memory_profile6_log - INFO - size of big_frame: 108.51 MB
2018-04-30 11:31:13,957 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-04-30 11:31:40,109 - memory_profile6_log - INFO - size of df: 23.35 MB
2018-04-30 11:31:40,111 - memory_profile6_log - INFO - getting total: 94940 training data(current date interest)
2018-04-30 11:31:40,153 - memory_profile6_log - INFO - size of current_frame: 24.08 MB
2018-04-30 11:31:40,154 - memory_profile6_log - INFO - loading time of: 522575 total genuine-current interest data ~ take 409.517s
2018-04-30 11:31:40,186 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 11:31:40,187 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 11:31:40,187 - memory_profile6_log - INFO - ================================================

2018-04-30 11:31:40,188 - memory_profile6_log - INFO -    363     86.9 MiB     86.9 MiB   @profile

2018-04-30 11:31:40,188 - memory_profile6_log - INFO -    364                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-30 11:31:40,190 - memory_profile6_log - INFO -    365     87.0 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-30 11:31:40,190 - memory_profile6_log - INFO -    366     87.0 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 11:31:40,191 - memory_profile6_log - INFO -    367                             

2018-04-30 11:31:40,191 - memory_profile6_log - INFO -    368                                 # ~~~ Begin collecting data ~~~

2018-04-30 11:31:40,194 - memory_profile6_log - INFO -    369     87.0 MiB      0.0 MiB       t0 = time.time()

2018-04-30 11:31:40,194 - memory_profile6_log - INFO -    370    679.6 MiB    592.6 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-30 11:31:40,194 - memory_profile6_log - INFO -    371    679.6 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-30 11:31:40,198 - memory_profile6_log - INFO -    372                                     logger.info("Training cannot be empty..")

2018-04-30 11:31:40,200 - memory_profile6_log - INFO -    373                                     return False

2018-04-30 11:31:40,201 - memory_profile6_log - INFO -    374    705.8 MiB     26.2 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-30 11:31:40,201 - memory_profile6_log - INFO -    375                                 # print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-30 11:31:40,203 - memory_profile6_log - INFO -    376    705.8 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-30 11:31:40,203 - memory_profile6_log - INFO -    377                             

2018-04-30 11:31:40,203 - memory_profile6_log - INFO -    378    715.8 MiB      9.9 MiB       big_frame = pd.concat(datalist)

2018-04-30 11:31:40,204 - memory_profile6_log - INFO -    379    715.8 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-30 11:31:40,206 - memory_profile6_log - INFO -    380    706.0 MiB     -9.8 MiB       del datalist

2018-04-30 11:31:40,206 - memory_profile6_log - INFO -    381                             

2018-04-30 11:31:40,209 - memory_profile6_log - INFO -    382    706.0 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 11:31:40,210 - memory_profile6_log - INFO -    383                             

2018-04-30 11:31:40,210 - memory_profile6_log - INFO -    384                                 # ~ get current news interest ~

2018-04-30 11:31:40,211 - memory_profile6_log - INFO -    385    706.0 MiB      0.0 MiB       if not cd:

2018-04-30 11:31:40,211 - memory_profile6_log - INFO -    386    706.0 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-04-30 11:31:40,213 - memory_profile6_log - INFO -    387    734.2 MiB     28.2 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-30 11:31:40,213 - memory_profile6_log - INFO -    388    734.2 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-30 11:31:40,213 - memory_profile6_log - INFO -    389                                 else:

2018-04-30 11:31:40,214 - memory_profile6_log - INFO -    390                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-30 11:31:40,214 - memory_profile6_log - INFO -    391                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-30 11:31:40,214 - memory_profile6_log - INFO -    392                             

2018-04-30 11:31:40,216 - memory_profile6_log - INFO -    393                                     # safe handling of query parameter

2018-04-30 11:31:40,216 - memory_profile6_log - INFO -    394                                     query_params = [

2018-04-30 11:31:40,216 - memory_profile6_log - INFO -    395                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-30 11:31:40,217 - memory_profile6_log - INFO -    396                                     ]

2018-04-30 11:31:40,217 - memory_profile6_log - INFO -    397                             

2018-04-30 11:31:40,217 - memory_profile6_log - INFO -    398                                     job_config.query_parameters = query_params

2018-04-30 11:31:40,220 - memory_profile6_log - INFO -    399                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-30 11:31:40,220 - memory_profile6_log - INFO -    400                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-30 11:31:40,223 - memory_profile6_log - INFO -    401                             

2018-04-30 11:31:40,223 - memory_profile6_log - INFO -    402    734.2 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-30 11:31:40,223 - memory_profile6_log - INFO -    403    734.2 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-30 11:31:40,224 - memory_profile6_log - INFO -    404    735.2 MiB      1.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-30 11:31:40,226 - memory_profile6_log - INFO -    405    735.2 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-30 11:31:40,226 - memory_profile6_log - INFO -    406    735.2 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-30 11:31:40,227 - memory_profile6_log - INFO -    407    735.2 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-30 11:31:40,227 - memory_profile6_log - INFO -    408                             

2018-04-30 11:31:40,229 - memory_profile6_log - INFO -    409    735.2 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-30 11:31:40,229 - memory_profile6_log - INFO - 


2018-04-30 11:31:40,236 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-30 11:31:40,266 - memory_profile6_log - INFO - train on: 427635 total genuine interest data(D(u, t))
2018-04-30 11:31:40,266 - memory_profile6_log - INFO - transform on: 94940 total current data(D(t))
2018-04-30 11:31:40,267 - memory_profile6_log - INFO - apply on: 427635 total history...)
2018-04-30 11:31:40,642 - memory_profile6_log - INFO - len of uniques_fit_hist:427635
2018-04-30 11:31:40,907 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:52258
2018-04-30 11:31:42,170 - memory_profile6_log - INFO - Len of model_fit: 427635
2018-04-30 11:31:42,171 - memory_profile6_log - INFO - Len of df_dut: 427635
2018-04-30 11:31:42,963 - memory_profile6_log - INFO - Len of fitted_models on main class: 427635
2018-04-30 11:31:42,964 - memory_profile6_log - INFO - 

2018-04-30 11:31:42,966 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-30 11:31:42,967 - memory_profile6_log - INFO - 

2018-04-30 11:31:43,046 - memory_profile6_log - INFO - len of fitted models after concat: 855270
2018-04-30 11:31:43,049 - memory_profile6_log - INFO - 

2018-04-30 11:31:43,049 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-30 11:31:43,051 - memory_profile6_log - INFO - 

2018-04-30 11:31:43,688 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 427635
2018-04-30 11:31:43,690 - memory_profile6_log - INFO - 

2018-04-30 11:32:43,444 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-30 11:32:43,484 - memory_profile6_log - INFO - Total train time: 63.219s
2018-04-30 11:32:43,486 - memory_profile6_log - INFO - memory left before cleaning: 78.400 percent memory...
2018-04-30 11:32:43,487 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-30 11:32:43,489 - memory_profile6_log - INFO - deleting df_dut...
2018-04-30 11:32:43,490 - memory_profile6_log - INFO - deleting df_dt...
2018-04-30 11:32:43,492 - memory_profile6_log - INFO - deleting df_input...
2018-04-30 11:32:43,496 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-30 11:32:43,497 - memory_profile6_log - INFO - deleting df_current...
2018-04-30 11:32:43,500 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-30 11:32:43,516 - memory_profile6_log - INFO - deleting model_fit...
2018-04-30 11:32:43,517 - memory_profile6_log - INFO - deleting result...
2018-04-30 11:32:43,543 - memory_profile6_log - INFO - deleting model_transform...
2018-04-30 11:32:43,545 - memory_profile6_log - INFO - memory left after cleaning: 78.100 percent memory...
2018-04-30 11:32:43,546 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-30 11:32:43,548 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-30 11:32:43,549 - memory_profile6_log - INFO - Saving main Transform model to Google DataStore...
2018-04-30 11:32:43,549 - memory_profile6_log - INFO - Saving total data: 340217
2018-04-30 11:32:43,723 - memory_profile6_log - INFO - Saving fitted_models as history to Google DataStore...
2018-04-30 11:32:43,815 - memory_profile6_log - INFO - Saving total data: 427635
2018-04-30 11:32:43,858 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 10
2018-04-30 11:32:43,858 - memory_profile6_log - INFO - processing batch-0
2018-04-30 11:32:44,065 - memory_profile6_log - INFO - processing batch-1
2018-04-30 11:32:44,259 - memory_profile6_log - INFO - processing batch-2
2018-04-30 11:32:44,453 - memory_profile6_log - INFO - processing batch-3
2018-04-30 11:32:44,642 - memory_profile6_log - INFO - processing batch-4
2018-04-30 11:32:44,841 - memory_profile6_log - INFO - processing batch-5
2018-04-30 11:32:45,035 - memory_profile6_log - INFO - processing batch-6
2018-04-30 11:32:45,223 - memory_profile6_log - INFO - processing batch-7
2018-04-30 11:32:45,417 - memory_profile6_log - INFO - processing batch-8
2018-04-30 11:32:45,611 - memory_profile6_log - INFO - processing batch-9
2018-04-30 11:32:45,819 - memory_profile6_log - INFO - deleting X_split...
2018-04-30 11:32:45,822 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-04-30 11:32:45,823 - memory_profile6_log - INFO - deleting BR...
2018-04-30 11:32:45,851 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 11:32:45,851 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 11:32:45,852 - memory_profile6_log - INFO - ================================================

2018-04-30 11:32:45,852 - memory_profile6_log - INFO -    113    663.8 MiB    663.8 MiB   @profile

2018-04-30 11:32:45,854 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-30 11:32:45,854 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-30 11:32:45,854 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-30 11:32:45,855 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-30 11:32:45,855 - memory_profile6_log - INFO -    118                                      saveto="datastore", fitted_models_hist=None):

2018-04-30 11:32:45,855 - memory_profile6_log - INFO -    119                                 """

2018-04-30 11:32:45,855 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-30 11:32:45,855 - memory_profile6_log - INFO -    121                                 """

2018-04-30 11:32:45,857 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-30 11:32:45,857 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-30 11:32:45,858 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-30 11:32:45,858 - memory_profile6_log - INFO -    125    663.8 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-30 11:32:45,858 - memory_profile6_log - INFO -    126    677.3 MiB     13.5 MiB       df_dut = df_input.copy(deep=True)

2018-04-30 11:32:45,858 - memory_profile6_log - INFO -    127    677.3 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 11:32:45,858 - memory_profile6_log - INFO -    128                             

2018-04-30 11:32:45,859 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-30 11:32:45,859 - memory_profile6_log - INFO -    130    679.8 MiB      2.4 MiB       df_dt = df_current.copy(deep=True)

2018-04-30 11:32:45,859 - memory_profile6_log - INFO -    131    680.4 MiB      0.6 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 11:32:45,864 - memory_profile6_log - INFO -    132                             

2018-04-30 11:32:45,865 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-30 11:32:45,865 - memory_profile6_log - INFO -    134    680.4 MiB      0.0 MiB       t0 = time.time()

2018-04-30 11:32:45,867 - memory_profile6_log - INFO -    135    680.4 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-30 11:32:45,867 - memory_profile6_log - INFO -    136    680.4 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-30 11:32:45,867 - memory_profile6_log - INFO -    137    680.4 MiB      0.0 MiB       logger.info("apply on: %d total history...)", len(df_hist))

2018-04-30 11:32:45,868 - memory_profile6_log - INFO -    138                             

2018-04-30 11:32:45,868 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-30 11:32:45,868 - memory_profile6_log - INFO -    140    680.4 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-30 11:32:45,868 - memory_profile6_log - INFO -    141                             

2018-04-30 11:32:45,871 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-30 11:32:45,871 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-30 11:32:45,875 - memory_profile6_log - INFO -    144    682.9 MiB      2.5 MiB       NB = BR.processX(df_dut)

2018-04-30 11:32:45,877 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-30 11:32:45,878 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-30 11:32:45,878 - memory_profile6_log - INFO -    147    699.4 MiB     16.5 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-30 11:32:45,878 - memory_profile6_log - INFO -    148                                 """

2018-04-30 11:32:45,878 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-30 11:32:45,878 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-30 11:32:45,880 - memory_profile6_log - INFO -    151                                 """

2018-04-30 11:32:45,880 - memory_profile6_log - INFO -    152    699.4 MiB      0.0 MiB       fitby_sigmant = False

2018-04-30 11:32:45,881 - memory_profile6_log - INFO -    153    699.4 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-30 11:32:45,881 - memory_profile6_log - INFO -    154    699.4 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-30 11:32:45,881 - memory_profile6_log - INFO -    155    715.7 MiB     16.3 MiB                            'is_general']]

2018-04-30 11:32:45,881 - memory_profile6_log - INFO -    156                                 # get sigma_Nt from fitted_models_hist

2018-04-30 11:32:45,882 - memory_profile6_log - INFO -    157    722.2 MiB      6.5 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-04-30 11:32:45,882 - memory_profile6_log - INFO -    158    722.2 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-04-30 11:32:45,882 - memory_profile6_log - INFO -    159    718.7 MiB     -3.6 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-04-30 11:32:45,884 - memory_profile6_log - INFO -    160    718.7 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-04-30 11:32:45,884 - memory_profile6_log - INFO -    161                             

2018-04-30 11:32:45,888 - memory_profile6_log - INFO -    162                                 # begin fit

2018-04-30 11:32:45,888 - memory_profile6_log - INFO -    163    718.7 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-30 11:32:45,890 - memory_profile6_log - INFO -    164    718.7 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-30 11:32:45,890 - memory_profile6_log - INFO -    165    761.1 MiB     42.5 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-04-30 11:32:45,890 - memory_profile6_log - INFO -    166    761.1 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-30 11:32:45,891 - memory_profile6_log - INFO -    167    761.1 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-30 11:32:45,891 - memory_profile6_log - INFO -    168                                 # print model_fit[['num_x', 'num_x', 'sigma_Nt', 'date_all_click']].loc[model_fit['user_id']=="1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157"].head(5)

2018-04-30 11:32:45,891 - memory_profile6_log - INFO -    169                             

2018-04-30 11:32:45,891 - memory_profile6_log - INFO -    170                                 # ~~ and Transform ~~

2018-04-30 11:32:45,892 - memory_profile6_log - INFO -    171                                 #   handling current news interest == current date

2018-04-30 11:32:45,892 - memory_profile6_log - INFO -    172    761.1 MiB      0.0 MiB       if df_dt.empty:

2018-04-30 11:32:45,892 - memory_profile6_log - INFO -    173                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-30 11:32:45,894 - memory_profile6_log - INFO -    174                                     return None

2018-04-30 11:32:45,894 - memory_profile6_log - INFO -    175    765.2 MiB      4.1 MiB       NB = BR.processX(df_dt)

2018-04-30 11:32:45,894 - memory_profile6_log - INFO -    176    766.6 MiB      1.4 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-30 11:32:45,894 - memory_profile6_log - INFO -    177                             

2018-04-30 11:32:45,895 - memory_profile6_log - INFO -    178    766.6 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-30 11:32:45,895 - memory_profile6_log - INFO -    179    754.6 MiB    -12.0 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-30 11:32:45,895 - memory_profile6_log - INFO -    180                             

2018-04-30 11:32:45,901 - memory_profile6_log - INFO -    181                                 # print "model_fit dtypes:\n", model_fit.dtypes

2018-04-30 11:32:45,901 - memory_profile6_log - INFO -    182                                 # print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-04-30 11:32:45,901 - memory_profile6_log - INFO -    183    754.6 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-30 11:32:45,903 - memory_profile6_log - INFO -    184    754.6 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-30 11:32:45,903 - memory_profile6_log - INFO -    185    754.6 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-04-30 11:32:45,904 - memory_profile6_log - INFO -    186    764.4 MiB      9.8 MiB                                                                                           "topic_id", "user_id"]],

2018-04-30 11:32:45,904 - memory_profile6_log - INFO -    187    804.4 MiB     40.0 MiB                                                     verbose=False)

2018-04-30 11:32:45,904 - memory_profile6_log - INFO -    188                                 # ~~~ filter is general and specific topic ~~~

2018-04-30 11:32:45,904 - memory_profile6_log - INFO -    189                                 # the idea is just we need to rerank every topic according

2018-04-30 11:32:45,905 - memory_profile6_log - INFO -    190                                 # user_id and and is_general by p0_posterior

2018-04-30 11:32:45,907 - memory_profile6_log - INFO -    191    804.4 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-30 11:32:45,907 - memory_profile6_log - INFO -    192    807.7 MiB      3.3 MiB                                     'is_general']].groupby(['topic_id',

2018-04-30 11:32:45,907 - memory_profile6_log - INFO -    193    804.4 MiB     -3.2 MiB                                                             'is_general']

2018-04-30 11:32:45,911 - memory_profile6_log - INFO -    194                                                                                      ).size().to_frame().reset_index()

2018-04-30 11:32:45,913 - memory_profile6_log - INFO -    195                             

2018-04-30 11:32:45,914 - memory_profile6_log - INFO -    196                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-30 11:32:45,914 - memory_profile6_log - INFO -    197                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-30 11:32:45,914 - memory_profile6_log - INFO -    198    804.5 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-30 11:32:45,914 - memory_profile6_log - INFO -    199                             

2018-04-30 11:32:45,914 - memory_profile6_log - INFO -    200                                 # ~ start by provide rank for each topic type ~

2018-04-30 11:32:45,915 - memory_profile6_log - INFO -    201    814.3 MiB      9.9 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-30 11:32:45,915 - memory_profile6_log - INFO -    202    822.5 MiB      8.2 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-30 11:32:45,917 - memory_profile6_log - INFO -    203                             

2018-04-30 11:32:45,917 - memory_profile6_log - INFO -    204                                 # ~ set threshold to filter output

2018-04-30 11:32:45,917 - memory_profile6_log - INFO -    205    822.5 MiB      0.0 MiB       if threshold > 0:

2018-04-30 11:32:45,917 - memory_profile6_log - INFO -    206    822.5 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-30 11:32:45,917 - memory_profile6_log - INFO -    207    822.5 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-30 11:32:45,918 - memory_profile6_log - INFO -    208    818.2 MiB     -4.3 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-30 11:32:45,918 - memory_profile6_log - INFO -    209                             

2018-04-30 11:32:45,920 - memory_profile6_log - INFO -    210    818.2 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-30 11:32:45,920 - memory_profile6_log - INFO -    211    818.2 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-30 11:32:45,920 - memory_profile6_log - INFO -    212                             

2018-04-30 11:32:45,924 - memory_profile6_log - INFO -    213    818.2 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-30 11:32:45,926 - memory_profile6_log - INFO -    214                             

2018-04-30 11:32:45,927 - memory_profile6_log - INFO -    215    818.2 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-30 11:32:45,927 - memory_profile6_log - INFO -    216    818.2 MiB      0.0 MiB       del df_dut

2018-04-30 11:32:45,927 - memory_profile6_log - INFO -    217    818.2 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-30 11:32:45,927 - memory_profile6_log - INFO -    218    818.2 MiB      0.0 MiB       del df_dt

2018-04-30 11:32:45,928 - memory_profile6_log - INFO -    219    818.2 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-30 11:32:45,930 - memory_profile6_log - INFO -    220    818.2 MiB      0.0 MiB       del df_input

2018-04-30 11:32:45,930 - memory_profile6_log - INFO -    221    818.2 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-30 11:32:45,930 - memory_profile6_log - INFO -    222    815.3 MiB     -2.9 MiB       del df_input_X

2018-04-30 11:32:45,930 - memory_profile6_log - INFO -    223    815.3 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-30 11:32:45,930 - memory_profile6_log - INFO -    224    815.3 MiB      0.0 MiB       del df_current

2018-04-30 11:32:45,931 - memory_profile6_log - INFO -    225    815.3 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-30 11:32:45,931 - memory_profile6_log - INFO -    226    815.3 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-30 11:32:45,936 - memory_profile6_log - INFO -    227    815.3 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-30 11:32:45,937 - memory_profile6_log - INFO -    228    779.4 MiB    -35.9 MiB       del model_fit

2018-04-30 11:32:45,937 - memory_profile6_log - INFO -    229    779.4 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-30 11:32:45,937 - memory_profile6_log - INFO -    230    779.4 MiB      0.0 MiB       del result

2018-04-30 11:32:45,937 - memory_profile6_log - INFO -    231    779.4 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-30 11:32:45,938 - memory_profile6_log - INFO -    232                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 11:32:45,940 - memory_profile6_log - INFO -    233                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 11:32:45,940 - memory_profile6_log - INFO -    234                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 11:32:45,940 - memory_profile6_log - INFO -    235    779.4 MiB      0.0 MiB       if savetrain:

2018-04-30 11:32:45,940 - memory_profile6_log - INFO -    236    787.2 MiB      7.8 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-30 11:32:45,941 - memory_profile6_log - INFO -    237    787.2 MiB      0.0 MiB           del model_transform

2018-04-30 11:32:45,941 - memory_profile6_log - INFO -    238    787.2 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-30 11:32:45,943 - memory_profile6_log - INFO -    239    787.2 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-30 11:32:45,943 - memory_profile6_log - INFO -    240                             

2018-04-30 11:32:45,943 - memory_profile6_log - INFO -    241    787.2 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-30 11:32:45,947 - memory_profile6_log - INFO -    242                                     # ~ Place your code to save the training model here ~

2018-04-30 11:32:45,947 - memory_profile6_log - INFO -    243    787.2 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-30 11:32:45,947 - memory_profile6_log - INFO -    244    787.2 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-30 11:32:45,948 - memory_profile6_log - INFO -    245    787.2 MiB      0.0 MiB               if multproc:

2018-04-30 11:32:45,948 - memory_profile6_log - INFO -    246                                             # ~ save transform models ~

2018-04-30 11:32:45,950 - memory_profile6_log - INFO -    247    787.2 MiB      0.0 MiB                   logger.info("Saving main Transform model to Google DataStore...")

2018-04-30 11:32:45,950 - memory_profile6_log - INFO -    248    787.2 MiB      0.0 MiB                   logger.info("Saving total data: %d", len(model_transformsv))

2018-04-30 11:32:45,950 - memory_profile6_log - INFO -    249    752.2 MiB    -35.0 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-30 11:32:45,950 - memory_profile6_log - INFO -    250                             

2018-04-30 11:32:45,950 - memory_profile6_log - INFO -    251                                             # ~ save fitted models ~

2018-04-30 11:32:45,951 - memory_profile6_log - INFO -    252    752.2 MiB      0.0 MiB                   logger.info("Saving fitted_models as history to Google DataStore...")

2018-04-30 11:32:45,951 - memory_profile6_log - INFO -    253    752.5 MiB      0.3 MiB                   save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-30 11:32:45,953 - memory_profile6_log - INFO -    254    776.9 MiB     24.4 MiB                   fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-30 11:32:45,953 - memory_profile6_log - INFO -    255    776.9 MiB      0.0 MiB                   logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-04-30 11:32:45,953 - memory_profile6_log - INFO -    256    790.1 MiB     13.2 MiB                   X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-30 11:32:45,953 - memory_profile6_log - INFO -    257    790.1 MiB      0.0 MiB                   logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-30 11:32:45,953 - memory_profile6_log - INFO -    258    791.6 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-30 11:32:45,957 - memory_profile6_log - INFO -    259    791.6 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-30 11:32:45,957 - memory_profile6_log - INFO -    260    791.6 MiB      1.5 MiB                       mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-30 11:32:45,959 - memory_profile6_log - INFO -    261                             

2018-04-30 11:32:45,960 - memory_profile6_log - INFO -    262    786.4 MiB     -5.2 MiB                   del X_split

2018-04-30 11:32:45,960 - memory_profile6_log - INFO -    263    786.4 MiB      0.0 MiB                   logger.info("deleting X_split...")

2018-04-30 11:32:45,960 - memory_profile6_log - INFO -    264    786.4 MiB      0.0 MiB                   del save_sigma_nt

2018-04-30 11:32:45,960 - memory_profile6_log - INFO -    265    786.4 MiB      0.0 MiB                   logger.info("deleting save_sigma_nt...")

2018-04-30 11:32:45,960 - memory_profile6_log - INFO -    266                             

2018-04-30 11:32:45,961 - memory_profile6_log - INFO -    267    786.4 MiB      0.0 MiB                   del BR

2018-04-30 11:32:45,961 - memory_profile6_log - INFO -    268    786.4 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-30 11:32:45,963 - memory_profile6_log - INFO -    269                             

2018-04-30 11:32:45,963 - memory_profile6_log - INFO -    270                                     elif str(saveto).lower() == "elastic":

2018-04-30 11:32:45,963 - memory_profile6_log - INFO -    271                                         logger.info("Using ElasticSearch as storage...")

2018-04-30 11:32:45,963 - memory_profile6_log - INFO -    272                                         logging.info("Saving main Transform model to Elasticsearch...")

2018-04-30 11:32:45,963 - memory_profile6_log - INFO -    273                                         mh.saveElasticS(model_transformsv)

2018-04-30 11:32:45,964 - memory_profile6_log - INFO -    274                             

2018-04-30 11:32:45,964 - memory_profile6_log - INFO -    275                                     # need save sigma_nt for daily train

2018-04-30 11:32:45,966 - memory_profile6_log - INFO -    276                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-30 11:32:45,966 - memory_profile6_log - INFO -    277                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-30 11:32:45,966 - memory_profile6_log - INFO -    278    786.4 MiB      0.0 MiB           if start_date and end_date:

2018-04-30 11:32:45,970 - memory_profile6_log - INFO -    279                                         if not fitby_sigmant:

2018-04-30 11:32:45,970 - memory_profile6_log - INFO -    280                                             logging.info("Saving sigma Nt...")

2018-04-30 11:32:45,971 - memory_profile6_log - INFO -    281                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-30 11:32:45,973 - memory_profile6_log - INFO -    282                                             save_sigma_nt['start_date'] = start_date

2018-04-30 11:32:45,973 - memory_profile6_log - INFO -    283                                             save_sigma_nt['end_date'] = end_date

2018-04-30 11:32:45,974 - memory_profile6_log - INFO -    284                                             print save_sigma_nt.head(5)

2018-04-30 11:32:45,974 - memory_profile6_log - INFO -    285                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-30 11:32:45,976 - memory_profile6_log - INFO -    286                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-30 11:32:45,976 - memory_profile6_log - INFO -    287    786.4 MiB      0.0 MiB       return

2018-04-30 11:32:45,976 - memory_profile6_log - INFO - 


2018-04-30 11:32:45,976 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-30 11:36:07,131 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-30 11:36:07,134 - memory_profile6_log - INFO - date_generated: 
2018-04-30 11:36:07,135 - memory_profile6_log - INFO -  
2018-04-30 11:36:07,135 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 10, 0, 0)]
2018-04-30 11:36:07,135 - memory_profile6_log - INFO - 

2018-04-30 11:36:07,137 - memory_profile6_log - INFO - using current date: 2018-04-11
2018-04-30 11:36:07,137 - memory_profile6_log - INFO - using start date: 2018-04-10 00:00:00
2018-04-30 11:36:07,137 - memory_profile6_log - INFO - using end date: 2018-04-10
2018-04-30 11:36:07,286 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-30 11:36:07,292 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-10
2018-04-30 11:38:02,815 - memory_profile6_log - INFO - size of df: 108.51 MB
2018-04-30 11:38:02,816 - memory_profile6_log - INFO - getting total: 427635 training data(genuine interest) for date: 2018-04-10
2018-04-30 11:38:02,881 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-30 11:38:02,882 - memory_profile6_log - INFO - Appending history data...
2018-04-30 11:38:02,884 - memory_profile6_log - INFO - processing batch-0
2018-04-30 11:38:02,884 - memory_profile6_log - INFO - creating list history data...
2018-04-30 11:38:02,993 - memory_profile6_log - INFO - call history data...
2018-04-30 11:38:54,013 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 11:38:55,691 - memory_profile6_log - INFO - processing batch-1
2018-04-30 11:38:55,694 - memory_profile6_log - INFO - creating list history data...
2018-04-30 11:38:55,770 - memory_profile6_log - INFO - call history data...
2018-04-30 11:39:49,144 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 11:39:50,882 - memory_profile6_log - INFO - processing batch-2
2018-04-30 11:39:50,884 - memory_profile6_log - INFO - creating list history data...
2018-04-30 11:39:50,963 - memory_profile6_log - INFO - call history data...
2018-04-30 11:40:40,236 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 11:40:41,957 - memory_profile6_log - INFO - processing batch-3
2018-04-30 11:40:41,960 - memory_profile6_log - INFO - creating list history data...
2018-04-30 11:40:42,039 - memory_profile6_log - INFO - call history data...
2018-04-30 11:41:29,654 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 11:41:31,351 - memory_profile6_log - INFO - processing batch-4
2018-04-30 11:41:31,352 - memory_profile6_log - INFO - creating list history data...
2018-04-30 11:41:31,437 - memory_profile6_log - INFO - call history data...
2018-04-30 11:42:21,112 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 11:42:23,059 - memory_profile6_log - INFO - Appending training data...
2018-04-30 11:42:23,059 - memory_profile6_log - INFO - len datalist: 1
2018-04-30 11:42:23,062 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-30 11:42:23,078 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 11:42:23,082 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 11:42:23,082 - memory_profile6_log - INFO - ================================================

2018-04-30 11:42:23,084 - memory_profile6_log - INFO -    315     86.7 MiB     86.7 MiB   @profile

2018-04-30 11:42:23,085 - memory_profile6_log - INFO -    316                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-30 11:42:23,085 - memory_profile6_log - INFO -    317     86.7 MiB      0.0 MiB       bq_client = client

2018-04-30 11:42:23,086 - memory_profile6_log - INFO -    318     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 11:42:23,088 - memory_profile6_log - INFO -    319                             

2018-04-30 11:42:23,088 - memory_profile6_log - INFO -    320     86.7 MiB      0.0 MiB       datalist = []

2018-04-30 11:42:23,088 - memory_profile6_log - INFO -    321     86.7 MiB      0.0 MiB       datalist_hist = []

2018-04-30 11:42:23,092 - memory_profile6_log - INFO -    322                             

2018-04-30 11:42:23,092 - memory_profile6_log - INFO -    323     86.7 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-30 11:42:23,094 - memory_profile6_log - INFO -    324    687.9 MiB      0.0 MiB       for ndate in date_generated:

2018-04-30 11:42:23,095 - memory_profile6_log - INFO -    325    393.7 MiB    306.9 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-30 11:42:23,095 - memory_profile6_log - INFO -    326    393.7 MiB      0.0 MiB           if tframe is not None:

2018-04-30 11:42:23,096 - memory_profile6_log - INFO -    327    393.7 MiB      0.0 MiB               if not tframe.empty:

2018-04-30 11:42:23,098 - memory_profile6_log - INFO -    328    407.6 MiB     13.9 MiB                   X_split = np.array_split(tframe, 5)

2018-04-30 11:42:23,098 - memory_profile6_log - INFO -    329    407.6 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-30 11:42:23,098 - memory_profile6_log - INFO -    330    407.6 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-30 11:42:23,099 - memory_profile6_log - INFO -    331    687.9 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-30 11:42:23,102 - memory_profile6_log - INFO -    332                                                 # ~ loading history

2018-04-30 11:42:23,105 - memory_profile6_log - INFO -    333                                                 """

2018-04-30 11:42:23,105 - memory_profile6_log - INFO -    334                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-30 11:42:23,105 - memory_profile6_log - INFO -    335                                                 """

2018-04-30 11:42:23,107 - memory_profile6_log - INFO -    336    660.6 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-30 11:42:23,108 - memory_profile6_log - INFO -    337                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-30 11:42:23,108 - memory_profile6_log - INFO -    338    660.6 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-30 11:42:23,108 - memory_profile6_log - INFO -    339                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-30 11:42:23,109 - memory_profile6_log - INFO -    340    662.7 MiB      6.2 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-30 11:42:23,109 - memory_profile6_log - INFO -    341                             

2018-04-30 11:42:23,111 - memory_profile6_log - INFO -    342    662.7 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-30 11:42:23,111 - memory_profile6_log - INFO -    343    740.5 MiB    560.8 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-30 11:42:23,115 - memory_profile6_log - INFO -    344                             

2018-04-30 11:42:23,115 - memory_profile6_log - INFO -    345                                                 # me = os.getpid()

2018-04-30 11:42:23,117 - memory_profile6_log - INFO -    346                                                 # kill_proc_tree(me)

2018-04-30 11:42:23,117 - memory_profile6_log - INFO -    347                             

2018-04-30 11:42:23,118 - memory_profile6_log - INFO -    348    740.5 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-30 11:42:23,118 - memory_profile6_log - INFO -    349    742.4 MiB     -1.6 MiB                       for m in h_frame:

2018-04-30 11:42:23,118 - memory_profile6_log - INFO -    350    742.4 MiB     -1.6 MiB                           if m is not None:

2018-04-30 11:42:23,119 - memory_profile6_log - INFO -    351    742.4 MiB     -1.6 MiB                               if len(m) > 0:

2018-04-30 11:42:23,121 - memory_profile6_log - INFO -    352    742.4 MiB      8.0 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-30 11:42:23,121 - memory_profile6_log - INFO -    353    688.8 MiB   -293.4 MiB                       del h_frame

2018-04-30 11:42:23,121 - memory_profile6_log - INFO -    354    687.9 MiB     -2.8 MiB                       del lhistory

2018-04-30 11:42:23,122 - memory_profile6_log - INFO -    355                             

2018-04-30 11:42:23,122 - memory_profile6_log - INFO -    356    687.9 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-30 11:42:23,127 - memory_profile6_log - INFO -    357    687.9 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-30 11:42:23,127 - memory_profile6_log - INFO -    358                                     else: 

2018-04-30 11:42:23,128 - memory_profile6_log - INFO -    359                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-30 11:42:23,128 - memory_profile6_log - INFO -    360    687.9 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-30 11:42:23,130 - memory_profile6_log - INFO -    361    687.9 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-30 11:42:23,130 - memory_profile6_log - INFO -    362                             

2018-04-30 11:42:23,131 - memory_profile6_log - INFO -    363    687.9 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-30 11:42:23,131 - memory_profile6_log - INFO - 


2018-04-30 11:42:24,536 - memory_profile6_log - INFO - size of big_frame_hist: 117.89 MB
2018-04-30 11:42:24,655 - memory_profile6_log - INFO - size of big_frame: 108.51 MB
2018-04-30 11:42:24,674 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-04-30 11:42:44,963 - memory_profile6_log - INFO - size of df: 23.35 MB
2018-04-30 11:42:44,963 - memory_profile6_log - INFO - getting total: 94940 training data(current date interest)
2018-04-30 11:42:45,019 - memory_profile6_log - INFO - size of current_frame: 24.08 MB
2018-04-30 11:42:45,023 - memory_profile6_log - INFO - loading time of: 522575 total genuine-current interest data ~ take 397.763s
2018-04-30 11:42:45,059 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 11:42:45,062 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 11:42:45,062 - memory_profile6_log - INFO - ================================================

2018-04-30 11:42:45,063 - memory_profile6_log - INFO -    365     86.6 MiB     86.6 MiB   @profile

2018-04-30 11:42:45,065 - memory_profile6_log - INFO -    366                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-30 11:42:45,065 - memory_profile6_log - INFO -    367     86.7 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-30 11:42:45,066 - memory_profile6_log - INFO -    368     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 11:42:45,066 - memory_profile6_log - INFO -    369                             

2018-04-30 11:42:45,068 - memory_profile6_log - INFO -    370                                 # ~~~ Begin collecting data ~~~

2018-04-30 11:42:45,068 - memory_profile6_log - INFO -    371     86.7 MiB      0.0 MiB       t0 = time.time()

2018-04-30 11:42:45,069 - memory_profile6_log - INFO -    372    677.7 MiB    591.0 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-30 11:42:45,069 - memory_profile6_log - INFO -    373    677.7 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-30 11:42:45,072 - memory_profile6_log - INFO -    374                                     logger.info("Training cannot be empty..")

2018-04-30 11:42:45,073 - memory_profile6_log - INFO -    375                                     return False

2018-04-30 11:42:45,073 - memory_profile6_log - INFO -    376    702.5 MiB     24.8 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-30 11:42:45,075 - memory_profile6_log - INFO -    377                                 # print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-30 11:42:45,075 - memory_profile6_log - INFO -    378    702.5 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-30 11:42:45,075 - memory_profile6_log - INFO -    379                             

2018-04-30 11:42:45,076 - memory_profile6_log - INFO -    380    712.7 MiB     10.2 MiB       big_frame = pd.concat(datalist)

2018-04-30 11:42:45,076 - memory_profile6_log - INFO -    381    712.7 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-30 11:42:45,078 - memory_profile6_log - INFO -    382    702.9 MiB     -9.8 MiB       del datalist

2018-04-30 11:42:45,078 - memory_profile6_log - INFO -    383                             

2018-04-30 11:42:45,078 - memory_profile6_log - INFO -    384    702.9 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 11:42:45,078 - memory_profile6_log - INFO -    385                             

2018-04-30 11:42:45,079 - memory_profile6_log - INFO -    386                                 # ~ get current news interest ~

2018-04-30 11:42:45,079 - memory_profile6_log - INFO -    387    702.9 MiB      0.0 MiB       if not cd:

2018-04-30 11:42:45,084 - memory_profile6_log - INFO -    388    702.9 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-04-30 11:42:45,085 - memory_profile6_log - INFO -    389    734.2 MiB     31.2 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-30 11:42:45,085 - memory_profile6_log - INFO -    390    734.2 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-30 11:42:45,085 - memory_profile6_log - INFO -    391                                 else:

2018-04-30 11:42:45,086 - memory_profile6_log - INFO -    392                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-30 11:42:45,086 - memory_profile6_log - INFO -    393                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-30 11:42:45,088 - memory_profile6_log - INFO -    394                             

2018-04-30 11:42:45,088 - memory_profile6_log - INFO -    395                                     # safe handling of query parameter

2018-04-30 11:42:45,089 - memory_profile6_log - INFO -    396                                     query_params = [

2018-04-30 11:42:45,089 - memory_profile6_log - INFO -    397                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-30 11:42:45,091 - memory_profile6_log - INFO -    398                                     ]

2018-04-30 11:42:45,091 - memory_profile6_log - INFO -    399                             

2018-04-30 11:42:45,092 - memory_profile6_log - INFO -    400                                     job_config.query_parameters = query_params

2018-04-30 11:42:45,092 - memory_profile6_log - INFO -    401                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-30 11:42:45,095 - memory_profile6_log - INFO -    402                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-30 11:42:45,095 - memory_profile6_log - INFO -    403                             

2018-04-30 11:42:45,096 - memory_profile6_log - INFO -    404    734.2 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-30 11:42:45,096 - memory_profile6_log - INFO -    405    734.2 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-30 11:42:45,096 - memory_profile6_log - INFO -    406    735.6 MiB      1.5 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-30 11:42:45,098 - memory_profile6_log - INFO -    407    735.6 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-30 11:42:45,098 - memory_profile6_log - INFO -    408    735.6 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-30 11:42:45,098 - memory_profile6_log - INFO -    409    735.6 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-30 11:42:45,099 - memory_profile6_log - INFO -    410                             

2018-04-30 11:42:45,099 - memory_profile6_log - INFO -    411    735.6 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-30 11:42:45,101 - memory_profile6_log - INFO - 


2018-04-30 11:42:45,104 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-30 11:42:45,138 - memory_profile6_log - INFO - train on: 427635 total genuine interest data(D(u, t))
2018-04-30 11:42:45,140 - memory_profile6_log - INFO - transform on: 94940 total current data(D(t))
2018-04-30 11:42:45,141 - memory_profile6_log - INFO - apply on: 427635 total history...)
2018-04-30 11:42:45,542 - memory_profile6_log - INFO - len of uniques_fit_hist:427635
2018-04-30 11:42:45,858 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:52258
2018-04-30 11:42:47,250 - memory_profile6_log - INFO - Len of model_fit: 427635
2018-04-30 11:42:47,250 - memory_profile6_log - INFO - Len of df_dut: 427635
2018-04-30 11:42:48,082 - memory_profile6_log - INFO - Len of fitted_models on main class: 427635
2018-04-30 11:42:48,082 - memory_profile6_log - INFO - 

2018-04-30 11:42:48,084 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-30 11:42:48,085 - memory_profile6_log - INFO - 

2018-04-30 11:42:48,167 - memory_profile6_log - INFO - len of fitted models after concat: 855270
2018-04-30 11:42:48,170 - memory_profile6_log - INFO - 

2018-04-30 11:42:48,171 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-30 11:42:48,173 - memory_profile6_log - INFO - 

2018-04-30 11:42:48,836 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 427635
2018-04-30 11:42:48,838 - memory_profile6_log - INFO - 

2018-04-30 11:43:00,431 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-30 11:43:00,434 - memory_profile6_log - INFO - date_generated: 
2018-04-30 11:43:00,436 - memory_profile6_log - INFO -  
2018-04-30 11:43:00,436 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 10, 0, 0)]
2018-04-30 11:43:00,437 - memory_profile6_log - INFO - 

2018-04-30 11:43:00,437 - memory_profile6_log - INFO - using current date: 2018-04-11
2018-04-30 11:43:00,437 - memory_profile6_log - INFO - using start date: 2018-04-10 00:00:00
2018-04-30 11:43:00,437 - memory_profile6_log - INFO - using end date: 2018-04-10
2018-04-30 11:43:00,575 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-30 11:43:00,578 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-10
2018-04-30 11:44:56,779 - memory_profile6_log - INFO - size of df: 108.51 MB
2018-04-30 11:44:56,780 - memory_profile6_log - INFO - getting total: 427635 training data(genuine interest) for date: 2018-04-10
2018-04-30 11:44:56,841 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-30 11:44:56,842 - memory_profile6_log - INFO - Appending history data...
2018-04-30 11:44:56,844 - memory_profile6_log - INFO - processing batch-0
2018-04-30 11:44:56,845 - memory_profile6_log - INFO - creating list history data...
2018-04-30 11:44:56,950 - memory_profile6_log - INFO - call history data...
2018-04-30 11:45:45,542 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 11:45:47,246 - memory_profile6_log - INFO - processing batch-1
2018-04-30 11:45:47,247 - memory_profile6_log - INFO - creating list history data...
2018-04-30 11:45:47,326 - memory_profile6_log - INFO - call history data...
2018-04-30 11:46:40,368 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 11:46:42,092 - memory_profile6_log - INFO - processing batch-2
2018-04-30 11:46:42,092 - memory_profile6_log - INFO - creating list history data...
2018-04-30 11:46:42,168 - memory_profile6_log - INFO - call history data...
2018-04-30 11:47:32,240 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 11:47:34,013 - memory_profile6_log - INFO - processing batch-3
2018-04-30 11:47:34,016 - memory_profile6_log - INFO - creating list history data...
2018-04-30 11:47:34,095 - memory_profile6_log - INFO - call history data...
2018-04-30 11:48:21,253 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 11:48:22,963 - memory_profile6_log - INFO - processing batch-4
2018-04-30 11:48:22,964 - memory_profile6_log - INFO - creating list history data...
2018-04-30 11:48:23,039 - memory_profile6_log - INFO - call history data...
2018-04-30 11:49:11,621 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 11:49:13,351 - memory_profile6_log - INFO - Appending training data...
2018-04-30 11:49:13,352 - memory_profile6_log - INFO - len datalist: 1
2018-04-30 11:49:13,355 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-30 11:49:13,368 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 11:49:13,369 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 11:49:13,371 - memory_profile6_log - INFO - ================================================

2018-04-30 11:49:13,372 - memory_profile6_log - INFO -    315     86.7 MiB     86.7 MiB   @profile

2018-04-30 11:49:13,375 - memory_profile6_log - INFO -    316                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-30 11:49:13,375 - memory_profile6_log - INFO -    317     86.7 MiB      0.0 MiB       bq_client = client

2018-04-30 11:49:13,377 - memory_profile6_log - INFO -    318     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 11:49:13,378 - memory_profile6_log - INFO -    319                             

2018-04-30 11:49:13,378 - memory_profile6_log - INFO -    320     86.7 MiB      0.0 MiB       datalist = []

2018-04-30 11:49:13,378 - memory_profile6_log - INFO -    321     86.7 MiB      0.0 MiB       datalist_hist = []

2018-04-30 11:49:13,380 - memory_profile6_log - INFO -    322                             

2018-04-30 11:49:13,381 - memory_profile6_log - INFO -    323     86.7 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-30 11:49:13,384 - memory_profile6_log - INFO -    324    688.1 MiB      0.0 MiB       for ndate in date_generated:

2018-04-30 11:49:13,387 - memory_profile6_log - INFO -    325    392.9 MiB    306.1 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-30 11:49:13,388 - memory_profile6_log - INFO -    326    392.9 MiB      0.0 MiB           if tframe is not None:

2018-04-30 11:49:13,388 - memory_profile6_log - INFO -    327    392.9 MiB      0.0 MiB               if not tframe.empty:

2018-04-30 11:49:13,390 - memory_profile6_log - INFO -    328    406.8 MiB     13.9 MiB                   X_split = np.array_split(tframe, 5)

2018-04-30 11:49:13,391 - memory_profile6_log - INFO -    329    406.8 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-30 11:49:13,391 - memory_profile6_log - INFO -    330    406.8 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-30 11:49:13,394 - memory_profile6_log - INFO -    331    688.1 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-30 11:49:13,395 - memory_profile6_log - INFO -    332                                                 # ~ loading history

2018-04-30 11:49:13,398 - memory_profile6_log - INFO -    333                                                 """

2018-04-30 11:49:13,398 - memory_profile6_log - INFO -    334                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-30 11:49:13,398 - memory_profile6_log - INFO -    335                                                 """

2018-04-30 11:49:13,400 - memory_profile6_log - INFO -    336    660.8 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-30 11:49:13,401 - memory_profile6_log - INFO -    337                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-30 11:49:13,403 - memory_profile6_log - INFO -    338    660.8 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-30 11:49:13,404 - memory_profile6_log - INFO -    339                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-30 11:49:13,404 - memory_profile6_log - INFO -    340    662.1 MiB      5.3 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-30 11:49:13,405 - memory_profile6_log - INFO -    341                             

2018-04-30 11:49:13,407 - memory_profile6_log - INFO -    342    662.1 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-30 11:49:13,407 - memory_profile6_log - INFO -    343    738.8 MiB    555.9 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-30 11:49:13,408 - memory_profile6_log - INFO -    344                             

2018-04-30 11:49:13,408 - memory_profile6_log - INFO -    345                                                 # me = os.getpid()

2018-04-30 11:49:13,410 - memory_profile6_log - INFO -    346                                                 # kill_proc_tree(me)

2018-04-30 11:49:13,411 - memory_profile6_log - INFO -    347                             

2018-04-30 11:49:13,414 - memory_profile6_log - INFO -    348    738.8 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-30 11:49:13,415 - memory_profile6_log - INFO -    349    740.6 MiB   -494.4 MiB                       for m in h_frame:

2018-04-30 11:49:13,417 - memory_profile6_log - INFO -    350    740.6 MiB   -491.9 MiB                           if m is not None:

2018-04-30 11:49:13,417 - memory_profile6_log - INFO -    351    740.6 MiB   -492.1 MiB                               if len(m) > 0:

2018-04-30 11:49:13,418 - memory_profile6_log - INFO -    352    740.6 MiB   -486.1 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-30 11:49:13,420 - memory_profile6_log - INFO -    353    688.1 MiB   -285.0 MiB                       del h_frame

2018-04-30 11:49:13,420 - memory_profile6_log - INFO -    354    688.1 MiB     -2.7 MiB                       del lhistory

2018-04-30 11:49:13,421 - memory_profile6_log - INFO -    355                             

2018-04-30 11:49:13,421 - memory_profile6_log - INFO -    356    688.1 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-30 11:49:13,424 - memory_profile6_log - INFO -    357    688.1 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-30 11:49:13,426 - memory_profile6_log - INFO -    358                                     else: 

2018-04-30 11:49:13,427 - memory_profile6_log - INFO -    359                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-30 11:49:13,427 - memory_profile6_log - INFO -    360    688.1 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-30 11:49:13,428 - memory_profile6_log - INFO -    361    688.1 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-30 11:49:13,430 - memory_profile6_log - INFO -    362                             

2018-04-30 11:49:13,430 - memory_profile6_log - INFO -    363    688.1 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-30 11:49:13,431 - memory_profile6_log - INFO - 


2018-04-30 11:49:14,724 - memory_profile6_log - INFO - size of big_frame_hist: 117.89 MB
2018-04-30 11:49:14,844 - memory_profile6_log - INFO - size of big_frame: 108.51 MB
2018-04-30 11:49:14,864 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-04-30 11:50:32,098 - memory_profile6_log - INFO - size of df: 86.42 MB
2018-04-30 11:50:32,099 - memory_profile6_log - INFO - getting total: 352296 training data(current date interest)
2018-04-30 11:50:32,240 - memory_profile6_log - INFO - size of current_frame: 89.11 MB
2018-04-30 11:50:32,242 - memory_profile6_log - INFO - loading time of: 779931 total genuine-current interest data ~ take 451.688s
2018-04-30 11:50:32,279 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 11:50:32,280 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 11:50:32,282 - memory_profile6_log - INFO - ================================================

2018-04-30 11:50:32,282 - memory_profile6_log - INFO -    365     86.6 MiB     86.6 MiB   @profile

2018-04-30 11:50:32,282 - memory_profile6_log - INFO -    366                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-30 11:50:32,283 - memory_profile6_log - INFO -    367     86.7 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-30 11:50:32,285 - memory_profile6_log - INFO -    368     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 11:50:32,286 - memory_profile6_log - INFO -    369                             

2018-04-30 11:50:32,286 - memory_profile6_log - INFO -    370                                 # ~~~ Begin collecting data ~~~

2018-04-30 11:50:32,288 - memory_profile6_log - INFO -    371     86.7 MiB      0.0 MiB       t0 = time.time()

2018-04-30 11:50:32,289 - memory_profile6_log - INFO -    372    677.7 MiB    591.0 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-30 11:50:32,289 - memory_profile6_log - INFO -    373    677.7 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-30 11:50:32,293 - memory_profile6_log - INFO -    374                                     logger.info("Training cannot be empty..")

2018-04-30 11:50:32,296 - memory_profile6_log - INFO -    375                                     return False

2018-04-30 11:50:32,296 - memory_profile6_log - INFO -    376    704.1 MiB     26.3 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-30 11:50:32,298 - memory_profile6_log - INFO -    377                                 # print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-30 11:50:32,299 - memory_profile6_log - INFO -    378    704.1 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-30 11:50:32,301 - memory_profile6_log - INFO -    379                             

2018-04-30 11:50:32,301 - memory_profile6_log - INFO -    380    714.3 MiB     10.2 MiB       big_frame = pd.concat(datalist)

2018-04-30 11:50:32,302 - memory_profile6_log - INFO -    381    714.3 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-30 11:50:32,305 - memory_profile6_log - INFO -    382    703.4 MiB    -10.9 MiB       del datalist

2018-04-30 11:50:32,305 - memory_profile6_log - INFO -    383                             

2018-04-30 11:50:32,306 - memory_profile6_log - INFO -    384    703.4 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 11:50:32,306 - memory_profile6_log - INFO -    385                             

2018-04-30 11:50:32,308 - memory_profile6_log - INFO -    386                                 # ~ get current news interest ~

2018-04-30 11:50:32,309 - memory_profile6_log - INFO -    387    703.4 MiB      0.0 MiB       if not cd:

2018-04-30 11:50:32,309 - memory_profile6_log - INFO -    388    703.4 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-04-30 11:50:32,309 - memory_profile6_log - INFO -    389    745.5 MiB     42.1 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-30 11:50:32,309 - memory_profile6_log - INFO -    390    745.5 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-30 11:50:32,311 - memory_profile6_log - INFO -    391                                 else:

2018-04-30 11:50:32,311 - memory_profile6_log - INFO -    392                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-30 11:50:32,312 - memory_profile6_log - INFO -    393                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-30 11:50:32,312 - memory_profile6_log - INFO -    394                             

2018-04-30 11:50:32,316 - memory_profile6_log - INFO -    395                                     # safe handling of query parameter

2018-04-30 11:50:32,318 - memory_profile6_log - INFO -    396                                     query_params = [

2018-04-30 11:50:32,319 - memory_profile6_log - INFO -    397                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-30 11:50:32,319 - memory_profile6_log - INFO -    398                                     ]

2018-04-30 11:50:32,319 - memory_profile6_log - INFO -    399                             

2018-04-30 11:50:32,321 - memory_profile6_log - INFO -    400                                     job_config.query_parameters = query_params

2018-04-30 11:50:32,321 - memory_profile6_log - INFO -    401                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-30 11:50:32,322 - memory_profile6_log - INFO -    402                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-30 11:50:32,322 - memory_profile6_log - INFO -    403                             

2018-04-30 11:50:32,322 - memory_profile6_log - INFO -    404    748.2 MiB      2.7 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-30 11:50:32,323 - memory_profile6_log - INFO -    405    748.2 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-30 11:50:32,323 - memory_profile6_log - INFO -    406    748.3 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-30 11:50:32,326 - memory_profile6_log - INFO -    407    748.3 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-30 11:50:32,328 - memory_profile6_log - INFO -    408    748.3 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-30 11:50:32,328 - memory_profile6_log - INFO -    409    748.3 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-30 11:50:32,329 - memory_profile6_log - INFO -    410                             

2018-04-30 11:50:32,331 - memory_profile6_log - INFO -    411    748.3 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-30 11:50:32,331 - memory_profile6_log - INFO - 


2018-04-30 11:50:32,335 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-30 11:50:32,388 - memory_profile6_log - INFO - train on: 427635 total genuine interest data(D(u, t))
2018-04-30 11:50:32,388 - memory_profile6_log - INFO - transform on: 352296 total current data(D(t))
2018-04-30 11:50:32,391 - memory_profile6_log - INFO - apply on: 427635 total history...)
2018-04-30 11:50:32,823 - memory_profile6_log - INFO - len of uniques_fit_hist:427635
2018-04-30 11:50:33,137 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:52258
2018-04-30 11:50:34,707 - memory_profile6_log - INFO - Len of model_fit: 427635
2018-04-30 11:50:34,709 - memory_profile6_log - INFO - Len of df_dut: 427635
2018-04-30 11:50:36,249 - memory_profile6_log - INFO - Len of fitted_models on main class: 427635
2018-04-30 11:50:36,250 - memory_profile6_log - INFO - 

2018-04-30 11:50:36,250 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-30 11:50:36,250 - memory_profile6_log - INFO - 

2018-04-30 11:50:36,339 - memory_profile6_log - INFO - len of fitted models after concat: 855270
2018-04-30 11:50:36,341 - memory_profile6_log - INFO - 

2018-04-30 11:50:36,342 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-30 11:50:36,342 - memory_profile6_log - INFO - 

2018-04-30 11:50:37,088 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 427635
2018-04-30 11:50:37,089 - memory_profile6_log - INFO - 

2018-04-30 11:51:39,936 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-30 11:51:40,023 - memory_profile6_log - INFO - Len of model_transform: 366673
2018-04-30 11:51:40,023 - memory_profile6_log - INFO - Len of df_dt: 352296
2018-04-30 11:51:40,025 - memory_profile6_log - INFO - Total train time: 67.635s
2018-04-30 11:51:40,026 - memory_profile6_log - INFO - memory left before cleaning: 76.500 percent memory...
2018-04-30 11:51:40,026 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-30 11:51:40,028 - memory_profile6_log - INFO - deleting df_dut...
2018-04-30 11:51:40,029 - memory_profile6_log - INFO - deleting df_dt...
2018-04-30 11:51:40,029 - memory_profile6_log - INFO - deleting df_input...
2018-04-30 11:51:40,043 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-30 11:51:40,045 - memory_profile6_log - INFO - deleting df_current...
2018-04-30 11:51:40,046 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-30 11:51:40,062 - memory_profile6_log - INFO - deleting model_fit...
2018-04-30 11:51:40,063 - memory_profile6_log - INFO - deleting result...
2018-04-30 11:51:40,095 - memory_profile6_log - INFO - deleting model_transform...
2018-04-30 11:51:40,096 - memory_profile6_log - INFO - memory left after cleaning: 76.200 percent memory...
2018-04-30 11:51:40,096 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-30 11:51:40,098 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-30 11:51:40,098 - memory_profile6_log - INFO - Saving main Transform model to Google DataStore...
2018-04-30 11:51:40,098 - memory_profile6_log - INFO - Saving total data: 366673
2018-04-30 11:51:40,292 - memory_profile6_log - INFO - Saving fitted_models as history to Google DataStore...
2018-04-30 11:51:40,388 - memory_profile6_log - INFO - Saving total data: 427635
2018-04-30 11:51:40,430 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 10
2018-04-30 11:51:40,430 - memory_profile6_log - INFO - processing batch-0
2018-04-30 11:51:40,625 - memory_profile6_log - INFO - processing batch-1
2018-04-30 11:51:40,813 - memory_profile6_log - INFO - processing batch-2
2018-04-30 11:51:41,006 - memory_profile6_log - INFO - processing batch-3
2018-04-30 11:51:41,191 - memory_profile6_log - INFO - processing batch-4
2018-04-30 11:51:41,384 - memory_profile6_log - INFO - processing batch-5
2018-04-30 11:51:41,569 - memory_profile6_log - INFO - processing batch-6
2018-04-30 11:51:41,762 - memory_profile6_log - INFO - processing batch-7
2018-04-30 11:51:41,953 - memory_profile6_log - INFO - processing batch-8
2018-04-30 11:51:42,148 - memory_profile6_log - INFO - processing batch-9
2018-04-30 11:51:42,348 - memory_profile6_log - INFO - deleting X_split...
2018-04-30 11:51:42,349 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-04-30 11:51:42,351 - memory_profile6_log - INFO - deleting BR...
2018-04-30 11:51:42,371 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 11:51:42,371 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 11:51:42,371 - memory_profile6_log - INFO - ================================================

2018-04-30 11:51:42,371 - memory_profile6_log - INFO -    113    677.1 MiB    677.1 MiB   @profile

2018-04-30 11:51:42,372 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-30 11:51:42,372 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-30 11:51:42,374 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-30 11:51:42,374 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-30 11:51:42,374 - memory_profile6_log - INFO -    118                                      saveto="datastore", fitted_models_hist=None):

2018-04-30 11:51:42,375 - memory_profile6_log - INFO -    119                                 """

2018-04-30 11:51:42,375 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-30 11:51:42,375 - memory_profile6_log - INFO -    121                                 """

2018-04-30 11:51:42,375 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-30 11:51:42,375 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-30 11:51:42,377 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-30 11:51:42,377 - memory_profile6_log - INFO -    125    677.1 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-30 11:51:42,377 - memory_profile6_log - INFO -    126    690.5 MiB     13.5 MiB       df_dut = df_input.copy(deep=True)

2018-04-30 11:51:42,378 - memory_profile6_log - INFO -    127    690.5 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 11:51:42,378 - memory_profile6_log - INFO -    128                             

2018-04-30 11:51:42,378 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-30 11:51:42,378 - memory_profile6_log - INFO -    130    701.6 MiB     11.1 MiB       df_dt = df_current.copy(deep=True)

2018-04-30 11:51:42,378 - memory_profile6_log - INFO -    131    701.6 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 11:51:42,380 - memory_profile6_log - INFO -    132                             

2018-04-30 11:51:42,384 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-30 11:51:42,384 - memory_profile6_log - INFO -    134    701.6 MiB      0.0 MiB       t0 = time.time()

2018-04-30 11:51:42,384 - memory_profile6_log - INFO -    135    701.6 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-30 11:51:42,384 - memory_profile6_log - INFO -    136    701.6 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-30 11:51:42,384 - memory_profile6_log - INFO -    137    701.6 MiB      0.0 MiB       logger.info("apply on: %d total history...)", len(df_hist))

2018-04-30 11:51:42,385 - memory_profile6_log - INFO -    138                             

2018-04-30 11:51:42,385 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-30 11:51:42,387 - memory_profile6_log - INFO -    140    701.6 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-30 11:51:42,387 - memory_profile6_log - INFO -    141                             

2018-04-30 11:51:42,388 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-30 11:51:42,388 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-30 11:51:42,388 - memory_profile6_log - INFO -    144    704.3 MiB      2.6 MiB       NB = BR.processX(df_dut)

2018-04-30 11:51:42,388 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-30 11:51:42,388 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-30 11:51:42,388 - memory_profile6_log - INFO -    147    720.7 MiB     16.4 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-30 11:51:42,390 - memory_profile6_log - INFO -    148                                 """

2018-04-30 11:51:42,390 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-30 11:51:42,391 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-30 11:51:42,391 - memory_profile6_log - INFO -    151                                 """

2018-04-30 11:51:42,391 - memory_profile6_log - INFO -    152    720.7 MiB      0.0 MiB       fitby_sigmant = False

2018-04-30 11:51:42,392 - memory_profile6_log - INFO -    153    720.7 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-30 11:51:42,397 - memory_profile6_log - INFO -    154    720.7 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-30 11:51:42,397 - memory_profile6_log - INFO -    155    737.0 MiB     16.3 MiB                            'is_general']]

2018-04-30 11:51:42,397 - memory_profile6_log - INFO -    156                                 # get sigma_Nt from fitted_models_hist

2018-04-30 11:51:42,398 - memory_profile6_log - INFO -    157    743.6 MiB      6.5 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-04-30 11:51:42,398 - memory_profile6_log - INFO -    158    743.6 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-04-30 11:51:42,398 - memory_profile6_log - INFO -    159    739.8 MiB     -3.8 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-04-30 11:51:42,398 - memory_profile6_log - INFO -    160    739.8 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-04-30 11:51:42,398 - memory_profile6_log - INFO -    161                             

2018-04-30 11:51:42,400 - memory_profile6_log - INFO -    162                                 # begin fit

2018-04-30 11:51:42,400 - memory_profile6_log - INFO -    163    739.8 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-30 11:51:42,401 - memory_profile6_log - INFO -    164    739.8 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-30 11:51:42,401 - memory_profile6_log - INFO -    165    781.8 MiB     42.1 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-04-30 11:51:42,401 - memory_profile6_log - INFO -    166    781.8 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-30 11:51:42,401 - memory_profile6_log - INFO -    167    781.8 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-30 11:51:42,401 - memory_profile6_log - INFO -    168                                 # print model_fit[['num_x', 'num_x', 'sigma_Nt', 'date_all_click']].loc[model_fit['user_id']=="1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157"].head(5)

2018-04-30 11:51:42,401 - memory_profile6_log - INFO -    169                             

2018-04-30 11:51:42,403 - memory_profile6_log - INFO -    170                                 # ~~ and Transform ~~

2018-04-30 11:51:42,403 - memory_profile6_log - INFO -    171                                 #   handling current news interest == current date

2018-04-30 11:51:42,404 - memory_profile6_log - INFO -    172    781.8 MiB      0.0 MiB       if df_dt.empty:

2018-04-30 11:51:42,404 - memory_profile6_log - INFO -    173                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-30 11:51:42,404 - memory_profile6_log - INFO -    174                                     return None

2018-04-30 11:51:42,408 - memory_profile6_log - INFO -    175    781.9 MiB      0.1 MiB       NB = BR.processX(df_dt)

2018-04-30 11:51:42,408 - memory_profile6_log - INFO -    176    798.0 MiB     16.1 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-30 11:51:42,410 - memory_profile6_log - INFO -    177                             

2018-04-30 11:51:42,410 - memory_profile6_log - INFO -    178    798.0 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-30 11:51:42,410 - memory_profile6_log - INFO -    179    795.1 MiB     -2.9 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-30 11:51:42,411 - memory_profile6_log - INFO -    180                             

2018-04-30 11:51:42,411 - memory_profile6_log - INFO -    181                                 # print "model_fit dtypes:\n", model_fit.dtypes

2018-04-30 11:51:42,411 - memory_profile6_log - INFO -    182                                 # print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-04-30 11:51:42,411 - memory_profile6_log - INFO -    183    795.1 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-30 11:51:42,411 - memory_profile6_log - INFO -    184    795.1 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-30 11:51:42,413 - memory_profile6_log - INFO -    185    795.1 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-04-30 11:51:42,413 - memory_profile6_log - INFO -    186    804.9 MiB      9.8 MiB                                                                                           "topic_id", "user_id"]],

2018-04-30 11:51:42,413 - memory_profile6_log - INFO -    187    843.1 MiB     38.2 MiB                                                     verbose=False)

2018-04-30 11:51:42,414 - memory_profile6_log - INFO -    188                                 # ~~~ filter is general and specific topic ~~~

2018-04-30 11:51:42,414 - memory_profile6_log - INFO -    189                                 # the idea is just we need to rerank every topic according

2018-04-30 11:51:42,414 - memory_profile6_log - INFO -    190                                 # user_id and and is_general by p0_posterior

2018-04-30 11:51:42,414 - memory_profile6_log - INFO -    191    843.1 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-30 11:51:42,414 - memory_profile6_log - INFO -    192    846.4 MiB      3.3 MiB                                     'is_general']].groupby(['topic_id',

2018-04-30 11:51:42,415 - memory_profile6_log - INFO -    193    843.2 MiB     -3.2 MiB                                                             'is_general']

2018-04-30 11:51:42,415 - memory_profile6_log - INFO -    194                                                                                      ).size().to_frame().reset_index()

2018-04-30 11:51:42,420 - memory_profile6_log - INFO -    195                             

2018-04-30 11:51:42,421 - memory_profile6_log - INFO -    196                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-30 11:51:42,421 - memory_profile6_log - INFO -    197                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-30 11:51:42,421 - memory_profile6_log - INFO -    198    843.2 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-30 11:51:42,421 - memory_profile6_log - INFO -    199                             

2018-04-30 11:51:42,421 - memory_profile6_log - INFO -    200                                 # ~ start by provide rank for each topic type ~

2018-04-30 11:51:42,423 - memory_profile6_log - INFO -    201    853.0 MiB      9.8 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-30 11:51:42,423 - memory_profile6_log - INFO -    202    861.2 MiB      8.2 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-30 11:51:42,423 - memory_profile6_log - INFO -    203                             

2018-04-30 11:51:42,424 - memory_profile6_log - INFO -    204                                 # ~ set threshold to filter output

2018-04-30 11:51:42,424 - memory_profile6_log - INFO -    205    861.2 MiB      0.0 MiB       if threshold > 0:

2018-04-30 11:51:42,424 - memory_profile6_log - INFO -    206    861.2 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-30 11:51:42,424 - memory_profile6_log - INFO -    207    861.2 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-30 11:51:42,424 - memory_profile6_log - INFO -    208    858.1 MiB     -3.1 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-30 11:51:42,426 - memory_profile6_log - INFO -    209                             

2018-04-30 11:51:42,426 - memory_profile6_log - INFO -    210    858.1 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-30 11:51:42,426 - memory_profile6_log - INFO -    211    858.1 MiB      0.0 MiB       logger.info("Len of model_transform: %d", len(model_transform))

2018-04-30 11:51:42,427 - memory_profile6_log - INFO -    212    858.1 MiB      0.0 MiB       logger.info("Len of df_dt: %d", len(df_dt))

2018-04-30 11:51:42,427 - memory_profile6_log - INFO -    213    858.1 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-30 11:51:42,427 - memory_profile6_log - INFO -    214                             

2018-04-30 11:51:42,427 - memory_profile6_log - INFO -    215    858.1 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-30 11:51:42,427 - memory_profile6_log - INFO -    216                             

2018-04-30 11:51:42,427 - memory_profile6_log - INFO -    217    858.1 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-30 11:51:42,433 - memory_profile6_log - INFO -    218    858.1 MiB      0.0 MiB       del df_dut

2018-04-30 11:51:42,434 - memory_profile6_log - INFO -    219    858.1 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-30 11:51:42,434 - memory_profile6_log - INFO -    220    858.1 MiB      0.0 MiB       del df_dt

2018-04-30 11:51:42,434 - memory_profile6_log - INFO -    221    858.1 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-30 11:51:42,434 - memory_profile6_log - INFO -    222    858.1 MiB      0.0 MiB       del df_input

2018-04-30 11:51:42,434 - memory_profile6_log - INFO -    223    858.1 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-30 11:51:42,434 - memory_profile6_log - INFO -    224    844.6 MiB    -13.4 MiB       del df_input_X

2018-04-30 11:51:42,436 - memory_profile6_log - INFO -    225    844.6 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-30 11:51:42,436 - memory_profile6_log - INFO -    226    844.6 MiB      0.0 MiB       del df_current

2018-04-30 11:51:42,437 - memory_profile6_log - INFO -    227    844.6 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-30 11:51:42,437 - memory_profile6_log - INFO -    228    844.6 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-30 11:51:42,437 - memory_profile6_log - INFO -    229    844.6 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-30 11:51:42,437 - memory_profile6_log - INFO -    230    808.7 MiB    -35.9 MiB       del model_fit

2018-04-30 11:51:42,437 - memory_profile6_log - INFO -    231    808.7 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-30 11:51:42,438 - memory_profile6_log - INFO -    232    808.7 MiB      0.0 MiB       del result

2018-04-30 11:51:42,438 - memory_profile6_log - INFO -    233    808.7 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-30 11:51:42,438 - memory_profile6_log - INFO -    234                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 11:51:42,440 - memory_profile6_log - INFO -    235                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 11:51:42,440 - memory_profile6_log - INFO -    236                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 11:51:42,440 - memory_profile6_log - INFO -    237    808.7 MiB      0.0 MiB       if savetrain:

2018-04-30 11:51:42,440 - memory_profile6_log - INFO -    238    817.1 MiB      8.4 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-30 11:51:42,440 - memory_profile6_log - INFO -    239    817.1 MiB      0.0 MiB           del model_transform

2018-04-30 11:51:42,444 - memory_profile6_log - INFO -    240    817.1 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-30 11:51:42,446 - memory_profile6_log - INFO -    241    817.1 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-30 11:51:42,446 - memory_profile6_log - INFO -    242                             

2018-04-30 11:51:42,447 - memory_profile6_log - INFO -    243    817.1 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-30 11:51:42,447 - memory_profile6_log - INFO -    244                                     # ~ Place your code to save the training model here ~

2018-04-30 11:51:42,447 - memory_profile6_log - INFO -    245    817.1 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-30 11:51:42,447 - memory_profile6_log - INFO -    246    817.1 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-30 11:51:42,447 - memory_profile6_log - INFO -    247    817.1 MiB      0.0 MiB               if multproc:

2018-04-30 11:51:42,448 - memory_profile6_log - INFO -    248                                             # ~ save transform models ~

2018-04-30 11:51:42,448 - memory_profile6_log - INFO -    249    817.1 MiB      0.0 MiB                   logger.info("Saving main Transform model to Google DataStore...")

2018-04-30 11:51:42,450 - memory_profile6_log - INFO -    250    817.1 MiB      0.0 MiB                   logger.info("Saving total data: %d", len(model_transformsv))

2018-04-30 11:51:42,450 - memory_profile6_log - INFO -    251    780.0 MiB    -37.1 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-30 11:51:42,450 - memory_profile6_log - INFO -    252                             

2018-04-30 11:51:42,450 - memory_profile6_log - INFO -    253                                             # ~ save fitted models ~

2018-04-30 11:51:42,450 - memory_profile6_log - INFO -    254    780.0 MiB      0.0 MiB                   logger.info("Saving fitted_models as history to Google DataStore...")

2018-04-30 11:51:42,451 - memory_profile6_log - INFO -    255    780.3 MiB      0.3 MiB                   save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-30 11:51:42,451 - memory_profile6_log - INFO -    256    805.1 MiB     24.8 MiB                   fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-30 11:51:42,451 - memory_profile6_log - INFO -    257    805.1 MiB      0.0 MiB                   logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-04-30 11:51:42,453 - memory_profile6_log - INFO -    258    820.2 MiB     15.1 MiB                   X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-30 11:51:42,453 - memory_profile6_log - INFO -    259    820.2 MiB      0.0 MiB                   logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-30 11:51:42,453 - memory_profile6_log - INFO -    260    822.2 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-30 11:51:42,453 - memory_profile6_log - INFO -    261    822.2 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-30 11:51:42,457 - memory_profile6_log - INFO -    262    822.2 MiB      2.0 MiB                       mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-30 11:51:42,457 - memory_profile6_log - INFO -    263                             

2018-04-30 11:51:42,459 - memory_profile6_log - INFO -    264    819.2 MiB     -3.0 MiB                   del X_split

2018-04-30 11:51:42,459 - memory_profile6_log - INFO -    265    819.2 MiB      0.0 MiB                   logger.info("deleting X_split...")

2018-04-30 11:51:42,459 - memory_profile6_log - INFO -    266    819.2 MiB      0.0 MiB                   del save_sigma_nt

2018-04-30 11:51:42,460 - memory_profile6_log - INFO -    267    819.2 MiB      0.0 MiB                   logger.info("deleting save_sigma_nt...")

2018-04-30 11:51:42,460 - memory_profile6_log - INFO -    268                             

2018-04-30 11:51:42,460 - memory_profile6_log - INFO -    269    819.2 MiB      0.0 MiB                   del BR

2018-04-30 11:51:42,460 - memory_profile6_log - INFO -    270    819.2 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-30 11:51:42,460 - memory_profile6_log - INFO -    271                             

2018-04-30 11:51:42,460 - memory_profile6_log - INFO -    272                                     elif str(saveto).lower() == "elastic":

2018-04-30 11:51:42,461 - memory_profile6_log - INFO -    273                                         logger.info("Using ElasticSearch as storage...")

2018-04-30 11:51:42,461 - memory_profile6_log - INFO -    274                                         logging.info("Saving main Transform model to Elasticsearch...")

2018-04-30 11:51:42,461 - memory_profile6_log - INFO -    275                                         mh.saveElasticS(model_transformsv)

2018-04-30 11:51:42,463 - memory_profile6_log - INFO -    276                             

2018-04-30 11:51:42,463 - memory_profile6_log - INFO -    277                                     # need save sigma_nt for daily train

2018-04-30 11:51:42,463 - memory_profile6_log - INFO -    278                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-30 11:51:42,463 - memory_profile6_log - INFO -    279                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-30 11:51:42,463 - memory_profile6_log - INFO -    280    819.2 MiB      0.0 MiB           if start_date and end_date:

2018-04-30 11:51:42,464 - memory_profile6_log - INFO -    281                                         if not fitby_sigmant:

2018-04-30 11:51:42,464 - memory_profile6_log - INFO -    282                                             logging.info("Saving sigma Nt...")

2018-04-30 11:51:42,466 - memory_profile6_log - INFO -    283                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-30 11:51:42,466 - memory_profile6_log - INFO -    284                                             save_sigma_nt['start_date'] = start_date

2018-04-30 11:51:42,466 - memory_profile6_log - INFO -    285                                             save_sigma_nt['end_date'] = end_date

2018-04-30 11:51:42,470 - memory_profile6_log - INFO -    286                                             print save_sigma_nt.head(5)

2018-04-30 11:51:42,470 - memory_profile6_log - INFO -    287                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-30 11:51:42,471 - memory_profile6_log - INFO -    288                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-30 11:51:42,471 - memory_profile6_log - INFO -    289    819.2 MiB      0.0 MiB       return

2018-04-30 11:51:42,471 - memory_profile6_log - INFO - 


2018-04-30 11:51:42,473 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-30 12:14:10,806 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-30 12:14:10,811 - memory_profile6_log - INFO - date_generated: 
2018-04-30 12:14:10,812 - memory_profile6_log - INFO -  
2018-04-30 12:14:10,812 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 10, 0, 0)]
2018-04-30 12:14:10,812 - memory_profile6_log - INFO - 

2018-04-30 12:14:10,812 - memory_profile6_log - INFO - using current date: 2018-04-11
2018-04-30 12:14:10,812 - memory_profile6_log - INFO - using start date: 2018-04-10 00:00:00
2018-04-30 12:14:10,813 - memory_profile6_log - INFO - using end date: 2018-04-10
2018-04-30 12:14:10,959 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-30 12:14:10,961 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-10
2018-04-30 12:16:06,246 - memory_profile6_log - INFO - size of df: 108.51 MB
2018-04-30 12:16:06,247 - memory_profile6_log - INFO - getting total: 427635 training data(genuine interest) for date: 2018-04-10
2018-04-30 12:16:06,313 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-30 12:16:06,315 - memory_profile6_log - INFO - Appending history data...
2018-04-30 12:16:06,316 - memory_profile6_log - INFO - processing batch-0
2018-04-30 12:16:06,318 - memory_profile6_log - INFO - creating list history data...
2018-04-30 12:16:06,424 - memory_profile6_log - INFO - call history data...
2018-04-30 12:16:55,765 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 12:16:57,426 - memory_profile6_log - INFO - processing batch-1
2018-04-30 12:16:57,427 - memory_profile6_log - INFO - creating list history data...
2018-04-30 12:16:57,505 - memory_profile6_log - INFO - call history data...
2018-04-30 12:17:44,609 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 12:17:46,322 - memory_profile6_log - INFO - processing batch-2
2018-04-30 12:17:46,323 - memory_profile6_log - INFO - creating list history data...
2018-04-30 12:17:46,413 - memory_profile6_log - INFO - call history data...
2018-04-30 12:18:34,213 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 12:18:35,950 - memory_profile6_log - INFO - processing batch-3
2018-04-30 12:18:35,951 - memory_profile6_log - INFO - creating list history data...
2018-04-30 12:18:36,030 - memory_profile6_log - INFO - call history data...
2018-04-30 12:19:28,138 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 12:19:29,838 - memory_profile6_log - INFO - processing batch-4
2018-04-30 12:19:29,841 - memory_profile6_log - INFO - creating list history data...
2018-04-30 12:19:29,915 - memory_profile6_log - INFO - call history data...
2018-04-30 12:20:31,063 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 12:20:32,950 - memory_profile6_log - INFO - Appending training data...
2018-04-30 12:20:32,950 - memory_profile6_log - INFO - len datalist: 1
2018-04-30 12:20:32,953 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-30 12:20:32,970 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 12:20:32,970 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 12:20:32,973 - memory_profile6_log - INFO - ================================================

2018-04-30 12:20:32,976 - memory_profile6_log - INFO -    315     86.8 MiB     86.8 MiB   @profile

2018-04-30 12:20:32,976 - memory_profile6_log - INFO -    316                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-30 12:20:32,977 - memory_profile6_log - INFO -    317     86.8 MiB      0.0 MiB       bq_client = client

2018-04-30 12:20:32,979 - memory_profile6_log - INFO -    318     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 12:20:32,980 - memory_profile6_log - INFO -    319                             

2018-04-30 12:20:32,982 - memory_profile6_log - INFO -    320     86.8 MiB      0.0 MiB       datalist = []

2018-04-30 12:20:32,983 - memory_profile6_log - INFO -    321     86.8 MiB      0.0 MiB       datalist_hist = []

2018-04-30 12:20:32,984 - memory_profile6_log - INFO -    322                             

2018-04-30 12:20:32,986 - memory_profile6_log - INFO -    323     86.8 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-30 12:20:32,986 - memory_profile6_log - INFO -    324    689.7 MiB      0.0 MiB       for ndate in date_generated:

2018-04-30 12:20:32,987 - memory_profile6_log - INFO -    325    394.6 MiB    307.8 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-30 12:20:32,989 - memory_profile6_log - INFO -    326    394.6 MiB      0.0 MiB           if tframe is not None:

2018-04-30 12:20:32,990 - memory_profile6_log - INFO -    327    394.6 MiB      0.0 MiB               if not tframe.empty:

2018-04-30 12:20:32,990 - memory_profile6_log - INFO -    328    407.8 MiB     13.3 MiB                   X_split = np.array_split(tframe, 5)

2018-04-30 12:20:32,994 - memory_profile6_log - INFO -    329    407.8 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-30 12:20:32,996 - memory_profile6_log - INFO -    330    407.8 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-30 12:20:32,996 - memory_profile6_log - INFO -    331    689.7 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-30 12:20:32,997 - memory_profile6_log - INFO -    332                                                 # ~ loading history

2018-04-30 12:20:33,000 - memory_profile6_log - INFO -    333                                                 """

2018-04-30 12:20:33,002 - memory_profile6_log - INFO -    334                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-30 12:20:33,003 - memory_profile6_log - INFO -    335                                                 """

2018-04-30 12:20:33,003 - memory_profile6_log - INFO -    336    663.8 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-30 12:20:33,005 - memory_profile6_log - INFO -    337                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-30 12:20:33,006 - memory_profile6_log - INFO -    338    663.8 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-30 12:20:33,006 - memory_profile6_log - INFO -    339                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-30 12:20:33,006 - memory_profile6_log - INFO -    340    665.2 MiB      6.4 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-30 12:20:33,007 - memory_profile6_log - INFO -    341                             

2018-04-30 12:20:33,009 - memory_profile6_log - INFO -    342    665.2 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-30 12:20:33,009 - memory_profile6_log - INFO -    343    738.7 MiB    546.9 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-30 12:20:33,009 - memory_profile6_log - INFO -    344                             

2018-04-30 12:20:33,013 - memory_profile6_log - INFO -    345                                                 # me = os.getpid()

2018-04-30 12:20:33,015 - memory_profile6_log - INFO -    346                                                 # kill_proc_tree(me)

2018-04-30 12:20:33,016 - memory_profile6_log - INFO -    347                             

2018-04-30 12:20:33,016 - memory_profile6_log - INFO -    348    738.7 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-30 12:20:33,016 - memory_profile6_log - INFO -    349    740.7 MiB     -1.4 MiB                       for m in h_frame:

2018-04-30 12:20:33,017 - memory_profile6_log - INFO -    350    740.7 MiB     -1.4 MiB                           if m is not None:

2018-04-30 12:20:33,019 - memory_profile6_log - INFO -    351    740.7 MiB     -1.4 MiB                               if len(m) > 0:

2018-04-30 12:20:33,019 - memory_profile6_log - INFO -    352    740.7 MiB      9.2 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-30 12:20:33,020 - memory_profile6_log - INFO -    353    689.7 MiB   -280.9 MiB                       del h_frame

2018-04-30 12:20:33,023 - memory_profile6_log - INFO -    354    689.7 MiB     -1.2 MiB                       del lhistory

2018-04-30 12:20:33,026 - memory_profile6_log - INFO -    355                             

2018-04-30 12:20:33,028 - memory_profile6_log - INFO -    356    689.7 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-30 12:20:33,029 - memory_profile6_log - INFO -    357    689.7 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-30 12:20:33,029 - memory_profile6_log - INFO -    358                                     else: 

2018-04-30 12:20:33,029 - memory_profile6_log - INFO -    359                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-30 12:20:33,030 - memory_profile6_log - INFO -    360    689.7 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-30 12:20:33,032 - memory_profile6_log - INFO -    361    689.7 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-30 12:20:33,032 - memory_profile6_log - INFO -    362                             

2018-04-30 12:20:33,036 - memory_profile6_log - INFO -    363    689.7 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-30 12:20:33,036 - memory_profile6_log - INFO - 


2018-04-30 12:20:34,572 - memory_profile6_log - INFO - size of big_frame_hist: 117.89 MB
2018-04-30 12:20:34,704 - memory_profile6_log - INFO - size of big_frame: 108.51 MB
2018-04-30 12:20:34,732 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-04-30 12:23:46,466 - memory_profile6_log - INFO - size of df: 245.42 MB
2018-04-30 12:23:46,466 - memory_profile6_log - INFO - getting total: 1004890 training data(current date interest)
2018-04-30 12:23:46,788 - memory_profile6_log - INFO - size of current_frame: 253.09 MB
2018-04-30 12:23:46,789 - memory_profile6_log - INFO - loading time of: 1432525 total genuine-current interest data ~ take 575.851s
2018-04-30 12:23:46,818 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 12:23:46,819 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 12:23:46,819 - memory_profile6_log - INFO - ================================================

2018-04-30 12:23:46,819 - memory_profile6_log - INFO -    365     86.7 MiB     86.7 MiB   @profile

2018-04-30 12:23:46,819 - memory_profile6_log - INFO -    366                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-30 12:23:46,819 - memory_profile6_log - INFO -    367     86.8 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-30 12:23:46,819 - memory_profile6_log - INFO -    368     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 12:23:46,819 - memory_profile6_log - INFO -    369                             

2018-04-30 12:23:46,826 - memory_profile6_log - INFO -    370                                 # ~~~ Begin collecting data ~~~

2018-04-30 12:23:46,826 - memory_profile6_log - INFO -    371     86.8 MiB      0.0 MiB       t0 = time.time()

2018-04-30 12:23:46,826 - memory_profile6_log - INFO -    372    678.4 MiB    591.6 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-30 12:23:46,832 - memory_profile6_log - INFO -    373    678.4 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-30 12:23:46,832 - memory_profile6_log - INFO -    374                                     logger.info("Training cannot be empty..")

2018-04-30 12:23:46,832 - memory_profile6_log - INFO -    375                                     return False

2018-04-30 12:23:46,835 - memory_profile6_log - INFO -    376    702.2 MiB     23.8 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-30 12:23:46,835 - memory_profile6_log - INFO -    377                                 # print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-30 12:23:46,836 - memory_profile6_log - INFO -    378    702.2 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-30 12:23:46,836 - memory_profile6_log - INFO -    379                             

2018-04-30 12:23:46,838 - memory_profile6_log - INFO -    380    712.4 MiB     10.2 MiB       big_frame = pd.concat(datalist)

2018-04-30 12:23:46,838 - memory_profile6_log - INFO -    381    712.4 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-30 12:23:46,842 - memory_profile6_log - INFO -    382    702.6 MiB     -9.8 MiB       del datalist

2018-04-30 12:23:46,845 - memory_profile6_log - INFO -    383                             

2018-04-30 12:23:46,845 - memory_profile6_log - INFO -    384    702.6 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 12:23:46,845 - memory_profile6_log - INFO -    385                             

2018-04-30 12:23:46,846 - memory_profile6_log - INFO -    386                                 # ~ get current news interest ~

2018-04-30 12:23:46,848 - memory_profile6_log - INFO -    387    702.6 MiB      0.0 MiB       if not cd:

2018-04-30 12:23:46,849 - memory_profile6_log - INFO -    388    702.6 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-04-30 12:23:46,849 - memory_profile6_log - INFO -    389    863.9 MiB    161.2 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-30 12:23:46,851 - memory_profile6_log - INFO -    390    863.9 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-30 12:23:46,851 - memory_profile6_log - INFO -    391                                 else:

2018-04-30 12:23:46,855 - memory_profile6_log - INFO -    392                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-30 12:23:46,855 - memory_profile6_log - INFO -    393                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-30 12:23:46,858 - memory_profile6_log - INFO -    394                             

2018-04-30 12:23:46,858 - memory_profile6_log - INFO -    395                                     # safe handling of query parameter

2018-04-30 12:23:46,858 - memory_profile6_log - INFO -    396                                     query_params = [

2018-04-30 12:23:46,859 - memory_profile6_log - INFO -    397                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-30 12:23:46,859 - memory_profile6_log - INFO -    398                                     ]

2018-04-30 12:23:46,861 - memory_profile6_log - INFO -    399                             

2018-04-30 12:23:46,861 - memory_profile6_log - INFO -    400                                     job_config.query_parameters = query_params

2018-04-30 12:23:46,861 - memory_profile6_log - INFO -    401                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-30 12:23:46,862 - memory_profile6_log - INFO -    402                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-30 12:23:46,862 - memory_profile6_log - INFO -    403                             

2018-04-30 12:23:46,868 - memory_profile6_log - INFO -    404    871.5 MiB      7.7 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-30 12:23:46,868 - memory_profile6_log - INFO -    405    871.5 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-30 12:23:46,869 - memory_profile6_log - INFO -    406    871.6 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-30 12:23:46,871 - memory_profile6_log - INFO -    407    871.6 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-30 12:23:46,872 - memory_profile6_log - INFO -    408    871.6 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-30 12:23:46,872 - memory_profile6_log - INFO -    409    871.6 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-30 12:23:46,875 - memory_profile6_log - INFO -    410                             

2018-04-30 12:23:46,875 - memory_profile6_log - INFO -    411    871.6 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-30 12:23:46,875 - memory_profile6_log - INFO - 


2018-04-30 12:23:46,881 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-30 12:23:46,963 - memory_profile6_log - INFO - train on: 427635 total genuine interest data(D(u, t))
2018-04-30 12:23:46,964 - memory_profile6_log - INFO - transform on: 1004890 total current data(D(t))
2018-04-30 12:23:46,967 - memory_profile6_log - INFO - apply on: 427635 total history...)
2018-04-30 12:23:47,357 - memory_profile6_log - INFO - len of uniques_fit_hist:427635
2018-04-30 12:23:47,622 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:52258
2018-04-30 12:23:48,907 - memory_profile6_log - INFO - Len of model_fit: 427635
2018-04-30 12:23:48,908 - memory_profile6_log - INFO - Len of df_dut: 427635
2018-04-30 12:23:51,200 - memory_profile6_log - INFO - Len of fitted_models on main class: 427635
2018-04-30 12:23:51,201 - memory_profile6_log - INFO - 

2018-04-30 12:23:51,203 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-30 12:23:51,204 - memory_profile6_log - INFO - 

2018-04-30 12:23:51,286 - memory_profile6_log - INFO - len of fitted models after concat: 855270
2018-04-30 12:23:51,288 - memory_profile6_log - INFO - 

2018-04-30 12:23:51,293 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-30 12:23:51,296 - memory_profile6_log - INFO - 

2018-04-30 12:23:51,940 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 427635
2018-04-30 12:23:51,943 - memory_profile6_log - INFO - 

2018-04-30 12:24:53,121 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-30 12:24:53,170 - memory_profile6_log - INFO - Len of model_transform: 391679
2018-04-30 12:24:53,171 - memory_profile6_log - INFO - Len of df_dt: 1004890
2018-04-30 12:24:53,173 - memory_profile6_log - INFO - Total train time: 66.207s
2018-04-30 12:24:53,174 - memory_profile6_log - INFO - memory left before cleaning: 76.700 percent memory...
2018-04-30 12:24:53,176 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-30 12:24:53,177 - memory_profile6_log - INFO - deleting df_dut...
2018-04-30 12:24:53,177 - memory_profile6_log - INFO - deleting df_dt...
2018-04-30 12:24:53,178 - memory_profile6_log - INFO - deleting df_input...
2018-04-30 12:24:53,220 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-30 12:24:53,220 - memory_profile6_log - INFO - deleting df_current...
2018-04-30 12:24:53,223 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-30 12:24:53,243 - memory_profile6_log - INFO - deleting model_fit...
2018-04-30 12:24:53,246 - memory_profile6_log - INFO - deleting result...
2018-04-30 12:24:53,278 - memory_profile6_log - INFO - deleting model_transform...
2018-04-30 12:24:53,279 - memory_profile6_log - INFO - memory left after cleaning: 76.300 percent memory...
2018-04-30 12:24:53,279 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-30 12:24:53,280 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-30 12:24:53,282 - memory_profile6_log - INFO - Saving main Transform model to Google DataStore...
2018-04-30 12:24:53,282 - memory_profile6_log - INFO - Saving total data: 391679
2018-04-30 12:24:53,473 - memory_profile6_log - INFO - Saving fitted_models as history to Google DataStore...
2018-04-30 12:24:53,565 - memory_profile6_log - INFO - Saving total data: 427635
2018-04-30 12:24:53,605 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 10
2018-04-30 12:24:53,605 - memory_profile6_log - INFO - processing batch-0
2018-04-30 12:24:53,801 - memory_profile6_log - INFO - processing batch-1
2018-04-30 12:24:53,990 - memory_profile6_log - INFO - processing batch-2
2018-04-30 12:24:54,178 - memory_profile6_log - INFO - processing batch-3
2018-04-30 12:24:54,378 - memory_profile6_log - INFO - processing batch-4
2018-04-30 12:24:54,569 - memory_profile6_log - INFO - processing batch-5
2018-04-30 12:24:54,766 - memory_profile6_log - INFO - processing batch-6
2018-04-30 12:24:54,960 - memory_profile6_log - INFO - processing batch-7
2018-04-30 12:24:55,151 - memory_profile6_log - INFO - processing batch-8
2018-04-30 12:24:55,342 - memory_profile6_log - INFO - processing batch-9
2018-04-30 12:24:55,532 - memory_profile6_log - INFO - deleting X_split...
2018-04-30 12:24:55,533 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-04-30 12:24:55,536 - memory_profile6_log - INFO - deleting BR...
2018-04-30 12:24:55,556 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 12:24:55,558 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 12:24:55,558 - memory_profile6_log - INFO - ================================================

2018-04-30 12:24:55,559 - memory_profile6_log - INFO -    113    822.6 MiB    822.6 MiB   @profile

2018-04-30 12:24:55,559 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-30 12:24:55,559 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-30 12:24:55,559 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-30 12:24:55,559 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-30 12:24:55,561 - memory_profile6_log - INFO -    118                                      saveto="datastore", fitted_models_hist=None):

2018-04-30 12:24:55,561 - memory_profile6_log - INFO -    119                                 """

2018-04-30 12:24:55,562 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-30 12:24:55,562 - memory_profile6_log - INFO -    121                                 """

2018-04-30 12:24:55,562 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-30 12:24:55,562 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-30 12:24:55,562 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-30 12:24:55,563 - memory_profile6_log - INFO -    125    822.6 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-30 12:24:55,563 - memory_profile6_log - INFO -    126    836.1 MiB     13.5 MiB       df_dut = df_input.copy(deep=True)

2018-04-30 12:24:55,565 - memory_profile6_log - INFO -    127    836.1 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 12:24:55,571 - memory_profile6_log - INFO -    128                             

2018-04-30 12:24:55,571 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-30 12:24:55,572 - memory_profile6_log - INFO -    130    867.7 MiB     31.6 MiB       df_dt = df_current.copy(deep=True)

2018-04-30 12:24:55,572 - memory_profile6_log - INFO -    131    867.7 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 12:24:55,575 - memory_profile6_log - INFO -    132                             

2018-04-30 12:24:55,575 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-30 12:24:55,576 - memory_profile6_log - INFO -    134    867.7 MiB      0.0 MiB       t0 = time.time()

2018-04-30 12:24:55,576 - memory_profile6_log - INFO -    135    867.7 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-30 12:24:55,576 - memory_profile6_log - INFO -    136    867.7 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-30 12:24:55,578 - memory_profile6_log - INFO -    137    867.7 MiB      0.0 MiB       logger.info("apply on: %d total history...)", len(df_hist))

2018-04-30 12:24:55,578 - memory_profile6_log - INFO -    138                             

2018-04-30 12:24:55,578 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-30 12:24:55,582 - memory_profile6_log - INFO -    140    867.7 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-30 12:24:55,582 - memory_profile6_log - INFO -    141                             

2018-04-30 12:24:55,585 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-30 12:24:55,585 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-30 12:24:55,585 - memory_profile6_log - INFO -    144    871.2 MiB      3.5 MiB       NB = BR.processX(df_dut)

2018-04-30 12:24:55,585 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-30 12:24:55,585 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-30 12:24:55,586 - memory_profile6_log - INFO -    147    887.7 MiB     16.5 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-30 12:24:55,586 - memory_profile6_log - INFO -    148                                 """

2018-04-30 12:24:55,588 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-30 12:24:55,588 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-30 12:24:55,588 - memory_profile6_log - INFO -    151                                 """

2018-04-30 12:24:55,588 - memory_profile6_log - INFO -    152    887.7 MiB      0.0 MiB       fitby_sigmant = False

2018-04-30 12:24:55,592 - memory_profile6_log - INFO -    153    887.7 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-30 12:24:55,594 - memory_profile6_log - INFO -    154    887.7 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-30 12:24:55,595 - memory_profile6_log - INFO -    155    904.1 MiB     16.3 MiB                            'is_general']]

2018-04-30 12:24:55,595 - memory_profile6_log - INFO -    156                                 # get sigma_Nt from fitted_models_hist

2018-04-30 12:24:55,595 - memory_profile6_log - INFO -    157    910.6 MiB      6.5 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-04-30 12:24:55,596 - memory_profile6_log - INFO -    158    910.6 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-04-30 12:24:55,598 - memory_profile6_log - INFO -    159    906.3 MiB     -4.3 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-04-30 12:24:55,598 - memory_profile6_log - INFO -    160    906.3 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-04-30 12:24:55,599 - memory_profile6_log - INFO -    161                             

2018-04-30 12:24:55,602 - memory_profile6_log - INFO -    162                                 # begin fit

2018-04-30 12:24:55,605 - memory_profile6_log - INFO -    163    906.3 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-30 12:24:55,605 - memory_profile6_log - INFO -    164    906.3 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-30 12:24:55,605 - memory_profile6_log - INFO -    165    950.7 MiB     44.4 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-04-30 12:24:55,607 - memory_profile6_log - INFO -    166    950.7 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-30 12:24:55,608 - memory_profile6_log - INFO -    167    950.7 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-30 12:24:55,609 - memory_profile6_log - INFO -    168                                 # print model_fit[['num_x', 'num_x', 'sigma_Nt', 'date_all_click']].loc[model_fit['user_id']=="1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157"].head(5)

2018-04-30 12:24:55,609 - memory_profile6_log - INFO -    169                             

2018-04-30 12:24:55,611 - memory_profile6_log - INFO -    170                                 # ~~ and Transform ~~

2018-04-30 12:24:55,611 - memory_profile6_log - INFO -    171                                 #   handling current news interest == current date

2018-04-30 12:24:55,615 - memory_profile6_log - INFO -    172    950.7 MiB      0.0 MiB       if df_dt.empty:

2018-04-30 12:24:55,618 - memory_profile6_log - INFO -    173                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-30 12:24:55,618 - memory_profile6_log - INFO -    174                                     return None

2018-04-30 12:24:55,618 - memory_profile6_log - INFO -    175    955.8 MiB      5.1 MiB       NB = BR.processX(df_dt)

2018-04-30 12:24:55,619 - memory_profile6_log - INFO -    176   1001.9 MiB     46.1 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-30 12:24:55,621 - memory_profile6_log - INFO -    177                             

2018-04-30 12:24:55,621 - memory_profile6_log - INFO -    178   1001.9 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-30 12:24:55,621 - memory_profile6_log - INFO -    179   1023.9 MiB     22.0 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-30 12:24:55,622 - memory_profile6_log - INFO -    180                             

2018-04-30 12:24:55,625 - memory_profile6_log - INFO -    181                                 # print "model_fit dtypes:\n", model_fit.dtypes

2018-04-30 12:24:55,630 - memory_profile6_log - INFO -    182                                 # print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-04-30 12:24:55,630 - memory_profile6_log - INFO -    183   1023.9 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-30 12:24:55,631 - memory_profile6_log - INFO -    184   1023.9 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-30 12:24:55,631 - memory_profile6_log - INFO -    185   1023.9 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-04-30 12:24:55,634 - memory_profile6_log - INFO -    186   1033.7 MiB      9.8 MiB                                                                                           "topic_id", "user_id"]],

2018-04-30 12:24:55,634 - memory_profile6_log - INFO -    187   1055.5 MiB     21.8 MiB                                                     verbose=False)

2018-04-30 12:24:55,637 - memory_profile6_log - INFO -    188                                 # ~~~ filter is general and specific topic ~~~

2018-04-30 12:24:55,638 - memory_profile6_log - INFO -    189                                 # the idea is just we need to rerank every topic according

2018-04-30 12:24:55,640 - memory_profile6_log - INFO -    190                                 # user_id and and is_general by p0_posterior

2018-04-30 12:24:55,642 - memory_profile6_log - INFO -    191   1055.5 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-30 12:24:55,642 - memory_profile6_log - INFO -    192   1058.8 MiB      3.3 MiB                                     'is_general']].groupby(['topic_id',

2018-04-30 12:24:55,644 - memory_profile6_log - INFO -    193   1055.5 MiB     -3.2 MiB                                                             'is_general']

2018-04-30 12:24:55,644 - memory_profile6_log - INFO -    194                                                                                      ).size().to_frame().reset_index()

2018-04-30 12:24:55,644 - memory_profile6_log - INFO -    195                             

2018-04-30 12:24:55,648 - memory_profile6_log - INFO -    196                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-30 12:24:55,648 - memory_profile6_log - INFO -    197                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-30 12:24:55,651 - memory_profile6_log - INFO -    198   1055.5 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-30 12:24:55,654 - memory_profile6_log - INFO -    199                             

2018-04-30 12:24:55,654 - memory_profile6_log - INFO -    200                                 # ~ start by provide rank for each topic type ~

2018-04-30 12:24:55,654 - memory_profile6_log - INFO -    201   1096.7 MiB     41.1 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-30 12:24:55,655 - memory_profile6_log - INFO -    202   1103.3 MiB      6.6 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-30 12:24:55,658 - memory_profile6_log - INFO -    203                             

2018-04-30 12:24:55,660 - memory_profile6_log - INFO -    204                                 # ~ set threshold to filter output

2018-04-30 12:24:55,661 - memory_profile6_log - INFO -    205   1103.3 MiB      0.0 MiB       if threshold > 0:

2018-04-30 12:24:55,663 - memory_profile6_log - INFO -    206   1103.3 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-30 12:24:55,664 - memory_profile6_log - INFO -    207   1103.3 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-30 12:24:55,664 - memory_profile6_log - INFO -    208   1101.1 MiB     -2.2 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-30 12:24:55,670 - memory_profile6_log - INFO -    209                             

2018-04-30 12:24:55,670 - memory_profile6_log - INFO -    210   1101.1 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-30 12:24:55,671 - memory_profile6_log - INFO -    211   1101.1 MiB      0.0 MiB       logger.info("Len of model_transform: %d", len(model_transform))

2018-04-30 12:24:55,671 - memory_profile6_log - INFO -    212   1101.1 MiB      0.0 MiB       logger.info("Len of df_dt: %d", len(df_dt))

2018-04-30 12:24:55,673 - memory_profile6_log - INFO -    213   1101.1 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-30 12:24:55,673 - memory_profile6_log - INFO -    214                             

2018-04-30 12:24:55,676 - memory_profile6_log - INFO -    215   1101.1 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-30 12:24:55,676 - memory_profile6_log - INFO -    216                             

2018-04-30 12:24:55,677 - memory_profile6_log - INFO -    217   1101.1 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-30 12:24:55,677 - memory_profile6_log - INFO -    218   1101.1 MiB      0.0 MiB       del df_dut

2018-04-30 12:24:55,677 - memory_profile6_log - INFO -    219   1101.1 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-30 12:24:55,680 - memory_profile6_log - INFO -    220   1101.1 MiB      0.0 MiB       del df_dt

2018-04-30 12:24:55,681 - memory_profile6_log - INFO -    221   1101.1 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-30 12:24:55,683 - memory_profile6_log - INFO -    222   1101.1 MiB      0.0 MiB       del df_input

2018-04-30 12:24:55,684 - memory_profile6_log - INFO -    223   1101.1 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-30 12:24:55,684 - memory_profile6_log - INFO -    224   1062.7 MiB    -38.3 MiB       del df_input_X

2018-04-30 12:24:55,684 - memory_profile6_log - INFO -    225   1062.7 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-30 12:24:55,686 - memory_profile6_log - INFO -    226   1062.7 MiB      0.0 MiB       del df_current

2018-04-30 12:24:55,686 - memory_profile6_log - INFO -    227   1062.7 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-30 12:24:55,687 - memory_profile6_log - INFO -    228   1062.7 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-30 12:24:55,687 - memory_profile6_log - INFO -    229   1062.7 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-30 12:24:55,687 - memory_profile6_log - INFO -    230   1026.8 MiB    -35.9 MiB       del model_fit

2018-04-30 12:24:55,687 - memory_profile6_log - INFO -    231   1026.8 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-30 12:24:55,691 - memory_profile6_log - INFO -    232   1026.8 MiB      0.0 MiB       del result

2018-04-30 12:24:55,694 - memory_profile6_log - INFO -    233   1026.8 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-30 12:24:55,694 - memory_profile6_log - INFO -    234                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 12:24:55,696 - memory_profile6_log - INFO -    235                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 12:24:55,697 - memory_profile6_log - INFO -    236                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 12:24:55,697 - memory_profile6_log - INFO -    237   1026.8 MiB      0.0 MiB       if savetrain:

2018-04-30 12:24:55,697 - memory_profile6_log - INFO -    238   1035.8 MiB      9.0 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-30 12:24:55,697 - memory_profile6_log - INFO -    239   1035.8 MiB      0.0 MiB           del model_transform

2018-04-30 12:24:55,704 - memory_profile6_log - INFO -    240   1035.8 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-30 12:24:55,706 - memory_profile6_log - INFO -    241   1035.8 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-30 12:24:55,710 - memory_profile6_log - INFO -    242                             

2018-04-30 12:24:55,710 - memory_profile6_log - INFO -    243   1035.8 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-30 12:24:55,710 - memory_profile6_log - INFO -    244                                     # ~ Place your code to save the training model here ~

2018-04-30 12:24:55,710 - memory_profile6_log - INFO -    245   1035.8 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-30 12:24:55,711 - memory_profile6_log - INFO -    246   1035.8 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-30 12:24:55,711 - memory_profile6_log - INFO -    247   1035.8 MiB      0.0 MiB               if multproc:

2018-04-30 12:24:55,714 - memory_profile6_log - INFO -    248                                             # ~ save transform models ~

2018-04-30 12:24:55,716 - memory_profile6_log - INFO -    249   1035.8 MiB      0.0 MiB                   logger.info("Saving main Transform model to Google DataStore...")

2018-04-30 12:24:55,717 - memory_profile6_log - INFO -    250   1035.8 MiB      0.0 MiB                   logger.info("Saving total data: %d", len(model_transformsv))

2018-04-30 12:24:55,717 - memory_profile6_log - INFO -    251    995.8 MiB    -40.0 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-30 12:24:55,719 - memory_profile6_log - INFO -    252                             

2018-04-30 12:24:55,719 - memory_profile6_log - INFO -    253                                             # ~ save fitted models ~

2018-04-30 12:24:55,720 - memory_profile6_log - INFO -    254    995.8 MiB      0.0 MiB                   logger.info("Saving fitted_models as history to Google DataStore...")

2018-04-30 12:24:55,720 - memory_profile6_log - INFO -    255    995.8 MiB      0.0 MiB                   save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-30 12:24:55,720 - memory_profile6_log - INFO -    256   1019.6 MiB     23.7 MiB                   fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-30 12:24:55,720 - memory_profile6_log - INFO -    257   1019.6 MiB      0.0 MiB                   logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-04-30 12:24:55,720 - memory_profile6_log - INFO -    258   1030.8 MiB     11.2 MiB                   X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-30 12:24:55,721 - memory_profile6_log - INFO -    259   1030.8 MiB      0.0 MiB                   logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-30 12:24:55,723 - memory_profile6_log - INFO -    260   1033.4 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-30 12:24:55,727 - memory_profile6_log - INFO -    261   1033.4 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-30 12:24:55,730 - memory_profile6_log - INFO -    262   1033.4 MiB      2.6 MiB                       mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-30 12:24:55,730 - memory_profile6_log - INFO -    263                             

2018-04-30 12:24:55,730 - memory_profile6_log - INFO -    264   1027.4 MiB     -5.9 MiB                   del X_split

2018-04-30 12:24:55,732 - memory_profile6_log - INFO -    265   1027.4 MiB      0.0 MiB                   logger.info("deleting X_split...")

2018-04-30 12:24:55,733 - memory_profile6_log - INFO -    266   1027.4 MiB      0.0 MiB                   del save_sigma_nt

2018-04-30 12:24:55,734 - memory_profile6_log - INFO -    267   1027.4 MiB      0.0 MiB                   logger.info("deleting save_sigma_nt...")

2018-04-30 12:24:55,736 - memory_profile6_log - INFO -    268                             

2018-04-30 12:24:55,736 - memory_profile6_log - INFO -    269   1027.4 MiB      0.0 MiB                   del BR

2018-04-30 12:24:55,740 - memory_profile6_log - INFO -    270   1027.4 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-30 12:24:55,740 - memory_profile6_log - INFO -    271                             

2018-04-30 12:24:55,743 - memory_profile6_log - INFO -    272                                     elif str(saveto).lower() == "elastic":

2018-04-30 12:24:55,743 - memory_profile6_log - INFO -    273                                         logger.info("Using ElasticSearch as storage...")

2018-04-30 12:24:55,743 - memory_profile6_log - INFO -    274                                         logging.info("Saving main Transform model to Elasticsearch...")

2018-04-30 12:24:55,743 - memory_profile6_log - INFO -    275                                         mh.saveElasticS(model_transformsv)

2018-04-30 12:24:55,744 - memory_profile6_log - INFO -    276                             

2018-04-30 12:24:55,744 - memory_profile6_log - INFO -    277                                     # need save sigma_nt for daily train

2018-04-30 12:24:55,746 - memory_profile6_log - INFO -    278                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-30 12:24:55,746 - memory_profile6_log - INFO -    279                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-30 12:24:55,746 - memory_profile6_log - INFO -    280   1027.4 MiB      0.0 MiB           if start_date and end_date:

2018-04-30 12:24:55,746 - memory_profile6_log - INFO -    281                                         if not fitby_sigmant:

2018-04-30 12:24:55,747 - memory_profile6_log - INFO -    282                                             logging.info("Saving sigma Nt...")

2018-04-30 12:24:55,747 - memory_profile6_log - INFO -    283                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-30 12:24:55,753 - memory_profile6_log - INFO -    284                                             save_sigma_nt['start_date'] = start_date

2018-04-30 12:24:55,753 - memory_profile6_log - INFO -    285                                             save_sigma_nt['end_date'] = end_date

2018-04-30 12:24:55,755 - memory_profile6_log - INFO -    286                                             print save_sigma_nt.head(5)

2018-04-30 12:24:55,756 - memory_profile6_log - INFO -    287                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-30 12:24:55,756 - memory_profile6_log - INFO -    288                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-30 12:24:55,759 - memory_profile6_log - INFO -    289   1027.4 MiB      0.0 MiB       return

2018-04-30 12:24:55,759 - memory_profile6_log - INFO - 


2018-04-30 12:24:55,759 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-30 12:28:43,887 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-30 12:28:43,890 - memory_profile6_log - INFO - date_generated: 
2018-04-30 12:28:43,891 - memory_profile6_log - INFO -  
2018-04-30 12:28:43,891 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 10, 0, 0)]
2018-04-30 12:28:43,891 - memory_profile6_log - INFO - 

2018-04-30 12:28:43,891 - memory_profile6_log - INFO - using current date: 2018-04-11
2018-04-30 12:28:43,892 - memory_profile6_log - INFO - using start date: 2018-04-10 00:00:00
2018-04-30 12:28:43,892 - memory_profile6_log - INFO - using end date: 2018-04-10
2018-04-30 12:28:44,032 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-30 12:28:44,035 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-10
2018-04-30 12:30:44,026 - memory_profile6_log - INFO - size of df: 108.51 MB
2018-04-30 12:30:44,028 - memory_profile6_log - INFO - getting total: 427635 training data(genuine interest) for date: 2018-04-10
2018-04-30 12:30:44,114 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-30 12:30:44,115 - memory_profile6_log - INFO - Appending history data...
2018-04-30 12:30:44,117 - memory_profile6_log - INFO - processing batch-0
2018-04-30 12:30:44,118 - memory_profile6_log - INFO - creating list history data...
2018-04-30 12:30:44,265 - memory_profile6_log - INFO - call history data...
2018-04-30 12:32:12,625 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-30 12:32:12,628 - memory_profile6_log - INFO - date_generated: 
2018-04-30 12:32:12,628 - memory_profile6_log - INFO -  
2018-04-30 12:32:12,630 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 10, 0, 0)]
2018-04-30 12:32:12,630 - memory_profile6_log - INFO - 

2018-04-30 12:32:12,631 - memory_profile6_log - INFO - using current date: 2018-04-11
2018-04-30 12:32:12,631 - memory_profile6_log - INFO - using start date: 2018-04-10 00:00:00
2018-04-30 12:32:12,631 - memory_profile6_log - INFO - using end date: 2018-04-10
2018-04-30 12:32:12,769 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-30 12:32:12,773 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-10
2018-04-30 12:33:07,244 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-30 12:33:07,246 - memory_profile6_log - INFO - date_generated: 
2018-04-30 12:33:07,247 - memory_profile6_log - INFO -  
2018-04-30 12:33:07,249 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 10, 0, 0)]
2018-04-30 12:33:07,249 - memory_profile6_log - INFO - 

2018-04-30 12:33:07,250 - memory_profile6_log - INFO - using current date: 2018-04-11
2018-04-30 12:33:07,250 - memory_profile6_log - INFO - using start date: 2018-04-10 00:00:00
2018-04-30 12:33:07,250 - memory_profile6_log - INFO - using end date: 2018-04-10
2018-04-30 12:33:07,417 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-30 12:33:07,424 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-10
2018-04-30 12:35:07,769 - memory_profile6_log - INFO - size of df: 108.51 MB
2018-04-30 12:35:07,769 - memory_profile6_log - INFO - getting total: 427635 training data(genuine interest) for date: 2018-04-10
2018-04-30 12:35:07,832 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-30 12:35:07,835 - memory_profile6_log - INFO - Appending history data...
2018-04-30 12:35:07,835 - memory_profile6_log - INFO - processing batch-0
2018-04-30 12:35:07,838 - memory_profile6_log - INFO - creating list history data...
2018-04-30 12:35:07,947 - memory_profile6_log - INFO - call history data...
2018-04-30 12:35:59,851 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 12:36:01,576 - memory_profile6_log - INFO - processing batch-1
2018-04-30 12:36:01,578 - memory_profile6_log - INFO - creating list history data...
2018-04-30 12:36:01,665 - memory_profile6_log - INFO - call history data...
2018-04-30 12:36:53,385 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 12:36:55,121 - memory_profile6_log - INFO - processing batch-2
2018-04-30 12:36:55,124 - memory_profile6_log - INFO - creating list history data...
2018-04-30 12:36:55,200 - memory_profile6_log - INFO - call history data...
2018-04-30 12:37:46,140 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 12:37:47,875 - memory_profile6_log - INFO - processing batch-3
2018-04-30 12:37:47,877 - memory_profile6_log - INFO - creating list history data...
2018-04-30 12:37:47,961 - memory_profile6_log - INFO - call history data...
2018-04-30 12:38:40,872 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 12:38:42,640 - memory_profile6_log - INFO - processing batch-4
2018-04-30 12:38:42,641 - memory_profile6_log - INFO - creating list history data...
2018-04-30 12:38:42,720 - memory_profile6_log - INFO - call history data...
2018-04-30 12:39:36,210 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 12:39:37,961 - memory_profile6_log - INFO - Appending training data...
2018-04-30 12:39:37,963 - memory_profile6_log - INFO - len datalist: 1
2018-04-30 12:39:37,964 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-30 12:39:37,980 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 12:39:37,982 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 12:39:37,983 - memory_profile6_log - INFO - ================================================

2018-04-30 12:39:37,984 - memory_profile6_log - INFO -    323     86.9 MiB     86.9 MiB   @profile

2018-04-30 12:39:37,986 - memory_profile6_log - INFO -    324                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-30 12:39:37,986 - memory_profile6_log - INFO -    325     86.9 MiB      0.0 MiB       bq_client = client

2018-04-30 12:39:37,989 - memory_profile6_log - INFO -    326     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 12:39:37,993 - memory_profile6_log - INFO -    327                             

2018-04-30 12:39:37,996 - memory_profile6_log - INFO -    328     86.9 MiB      0.0 MiB       datalist = []

2018-04-30 12:39:37,996 - memory_profile6_log - INFO -    329     86.9 MiB      0.0 MiB       datalist_hist = []

2018-04-30 12:39:37,997 - memory_profile6_log - INFO -    330                             

2018-04-30 12:39:37,999 - memory_profile6_log - INFO -    331     86.9 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-30 12:39:38,000 - memory_profile6_log - INFO -    332    687.9 MiB      0.0 MiB       for ndate in date_generated:

2018-04-30 12:39:38,000 - memory_profile6_log - INFO -    333    393.4 MiB    306.5 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-30 12:39:38,002 - memory_profile6_log - INFO -    334    393.4 MiB      0.0 MiB           if tframe is not None:

2018-04-30 12:39:38,003 - memory_profile6_log - INFO -    335    393.4 MiB      0.0 MiB               if not tframe.empty:

2018-04-30 12:39:38,006 - memory_profile6_log - INFO -    336    407.3 MiB     13.9 MiB                   X_split = np.array_split(tframe, 5)

2018-04-30 12:39:38,006 - memory_profile6_log - INFO -    337    407.3 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-30 12:39:38,007 - memory_profile6_log - INFO -    338    407.3 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-30 12:39:38,009 - memory_profile6_log - INFO -    339    687.9 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-30 12:39:38,009 - memory_profile6_log - INFO -    340                                                 # ~ loading history

2018-04-30 12:39:38,010 - memory_profile6_log - INFO -    341                                                 """

2018-04-30 12:39:38,012 - memory_profile6_log - INFO -    342                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-30 12:39:38,013 - memory_profile6_log - INFO -    343                                                 """

2018-04-30 12:39:38,013 - memory_profile6_log - INFO -    344    659.1 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-30 12:39:38,017 - memory_profile6_log - INFO -    345                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-30 12:39:38,019 - memory_profile6_log - INFO -    346    659.1 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-30 12:39:38,020 - memory_profile6_log - INFO -    347                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-30 12:39:38,022 - memory_profile6_log - INFO -    348    660.4 MiB      6.9 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-30 12:39:38,023 - memory_profile6_log - INFO -    349                             

2018-04-30 12:39:38,023 - memory_profile6_log - INFO -    350    660.4 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-30 12:39:38,026 - memory_profile6_log - INFO -    351    738.6 MiB    565.7 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-30 12:39:38,026 - memory_profile6_log - INFO -    352                             

2018-04-30 12:39:38,029 - memory_profile6_log - INFO -    353                                                 # me = os.getpid()

2018-04-30 12:39:38,029 - memory_profile6_log - INFO -    354                                                 # kill_proc_tree(me)

2018-04-30 12:39:38,030 - memory_profile6_log - INFO -    355                             

2018-04-30 12:39:38,032 - memory_profile6_log - INFO -    356    738.6 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-30 12:39:38,032 - memory_profile6_log - INFO -    357    740.6 MiB   -777.6 MiB                       for m in h_frame:

2018-04-30 12:39:38,032 - memory_profile6_log - INFO -    358    740.6 MiB   -771.9 MiB                           if m is not None:

2018-04-30 12:39:38,033 - memory_profile6_log - INFO -    359    740.6 MiB   -771.9 MiB                               if len(m) > 0:

2018-04-30 12:39:38,036 - memory_profile6_log - INFO -    360    740.6 MiB   -767.9 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-30 12:39:38,038 - memory_profile6_log - INFO -    361    688.9 MiB   -298.2 MiB                       del h_frame

2018-04-30 12:39:38,039 - memory_profile6_log - INFO -    362    687.9 MiB     -3.3 MiB                       del lhistory

2018-04-30 12:39:38,039 - memory_profile6_log - INFO -    363                             

2018-04-30 12:39:38,040 - memory_profile6_log - INFO -    364    687.9 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-30 12:39:38,042 - memory_profile6_log - INFO -    365    687.9 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-30 12:39:38,042 - memory_profile6_log - INFO -    366                                     else: 

2018-04-30 12:39:38,043 - memory_profile6_log - INFO -    367                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-30 12:39:38,045 - memory_profile6_log - INFO -    368    687.9 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-30 12:39:38,046 - memory_profile6_log - INFO -    369    687.9 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-30 12:39:38,049 - memory_profile6_log - INFO -    370                             

2018-04-30 12:39:38,049 - memory_profile6_log - INFO -    371    687.9 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-30 12:39:38,052 - memory_profile6_log - INFO - 


2018-04-30 12:39:39,390 - memory_profile6_log - INFO - size of big_frame_hist: 117.89 MB
2018-04-30 12:39:39,506 - memory_profile6_log - INFO - size of big_frame: 108.51 MB
2018-04-30 12:39:39,526 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-04-30 12:43:27,658 - memory_profile6_log - INFO - size of df: 305.27 MB
2018-04-30 12:43:27,660 - memory_profile6_log - INFO - getting total: 1249791 training data(current date interest)
2018-04-30 12:43:28,049 - memory_profile6_log - INFO - size of current_frame: 314.81 MB
2018-04-30 12:43:28,052 - memory_profile6_log - INFO - loading time of: 1677426 total genuine-current interest data ~ take 620.676s
2018-04-30 12:43:28,075 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 12:43:28,076 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 12:43:28,078 - memory_profile6_log - INFO - ================================================

2018-04-30 12:43:28,078 - memory_profile6_log - INFO -    373     86.8 MiB     86.8 MiB   @profile

2018-04-30 12:43:28,079 - memory_profile6_log - INFO -    374                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-30 12:43:28,081 - memory_profile6_log - INFO -    375     86.9 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-30 12:43:28,082 - memory_profile6_log - INFO -    376     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 12:43:28,082 - memory_profile6_log - INFO -    377                             

2018-04-30 12:43:28,082 - memory_profile6_log - INFO -    378                                 # ~~~ Begin collecting data ~~~

2018-04-30 12:43:28,084 - memory_profile6_log - INFO -    379     86.9 MiB      0.0 MiB       t0 = time.time()

2018-04-30 12:43:28,085 - memory_profile6_log - INFO -    380    675.4 MiB    588.5 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-30 12:43:28,085 - memory_profile6_log - INFO -    381    675.4 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-30 12:43:28,085 - memory_profile6_log - INFO -    382                                     logger.info("Training cannot be empty..")

2018-04-30 12:43:28,089 - memory_profile6_log - INFO -    383                                     return False

2018-04-30 12:43:28,092 - memory_profile6_log - INFO -    384    700.8 MiB     25.3 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-30 12:43:28,092 - memory_profile6_log - INFO -    385                                 # print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-30 12:43:28,094 - memory_profile6_log - INFO -    386    700.8 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-30 12:43:28,094 - memory_profile6_log - INFO -    387                             

2018-04-30 12:43:28,095 - memory_profile6_log - INFO -    388    710.9 MiB     10.1 MiB       big_frame = pd.concat(datalist)

2018-04-30 12:43:28,095 - memory_profile6_log - INFO -    389    710.9 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-30 12:43:28,095 - memory_profile6_log - INFO -    390    700.7 MiB    -10.2 MiB       del datalist

2018-04-30 12:43:28,101 - memory_profile6_log - INFO -    391                             

2018-04-30 12:43:28,101 - memory_profile6_log - INFO -    392    700.7 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 12:43:28,102 - memory_profile6_log - INFO -    393                             

2018-04-30 12:43:28,104 - memory_profile6_log - INFO -    394                                 # ~ get current news interest ~

2018-04-30 12:43:28,104 - memory_profile6_log - INFO -    395    700.7 MiB      0.0 MiB       if not cd:

2018-04-30 12:43:28,107 - memory_profile6_log - INFO -    396    700.7 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-04-30 12:43:28,107 - memory_profile6_log - INFO -    397    894.9 MiB    194.2 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-30 12:43:28,108 - memory_profile6_log - INFO -    398    894.9 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-30 12:43:28,108 - memory_profile6_log - INFO -    399                                 else:

2018-04-30 12:43:28,108 - memory_profile6_log - INFO -    400                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-30 12:43:28,108 - memory_profile6_log - INFO -    401                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-30 12:43:28,112 - memory_profile6_log - INFO -    402                             

2018-04-30 12:43:28,114 - memory_profile6_log - INFO -    403                                     # safe handling of query parameter

2018-04-30 12:43:28,115 - memory_profile6_log - INFO -    404                                     query_params = [

2018-04-30 12:43:28,115 - memory_profile6_log - INFO -    405                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-30 12:43:28,117 - memory_profile6_log - INFO -    406                                     ]

2018-04-30 12:43:28,117 - memory_profile6_log - INFO -    407                             

2018-04-30 12:43:28,117 - memory_profile6_log - INFO -    408                                     job_config.query_parameters = query_params

2018-04-30 12:43:28,118 - memory_profile6_log - INFO -    409                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-30 12:43:28,118 - memory_profile6_log - INFO -    410                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-30 12:43:28,118 - memory_profile6_log - INFO -    411                             

2018-04-30 12:43:28,118 - memory_profile6_log - INFO -    412    904.4 MiB      9.5 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-30 12:43:28,119 - memory_profile6_log - INFO -    413    904.4 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-30 12:43:28,119 - memory_profile6_log - INFO -    414    904.5 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-30 12:43:28,124 - memory_profile6_log - INFO -    415    904.5 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-30 12:43:28,125 - memory_profile6_log - INFO -    416    904.5 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-30 12:43:28,125 - memory_profile6_log - INFO -    417    904.5 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-30 12:43:28,125 - memory_profile6_log - INFO -    418                             

2018-04-30 12:43:28,127 - memory_profile6_log - INFO -    419    904.5 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-30 12:43:28,128 - memory_profile6_log - INFO - 


2018-04-30 12:43:28,131 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-30 12:43:28,239 - memory_profile6_log - INFO - train on: 427635 total genuine interest data(D(u, t))
2018-04-30 12:43:28,240 - memory_profile6_log - INFO - transform on: 1249791 total current data(D(t))
2018-04-30 12:43:28,240 - memory_profile6_log - INFO - apply on: 427635 total history...)
2018-04-30 12:43:28,642 - memory_profile6_log - INFO - len of uniques_fit_hist:427635
2018-04-30 12:43:28,951 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:52258
2018-04-30 12:43:30,244 - memory_profile6_log - INFO - Len of model_fit: 427635
2018-04-30 12:43:30,246 - memory_profile6_log - INFO - Len of df_dut: 427635
2018-04-30 12:43:33,141 - memory_profile6_log - INFO - Len of fitted_models on main class: 427635
2018-04-30 12:43:33,141 - memory_profile6_log - INFO - 

2018-04-30 12:44:33,727 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-30 12:44:34,086 - memory_profile6_log - INFO -                                                   user_id           topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt  p0_posterior  is_general  rank
246500  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22601470         261.954057             271.954057   0.062761       128      0.123681        True   1.0
246509  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27431099         302.340224             312.340224   0.050504       128      0.114307        True   2.0
246499  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22553543          73.975879              83.975879   0.126607       128      0.077043        True   3.0
246497  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22552349         460.150210             470.150210   0.006228       128      0.021218        True   4.0
246506  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27428266         280.430762             290.430762   0.008460       128      0.017806        True   5.0
246503  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22661796          63.773328              73.773328   0.016518       128      0.008830        True   6.0
246498  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22553321         106.435097             116.435097   0.008790       128      0.007416        True   7.0
246496  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22552186         107.096185             117.096185   0.008537       128      0.007244        True   8.0
246495  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22551890         597.511881             607.511881   0.001489       128      0.006554        True   9.0
246501  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22615029        1790.762611            1800.762611   0.000271       128      0.003534        True  10.0
246504  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22662160          66.164565              76.164565   0.001654       128      0.000913        True  11.0
246489  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1164316987        1495.630731            1505.630731   0.024410       128      0.266317       False   1.0
246526  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           36060446         675.608172             685.608172   0.021372       128      0.106180       False   2.0
246502  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22661658       15474.025641           15484.025641   0.000180       128      0.020214       False   3.0
246529  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           39301645         142.030360             152.030360   0.015590       128      0.017176       False   4.0
246531  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           40410447        2793.921296            2803.921296   0.000800       128      0.016262       False   5.0
246538  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           67524823       44702.740741           44712.740741   0.000045       128      0.014500       False   6.0
246508  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27430823          79.731404              89.731404   0.017126       128      0.011136       False   7.0
246536  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           47091877         679.220034             689.220034   0.001193       128      0.005957       False   8.0
246477  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1017140824       40232.466667           40242.466667   0.000018       128      0.005354       False   9.0
246483  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1039394600        2698.153502            2708.153502   0.000209       128      0.004098       False  10.0
246507  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27428824         612.055781             622.055781   0.000811       128      0.003657       False  11.0
246505  1616f009d96b1-0285d8288a5bce-70217860-38400-16...          271947679         879.718659             889.718659   0.000499       128      0.003218       False  12.0
246512  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790313513        2601.237069            2611.237069   0.000137       128      0.002595       False  13.0
246533  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           40799617         290.697013             300.697013   0.000645       128      0.001406       False  14.0
246492  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1164317385       50290.583333           50300.583333   0.000003       128      0.001255       False  15.0
2018-04-30 12:44:34,092 - memory_profile6_log - INFO - 

2018-04-30 12:44:34,094 - memory_profile6_log - INFO - Len of model_transform: 393411
2018-04-30 12:44:34,095 - memory_profile6_log - INFO - Len of df_dt: 1249791
2018-04-30 12:44:34,095 - memory_profile6_log - INFO - Total train time: 65.546s
2018-04-30 12:44:34,096 - memory_profile6_log - INFO - memory left before cleaning: 76.400 percent memory...
2018-04-30 12:44:34,096 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-30 12:44:34,098 - memory_profile6_log - INFO - deleting df_dut...
2018-04-30 12:44:34,099 - memory_profile6_log - INFO - deleting df_dt...
2018-04-30 12:44:34,101 - memory_profile6_log - INFO - deleting df_input...
2018-04-30 12:44:34,157 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-30 12:44:34,158 - memory_profile6_log - INFO - deleting df_current...
2018-04-30 12:44:34,160 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-30 12:44:34,183 - memory_profile6_log - INFO - deleting model_fit...
2018-04-30 12:44:34,184 - memory_profile6_log - INFO - deleting result...
2018-04-30 12:44:34,220 - memory_profile6_log - INFO - deleting model_transform...
2018-04-30 12:44:34,221 - memory_profile6_log - INFO - memory left after cleaning: 75.900 percent memory...
2018-04-30 12:44:34,223 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-30 12:44:34,226 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-30 12:44:34,229 - memory_profile6_log - INFO - Saving main Transform model to Google DataStore...
2018-04-30 12:44:34,230 - memory_profile6_log - INFO - Saving total data: 393411
2018-04-30 12:44:34,423 - memory_profile6_log - INFO - Saving fitted_models as history to Google DataStore...
2018-04-30 12:44:34,515 - memory_profile6_log - INFO - Saving total data: 427635
2018-04-30 12:44:34,552 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 10
2018-04-30 12:44:34,559 - memory_profile6_log - INFO - processing batch-0
2018-04-30 12:44:34,750 - memory_profile6_log - INFO - processing batch-1
2018-04-30 12:44:34,936 - memory_profile6_log - INFO - processing batch-2
2018-04-30 12:44:35,125 - memory_profile6_log - INFO - processing batch-3
2018-04-30 12:44:35,312 - memory_profile6_log - INFO - processing batch-4
2018-04-30 12:44:35,497 - memory_profile6_log - INFO - processing batch-5
2018-04-30 12:44:35,686 - memory_profile6_log - INFO - processing batch-6
2018-04-30 12:44:35,875 - memory_profile6_log - INFO - processing batch-7
2018-04-30 12:44:36,075 - memory_profile6_log - INFO - processing batch-8
2018-04-30 12:44:36,263 - memory_profile6_log - INFO - processing batch-9
2018-04-30 12:44:36,460 - memory_profile6_log - INFO - deleting X_split...
2018-04-30 12:44:36,463 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-04-30 12:44:36,463 - memory_profile6_log - INFO - deleting BR...
2018-04-30 12:44:36,483 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 12:44:36,483 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 12:44:36,484 - memory_profile6_log - INFO - ================================================

2018-04-30 12:44:36,484 - memory_profile6_log - INFO -    113    897.3 MiB    897.3 MiB   @profile

2018-04-30 12:44:36,486 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-30 12:44:36,487 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-30 12:44:36,487 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-30 12:44:36,489 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-30 12:44:36,489 - memory_profile6_log - INFO -    118                                      saveto="datastore", fitted_models_hist=None):

2018-04-30 12:44:36,492 - memory_profile6_log - INFO -    119                                 """

2018-04-30 12:44:36,492 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-30 12:44:36,492 - memory_profile6_log - INFO -    121                                 """

2018-04-30 12:44:36,493 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-30 12:44:36,493 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-30 12:44:36,493 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-30 12:44:36,496 - memory_profile6_log - INFO -    125    897.3 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-30 12:44:36,497 - memory_profile6_log - INFO -    126    910.3 MiB     13.1 MiB       df_dut = df_input.copy(deep=True)

2018-04-30 12:44:36,497 - memory_profile6_log - INFO -    127    910.3 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 12:44:36,500 - memory_profile6_log - INFO -    128                             

2018-04-30 12:44:36,500 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-30 12:44:36,500 - memory_profile6_log - INFO -    130    949.7 MiB     39.3 MiB       df_dt = df_current.copy(deep=True)

2018-04-30 12:44:36,500 - memory_profile6_log - INFO -    131    949.7 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 12:44:36,500 - memory_profile6_log - INFO -    132                             

2018-04-30 12:44:36,500 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-30 12:44:36,502 - memory_profile6_log - INFO -    134    949.7 MiB      0.0 MiB       t0 = time.time()

2018-04-30 12:44:36,502 - memory_profile6_log - INFO -    135    949.7 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-30 12:44:36,502 - memory_profile6_log - INFO -    136    949.7 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-30 12:44:36,503 - memory_profile6_log - INFO -    137    949.7 MiB      0.0 MiB       logger.info("apply on: %d total history...)", len(df_hist))

2018-04-30 12:44:36,503 - memory_profile6_log - INFO -    138                             

2018-04-30 12:44:36,503 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-30 12:44:36,507 - memory_profile6_log - INFO -    140    949.7 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-30 12:44:36,510 - memory_profile6_log - INFO -    141                             

2018-04-30 12:44:36,510 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-30 12:44:36,510 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-30 12:44:36,512 - memory_profile6_log - INFO -    144    950.7 MiB      1.0 MiB       NB = BR.processX(df_dut)

2018-04-30 12:44:36,513 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-30 12:44:36,513 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-30 12:44:36,513 - memory_profile6_log - INFO -    147    967.2 MiB     16.5 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-30 12:44:36,519 - memory_profile6_log - INFO -    148                                 """

2018-04-30 12:44:36,519 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-30 12:44:36,523 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-30 12:44:36,523 - memory_profile6_log - INFO -    151                                 """

2018-04-30 12:44:36,523 - memory_profile6_log - INFO -    152    967.2 MiB      0.0 MiB       fitby_sigmant = False

2018-04-30 12:44:36,526 - memory_profile6_log - INFO -    153    967.2 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-30 12:44:36,526 - memory_profile6_log - INFO -    154    967.2 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-30 12:44:36,526 - memory_profile6_log - INFO -    155    983.5 MiB     16.3 MiB                            'is_general']]

2018-04-30 12:44:36,526 - memory_profile6_log - INFO -    156                                 # get sigma_Nt from fitted_models_hist

2018-04-30 12:44:36,529 - memory_profile6_log - INFO -    157    990.1 MiB      6.5 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-04-30 12:44:36,530 - memory_profile6_log - INFO -    158    990.1 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-04-30 12:44:36,532 - memory_profile6_log - INFO -    159    984.8 MiB     -5.3 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-04-30 12:44:36,532 - memory_profile6_log - INFO -    160    984.8 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-04-30 12:44:36,532 - memory_profile6_log - INFO -    161                             

2018-04-30 12:44:36,533 - memory_profile6_log - INFO -    162                                 # begin fit

2018-04-30 12:44:36,533 - memory_profile6_log - INFO -    163                                 """model_fit = BR.fit(df_dut, df_input_X,

2018-04-30 12:44:36,533 - memory_profile6_log - INFO -    164                                                    full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-30 12:44:36,535 - memory_profile6_log - INFO -    165                                                    sigma_nt_hist=uniques_fit_hist, verbose=False)"""

2018-04-30 12:44:36,535 - memory_profile6_log - INFO -    166    984.8 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-30 12:44:36,535 - memory_profile6_log - INFO -    167    984.8 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-30 12:44:36,536 - memory_profile6_log - INFO -    168   1023.3 MiB     38.5 MiB                          sigma_nt_hist=None, verbose=False)

2018-04-30 12:44:36,536 - memory_profile6_log - INFO -    169   1023.3 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-30 12:44:36,536 - memory_profile6_log - INFO -    170   1023.3 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-30 12:44:36,536 - memory_profile6_log - INFO -    171                                 # print model_fit[['num_x', 'num_x', 'sigma_Nt', 'date_all_click']].loc[model_fit['user_id']=="1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157"].head(5)

2018-04-30 12:44:36,540 - memory_profile6_log - INFO -    172                             

2018-04-30 12:44:36,542 - memory_profile6_log - INFO -    173                                 # ~~ and Transform ~~

2018-04-30 12:44:36,543 - memory_profile6_log - INFO -    174                                 #   handling current news interest == current date

2018-04-30 12:44:36,545 - memory_profile6_log - INFO -    175   1023.3 MiB      0.0 MiB       if df_dt.empty:

2018-04-30 12:44:36,546 - memory_profile6_log - INFO -    176                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-30 12:44:36,546 - memory_profile6_log - INFO -    177                                     return None

2018-04-30 12:44:36,546 - memory_profile6_log - INFO -    178   1026.9 MiB      3.6 MiB       NB = BR.processX(df_dt)

2018-04-30 12:44:36,548 - memory_profile6_log - INFO -    179   1085.5 MiB     58.6 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-30 12:44:36,553 - memory_profile6_log - INFO -    180                             

2018-04-30 12:44:36,555 - memory_profile6_log - INFO -    181   1085.5 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-30 12:44:36,555 - memory_profile6_log - INFO -    182   1118.0 MiB     32.6 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-30 12:44:36,556 - memory_profile6_log - INFO -    183                             

2018-04-30 12:44:36,559 - memory_profile6_log - INFO -    184                                 # print "model_fit dtypes:\n", model_fit.dtypes

2018-04-30 12:44:36,559 - memory_profile6_log - INFO -    185                                 # print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-04-30 12:44:36,559 - memory_profile6_log - INFO -    186                                 """model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-30 12:44:36,559 - memory_profile6_log - INFO -    187                                                                               fitted_model=model_fit,

2018-04-30 12:44:36,561 - memory_profile6_log - INFO -    188                                                                               fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-04-30 12:44:36,565 - memory_profile6_log - INFO -    189                                                                                                                     "topic_id", "user_id"]],

2018-04-30 12:44:36,565 - memory_profile6_log - INFO -    190                                                                               verbose=False)"""

2018-04-30 12:44:36,568 - memory_profile6_log - INFO -    191   1118.0 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-30 12:44:36,569 - memory_profile6_log - INFO -    192   1118.0 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-30 12:44:36,569 - memory_profile6_log - INFO -    193   1118.0 MiB      0.0 MiB                                                     fitted_model_hist=None,

2018-04-30 12:44:36,569 - memory_profile6_log - INFO -    194   1145.6 MiB     27.6 MiB                                                     verbose=False)

2018-04-30 12:44:36,571 - memory_profile6_log - INFO -    195                                 # ~~~ filter is general and specific topic ~~~

2018-04-30 12:44:36,575 - memory_profile6_log - INFO -    196                                 # the idea is just we need to rerank every topic according

2018-04-30 12:44:36,576 - memory_profile6_log - INFO -    197                                 # user_id and and is_general by p0_posterior

2018-04-30 12:44:36,578 - memory_profile6_log - INFO -    198   1145.6 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-30 12:44:36,579 - memory_profile6_log - INFO -    199   1148.9 MiB      3.3 MiB                                     'is_general']].groupby(['topic_id',

2018-04-30 12:44:36,581 - memory_profile6_log - INFO -    200   1145.6 MiB     -3.2 MiB                                                             'is_general']

2018-04-30 12:44:36,582 - memory_profile6_log - INFO -    201                                                                                      ).size().to_frame().reset_index()

2018-04-30 12:44:36,584 - memory_profile6_log - INFO -    202                             

2018-04-30 12:44:36,586 - memory_profile6_log - INFO -    203                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-30 12:44:36,586 - memory_profile6_log - INFO -    204                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-30 12:44:36,588 - memory_profile6_log - INFO -    205   1145.7 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-30 12:44:36,589 - memory_profile6_log - INFO -    206                             

2018-04-30 12:44:36,589 - memory_profile6_log - INFO -    207                                 # ~ start by provide rank for each topic type ~

2018-04-30 12:44:36,592 - memory_profile6_log - INFO -    208   1182.2 MiB     36.5 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-30 12:44:36,592 - memory_profile6_log - INFO -    209   1188.9 MiB      6.8 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-30 12:44:36,592 - memory_profile6_log - INFO -    210                             

2018-04-30 12:44:36,594 - memory_profile6_log - INFO -    211                                 # ~ set threshold to filter output

2018-04-30 12:44:36,594 - memory_profile6_log - INFO -    212   1188.9 MiB      0.0 MiB       if threshold > 0:

2018-04-30 12:44:36,598 - memory_profile6_log - INFO -    213   1188.9 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-30 12:44:36,598 - memory_profile6_log - INFO -    214   1188.9 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-30 12:44:36,601 - memory_profile6_log - INFO -    215   1186.9 MiB     -2.0 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-30 12:44:36,602 - memory_profile6_log - INFO -    216                             

2018-04-30 12:44:36,602 - memory_profile6_log - INFO -    217   1186.9 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-30 12:44:36,604 - memory_profile6_log - INFO -    218   1167.8 MiB    -19.1 MiB       print model_transform[model_transform['user_id']=='1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157']

2018-04-30 12:44:36,604 - memory_profile6_log - INFO -    219   1167.8 MiB      0.0 MiB       logger.info("Len of model_transform: %d", len(model_transform))

2018-04-30 12:44:36,605 - memory_profile6_log - INFO -    220   1167.8 MiB      0.0 MiB       logger.info("Len of df_dt: %d", len(df_dt))

2018-04-30 12:44:36,605 - memory_profile6_log - INFO -    221   1167.8 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-30 12:44:36,605 - memory_profile6_log - INFO -    222                             

2018-04-30 12:44:36,611 - memory_profile6_log - INFO -    223   1167.8 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-30 12:44:36,612 - memory_profile6_log - INFO -    224                             

2018-04-30 12:44:36,614 - memory_profile6_log - INFO -    225   1167.8 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-30 12:44:36,615 - memory_profile6_log - INFO -    226   1167.8 MiB      0.0 MiB       del df_dut

2018-04-30 12:44:36,617 - memory_profile6_log - INFO -    227   1167.8 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-30 12:44:36,618 - memory_profile6_log - INFO -    228   1167.8 MiB      0.0 MiB       del df_dt

2018-04-30 12:44:36,618 - memory_profile6_log - INFO -    229   1167.8 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-30 12:44:36,621 - memory_profile6_log - INFO -    230   1167.8 MiB      0.0 MiB       del df_input

2018-04-30 12:44:36,621 - memory_profile6_log - INFO -    231   1167.8 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-30 12:44:36,625 - memory_profile6_log - INFO -    232   1118.9 MiB    -48.9 MiB       del df_input_X

2018-04-30 12:44:36,627 - memory_profile6_log - INFO -    233   1118.9 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-30 12:44:36,628 - memory_profile6_log - INFO -    234   1118.9 MiB      0.0 MiB       del df_current

2018-04-30 12:44:36,628 - memory_profile6_log - INFO -    235   1118.9 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-30 12:44:36,631 - memory_profile6_log - INFO -    236   1118.9 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-30 12:44:36,632 - memory_profile6_log - INFO -    237   1118.9 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-30 12:44:36,634 - memory_profile6_log - INFO -    238   1083.0 MiB    -35.9 MiB       del model_fit

2018-04-30 12:44:36,635 - memory_profile6_log - INFO -    239   1083.0 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-30 12:44:36,637 - memory_profile6_log - INFO -    240   1083.0 MiB      0.0 MiB       del result

2018-04-30 12:44:36,638 - memory_profile6_log - INFO -    241   1083.0 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-30 12:44:36,638 - memory_profile6_log - INFO -    242                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 12:44:36,638 - memory_profile6_log - INFO -    243                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 12:44:36,644 - memory_profile6_log - INFO -    244                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 12:44:36,644 - memory_profile6_log - INFO -    245   1083.0 MiB      0.0 MiB       if savetrain:

2018-04-30 12:44:36,647 - memory_profile6_log - INFO -    246   1092.0 MiB      9.0 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-30 12:44:36,647 - memory_profile6_log - INFO -    247   1092.0 MiB      0.0 MiB           del model_transform

2018-04-30 12:44:36,648 - memory_profile6_log - INFO -    248   1092.0 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-30 12:44:36,651 - memory_profile6_log - INFO -    249   1092.0 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-30 12:44:36,651 - memory_profile6_log - INFO -    250                             

2018-04-30 12:44:36,651 - memory_profile6_log - INFO -    251   1092.0 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-30 12:44:36,651 - memory_profile6_log - INFO -    252                                     # ~ Place your code to save the training model here ~

2018-04-30 12:44:36,654 - memory_profile6_log - INFO -    253   1092.0 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-30 12:44:36,657 - memory_profile6_log - INFO -    254   1092.0 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-30 12:44:36,657 - memory_profile6_log - INFO -    255   1092.0 MiB      0.0 MiB               if multproc:

2018-04-30 12:44:36,658 - memory_profile6_log - INFO -    256                                             # ~ save transform models ~

2018-04-30 12:44:36,658 - memory_profile6_log - INFO -    257   1092.0 MiB      0.0 MiB                   logger.info("Saving main Transform model to Google DataStore...")

2018-04-30 12:44:36,660 - memory_profile6_log - INFO -    258   1092.0 MiB      0.0 MiB                   logger.info("Saving total data: %d", len(model_transformsv))

2018-04-30 12:44:36,661 - memory_profile6_log - INFO -    259   1071.5 MiB    -20.6 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-30 12:44:36,661 - memory_profile6_log - INFO -    260                             

2018-04-30 12:44:36,661 - memory_profile6_log - INFO -    261                                             # ~ save fitted models ~

2018-04-30 12:44:36,665 - memory_profile6_log - INFO -    262   1071.5 MiB      0.0 MiB                   logger.info("Saving fitted_models as history to Google DataStore...")

2018-04-30 12:44:36,667 - memory_profile6_log - INFO -    263   1071.5 MiB      0.0 MiB                   save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-30 12:44:36,667 - memory_profile6_log - INFO -    264   1095.5 MiB     24.0 MiB                   fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-30 12:44:36,668 - memory_profile6_log - INFO -    265   1095.5 MiB      0.0 MiB                   logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-04-30 12:44:36,670 - memory_profile6_log - INFO -    266   1111.0 MiB     15.5 MiB                   X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-30 12:44:36,670 - memory_profile6_log - INFO -    267   1111.0 MiB      0.0 MiB                   logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-30 12:44:36,671 - memory_profile6_log - INFO -    268   1114.0 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-30 12:44:36,671 - memory_profile6_log - INFO -    269   1114.0 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-30 12:44:36,673 - memory_profile6_log - INFO -    270   1114.0 MiB      3.0 MiB                       mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-30 12:44:36,677 - memory_profile6_log - INFO -    271                             

2018-04-30 12:44:36,678 - memory_profile6_log - INFO -    272   1106.4 MiB     -7.6 MiB                   del X_split

2018-04-30 12:44:36,680 - memory_profile6_log - INFO -    273   1106.4 MiB      0.0 MiB                   logger.info("deleting X_split...")

2018-04-30 12:44:36,680 - memory_profile6_log - INFO -    274   1106.4 MiB      0.0 MiB                   del save_sigma_nt

2018-04-30 12:44:36,681 - memory_profile6_log - INFO -    275   1106.4 MiB      0.0 MiB                   logger.info("deleting save_sigma_nt...")

2018-04-30 12:44:36,684 - memory_profile6_log - INFO -    276                             

2018-04-30 12:44:36,684 - memory_profile6_log - INFO -    277   1106.4 MiB      0.0 MiB                   del BR

2018-04-30 12:44:36,687 - memory_profile6_log - INFO -    278   1106.4 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-30 12:44:36,687 - memory_profile6_log - INFO -    279                             

2018-04-30 12:44:36,691 - memory_profile6_log - INFO -    280                                     elif str(saveto).lower() == "elastic":

2018-04-30 12:44:36,693 - memory_profile6_log - INFO -    281                                         logger.info("Using ElasticSearch as storage...")

2018-04-30 12:44:36,694 - memory_profile6_log - INFO -    282                                         logging.info("Saving main Transform model to Elasticsearch...")

2018-04-30 12:44:36,694 - memory_profile6_log - INFO -    283                                         mh.saveElasticS(model_transformsv)

2018-04-30 12:44:36,696 - memory_profile6_log - INFO -    284                             

2018-04-30 12:44:36,700 - memory_profile6_log - INFO -    285                                     # need save sigma_nt for daily train

2018-04-30 12:44:36,700 - memory_profile6_log - INFO -    286                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-30 12:44:36,703 - memory_profile6_log - INFO -    287                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-30 12:44:36,703 - memory_profile6_log - INFO -    288   1106.4 MiB      0.0 MiB           if start_date and end_date:

2018-04-30 12:44:36,704 - memory_profile6_log - INFO -    289                                         if not fitby_sigmant:

2018-04-30 12:44:36,704 - memory_profile6_log - INFO -    290                                             logging.info("Saving sigma Nt...")

2018-04-30 12:44:36,706 - memory_profile6_log - INFO -    291                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-30 12:44:36,707 - memory_profile6_log - INFO -    292                                             save_sigma_nt['start_date'] = start_date

2018-04-30 12:44:36,709 - memory_profile6_log - INFO -    293                                             save_sigma_nt['end_date'] = end_date

2018-04-30 12:44:36,710 - memory_profile6_log - INFO -    294                                             print save_sigma_nt.head(5)

2018-04-30 12:44:36,711 - memory_profile6_log - INFO -    295                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-30 12:44:36,713 - memory_profile6_log - INFO -    296                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-30 12:44:36,713 - memory_profile6_log - INFO -    297   1106.4 MiB      0.0 MiB       return

2018-04-30 12:44:36,714 - memory_profile6_log - INFO - 


2018-04-30 12:44:36,717 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-30 12:47:18,138 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-30 12:47:18,141 - memory_profile6_log - INFO - date_generated: 
2018-04-30 12:47:18,141 - memory_profile6_log - INFO -  
2018-04-30 12:47:18,141 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 10, 0, 0)]
2018-04-30 12:47:18,142 - memory_profile6_log - INFO - 

2018-04-30 12:47:18,142 - memory_profile6_log - INFO - using current date: 2018-04-11
2018-04-30 12:47:18,144 - memory_profile6_log - INFO - using start date: 2018-04-10 00:00:00
2018-04-30 12:47:18,144 - memory_profile6_log - INFO - using end date: 2018-04-10
2018-04-30 12:47:18,283 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-30 12:47:18,289 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-10
2018-04-30 12:47:54,154 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-30 12:47:54,157 - memory_profile6_log - INFO - date_generated: 
2018-04-30 12:47:54,157 - memory_profile6_log - INFO -  
2018-04-30 12:47:54,158 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 10, 0, 0)]
2018-04-30 12:47:54,160 - memory_profile6_log - INFO - 

2018-04-30 12:47:54,160 - memory_profile6_log - INFO - using current date: 2018-04-11
2018-04-30 12:47:54,161 - memory_profile6_log - INFO - using start date: 2018-04-10 00:00:00
2018-04-30 12:47:54,161 - memory_profile6_log - INFO - using end date: 2018-04-10
2018-04-30 12:47:54,316 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-30 12:47:54,322 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-10
2018-04-30 12:49:52,486 - memory_profile6_log - INFO - size of df: 108.51 MB
2018-04-30 12:49:52,489 - memory_profile6_log - INFO - getting total: 427635 training data(genuine interest) for date: 2018-04-10
2018-04-30 12:49:52,553 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-30 12:49:52,555 - memory_profile6_log - INFO - Appending history data...
2018-04-30 12:49:52,556 - memory_profile6_log - INFO - processing batch-0
2018-04-30 12:49:52,558 - memory_profile6_log - INFO - creating list history data...
2018-04-30 12:49:52,664 - memory_profile6_log - INFO - call history data...
2018-04-30 12:50:42,088 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 12:50:43,812 - memory_profile6_log - INFO - processing batch-1
2018-04-30 12:50:43,815 - memory_profile6_log - INFO - creating list history data...
2018-04-30 12:50:43,894 - memory_profile6_log - INFO - call history data...
2018-04-30 12:51:33,940 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 12:51:35,805 - memory_profile6_log - INFO - processing batch-2
2018-04-30 12:51:35,808 - memory_profile6_log - INFO - creating list history data...
2018-04-30 12:51:35,892 - memory_profile6_log - INFO - call history data...
2018-04-30 12:52:26,145 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 12:52:28,451 - memory_profile6_log - INFO - processing batch-3
2018-04-30 12:52:28,453 - memory_profile6_log - INFO - creating list history data...
2018-04-30 12:52:28,536 - memory_profile6_log - INFO - call history data...
2018-04-30 12:53:30,825 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 12:53:32,947 - memory_profile6_log - INFO - processing batch-4
2018-04-30 12:53:32,950 - memory_profile6_log - INFO - creating list history data...
2018-04-30 12:53:33,029 - memory_profile6_log - INFO - call history data...
2018-04-30 12:54:22,704 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 12:54:24,667 - memory_profile6_log - INFO - Appending training data...
2018-04-30 12:54:24,668 - memory_profile6_log - INFO - len datalist: 1
2018-04-30 12:54:24,670 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-30 12:54:24,688 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 12:54:24,690 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 12:54:24,694 - memory_profile6_log - INFO - ================================================

2018-04-30 12:54:24,697 - memory_profile6_log - INFO -    325     86.9 MiB     86.9 MiB   @profile

2018-04-30 12:54:24,701 - memory_profile6_log - INFO -    326                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-30 12:54:24,703 - memory_profile6_log - INFO -    327     86.9 MiB      0.0 MiB       bq_client = client

2018-04-30 12:54:24,706 - memory_profile6_log - INFO -    328     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 12:54:24,707 - memory_profile6_log - INFO -    329                             

2018-04-30 12:54:24,707 - memory_profile6_log - INFO -    330     86.9 MiB      0.0 MiB       datalist = []

2018-04-30 12:54:24,709 - memory_profile6_log - INFO -    331     86.9 MiB      0.0 MiB       datalist_hist = []

2018-04-30 12:54:24,711 - memory_profile6_log - INFO -    332                             

2018-04-30 12:54:24,713 - memory_profile6_log - INFO -    333     86.9 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-30 12:54:24,717 - memory_profile6_log - INFO -    334    689.7 MiB      0.0 MiB       for ndate in date_generated:

2018-04-30 12:54:24,719 - memory_profile6_log - INFO -    335    393.6 MiB    306.7 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-30 12:54:24,724 - memory_profile6_log - INFO -    336    393.6 MiB      0.0 MiB           if tframe is not None:

2018-04-30 12:54:24,726 - memory_profile6_log - INFO -    337    393.6 MiB      0.0 MiB               if not tframe.empty:

2018-04-30 12:54:24,727 - memory_profile6_log - INFO -    338    407.5 MiB     13.9 MiB                   X_split = np.array_split(tframe, 5)

2018-04-30 12:54:24,729 - memory_profile6_log - INFO -    339    407.5 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-30 12:54:24,730 - memory_profile6_log - INFO -    340    407.5 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-30 12:54:24,730 - memory_profile6_log - INFO -    341    689.7 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-30 12:54:24,732 - memory_profile6_log - INFO -    342                                                 # ~ loading history

2018-04-30 12:54:24,736 - memory_profile6_log - INFO -    343                                                 """

2018-04-30 12:54:24,737 - memory_profile6_log - INFO -    344                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-30 12:54:24,739 - memory_profile6_log - INFO -    345                                                 """

2018-04-30 12:54:24,740 - memory_profile6_log - INFO -    346    663.1 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-30 12:54:24,742 - memory_profile6_log - INFO -    347                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-30 12:54:24,743 - memory_profile6_log - INFO -    348    663.1 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-30 12:54:24,747 - memory_profile6_log - INFO -    349                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-30 12:54:24,750 - memory_profile6_log - INFO -    350    664.5 MiB      5.5 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-30 12:54:24,752 - memory_profile6_log - INFO -    351                             

2018-04-30 12:54:24,756 - memory_profile6_log - INFO -    352    664.5 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-30 12:54:24,756 - memory_profile6_log - INFO -    353    740.4 MiB    556.1 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-30 12:54:24,759 - memory_profile6_log - INFO -    354                             

2018-04-30 12:54:24,762 - memory_profile6_log - INFO -    355                                                 # me = os.getpid()

2018-04-30 12:54:24,763 - memory_profile6_log - INFO -    356                                                 # kill_proc_tree(me)

2018-04-30 12:54:24,766 - memory_profile6_log - INFO -    357                             

2018-04-30 12:54:24,769 - memory_profile6_log - INFO -    358    740.4 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-30 12:54:24,770 - memory_profile6_log - INFO -    359    742.4 MiB     -0.9 MiB                       for m in h_frame:

2018-04-30 12:54:24,772 - memory_profile6_log - INFO -    360    742.4 MiB     -0.9 MiB                           if m is not None:

2018-04-30 12:54:24,773 - memory_profile6_log - INFO -    361    742.4 MiB     -0.9 MiB                               if len(m) > 0:

2018-04-30 12:54:24,775 - memory_profile6_log - INFO -    362    742.4 MiB     10.4 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-30 12:54:24,778 - memory_profile6_log - INFO -    363    689.7 MiB   -289.5 MiB                       del h_frame

2018-04-30 12:54:24,779 - memory_profile6_log - INFO -    364    689.7 MiB     -1.2 MiB                       del lhistory

2018-04-30 12:54:24,779 - memory_profile6_log - INFO -    365                             

2018-04-30 12:54:24,782 - memory_profile6_log - INFO -    366    689.7 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-30 12:54:24,783 - memory_profile6_log - INFO -    367    689.7 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-30 12:54:24,790 - memory_profile6_log - INFO -    368                                     else: 

2018-04-30 12:54:24,792 - memory_profile6_log - INFO -    369                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-30 12:54:24,796 - memory_profile6_log - INFO -    370    689.7 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-30 12:54:24,796 - memory_profile6_log - INFO -    371    689.7 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-30 12:54:24,801 - memory_profile6_log - INFO -    372                             

2018-04-30 12:54:24,802 - memory_profile6_log - INFO -    373    689.7 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-30 12:54:24,803 - memory_profile6_log - INFO - 


2018-04-30 12:54:26,348 - memory_profile6_log - INFO - size of big_frame_hist: 117.89 MB
2018-04-30 12:54:26,470 - memory_profile6_log - INFO - size of big_frame: 108.51 MB
2018-04-30 12:54:26,493 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-04-30 12:58:16,644 - memory_profile6_log - INFO - size of df: 305.27 MB
2018-04-30 12:58:16,647 - memory_profile6_log - INFO - getting total: 1249791 training data(current date interest)
2018-04-30 12:58:17,098 - memory_profile6_log - INFO - size of current_frame: 314.81 MB
2018-04-30 12:58:17,098 - memory_profile6_log - INFO - loading time of: 1677426 total genuine-current interest data ~ take 622.806s
2018-04-30 12:58:17,130 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 12:58:17,131 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 12:58:17,131 - memory_profile6_log - INFO - ================================================

2018-04-30 12:58:17,135 - memory_profile6_log - INFO -    375     86.8 MiB     86.8 MiB   @profile

2018-04-30 12:58:17,138 - memory_profile6_log - INFO -    376                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-30 12:58:17,138 - memory_profile6_log - INFO -    377     86.9 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-30 12:58:17,141 - memory_profile6_log - INFO -    378     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 12:58:17,144 - memory_profile6_log - INFO -    379                             

2018-04-30 12:58:17,145 - memory_profile6_log - INFO -    380                                 # ~~~ Begin collecting data ~~~

2018-04-30 12:58:17,147 - memory_profile6_log - INFO -    381     86.9 MiB      0.0 MiB       t0 = time.time()

2018-04-30 12:58:17,148 - memory_profile6_log - INFO -    382    679.6 MiB    592.7 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-30 12:58:17,150 - memory_profile6_log - INFO -    383    679.6 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-30 12:58:17,154 - memory_profile6_log - INFO -    384                                     logger.info("Training cannot be empty..")

2018-04-30 12:58:17,155 - memory_profile6_log - INFO -    385                                     return False

2018-04-30 12:58:17,157 - memory_profile6_log - INFO -    386    705.9 MiB     26.3 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-30 12:58:17,158 - memory_profile6_log - INFO -    387                                 # print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-30 12:58:17,160 - memory_profile6_log - INFO -    388    705.9 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-30 12:58:17,164 - memory_profile6_log - INFO -    389                             

2018-04-30 12:58:17,167 - memory_profile6_log - INFO -    390    715.9 MiB     10.0 MiB       big_frame = pd.concat(datalist)

2018-04-30 12:58:17,170 - memory_profile6_log - INFO -    391    715.9 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-30 12:58:17,173 - memory_profile6_log - INFO -    392    706.1 MiB     -9.8 MiB       del datalist

2018-04-30 12:58:17,174 - memory_profile6_log - INFO -    393                             

2018-04-30 12:58:17,177 - memory_profile6_log - INFO -    394    706.1 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 12:58:17,184 - memory_profile6_log - INFO -    395                             

2018-04-30 12:58:17,188 - memory_profile6_log - INFO -    396                                 # ~ get current news interest ~

2018-04-30 12:58:17,190 - memory_profile6_log - INFO -    397    706.1 MiB      0.0 MiB       if not cd:

2018-04-30 12:58:17,193 - memory_profile6_log - INFO -    398    706.1 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-04-30 12:58:17,196 - memory_profile6_log - INFO -    399    894.3 MiB    188.1 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-30 12:58:17,197 - memory_profile6_log - INFO -    400    894.3 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-30 12:58:17,197 - memory_profile6_log - INFO -    401                                 else:

2018-04-30 12:58:17,198 - memory_profile6_log - INFO -    402                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-30 12:58:17,200 - memory_profile6_log - INFO -    403                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-30 12:58:17,200 - memory_profile6_log - INFO -    404                             

2018-04-30 12:58:17,200 - memory_profile6_log - INFO -    405                                     # safe handling of query parameter

2018-04-30 12:58:17,203 - memory_profile6_log - INFO -    406                                     query_params = [

2018-04-30 12:58:17,206 - memory_profile6_log - INFO -    407                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-30 12:58:17,207 - memory_profile6_log - INFO -    408                                     ]

2018-04-30 12:58:17,209 - memory_profile6_log - INFO -    409                             

2018-04-30 12:58:17,210 - memory_profile6_log - INFO -    410                                     job_config.query_parameters = query_params

2018-04-30 12:58:17,210 - memory_profile6_log - INFO -    411                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-30 12:58:17,210 - memory_profile6_log - INFO -    412                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-30 12:58:17,211 - memory_profile6_log - INFO -    413                             

2018-04-30 12:58:17,213 - memory_profile6_log - INFO -    414    903.8 MiB      9.5 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-30 12:58:17,217 - memory_profile6_log - INFO -    415    903.8 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-30 12:58:17,217 - memory_profile6_log - INFO -    416    903.8 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-30 12:58:17,219 - memory_profile6_log - INFO -    417    903.8 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-30 12:58:17,219 - memory_profile6_log - INFO -    418    903.8 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-30 12:58:17,220 - memory_profile6_log - INFO -    419    903.8 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-30 12:58:17,220 - memory_profile6_log - INFO -    420                             

2018-04-30 12:58:17,221 - memory_profile6_log - INFO -    421    903.8 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-30 12:58:17,223 - memory_profile6_log - INFO - 


2018-04-30 12:58:17,230 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-30 12:58:17,362 - memory_profile6_log - INFO - train on: 427635 total genuine interest data(D(u, t))
2018-04-30 12:58:17,365 - memory_profile6_log - INFO - transform on: 1249791 total current data(D(t))
2018-04-30 12:58:17,365 - memory_profile6_log - INFO - apply on: 427635 total history...)
2018-04-30 12:58:17,818 - memory_profile6_log - INFO - len of uniques_fit_hist:427635
2018-04-30 12:58:18,184 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:52258
2018-04-30 12:58:19,842 - memory_profile6_log - INFO - Len of model_fit: 427635
2018-04-30 12:58:19,844 - memory_profile6_log - INFO - Len of df_dut: 427635
2018-04-30 12:58:23,407 - memory_profile6_log - INFO - Len of fitted_models on main class: 427635
2018-04-30 12:58:23,407 - memory_profile6_log - INFO - 

2018-04-30 12:58:23,408 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-30 12:58:23,410 - memory_profile6_log - INFO - 

2018-04-30 12:58:23,503 - memory_profile6_log - INFO - len of fitted models after concat: 855270
2018-04-30 12:58:23,505 - memory_profile6_log - INFO - 

2018-04-30 12:58:23,506 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-30 12:58:23,507 - memory_profile6_log - INFO - 

2018-04-30 12:58:24,336 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 427635
2018-04-30 12:58:24,338 - memory_profile6_log - INFO - 

2018-04-30 12:59:42,365 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-30 12:59:42,733 - memory_profile6_log - INFO -                                                   user_id           topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt  p0_posterior  is_general  rank
246500  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22601470         345.673524             355.673524   0.062761      1389      0.015956        True   1.0
246509  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27431099         387.256535             397.256535   0.050504      1389      0.014341        True   2.0
246499  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22553543          89.967400              99.967400   0.126607      1389      0.009047        True   3.0
246497  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22552349         519.838302             529.838302   0.006228      1389      0.002359        True   4.0
246506  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27428266         372.562342             382.562342   0.008460      1389      0.002314        True   5.0
246503  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22661796         102.748815             112.748815   0.016518      1389      0.001331        True   6.0
246498  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22553321         168.384798             178.384798   0.008790      1389      0.001121        True   7.0
246496  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22552186         172.446407             182.446407   0.008537      1389      0.001113        True   8.0
246495  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22551890         892.148245             902.148245   0.001489      1389      0.000960        True   9.0
246501  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22615029        2221.501966            2231.501966   0.000271      1389      0.000432        True  10.0
246504  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22662160         153.675862             163.675862   0.001654      1389      0.000194        True  11.0
246489  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1164316987        2062.475707            2072.475707   0.024410      1389      0.036160       False   1.0
246526  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           36060446         833.369515             843.369515   0.021372      1389      0.012884       False   2.0
246529  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           39301645         236.906870             246.906870   0.015590      1389      0.002752       False   3.0
246502  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22661658       18980.340538           18990.340538   0.000180      1389      0.002445       False   4.0
246531  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           40410447        3239.018630            3249.018630   0.000800      1389      0.001859       False   5.0
246538  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           67524823       49435.537114           49445.537114   0.000045      1389      0.001582       False   6.0
246508  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27430823          97.318507             107.318507   0.017126      1389      0.001314       False   7.0
246536  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           47091877         873.363621             883.363621   0.001193      1389      0.000753       False   8.0
246477  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1017140824       44491.983402           44501.983402   0.000018      1389      0.000584       False   9.0
246483  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1039394600        3598.398906            3608.398906   0.000209      1389      0.000539       False  10.0
246507  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27428824         682.243035             692.243035   0.000811      1389      0.000401       False  11.0
246505  1616f009d96b1-0285d8288a5bce-70217860-38400-16...          271947679         972.856780             982.856780   0.000499      1389      0.000351       False  12.0
246512  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790313513        2987.377027            2997.377027   0.000137      1389      0.000294       False  13.0
246540  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           81568133        1538.348412            1548.348412   0.000265      1389      0.000293       False  14.0
246535  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           41566010          88.933715              98.933715   0.002808      1389      0.000199       False  15.0
2018-04-30 12:59:42,740 - memory_profile6_log - INFO - 

2018-04-30 12:59:42,743 - memory_profile6_log - INFO - Len of model_transform: 393414
2018-04-30 12:59:42,743 - memory_profile6_log - INFO - Len of df_dt: 1249791
2018-04-30 12:59:42,744 - memory_profile6_log - INFO - Total train time: 85.057s
2018-04-30 12:59:42,746 - memory_profile6_log - INFO - memory left before cleaning: 80.800 percent memory...
2018-04-30 12:59:42,749 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-30 12:59:42,750 - memory_profile6_log - INFO - deleting df_dut...
2018-04-30 12:59:42,750 - memory_profile6_log - INFO - deleting df_dt...
2018-04-30 12:59:42,753 - memory_profile6_log - INFO - deleting df_input...
2018-04-30 12:59:42,809 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-30 12:59:42,809 - memory_profile6_log - INFO - deleting df_current...
2018-04-30 12:59:42,812 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-30 12:59:42,835 - memory_profile6_log - INFO - deleting model_fit...
2018-04-30 12:59:42,836 - memory_profile6_log - INFO - deleting result...
2018-04-30 12:59:42,875 - memory_profile6_log - INFO - deleting model_transform...
2018-04-30 12:59:42,878 - memory_profile6_log - INFO - memory left after cleaning: 80.300 percent memory...
2018-04-30 12:59:42,878 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-30 12:59:42,880 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-30 12:59:42,881 - memory_profile6_log - INFO - Saving main Transform model to Google DataStore...
2018-04-30 12:59:42,881 - memory_profile6_log - INFO - Saving total data: 393414
2018-04-30 12:59:43,190 - memory_profile6_log - INFO - Saving fitted_models as history to Google DataStore...
2018-04-30 12:59:43,299 - memory_profile6_log - INFO - Saving total data: 427635
2018-04-30 12:59:43,349 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 10
2018-04-30 12:59:43,351 - memory_profile6_log - INFO - processing batch-0
2018-04-30 12:59:43,543 - memory_profile6_log - INFO - processing batch-1
2018-04-30 12:59:43,733 - memory_profile6_log - INFO - processing batch-2
2018-04-30 12:59:43,924 - memory_profile6_log - INFO - processing batch-3
2018-04-30 12:59:44,118 - memory_profile6_log - INFO - processing batch-4
2018-04-30 12:59:44,302 - memory_profile6_log - INFO - processing batch-5
2018-04-30 12:59:44,493 - memory_profile6_log - INFO - processing batch-6
2018-04-30 12:59:44,687 - memory_profile6_log - INFO - processing batch-7
2018-04-30 12:59:44,885 - memory_profile6_log - INFO - processing batch-8
2018-04-30 12:59:45,076 - memory_profile6_log - INFO - processing batch-9
2018-04-30 12:59:45,286 - memory_profile6_log - INFO - deleting X_split...
2018-04-30 12:59:45,289 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-04-30 12:59:45,292 - memory_profile6_log - INFO - deleting BR...
2018-04-30 12:59:45,318 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 12:59:45,319 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 12:59:45,321 - memory_profile6_log - INFO - ================================================

2018-04-30 12:59:45,323 - memory_profile6_log - INFO -    113    894.6 MiB    894.6 MiB   @profile

2018-04-30 12:59:45,325 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-30 12:59:45,325 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-30 12:59:45,325 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-30 12:59:45,326 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-30 12:59:45,331 - memory_profile6_log - INFO -    118                                      saveto="datastore", fitted_models_hist=None):

2018-04-30 12:59:45,332 - memory_profile6_log - INFO -    119                                 """

2018-04-30 12:59:45,334 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-30 12:59:45,335 - memory_profile6_log - INFO -    121                                 """

2018-04-30 12:59:45,335 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-30 12:59:45,335 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-30 12:59:45,338 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-30 12:59:45,338 - memory_profile6_log - INFO -    125    894.6 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-30 12:59:45,341 - memory_profile6_log - INFO -    126    908.1 MiB     13.5 MiB       df_dut = df_input.copy(deep=True)

2018-04-30 12:59:45,342 - memory_profile6_log - INFO -    127    908.1 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 12:59:45,344 - memory_profile6_log - INFO -    128                             

2018-04-30 12:59:45,345 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-30 12:59:45,345 - memory_profile6_log - INFO -    130    947.4 MiB     39.3 MiB       df_dt = df_current.copy(deep=True)

2018-04-30 12:59:45,345 - memory_profile6_log - INFO -    131    947.4 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 12:59:45,346 - memory_profile6_log - INFO -    132                             

2018-04-30 12:59:45,346 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-30 12:59:45,348 - memory_profile6_log - INFO -    134    947.4 MiB      0.0 MiB       t0 = time.time()

2018-04-30 12:59:45,349 - memory_profile6_log - INFO -    135    947.4 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-30 12:59:45,352 - memory_profile6_log - INFO -    136    947.4 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-30 12:59:45,354 - memory_profile6_log - INFO -    137    947.4 MiB      0.0 MiB       logger.info("apply on: %d total history...)", len(df_hist))

2018-04-30 12:59:45,355 - memory_profile6_log - INFO -    138                             

2018-04-30 12:59:45,355 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-30 12:59:45,357 - memory_profile6_log - INFO -    140    947.4 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-30 12:59:45,358 - memory_profile6_log - INFO -    141                             

2018-04-30 12:59:45,358 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-30 12:59:45,358 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-30 12:59:45,359 - memory_profile6_log - INFO -    144    950.4 MiB      3.0 MiB       NB = BR.processX(df_dut)

2018-04-30 12:59:45,359 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-30 12:59:45,361 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-30 12:59:45,364 - memory_profile6_log - INFO -    147    966.9 MiB     16.5 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-30 12:59:45,365 - memory_profile6_log - INFO -    148                                 """

2018-04-30 12:59:45,365 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-30 12:59:45,365 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-30 12:59:45,367 - memory_profile6_log - INFO -    151                                 """

2018-04-30 12:59:45,368 - memory_profile6_log - INFO -    152    966.9 MiB      0.0 MiB       fitby_sigmant = False

2018-04-30 12:59:45,368 - memory_profile6_log - INFO -    153    966.9 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-30 12:59:45,369 - memory_profile6_log - INFO -    154    966.9 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-30 12:59:45,369 - memory_profile6_log - INFO -    155    983.2 MiB     16.3 MiB                            'is_general']]

2018-04-30 12:59:45,371 - memory_profile6_log - INFO -    156                                 # get sigma_Nt from fitted_models_hist

2018-04-30 12:59:45,371 - memory_profile6_log - INFO -    157    989.7 MiB      6.5 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-04-30 12:59:45,375 - memory_profile6_log - INFO -    158    989.7 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-04-30 12:59:45,375 - memory_profile6_log - INFO -    159    985.9 MiB     -3.8 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-04-30 12:59:45,377 - memory_profile6_log - INFO -    160    985.9 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-04-30 12:59:45,377 - memory_profile6_log - INFO -    161                             

2018-04-30 12:59:45,378 - memory_profile6_log - INFO -    162                                 # begin fit

2018-04-30 12:59:45,378 - memory_profile6_log - INFO -    163    985.9 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-30 12:59:45,378 - memory_profile6_log - INFO -    164    985.9 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-30 12:59:45,380 - memory_profile6_log - INFO -    165   1024.6 MiB     38.7 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-04-30 12:59:45,381 - memory_profile6_log - INFO -    166                                 """

2018-04-30 12:59:45,381 - memory_profile6_log - INFO -    167                                 model_fit = BR.fit(df_dut, df_input_X,

2018-04-30 12:59:45,382 - memory_profile6_log - INFO -    168                                                    full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-30 12:59:45,382 - memory_profile6_log - INFO -    169                                                    sigma_nt_hist=None, verbose=False)"""

2018-04-30 12:59:45,387 - memory_profile6_log - INFO -    170   1024.6 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-30 12:59:45,388 - memory_profile6_log - INFO -    171   1024.6 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-30 12:59:45,388 - memory_profile6_log - INFO -    172                                 # print model_fit[['num_x', 'num_x', 'sigma_Nt', 'date_all_click']].loc[model_fit['user_id']=="1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157"].head(5)

2018-04-30 12:59:45,391 - memory_profile6_log - INFO -    173                             

2018-04-30 12:59:45,391 - memory_profile6_log - INFO -    174                                 # ~~ and Transform ~~

2018-04-30 12:59:45,392 - memory_profile6_log - INFO -    175                                 #   handling current news interest == current date

2018-04-30 12:59:45,392 - memory_profile6_log - INFO -    176   1024.6 MiB      0.0 MiB       if df_dt.empty:

2018-04-30 12:59:45,394 - memory_profile6_log - INFO -    177                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-30 12:59:45,394 - memory_profile6_log - INFO -    178                                     return None

2018-04-30 12:59:45,394 - memory_profile6_log - INFO -    179   1028.3 MiB      3.7 MiB       NB = BR.processX(df_dt)

2018-04-30 12:59:45,398 - memory_profile6_log - INFO -    180   1086.9 MiB     58.6 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-30 12:59:45,398 - memory_profile6_log - INFO -    181                             

2018-04-30 12:59:45,400 - memory_profile6_log - INFO -    182   1086.9 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-30 12:59:45,400 - memory_profile6_log - INFO -    183   1119.4 MiB     32.6 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-30 12:59:45,401 - memory_profile6_log - INFO -    184                             

2018-04-30 12:59:45,401 - memory_profile6_log - INFO -    185                                 # print "model_fit dtypes:\n", model_fit.dtypes

2018-04-30 12:59:45,401 - memory_profile6_log - INFO -    186                                 # print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-04-30 12:59:45,401 - memory_profile6_log - INFO -    187   1119.4 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-30 12:59:45,403 - memory_profile6_log - INFO -    188   1119.4 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-30 12:59:45,403 - memory_profile6_log - INFO -    189   1119.4 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-04-30 12:59:45,404 - memory_profile6_log - INFO -    190   1129.2 MiB      9.8 MiB                                                                                           "topic_id", "user_id"]],

2018-04-30 12:59:45,404 - memory_profile6_log - INFO -    191   1147.6 MiB     18.3 MiB                                                     verbose=False)

2018-04-30 12:59:45,404 - memory_profile6_log - INFO -    192                                 """

2018-04-30 12:59:45,405 - memory_profile6_log - INFO -    193                                 model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-30 12:59:45,410 - memory_profile6_log - INFO -    194                                                                               fitted_model=model_fit,

2018-04-30 12:59:45,411 - memory_profile6_log - INFO -    195                                                                               fitted_model_hist=None,

2018-04-30 12:59:45,411 - memory_profile6_log - INFO -    196                                                                               verbose=False)"""

2018-04-30 12:59:45,413 - memory_profile6_log - INFO -    197                                 # ~~~ filter is general and specific topic ~~~

2018-04-30 12:59:45,414 - memory_profile6_log - INFO -    198                                 # the idea is just we need to rerank every topic according

2018-04-30 12:59:45,415 - memory_profile6_log - INFO -    199                                 # user_id and and is_general by p0_posterior

2018-04-30 12:59:45,417 - memory_profile6_log - INFO -    200   1147.6 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-30 12:59:45,417 - memory_profile6_log - INFO -    201   1150.8 MiB      3.3 MiB                                     'is_general']].groupby(['topic_id',

2018-04-30 12:59:45,417 - memory_profile6_log - INFO -    202   1147.6 MiB     -3.2 MiB                                                             'is_general']

2018-04-30 12:59:45,417 - memory_profile6_log - INFO -    203                                                                                      ).size().to_frame().reset_index()

2018-04-30 12:59:45,417 - memory_profile6_log - INFO -    204                             

2018-04-30 12:59:45,421 - memory_profile6_log - INFO -    205                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-30 12:59:45,423 - memory_profile6_log - INFO -    206                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-30 12:59:45,423 - memory_profile6_log - INFO -    207   1147.6 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-30 12:59:45,424 - memory_profile6_log - INFO -    208                             

2018-04-30 12:59:45,424 - memory_profile6_log - INFO -    209                                 # ~ start by provide rank for each topic type ~

2018-04-30 12:59:45,424 - memory_profile6_log - INFO -    210   1176.2 MiB     28.6 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-30 12:59:45,426 - memory_profile6_log - INFO -    211   1182.4 MiB      6.2 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-30 12:59:45,427 - memory_profile6_log - INFO -    212                             

2018-04-30 12:59:45,428 - memory_profile6_log - INFO -    213                                 # ~ set threshold to filter output

2018-04-30 12:59:45,433 - memory_profile6_log - INFO -    214   1182.4 MiB      0.0 MiB       if threshold > 0:

2018-04-30 12:59:45,433 - memory_profile6_log - INFO -    215   1182.4 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-30 12:59:45,434 - memory_profile6_log - INFO -    216   1182.4 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-30 12:59:45,434 - memory_profile6_log - INFO -    217   1180.1 MiB     -2.3 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-30 12:59:45,436 - memory_profile6_log - INFO -    218                             

2018-04-30 12:59:45,437 - memory_profile6_log - INFO -    219   1180.1 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-30 12:59:45,437 - memory_profile6_log - INFO -    220   1160.9 MiB    -19.2 MiB       print model_transform[model_transform['user_id']=='1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157']

2018-04-30 12:59:45,437 - memory_profile6_log - INFO -    221   1160.9 MiB      0.0 MiB       logger.info("Len of model_transform: %d", len(model_transform))

2018-04-30 12:59:45,437 - memory_profile6_log - INFO -    222   1160.9 MiB      0.0 MiB       logger.info("Len of df_dt: %d", len(df_dt))

2018-04-30 12:59:45,438 - memory_profile6_log - INFO -    223   1160.9 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-30 12:59:45,440 - memory_profile6_log - INFO -    224                             

2018-04-30 12:59:45,443 - memory_profile6_log - INFO -    225   1160.9 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-30 12:59:45,444 - memory_profile6_log - INFO -    226                             

2018-04-30 12:59:45,444 - memory_profile6_log - INFO -    227   1160.9 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-30 12:59:45,444 - memory_profile6_log - INFO -    228   1160.9 MiB      0.0 MiB       del df_dut

2018-04-30 12:59:45,444 - memory_profile6_log - INFO -    229   1160.9 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-30 12:59:45,446 - memory_profile6_log - INFO -    230   1160.9 MiB      0.0 MiB       del df_dt

2018-04-30 12:59:45,447 - memory_profile6_log - INFO -    231   1160.9 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-30 12:59:45,447 - memory_profile6_log - INFO -    232   1160.9 MiB      0.0 MiB       del df_input

2018-04-30 12:59:45,448 - memory_profile6_log - INFO -    233   1160.9 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-30 12:59:45,450 - memory_profile6_log - INFO -    234   1112.0 MiB    -48.9 MiB       del df_input_X

2018-04-30 12:59:45,450 - memory_profile6_log - INFO -    235   1112.0 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-30 12:59:45,450 - memory_profile6_log - INFO -    236   1112.0 MiB      0.0 MiB       del df_current

2018-04-30 12:59:45,450 - memory_profile6_log - INFO -    237   1112.0 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-30 12:59:45,450 - memory_profile6_log - INFO -    238   1112.0 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-30 12:59:45,453 - memory_profile6_log - INFO -    239   1112.0 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-30 12:59:45,456 - memory_profile6_log - INFO -    240   1076.1 MiB    -35.9 MiB       del model_fit

2018-04-30 12:59:45,456 - memory_profile6_log - INFO -    241   1076.1 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-30 12:59:45,457 - memory_profile6_log - INFO -    242   1076.1 MiB      0.0 MiB       del result

2018-04-30 12:59:45,457 - memory_profile6_log - INFO -    243   1076.1 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-30 12:59:45,457 - memory_profile6_log - INFO -    244                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 12:59:45,457 - memory_profile6_log - INFO -    245                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 12:59:45,459 - memory_profile6_log - INFO -    246                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 12:59:45,459 - memory_profile6_log - INFO -    247   1076.1 MiB      0.0 MiB       if savetrain:

2018-04-30 12:59:45,460 - memory_profile6_log - INFO -    248   1085.1 MiB      9.0 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-30 12:59:45,460 - memory_profile6_log - INFO -    249   1085.1 MiB      0.0 MiB           del model_transform

2018-04-30 12:59:45,460 - memory_profile6_log - INFO -    250   1085.1 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-30 12:59:45,460 - memory_profile6_log - INFO -    251   1085.1 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-30 12:59:45,461 - memory_profile6_log - INFO -    252                             

2018-04-30 12:59:45,461 - memory_profile6_log - INFO -    253   1085.1 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-30 12:59:45,466 - memory_profile6_log - INFO -    254                                     # ~ Place your code to save the training model here ~

2018-04-30 12:59:45,467 - memory_profile6_log - INFO -    255   1085.1 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-30 12:59:45,467 - memory_profile6_log - INFO -    256   1085.1 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-30 12:59:45,467 - memory_profile6_log - INFO -    257   1085.1 MiB      0.0 MiB               if multproc:

2018-04-30 12:59:45,467 - memory_profile6_log - INFO -    258                                             # ~ save transform models ~

2018-04-30 12:59:45,469 - memory_profile6_log - INFO -    259   1085.1 MiB      0.0 MiB                   logger.info("Saving main Transform model to Google DataStore...")

2018-04-30 12:59:45,469 - memory_profile6_log - INFO -    260   1085.1 MiB      0.0 MiB                   logger.info("Saving total data: %d", len(model_transformsv))

2018-04-30 12:59:45,470 - memory_profile6_log - INFO -    261   1064.1 MiB    -21.0 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-30 12:59:45,470 - memory_profile6_log - INFO -    262                             

2018-04-30 12:59:45,470 - memory_profile6_log - INFO -    263                                             # ~ save fitted models ~

2018-04-30 12:59:45,470 - memory_profile6_log - INFO -    264   1064.1 MiB      0.0 MiB                   logger.info("Saving fitted_models as history to Google DataStore...")

2018-04-30 12:59:45,470 - memory_profile6_log - INFO -    265   1064.1 MiB      0.0 MiB                   save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-30 12:59:45,471 - memory_profile6_log - INFO -    266   1087.5 MiB     23.4 MiB                   fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-30 12:59:45,471 - memory_profile6_log - INFO -    267   1087.5 MiB      0.0 MiB                   logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-04-30 12:59:45,473 - memory_profile6_log - INFO -    268   1100.8 MiB     13.3 MiB                   X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-30 12:59:45,473 - memory_profile6_log - INFO -    269   1100.8 MiB      0.0 MiB                   logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-30 12:59:45,477 - memory_profile6_log - INFO -    270   1103.5 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-30 12:59:45,477 - memory_profile6_log - INFO -    271   1103.5 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-30 12:59:45,479 - memory_profile6_log - INFO -    272   1103.5 MiB      2.6 MiB                       mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-30 12:59:45,480 - memory_profile6_log - INFO -    273                             

2018-04-30 12:59:45,480 - memory_profile6_log - INFO -    274   1098.6 MiB     -4.9 MiB                   del X_split

2018-04-30 12:59:45,482 - memory_profile6_log - INFO -    275   1098.6 MiB      0.0 MiB                   logger.info("deleting X_split...")

2018-04-30 12:59:45,483 - memory_profile6_log - INFO -    276   1098.6 MiB      0.0 MiB                   del save_sigma_nt

2018-04-30 12:59:45,483 - memory_profile6_log - INFO -    277   1098.6 MiB      0.0 MiB                   logger.info("deleting save_sigma_nt...")

2018-04-30 12:59:45,483 - memory_profile6_log - INFO -    278                             

2018-04-30 12:59:45,484 - memory_profile6_log - INFO -    279   1098.6 MiB      0.0 MiB                   del BR

2018-04-30 12:59:45,484 - memory_profile6_log - INFO -    280   1098.6 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-30 12:59:45,487 - memory_profile6_log - INFO -    281                             

2018-04-30 12:59:45,489 - memory_profile6_log - INFO -    282                                     elif str(saveto).lower() == "elastic":

2018-04-30 12:59:45,490 - memory_profile6_log - INFO -    283                                         logger.info("Using ElasticSearch as storage...")

2018-04-30 12:59:45,490 - memory_profile6_log - INFO -    284                                         logging.info("Saving main Transform model to Elasticsearch...")

2018-04-30 12:59:45,490 - memory_profile6_log - INFO -    285                                         mh.saveElasticS(model_transformsv)

2018-04-30 12:59:45,492 - memory_profile6_log - INFO -    286                             

2018-04-30 12:59:45,492 - memory_profile6_log - INFO -    287                                     # need save sigma_nt for daily train

2018-04-30 12:59:45,493 - memory_profile6_log - INFO -    288                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-30 12:59:45,493 - memory_profile6_log - INFO -    289                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-30 12:59:45,493 - memory_profile6_log - INFO -    290   1098.6 MiB      0.0 MiB           if start_date and end_date:

2018-04-30 12:59:45,493 - memory_profile6_log - INFO -    291                                         if not fitby_sigmant:

2018-04-30 12:59:45,494 - memory_profile6_log - INFO -    292                                             logging.info("Saving sigma Nt...")

2018-04-30 12:59:45,496 - memory_profile6_log - INFO -    293                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-30 12:59:45,500 - memory_profile6_log - INFO -    294                                             save_sigma_nt['start_date'] = start_date

2018-04-30 12:59:45,502 - memory_profile6_log - INFO -    295                                             save_sigma_nt['end_date'] = end_date

2018-04-30 12:59:45,502 - memory_profile6_log - INFO -    296                                             print save_sigma_nt.head(5)

2018-04-30 12:59:45,503 - memory_profile6_log - INFO -    297                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-30 12:59:45,503 - memory_profile6_log - INFO -    298                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-30 12:59:45,503 - memory_profile6_log - INFO -    299   1098.6 MiB      0.0 MiB       return

2018-04-30 12:59:45,505 - memory_profile6_log - INFO - 


2018-04-30 12:59:45,506 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-30 18:16:35,540 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-30 18:16:35,611 - memory_profile6_log - INFO - date_generated: 
2018-04-30 18:16:35,612 - memory_profile6_log - INFO -  
2018-04-30 18:16:35,612 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 10, 0, 0)]
2018-04-30 18:16:35,614 - memory_profile6_log - INFO - 

2018-04-30 18:16:35,614 - memory_profile6_log - INFO - using current date: 2018-04-11
2018-04-30 18:16:35,615 - memory_profile6_log - INFO - using start date: 2018-04-10 00:00:00
2018-04-30 18:16:35,615 - memory_profile6_log - INFO - using end date: 2018-04-10
2018-04-30 18:16:35,750 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-30 18:16:35,753 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-10
2018-04-30 18:18:24,834 - memory_profile6_log - INFO - size of df: 108.51 MB
2018-04-30 18:18:24,834 - memory_profile6_log - INFO - getting total: 427635 training data(genuine interest) for date: 2018-04-10
2018-04-30 18:18:24,895 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-30 18:18:24,895 - memory_profile6_log - INFO - Appending history data...
2018-04-30 18:18:24,895 - memory_profile6_log - INFO - processing batch-0
2018-04-30 18:18:24,895 - memory_profile6_log - INFO - creating list history data...
2018-04-30 18:18:25,022 - memory_profile6_log - INFO - call history data...
2018-04-30 18:19:24,743 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 18:19:26,653 - memory_profile6_log - INFO - processing batch-1
2018-04-30 18:19:26,653 - memory_profile6_log - INFO - creating list history data...
2018-04-30 18:19:26,726 - memory_profile6_log - INFO - call history data...
2018-04-30 18:20:26,980 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 18:20:28,694 - memory_profile6_log - INFO - processing batch-2
2018-04-30 18:20:28,696 - memory_profile6_log - INFO - creating list history data...
2018-04-30 18:20:28,778 - memory_profile6_log - INFO - call history data...
2018-04-30 18:21:27,767 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 18:21:29,628 - memory_profile6_log - INFO - processing batch-3
2018-04-30 18:21:29,630 - memory_profile6_log - INFO - creating list history data...
2018-04-30 18:21:29,709 - memory_profile6_log - INFO - call history data...
2018-04-30 18:22:27,128 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 18:22:28,997 - memory_profile6_log - INFO - processing batch-4
2018-04-30 18:22:28,999 - memory_profile6_log - INFO - creating list history data...
2018-04-30 18:22:29,078 - memory_profile6_log - INFO - call history data...
2018-04-30 18:23:27,819 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 18:23:29,730 - memory_profile6_log - INFO - Appending training data...
2018-04-30 18:23:29,732 - memory_profile6_log - INFO - len datalist: 1
2018-04-30 18:23:29,733 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-30 18:23:29,746 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 18:23:29,749 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 18:23:29,750 - memory_profile6_log - INFO - ================================================

2018-04-30 18:23:29,750 - memory_profile6_log - INFO -    325     86.8 MiB     86.8 MiB   @profile

2018-04-30 18:23:29,753 - memory_profile6_log - INFO -    326                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-30 18:23:29,753 - memory_profile6_log - INFO -    327     86.8 MiB      0.0 MiB       bq_client = client

2018-04-30 18:23:29,753 - memory_profile6_log - INFO -    328     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 18:23:29,755 - memory_profile6_log - INFO -    329                             

2018-04-30 18:23:29,756 - memory_profile6_log - INFO -    330     86.8 MiB      0.0 MiB       datalist = []

2018-04-30 18:23:29,756 - memory_profile6_log - INFO -    331     86.8 MiB      0.0 MiB       datalist_hist = []

2018-04-30 18:23:29,756 - memory_profile6_log - INFO -    332                             

2018-04-30 18:23:29,756 - memory_profile6_log - INFO -    333     86.8 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-30 18:23:29,757 - memory_profile6_log - INFO -    334    689.6 MiB      0.0 MiB       for ndate in date_generated:

2018-04-30 18:23:29,759 - memory_profile6_log - INFO -    335    393.3 MiB    306.5 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-30 18:23:29,760 - memory_profile6_log - INFO -    336    393.3 MiB      0.0 MiB           if tframe is not None:

2018-04-30 18:23:29,762 - memory_profile6_log - INFO -    337    393.3 MiB      0.0 MiB               if not tframe.empty:

2018-04-30 18:23:29,763 - memory_profile6_log - INFO -    338    407.2 MiB     13.9 MiB                   X_split = np.array_split(tframe, 5)

2018-04-30 18:23:29,763 - memory_profile6_log - INFO -    339    407.2 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-30 18:23:29,765 - memory_profile6_log - INFO -    340    407.2 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-30 18:23:29,766 - memory_profile6_log - INFO -    341    689.6 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-30 18:23:29,766 - memory_profile6_log - INFO -    342                                                 # ~ loading history

2018-04-30 18:23:29,766 - memory_profile6_log - INFO -    343                                                 """

2018-04-30 18:23:29,767 - memory_profile6_log - INFO -    344                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-30 18:23:29,769 - memory_profile6_log - INFO -    345                                                 """

2018-04-30 18:23:29,772 - memory_profile6_log - INFO -    346    664.0 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-30 18:23:29,772 - memory_profile6_log - INFO -    347                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-30 18:23:29,773 - memory_profile6_log - INFO -    348    664.0 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-30 18:23:29,773 - memory_profile6_log - INFO -    349                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-30 18:23:29,775 - memory_profile6_log - INFO -    350    664.8 MiB      6.2 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-30 18:23:29,775 - memory_profile6_log - INFO -    351                             

2018-04-30 18:23:29,776 - memory_profile6_log - INFO -    352    664.8 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-30 18:23:29,776 - memory_profile6_log - INFO -    353    739.4 MiB    553.0 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-30 18:23:29,778 - memory_profile6_log - INFO -    354                             

2018-04-30 18:23:29,778 - memory_profile6_log - INFO -    355                                                 # me = os.getpid()

2018-04-30 18:23:29,779 - memory_profile6_log - INFO -    356                                                 # kill_proc_tree(me)

2018-04-30 18:23:29,782 - memory_profile6_log - INFO -    357                             

2018-04-30 18:23:29,782 - memory_profile6_log - INFO -    358    739.4 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-30 18:23:29,782 - memory_profile6_log - INFO -    359    741.2 MiB     -1.5 MiB                       for m in h_frame:

2018-04-30 18:23:29,785 - memory_profile6_log - INFO -    360    741.2 MiB     -1.5 MiB                           if m is not None:

2018-04-30 18:23:29,785 - memory_profile6_log - INFO -    361    741.2 MiB     -1.5 MiB                               if len(m) > 0:

2018-04-30 18:23:29,786 - memory_profile6_log - INFO -    362    741.2 MiB      8.9 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-30 18:23:29,786 - memory_profile6_log - INFO -    363    689.6 MiB   -285.9 MiB                       del h_frame

2018-04-30 18:23:29,786 - memory_profile6_log - INFO -    364    689.6 MiB     -1.3 MiB                       del lhistory

2018-04-30 18:23:29,788 - memory_profile6_log - INFO -    365                             

2018-04-30 18:23:29,789 - memory_profile6_log - INFO -    366    689.6 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-30 18:23:29,789 - memory_profile6_log - INFO -    367    689.6 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-30 18:23:29,792 - memory_profile6_log - INFO -    368                                     else: 

2018-04-30 18:23:29,792 - memory_profile6_log - INFO -    369                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-30 18:23:29,792 - memory_profile6_log - INFO -    370    689.6 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-30 18:23:29,793 - memory_profile6_log - INFO -    371    689.6 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-30 18:23:29,795 - memory_profile6_log - INFO -    372                             

2018-04-30 18:23:29,796 - memory_profile6_log - INFO -    373    689.6 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-30 18:23:29,796 - memory_profile6_log - INFO - 


2018-04-30 18:23:31,312 - memory_profile6_log - INFO - size of big_frame_hist: 117.89 MB
2018-04-30 18:23:31,430 - memory_profile6_log - INFO - size of big_frame: 108.51 MB
2018-04-30 18:23:31,453 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-04-30 18:29:19,726 - memory_profile6_log - INFO - size of df: 471.89 MB
2018-04-30 18:29:19,726 - memory_profile6_log - INFO - getting total: 1929359 training data(current date interest)
2018-04-30 18:29:20,354 - memory_profile6_log - INFO - size of current_frame: 486.61 MB
2018-04-30 18:29:20,355 - memory_profile6_log - INFO - loading time of: 2356994 total genuine-current interest data ~ take 764.627s
2018-04-30 18:29:20,385 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 18:29:20,387 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 18:29:20,388 - memory_profile6_log - INFO - ================================================

2018-04-30 18:29:20,388 - memory_profile6_log - INFO -    375     86.6 MiB     86.6 MiB   @profile

2018-04-30 18:29:20,390 - memory_profile6_log - INFO -    376                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-30 18:29:20,390 - memory_profile6_log - INFO -    377     86.8 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-30 18:29:20,390 - memory_profile6_log - INFO -    378     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 18:29:20,391 - memory_profile6_log - INFO -    379                             

2018-04-30 18:29:20,391 - memory_profile6_log - INFO -    380                                 # ~~~ Begin collecting data ~~~

2018-04-30 18:29:20,391 - memory_profile6_log - INFO -    381     86.8 MiB      0.0 MiB       t0 = time.time()

2018-04-30 18:29:20,391 - memory_profile6_log - INFO -    382    681.8 MiB    595.1 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-30 18:29:20,392 - memory_profile6_log - INFO -    383    681.8 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-30 18:29:20,392 - memory_profile6_log - INFO -    384                                     logger.info("Training cannot be empty..")

2018-04-30 18:29:20,392 - memory_profile6_log - INFO -    385                                     return False

2018-04-30 18:29:20,394 - memory_profile6_log - INFO -    386    708.0 MiB     26.2 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-30 18:29:20,398 - memory_profile6_log - INFO -    387                                 # print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-30 18:29:20,398 - memory_profile6_log - INFO -    388    708.0 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-30 18:29:20,400 - memory_profile6_log - INFO -    389                             

2018-04-30 18:29:20,400 - memory_profile6_log - INFO -    390    718.0 MiB      9.9 MiB       big_frame = pd.concat(datalist)

2018-04-30 18:29:20,401 - memory_profile6_log - INFO -    391    718.0 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-30 18:29:20,401 - memory_profile6_log - INFO -    392    708.2 MiB     -9.8 MiB       del datalist

2018-04-30 18:29:20,401 - memory_profile6_log - INFO -    393                             

2018-04-30 18:29:20,401 - memory_profile6_log - INFO -    394    708.2 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 18:29:20,401 - memory_profile6_log - INFO -    395                             

2018-04-30 18:29:20,403 - memory_profile6_log - INFO -    396                                 # ~ get current news interest ~

2018-04-30 18:29:20,403 - memory_profile6_log - INFO -    397    708.2 MiB      0.0 MiB       if not cd:

2018-04-30 18:29:20,404 - memory_profile6_log - INFO -    398    708.2 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-04-30 18:29:20,404 - memory_profile6_log - INFO -    399   1068.9 MiB    360.7 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-30 18:29:20,404 - memory_profile6_log - INFO -    400   1068.9 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-30 18:29:20,404 - memory_profile6_log - INFO -    401                                 else:

2018-04-30 18:29:20,404 - memory_profile6_log - INFO -    402                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-30 18:29:20,404 - memory_profile6_log - INFO -    403                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-30 18:29:20,407 - memory_profile6_log - INFO -    404                             

2018-04-30 18:29:20,408 - memory_profile6_log - INFO -    405                                     # safe handling of query parameter

2018-04-30 18:29:20,410 - memory_profile6_log - INFO -    406                                     query_params = [

2018-04-30 18:29:20,411 - memory_profile6_log - INFO -    407                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-30 18:29:20,411 - memory_profile6_log - INFO -    408                                     ]

2018-04-30 18:29:20,411 - memory_profile6_log - INFO -    409                             

2018-04-30 18:29:20,411 - memory_profile6_log - INFO -    410                                     job_config.query_parameters = query_params

2018-04-30 18:29:20,413 - memory_profile6_log - INFO -    411                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-30 18:29:20,414 - memory_profile6_log - INFO -    412                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-30 18:29:20,414 - memory_profile6_log - INFO -    413                             

2018-04-30 18:29:20,415 - memory_profile6_log - INFO -    414   1083.6 MiB     14.7 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-30 18:29:20,415 - memory_profile6_log - INFO -    415   1083.6 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-30 18:29:20,415 - memory_profile6_log - INFO -    416   1083.7 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-30 18:29:20,418 - memory_profile6_log - INFO -    417   1083.7 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-30 18:29:20,420 - memory_profile6_log - INFO -    418   1083.7 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-30 18:29:20,421 - memory_profile6_log - INFO -    419   1083.7 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-30 18:29:20,421 - memory_profile6_log - INFO -    420                             

2018-04-30 18:29:20,421 - memory_profile6_log - INFO -    421   1083.7 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-30 18:29:20,421 - memory_profile6_log - INFO - 


2018-04-30 18:29:20,426 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-30 18:29:20,553 - memory_profile6_log - INFO - train on: 427635 total genuine interest data(D(u, t))
2018-04-30 18:29:20,553 - memory_profile6_log - INFO - transform on: 1929359 total current data(D(t))
2018-04-30 18:29:20,553 - memory_profile6_log - INFO - apply on: 427635 total history...)
2018-04-30 18:29:20,974 - memory_profile6_log - INFO - len of uniques_fit_hist:427635
2018-04-30 18:29:21,243 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:52258
2018-04-30 18:29:22,296 - memory_profile6_log - INFO - Len of model_fit: 427635
2018-04-30 18:29:22,296 - memory_profile6_log - INFO - Len of df_dut: 427635
2018-04-30 18:29:26,002 - memory_profile6_log - INFO - Len of fitted_models on main class: 427635
2018-04-30 18:29:26,003 - memory_profile6_log - INFO - 

2018-04-30 18:29:26,005 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-30 18:29:26,005 - memory_profile6_log - INFO - 

2018-04-30 18:29:26,095 - memory_profile6_log - INFO - len of fitted models after concat: 855270
2018-04-30 18:29:26,095 - memory_profile6_log - INFO - 

2018-04-30 18:29:26,096 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-30 18:29:26,098 - memory_profile6_log - INFO - 

2018-04-30 18:29:26,707 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 427635
2018-04-30 18:29:26,707 - memory_profile6_log - INFO - 

2018-04-30 18:30:25,302 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-30 18:30:25,617 - memory_profile6_log - INFO -                                                   user_id           topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt  p0_posterior  is_general  rank
246500  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22601470         345.673524             355.673524   0.064560       128      0.166393        True   1.0
246509  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27431099         387.256535             397.256535   0.051877       128      0.149337        True   2.0
246499  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22553543          89.967400              99.967400   0.131487       128      0.095249        True   3.0
246506  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27428266         372.562342             382.562342   0.009824       128      0.027233        True   4.0
246497  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22552349         519.838302             529.838302   0.005242       128      0.020127        True   5.0
246503  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22661796         102.748815             112.748815   0.016288       128      0.013308        True   6.0
246495  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22551890         892.148245             902.148245   0.001499       128      0.009800        True   7.0
246498  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22553321         168.384798             178.384798   0.005673       128      0.007333        True   8.0
246496  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22552186         172.446407             182.446407   0.005503       128      0.007275        True   9.0
246504  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22662160         153.675862             163.675862   0.002721       128      0.003227        True  10.0
246501  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22615029        2221.501966            2231.501966   0.000170       128      0.002745        True  11.0
246489  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1164316987        2062.475707            2072.475707   0.027965       128      0.419984       False   1.0
246526  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           36060446         833.369515             843.369515   0.026573       128      0.162399       False   2.0
246502  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22661658       18980.340538           18990.340538   0.000192       128      0.026395       False   3.0
246529  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           39301645         236.906870             246.906870   0.014040       128      0.025119       False   4.0
246531  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           40410447        3239.018630            3249.018630   0.001033       128      0.024318       False   5.0
246538  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           67524823       49435.537114           49445.537114   0.000046       128      0.016468       False   6.0
246483  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1039394600        3598.398906            3608.398906   0.000574       128      0.014999       False   7.0
246508  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27430823          97.318507             107.318507   0.012942       128      0.010065       False   8.0
246536  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           47091877         873.363621             883.363621   0.001373       128      0.008792       False   9.0
246487  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1151613972         939.971480             949.971480   0.000911       128      0.006271       False  10.0
246477  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1017140824       44491.983402           44501.983402   0.000018       128      0.005719       False  11.0
246512  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790313513        2987.377027            2997.377027   0.000257       128      0.005573       False  12.0
246486  1616f009d96b1-0285d8288a5bce-70217860-38400-16...          110282443         363.892994             373.892994   0.001339       128      0.003628       False  13.0
246505  1616f009d96b1-0285d8288a5bce-70217860-38400-16...          271947679         972.856780             982.856780   0.000443       128      0.003152       False  14.0
246535  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           41566010          88.933715              98.933715   0.003888       128      0.002787       False  15.0
2018-04-30 18:30:25,632 - memory_profile6_log - INFO - 

2018-04-30 18:30:25,634 - memory_profile6_log - INFO - Len of model_transform: 393224
2018-04-30 18:30:25,634 - memory_profile6_log - INFO - Len of df_dt: 1929359
2018-04-30 18:30:25,635 - memory_profile6_log - INFO - Total train time: 64.809s
2018-04-30 18:30:25,638 - memory_profile6_log - INFO - memory left before cleaning: 80.900 percent memory...
2018-04-30 18:30:25,638 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-30 18:30:25,638 - memory_profile6_log - INFO - deleting df_dut...
2018-04-30 18:30:25,641 - memory_profile6_log - INFO - deleting df_dt...
2018-04-30 18:30:25,642 - memory_profile6_log - INFO - deleting df_input...
2018-04-30 18:30:25,730 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-30 18:30:25,733 - memory_profile6_log - INFO - deleting df_current...
2018-04-30 18:30:25,733 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-30 18:30:25,750 - memory_profile6_log - INFO - deleting model_fit...
2018-04-30 18:30:25,750 - memory_profile6_log - INFO - deleting result...
2018-04-30 18:30:25,782 - memory_profile6_log - INFO - deleting model_transform...
2018-04-30 18:30:25,782 - memory_profile6_log - INFO - memory left after cleaning: 80.300 percent memory...
2018-04-30 18:30:25,782 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-30 18:30:25,782 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-30 18:30:25,782 - memory_profile6_log - INFO - Saving main Transform model to Google DataStore...
2018-04-30 18:30:25,782 - memory_profile6_log - INFO - Saving total data: 393224
2018-04-30 18:30:25,969 - memory_profile6_log - INFO - Saving fitted_models as history to Google DataStore...
2018-04-30 18:30:26,062 - memory_profile6_log - INFO - Saving total data: 427635
2018-04-30 18:30:26,094 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 10
2018-04-30 18:30:26,108 - memory_profile6_log - INFO - processing batch-0
2018-04-30 18:30:26,302 - memory_profile6_log - INFO - processing batch-1
2018-04-30 18:30:26,505 - memory_profile6_log - INFO - processing batch-2
2018-04-30 18:30:26,683 - memory_profile6_log - INFO - processing batch-3
2018-04-30 18:30:26,867 - memory_profile6_log - INFO - processing batch-4
2018-04-30 18:30:27,069 - memory_profile6_log - INFO - processing batch-5
2018-04-30 18:30:27,289 - memory_profile6_log - INFO - processing batch-6
2018-04-30 18:30:27,493 - memory_profile6_log - INFO - processing batch-7
2018-04-30 18:30:27,694 - memory_profile6_log - INFO - processing batch-8
2018-04-30 18:30:27,930 - memory_profile6_log - INFO - processing batch-9
2018-04-30 18:30:28,150 - memory_profile6_log - INFO - deleting X_split...
2018-04-30 18:30:28,151 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-04-30 18:30:28,153 - memory_profile6_log - INFO - deleting BR...
2018-04-30 18:30:28,165 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 18:30:28,165 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 18:30:28,165 - memory_profile6_log - INFO - ================================================

2018-04-30 18:30:28,165 - memory_profile6_log - INFO -    113   1073.5 MiB   1073.5 MiB   @profile

2018-04-30 18:30:28,165 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-30 18:30:28,165 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-30 18:30:28,165 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-30 18:30:28,165 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-30 18:30:28,165 - memory_profile6_log - INFO -    118                                      saveto="datastore", fitted_models_hist=None):

2018-04-30 18:30:28,165 - memory_profile6_log - INFO -    119                                 """

2018-04-30 18:30:28,165 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-30 18:30:28,165 - memory_profile6_log - INFO -    121                                 """

2018-04-30 18:30:28,165 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-30 18:30:28,165 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-30 18:30:28,165 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-30 18:30:28,165 - memory_profile6_log - INFO -    125   1073.5 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-30 18:30:28,165 - memory_profile6_log - INFO -    126   1086.9 MiB     13.5 MiB       df_dut = df_input.copy(deep=True)

2018-04-30 18:30:28,165 - memory_profile6_log - INFO -    127   1086.9 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 18:30:28,165 - memory_profile6_log - INFO -    128                             

2018-04-30 18:30:28,165 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-30 18:30:28,165 - memory_profile6_log - INFO -    130   1147.7 MiB     60.7 MiB       df_dt = df_current.copy(deep=True)

2018-04-30 18:30:28,165 - memory_profile6_log - INFO -    131   1147.7 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 18:30:28,165 - memory_profile6_log - INFO -    132                             

2018-04-30 18:30:28,165 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-30 18:30:28,165 - memory_profile6_log - INFO -    134   1147.7 MiB      0.0 MiB       t0 = time.time()

2018-04-30 18:30:28,165 - memory_profile6_log - INFO -    135   1147.7 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-30 18:30:28,165 - memory_profile6_log - INFO -    136   1147.7 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-30 18:30:28,165 - memory_profile6_log - INFO -    137   1147.7 MiB      0.0 MiB       logger.info("apply on: %d total history...)", len(df_hist))

2018-04-30 18:30:28,165 - memory_profile6_log - INFO -    138                             

2018-04-30 18:30:28,165 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-30 18:30:28,165 - memory_profile6_log - INFO -    140   1147.7 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-30 18:30:28,186 - memory_profile6_log - INFO -    141                             

2018-04-30 18:30:28,187 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-30 18:30:28,187 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-30 18:30:28,187 - memory_profile6_log - INFO -    144   1150.1 MiB      2.4 MiB       NB = BR.processX(df_dut)

2018-04-30 18:30:28,188 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-30 18:30:28,188 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-30 18:30:28,188 - memory_profile6_log - INFO -    147   1166.6 MiB     16.5 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-30 18:30:28,190 - memory_profile6_log - INFO -    148                                 """

2018-04-30 18:30:28,190 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-30 18:30:28,190 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-30 18:30:28,190 - memory_profile6_log - INFO -    151                                 """

2018-04-30 18:30:28,190 - memory_profile6_log - INFO -    152   1166.6 MiB      0.0 MiB       fitby_sigmant = False

2018-04-30 18:30:28,190 - memory_profile6_log - INFO -    153   1166.6 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-30 18:30:28,191 - memory_profile6_log - INFO -    154   1166.6 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-30 18:30:28,191 - memory_profile6_log - INFO -    155   1182.9 MiB     16.3 MiB                            'is_general']]

2018-04-30 18:30:28,193 - memory_profile6_log - INFO -    156                                 # get sigma_Nt from fitted_models_hist

2018-04-30 18:30:28,193 - memory_profile6_log - INFO -    157   1189.4 MiB      6.5 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-04-30 18:30:28,193 - memory_profile6_log - INFO -    158   1189.4 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    159   1185.1 MiB     -4.3 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    160   1185.1 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    161                             

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    162                                 # begin fit

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    163                                 """model_fit = BR.fit(df_dut, df_input_X,

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    164                                                    full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    165                                                    sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    166                                 """

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    167   1185.1 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    168   1185.1 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    169   1224.3 MiB     39.2 MiB                          sigma_nt_hist=None, verbose=False)

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    170   1224.3 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    171   1224.3 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    172                                 # print model_fit[['num_x', 'num_x', 'sigma_Nt', 'date_all_click']].loc[model_fit['user_id']=="1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157"].head(5)

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    173                             

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    174                                 # ~~ and Transform ~~

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    175                                 #   handling current news interest == current date

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    176   1224.3 MiB      0.0 MiB       if df_dt.empty:

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    177                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    178                                     return None

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    179   1230.1 MiB      5.8 MiB       NB = BR.processX(df_dt)

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    180   1320.6 MiB     90.4 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    181                             

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    182   1320.6 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    183   1379.7 MiB     59.1 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    184                             

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    185                                 # print "model_fit dtypes:\n", model_fit.dtypes

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    186                                 # print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    187   1379.7 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    188   1379.7 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    189   1379.7 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    190   1389.5 MiB      9.8 MiB                                                                                           "topic_id", "user_id"]],

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    191   1406.9 MiB     17.4 MiB                                                     verbose=False)

2018-04-30 18:30:28,194 - memory_profile6_log - INFO -    192                                 """

2018-04-30 18:30:28,210 - memory_profile6_log - INFO -    193                                 model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-30 18:30:28,210 - memory_profile6_log - INFO -    194                                                                               fitted_model=model_fit,

2018-04-30 18:30:28,210 - memory_profile6_log - INFO -    195                                                                               fitted_model_hist=None,

2018-04-30 18:30:28,210 - memory_profile6_log - INFO -    196                                                                               verbose=False)"""

2018-04-30 18:30:28,210 - memory_profile6_log - INFO -    197                                 # ~~~ filter is general and specific topic ~~~

2018-04-30 18:30:28,210 - memory_profile6_log - INFO -    198                                 # the idea is just we need to rerank every topic according

2018-04-30 18:30:28,210 - memory_profile6_log - INFO -    199                                 # user_id and and is_general by p0_posterior

2018-04-30 18:30:28,210 - memory_profile6_log - INFO -    200   1406.9 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-30 18:30:28,210 - memory_profile6_log - INFO -    201   1410.2 MiB      3.3 MiB                                     'is_general']].groupby(['topic_id',

2018-04-30 18:30:28,210 - memory_profile6_log - INFO -    202   1406.9 MiB     -3.2 MiB                                                             'is_general']

2018-04-30 18:30:28,210 - memory_profile6_log - INFO -    203                                                                                      ).size().to_frame().reset_index()

2018-04-30 18:30:28,210 - memory_profile6_log - INFO -    204                             

2018-04-30 18:30:28,210 - memory_profile6_log - INFO -    205                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-30 18:30:28,210 - memory_profile6_log - INFO -    206                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-30 18:30:28,210 - memory_profile6_log - INFO -    207   1407.0 MiB      0.1 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-30 18:30:28,210 - memory_profile6_log - INFO -    208                             

2018-04-30 18:30:28,210 - memory_profile6_log - INFO -    209                                 # ~ start by provide rank for each topic type ~

2018-04-30 18:30:28,210 - memory_profile6_log - INFO -    210   1441.7 MiB     34.7 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-30 18:30:28,210 - memory_profile6_log - INFO -    211   1449.0 MiB      7.3 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-30 18:30:28,210 - memory_profile6_log - INFO -    212                             

2018-04-30 18:30:28,210 - memory_profile6_log - INFO -    213                                 # ~ set threshold to filter output

2018-04-30 18:30:28,210 - memory_profile6_log - INFO -    214   1449.0 MiB      0.0 MiB       if threshold > 0:

2018-04-30 18:30:28,210 - memory_profile6_log - INFO -    215   1449.0 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-30 18:30:28,210 - memory_profile6_log - INFO -    216   1449.0 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-30 18:30:28,210 - memory_profile6_log - INFO -    217   1447.6 MiB     -1.4 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-30 18:30:28,210 - memory_profile6_log - INFO -    218                             

2018-04-30 18:30:28,210 - memory_profile6_log - INFO -    219   1447.6 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-30 18:30:28,210 - memory_profile6_log - INFO -    220   1428.5 MiB    -19.2 MiB       print model_transform[model_transform['user_id']=='1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157']

2018-04-30 18:30:28,210 - memory_profile6_log - INFO -    221   1428.5 MiB      0.0 MiB       logger.info("Len of model_transform: %d", len(model_transform))

2018-04-30 18:30:28,226 - memory_profile6_log - INFO -    222   1428.5 MiB      0.0 MiB       logger.info("Len of df_dt: %d", len(df_dt))

2018-04-30 18:30:28,226 - memory_profile6_log - INFO -    223   1428.5 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-30 18:30:28,226 - memory_profile6_log - INFO -    224                             

2018-04-30 18:30:28,226 - memory_profile6_log - INFO -    225   1428.5 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-30 18:30:28,226 - memory_profile6_log - INFO -    226                             

2018-04-30 18:30:28,226 - memory_profile6_log - INFO -    227   1428.5 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-30 18:30:28,226 - memory_profile6_log - INFO -    228   1428.5 MiB      0.0 MiB       del df_dut

2018-04-30 18:30:28,226 - memory_profile6_log - INFO -    229   1428.5 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-30 18:30:28,226 - memory_profile6_log - INFO -    230   1428.5 MiB      0.0 MiB       del df_dt

2018-04-30 18:30:28,226 - memory_profile6_log - INFO -    231   1428.5 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-30 18:30:28,226 - memory_profile6_log - INFO -    232   1428.5 MiB      0.0 MiB       del df_input

2018-04-30 18:30:28,226 - memory_profile6_log - INFO -    233   1428.5 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-30 18:30:28,226 - memory_profile6_log - INFO -    234   1353.0 MiB    -75.4 MiB       del df_input_X

2018-04-30 18:30:28,226 - memory_profile6_log - INFO -    235   1353.0 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-30 18:30:28,226 - memory_profile6_log - INFO -    236   1353.0 MiB      0.0 MiB       del df_current

2018-04-30 18:30:28,226 - memory_profile6_log - INFO -    237   1353.0 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-30 18:30:28,226 - memory_profile6_log - INFO -    238   1353.0 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-30 18:30:28,226 - memory_profile6_log - INFO -    239   1353.0 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-30 18:30:28,226 - memory_profile6_log - INFO -    240   1317.1 MiB    -35.9 MiB       del model_fit

2018-04-30 18:30:28,226 - memory_profile6_log - INFO -    241   1317.1 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-30 18:30:28,226 - memory_profile6_log - INFO -    242   1317.1 MiB      0.0 MiB       del result

2018-04-30 18:30:28,226 - memory_profile6_log - INFO -    243   1317.1 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-30 18:30:28,226 - memory_profile6_log - INFO -    244                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 18:30:28,226 - memory_profile6_log - INFO -    245                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 18:30:28,226 - memory_profile6_log - INFO -    246                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 18:30:28,226 - memory_profile6_log - INFO -    247   1317.1 MiB      0.0 MiB       if savetrain:

2018-04-30 18:30:28,226 - memory_profile6_log - INFO -    248   1326.2 MiB      9.0 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-30 18:30:28,240 - memory_profile6_log - INFO -    249   1326.2 MiB      0.0 MiB           del model_transform

2018-04-30 18:30:28,240 - memory_profile6_log - INFO -    250   1326.2 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-30 18:30:28,240 - memory_profile6_log - INFO -    251   1326.2 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-30 18:30:28,240 - memory_profile6_log - INFO -    252                             

2018-04-30 18:30:28,240 - memory_profile6_log - INFO -    253   1326.2 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-30 18:30:28,240 - memory_profile6_log - INFO -    254                                     # ~ Place your code to save the training model here ~

2018-04-30 18:30:28,240 - memory_profile6_log - INFO -    255   1326.2 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-30 18:30:28,240 - memory_profile6_log - INFO -    256   1326.2 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-30 18:30:28,240 - memory_profile6_log - INFO -    257   1326.2 MiB      0.0 MiB               if multproc:

2018-04-30 18:30:28,240 - memory_profile6_log - INFO -    258                                             # ~ save transform models ~

2018-04-30 18:30:28,240 - memory_profile6_log - INFO -    259   1326.2 MiB      0.0 MiB                   logger.info("Saving main Transform model to Google DataStore...")

2018-04-30 18:30:28,240 - memory_profile6_log - INFO -    260   1326.2 MiB      0.0 MiB                   logger.info("Saving total data: %d", len(model_transformsv))

2018-04-30 18:30:28,240 - memory_profile6_log - INFO -    261   1306.1 MiB    -20.0 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-30 18:30:28,240 - memory_profile6_log - INFO -    262                             

2018-04-30 18:30:28,240 - memory_profile6_log - INFO -    263                                             # ~ save fitted models ~

2018-04-30 18:30:28,240 - memory_profile6_log - INFO -    264   1306.1 MiB      0.0 MiB                   logger.info("Saving fitted_models as history to Google DataStore...")

2018-04-30 18:30:28,240 - memory_profile6_log - INFO -    265   1306.2 MiB      0.0 MiB                   save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-30 18:30:28,240 - memory_profile6_log - INFO -    266   1330.4 MiB     24.2 MiB                   fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-30 18:30:28,240 - memory_profile6_log - INFO -    267   1330.4 MiB      0.0 MiB                   logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-04-30 18:30:28,240 - memory_profile6_log - INFO -    268   1345.8 MiB     15.4 MiB                   X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-30 18:30:28,240 - memory_profile6_log - INFO -    269   1345.8 MiB      0.0 MiB                   logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-30 18:30:28,256 - memory_profile6_log - INFO -    270   1349.1 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-30 18:30:28,256 - memory_profile6_log - INFO -    271   1349.1 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-30 18:30:28,256 - memory_profile6_log - INFO -    272   1349.1 MiB      3.3 MiB                       mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-30 18:30:28,256 - memory_profile6_log - INFO -    273                             

2018-04-30 18:30:28,256 - memory_profile6_log - INFO -    274   1346.1 MiB     -3.0 MiB                   del X_split

2018-04-30 18:30:28,256 - memory_profile6_log - INFO -    275   1346.1 MiB      0.0 MiB                   logger.info("deleting X_split...")

2018-04-30 18:30:28,256 - memory_profile6_log - INFO -    276   1346.1 MiB      0.0 MiB                   del save_sigma_nt

2018-04-30 18:30:28,256 - memory_profile6_log - INFO -    277   1346.1 MiB      0.0 MiB                   logger.info("deleting save_sigma_nt...")

2018-04-30 18:30:28,256 - memory_profile6_log - INFO -    278                             

2018-04-30 18:30:28,256 - memory_profile6_log - INFO -    279   1346.1 MiB      0.0 MiB                   del BR

2018-04-30 18:30:28,256 - memory_profile6_log - INFO -    280   1346.1 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-30 18:30:28,256 - memory_profile6_log - INFO -    281                             

2018-04-30 18:30:28,256 - memory_profile6_log - INFO -    282                                     elif str(saveto).lower() == "elastic":

2018-04-30 18:30:28,256 - memory_profile6_log - INFO -    283                                         logger.info("Using ElasticSearch as storage...")

2018-04-30 18:30:28,256 - memory_profile6_log - INFO -    284                                         logging.info("Saving main Transform model to Elasticsearch...")

2018-04-30 18:30:28,256 - memory_profile6_log - INFO -    285                                         mh.saveElasticS(model_transformsv)

2018-04-30 18:30:28,256 - memory_profile6_log - INFO -    286                             

2018-04-30 18:30:28,256 - memory_profile6_log - INFO -    287                                     # need save sigma_nt for daily train

2018-04-30 18:30:28,256 - memory_profile6_log - INFO -    288                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-30 18:30:28,256 - memory_profile6_log - INFO -    289                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-30 18:30:28,256 - memory_profile6_log - INFO -    290   1346.1 MiB      0.0 MiB           if start_date and end_date:

2018-04-30 18:30:28,256 - memory_profile6_log - INFO -    291                                         if not fitby_sigmant:

2018-04-30 18:30:28,256 - memory_profile6_log - INFO -    292                                             logging.info("Saving sigma Nt...")

2018-04-30 18:30:28,256 - memory_profile6_log - INFO -    293                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-30 18:30:28,256 - memory_profile6_log - INFO -    294                                             save_sigma_nt['start_date'] = start_date

2018-04-30 18:30:28,256 - memory_profile6_log - INFO -    295                                             save_sigma_nt['end_date'] = end_date

2018-04-30 18:30:28,256 - memory_profile6_log - INFO -    296                                             print save_sigma_nt.head(5)

2018-04-30 18:30:28,273 - memory_profile6_log - INFO -    297                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-30 18:30:28,273 - memory_profile6_log - INFO -    298                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-30 18:30:28,273 - memory_profile6_log - INFO -    299   1346.1 MiB      0.0 MiB       return

2018-04-30 18:30:28,273 - memory_profile6_log - INFO - 


2018-04-30 18:30:28,273 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-30 19:09:44,051 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-30 19:09:44,051 - memory_profile6_log - INFO - date_generated: 
2018-04-30 19:09:44,051 - memory_profile6_log - INFO -  
2018-04-30 19:09:44,051 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-30 19:09:44,051 - memory_profile6_log - INFO - 

2018-04-30 19:09:44,051 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-30 19:09:44,051 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-30 19:09:44,051 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-30 19:09:44,176 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-30 19:09:44,191 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-30 19:10:07,522 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-30 19:10:07,522 - memory_profile6_log - INFO - date_generated: 
2018-04-30 19:10:07,522 - memory_profile6_log - INFO -  
2018-04-30 19:10:07,522 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-30 19:10:07,522 - memory_profile6_log - INFO - 

2018-04-30 19:10:07,522 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-30 19:10:07,522 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-30 19:10:07,522 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-30 19:10:07,647 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-30 19:10:07,663 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-30 19:11:18,483 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-30 19:11:18,483 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-30 19:11:18,529 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-30 19:11:18,529 - memory_profile6_log - INFO - Appending history data...
2018-04-30 19:11:18,529 - memory_profile6_log - INFO - processing batch-0
2018-04-30 19:11:18,529 - memory_profile6_log - INFO - creating list history data...
2018-04-30 19:11:18,608 - memory_profile6_log - INFO - call history data...
2018-04-30 19:11:59,453 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 19:12:00,868 - memory_profile6_log - INFO - processing batch-1
2018-04-30 19:12:00,868 - memory_profile6_log - INFO - creating list history data...
2018-04-30 19:12:00,914 - memory_profile6_log - INFO - call history data...
2018-04-30 19:12:39,483 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 19:12:40,627 - memory_profile6_log - INFO - processing batch-2
2018-04-30 19:12:40,627 - memory_profile6_log - INFO - creating list history data...
2018-04-30 19:12:40,674 - memory_profile6_log - INFO - call history data...
2018-04-30 19:13:19,739 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 19:13:20,897 - memory_profile6_log - INFO - processing batch-3
2018-04-30 19:13:20,897 - memory_profile6_log - INFO - creating list history data...
2018-04-30 19:13:20,950 - memory_profile6_log - INFO - call history data...
2018-04-30 19:13:58,960 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 19:14:00,170 - memory_profile6_log - INFO - processing batch-4
2018-04-30 19:14:00,170 - memory_profile6_log - INFO - creating list history data...
2018-04-30 19:14:00,232 - memory_profile6_log - INFO - call history data...
2018-04-30 19:14:38,338 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 19:14:39,561 - memory_profile6_log - INFO - Appending training data...
2018-04-30 19:14:39,561 - memory_profile6_log - INFO - len datalist: 1
2018-04-30 19:14:39,561 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-30 19:14:39,561 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 19:14:39,561 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 19:14:39,561 - memory_profile6_log - INFO - ================================================

2018-04-30 19:14:39,561 - memory_profile6_log - INFO -    320     87.5 MiB     87.5 MiB   @profile

2018-04-30 19:14:39,561 - memory_profile6_log - INFO -    321                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-30 19:14:39,561 - memory_profile6_log - INFO -    322     87.5 MiB      0.0 MiB       bq_client = client

2018-04-30 19:14:39,561 - memory_profile6_log - INFO -    323     87.5 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 19:14:39,561 - memory_profile6_log - INFO -    324                             

2018-04-30 19:14:39,576 - memory_profile6_log - INFO -    325     87.5 MiB      0.0 MiB       datalist = []

2018-04-30 19:14:39,576 - memory_profile6_log - INFO -    326     87.5 MiB      0.0 MiB       datalist_hist = []

2018-04-30 19:14:39,576 - memory_profile6_log - INFO -    327                             

2018-04-30 19:14:39,576 - memory_profile6_log - INFO -    328     87.5 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-30 19:14:39,576 - memory_profile6_log - INFO -    329    447.1 MiB      0.0 MiB       for ndate in date_generated:

2018-04-30 19:14:39,576 - memory_profile6_log - INFO -    330    339.9 MiB    252.4 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-30 19:14:39,576 - memory_profile6_log - INFO -    331    339.9 MiB      0.0 MiB           if tframe is not None:

2018-04-30 19:14:39,576 - memory_profile6_log - INFO -    332    339.9 MiB      0.0 MiB               if not tframe.empty:

2018-04-30 19:14:39,576 - memory_profile6_log - INFO -    333    347.8 MiB      7.9 MiB                   X_split = np.array_split(tframe, 5)

2018-04-30 19:14:39,576 - memory_profile6_log - INFO -    334    347.8 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-30 19:14:39,576 - memory_profile6_log - INFO -    335    347.8 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-30 19:14:39,576 - memory_profile6_log - INFO -    336    447.1 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-30 19:14:39,576 - memory_profile6_log - INFO -    337                                                 # ~ loading history

2018-04-30 19:14:39,576 - memory_profile6_log - INFO -    338                                                 """

2018-04-30 19:14:39,576 - memory_profile6_log - INFO -    339                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-30 19:14:39,576 - memory_profile6_log - INFO -    340                                                 """

2018-04-30 19:14:39,576 - memory_profile6_log - INFO -    341    431.1 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-30 19:14:39,576 - memory_profile6_log - INFO -    342                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-30 19:14:39,576 - memory_profile6_log - INFO -    343    431.1 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-30 19:14:39,576 - memory_profile6_log - INFO -    344                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-30 19:14:39,576 - memory_profile6_log - INFO -    345    431.4 MiB      3.6 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-30 19:14:39,592 - memory_profile6_log - INFO -    346                             

2018-04-30 19:14:39,592 - memory_profile6_log - INFO -    347    431.4 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-30 19:14:39,592 - memory_profile6_log - INFO -    348    475.3 MiB    247.0 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-30 19:14:39,592 - memory_profile6_log - INFO -    349                             

2018-04-30 19:14:39,592 - memory_profile6_log - INFO -    350                                                 # me = os.getpid()

2018-04-30 19:14:39,592 - memory_profile6_log - INFO -    351                                                 # kill_proc_tree(me)

2018-04-30 19:14:39,592 - memory_profile6_log - INFO -    352                             

2018-04-30 19:14:39,592 - memory_profile6_log - INFO -    353    475.3 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-30 19:14:39,592 - memory_profile6_log - INFO -    354    476.4 MiB     -1.3 MiB                       for m in h_frame:

2018-04-30 19:14:39,592 - memory_profile6_log - INFO -    355    476.4 MiB     -1.3 MiB                           if m is not None:

2018-04-30 19:14:39,592 - memory_profile6_log - INFO -    356    476.4 MiB     -1.3 MiB                               if len(m) > 0:

2018-04-30 19:14:39,592 - memory_profile6_log - INFO -    357    476.4 MiB      2.9 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-30 19:14:39,592 - memory_profile6_log - INFO -    358    447.1 MiB   -155.4 MiB                       del h_frame

2018-04-30 19:14:39,608 - memory_profile6_log - INFO -    359    447.1 MiB      0.0 MiB                       del lhistory

2018-04-30 19:14:39,608 - memory_profile6_log - INFO -    360                             

2018-04-30 19:14:39,608 - memory_profile6_log - INFO -    361    447.1 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-30 19:14:39,608 - memory_profile6_log - INFO -    362    447.1 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-30 19:14:39,608 - memory_profile6_log - INFO -    363                                     else: 

2018-04-30 19:14:39,608 - memory_profile6_log - INFO -    364                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-30 19:14:39,608 - memory_profile6_log - INFO -    365    447.1 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-30 19:14:39,608 - memory_profile6_log - INFO -    366    447.1 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-30 19:14:39,608 - memory_profile6_log - INFO -    367                             

2018-04-30 19:14:39,608 - memory_profile6_log - INFO -    368    447.1 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-30 19:14:39,608 - memory_profile6_log - INFO - 


2018-04-30 19:14:40,724 - memory_profile6_log - INFO - size of big_frame_hist: 69.87 MB
2018-04-30 19:14:40,789 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-30 19:14:40,789 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-04-30 19:20:01,512 - memory_profile6_log - INFO - size of df: 471.89 MB
2018-04-30 19:20:01,516 - memory_profile6_log - INFO - getting total: 1929359 training data(current date interest)
2018-04-30 19:20:02,150 - memory_profile6_log - INFO - size of current_frame: 486.61 MB
2018-04-30 19:20:02,151 - memory_profile6_log - INFO - loading time of: 2183354 total genuine-current interest data ~ take 594.520s
2018-04-30 19:20:02,174 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 19:20:02,176 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 19:20:02,177 - memory_profile6_log - INFO - ================================================

2018-04-30 19:20:02,178 - memory_profile6_log - INFO -    370     87.3 MiB     87.3 MiB   @profile

2018-04-30 19:20:02,180 - memory_profile6_log - INFO -    371                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-30 19:20:02,181 - memory_profile6_log - INFO -    372     87.5 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-30 19:20:02,183 - memory_profile6_log - INFO -    373     87.5 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 19:20:02,184 - memory_profile6_log - INFO -    374                             

2018-04-30 19:20:02,184 - memory_profile6_log - INFO -    375                                 # ~~~ Begin collecting data ~~~

2018-04-30 19:20:02,187 - memory_profile6_log - INFO -    376     87.5 MiB      0.0 MiB       t0 = time.time()

2018-04-30 19:20:02,188 - memory_profile6_log - INFO -    377    444.3 MiB    356.8 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-30 19:20:02,190 - memory_profile6_log - INFO -    378    444.3 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-30 19:20:02,190 - memory_profile6_log - INFO -    379                                     logger.info("Training cannot be empty..")

2018-04-30 19:20:02,193 - memory_profile6_log - INFO -    380                                     return False

2018-04-30 19:20:02,194 - memory_profile6_log - INFO -    381    459.2 MiB     15.0 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-30 19:20:02,194 - memory_profile6_log - INFO -    382                                 # print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-30 19:20:02,194 - memory_profile6_log - INFO -    383    459.2 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-30 19:20:02,196 - memory_profile6_log - INFO -    384                             

2018-04-30 19:20:02,198 - memory_profile6_log - INFO -    385    465.3 MiB      6.1 MiB       big_frame = pd.concat(datalist)

2018-04-30 19:20:02,198 - memory_profile6_log - INFO -    386    465.3 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-30 19:20:02,200 - memory_profile6_log - INFO -    387    459.5 MiB     -5.8 MiB       del datalist

2018-04-30 19:20:02,200 - memory_profile6_log - INFO -    388                             

2018-04-30 19:20:02,200 - memory_profile6_log - INFO -    389    459.5 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 19:20:02,201 - memory_profile6_log - INFO -    390                             

2018-04-30 19:20:02,203 - memory_profile6_log - INFO -    391                                 # ~ get current news interest ~

2018-04-30 19:20:02,203 - memory_profile6_log - INFO -    392    459.5 MiB      0.0 MiB       if not cd:

2018-04-30 19:20:02,206 - memory_profile6_log - INFO -    393    459.5 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-04-30 19:20:02,207 - memory_profile6_log - INFO -    394    948.6 MiB    489.1 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-30 19:20:02,207 - memory_profile6_log - INFO -    395    948.6 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-30 19:20:02,210 - memory_profile6_log - INFO -    396                                 else:

2018-04-30 19:20:02,210 - memory_profile6_log - INFO -    397                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-30 19:20:02,213 - memory_profile6_log - INFO -    398                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-30 19:20:02,213 - memory_profile6_log - INFO -    399                             

2018-04-30 19:20:02,213 - memory_profile6_log - INFO -    400                                     # safe handling of query parameter

2018-04-30 19:20:02,214 - memory_profile6_log - INFO -    401                                     query_params = [

2018-04-30 19:20:02,216 - memory_profile6_log - INFO -    402                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-30 19:20:02,216 - memory_profile6_log - INFO -    403                                     ]

2018-04-30 19:20:02,216 - memory_profile6_log - INFO -    404                             

2018-04-30 19:20:02,217 - memory_profile6_log - INFO -    405                                     job_config.query_parameters = query_params

2018-04-30 19:20:02,217 - memory_profile6_log - INFO -    406                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-30 19:20:02,217 - memory_profile6_log - INFO -    407                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-30 19:20:02,219 - memory_profile6_log - INFO -    408                             

2018-04-30 19:20:02,219 - memory_profile6_log - INFO -    409    963.3 MiB     14.7 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-30 19:20:02,219 - memory_profile6_log - INFO -    410    963.3 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-30 19:20:02,220 - memory_profile6_log - INFO -    411    963.4 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-30 19:20:02,223 - memory_profile6_log - INFO -    412    963.4 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-30 19:20:02,224 - memory_profile6_log - INFO -    413    963.4 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-30 19:20:02,226 - memory_profile6_log - INFO -    414    963.4 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-30 19:20:02,226 - memory_profile6_log - INFO -    415                             

2018-04-30 19:20:02,226 - memory_profile6_log - INFO -    416    963.4 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-30 19:20:02,226 - memory_profile6_log - INFO - 


2018-04-30 19:20:02,230 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-30 19:20:02,371 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-30 19:20:02,372 - memory_profile6_log - INFO - transform on: 1929359 total current data(D(t))
2018-04-30 19:20:02,374 - memory_profile6_log - INFO - apply on: 253995 total history...)
2018-04-30 19:20:03,342 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-30 19:20:03,344 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-30 19:20:07,155 - memory_profile6_log - INFO - Len of fitted_models on main class: 253995
2018-04-30 19:20:07,157 - memory_profile6_log - INFO - 

2018-04-30 19:20:07,157 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-30 19:20:07,158 - memory_profile6_log - INFO - 

2018-04-30 19:20:07,160 - memory_profile6_log - INFO - fitted_model_hist:

2018-04-30 19:20:07,161 - memory_profile6_log - INFO - 

2018-04-30 19:20:07,341 - memory_profile6_log - INFO -      pt_posterior_x_Nt           topic_id                                            user_id  sigma_Nt
23          294.636364           22551890  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
100          15.991521           22553543  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
293          83.719466           22601470  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
185         430.739355           22615029  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
9           516.753870           27313197  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
291         860.368557           27313228  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
23           84.916311           27431099  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
4           483.801449           27431145  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
259         426.610863           27433171  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
225        2528.962121           27434611  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
4           157.761342           36060446  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
183         187.225463           39062368  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
174          94.876510           39301645  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
14          209.556183           40281010  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
270         445.097333           40410447  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
145         192.516148           40710353  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
322         984.728614           43444400  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
23          621.644320           45900667  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
75          135.178376           47085613  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
34         7418.288889           47134121  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
26         1589.633333           51484995  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
233        1470.585903          403246831  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
205        2112.803797         1014424906  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
74          940.346479         1019508535  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
31         2073.434783         1036923500  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
48          951.062678         1046474724  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
51          858.156812         1060869149  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
274        1346.060484         1060896301  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
337        1420.523404         1068590702  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
184         748.482063         1164316760  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
320         165.998508         1443761312  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
302         724.914224  27431110790312055  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
223        5057.924242  27431110790312749  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
182         702.785263  27431110790313167  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
27         1094.501639  27431110790313263  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
22          738.546460  27431110790314020  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
20          655.840864  27431110790314045  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
34          194.422248  27431110790314052  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
2018-04-30 19:20:07,342 - memory_profile6_log - INFO - 

2018-04-30 19:20:07,401 - memory_profile6_log - INFO - len of fitted models after concat: 507990
2018-04-30 19:20:07,403 - memory_profile6_log - INFO - 

2018-04-30 19:20:07,404 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-30 19:20:07,405 - memory_profile6_log - INFO - 

2018-04-30 19:20:07,815 - memory_profile6_log - INFO - fitted_models after cobine:

2018-04-30 19:20:07,816 - memory_profile6_log - INFO - 

2018-04-30 19:20:08,003 - memory_profile6_log - INFO -      pt_posterior_x_Nt           topic_id                                            user_id  sigma_Nt
23          294.636364           22551890  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
100          15.991521           22553543  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
293          83.719466           22601470  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
185         430.739355           22615029  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
9           516.753870           27313197  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
291         860.368557           27313228  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
23           84.916311           27431099  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
4           483.801449           27431145  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
259         426.610863           27433171  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
225        2528.962121           27434611  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
4           157.761342           36060446  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
183         187.225463           39062368  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
174          94.876510           39301645  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
14          209.556183           40281010  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
270         445.097333           40410447  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
145         192.516148           40710353  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
322         984.728614           43444400  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
23          621.644320           45900667  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
75          135.178376           47085613  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
34         7418.288889           47134121  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
26         1589.633333           51484995  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
233        1470.585903          403246831  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
205        2112.803797         1014424906  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
74          940.346479         1019508535  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
31         2073.434783         1036923500  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
48          951.062678         1046474724  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
51          858.156812         1060869149  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
274        1346.060484         1060896301  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
337        1420.523404         1068590702  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
184         748.482063         1164316760  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
320         165.998508         1443761312  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
302         724.914224  27431110790312055  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
223        5057.924242  27431110790312749  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
182         702.785263  27431110790313167  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
27         1094.501639  27431110790313263  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
22          738.546460  27431110790314020  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
20          655.840864  27431110790314045  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
34          194.422248  27431110790314052  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
2018-04-30 19:20:08,005 - memory_profile6_log - INFO - 

2018-04-30 19:20:08,006 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 253995
2018-04-30 19:20:08,006 - memory_profile6_log - INFO - 

2018-04-30 19:20:49,894 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-30 19:20:50,094 - memory_profile6_log - INFO -                                                   user_id           topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt  p0_posterior  is_general  rank
137453  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22601470         167.438933             177.438933   0.064560        52      0.184765        True   1.0
137457  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27431099         169.832621             179.832621   0.051877        52      0.150471        True   2.0
137452  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22553543          31.983042              41.983042   0.131487        52      0.089036        True   3.0
137451  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22551890         589.272727             599.272727   0.001499        52      0.014489        True   4.0
137454  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22615029         861.478710             871.478710   0.000170        52      0.002386        True   5.0
137477  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           47085613         270.356752             280.356752   0.055402        52      0.250521       False   1.0
137458  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790312055        1449.828447            1459.828447   0.006265        52      0.147514       False   2.0
137468  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           36060446         315.522684             325.522684   0.026573        52      0.139519       False   3.0
137455  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27313197        1033.507740            1043.507740   0.005880        52      0.098970       False   4.0
137467  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27434611        5057.924242            5067.924242   0.000708        52      0.057864       False   5.0
137470  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           39301645         189.753020             199.753020   0.014040        52      0.045233       False   6.0
137478  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           47134121       14836.577778           14846.577778   0.000134        52      0.032065       False   7.0
137466  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27433171         853.221725             863.221725   0.001427        52      0.019863       False   8.0
137473  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           40410447         890.194667             900.194667   0.001033        52      0.014997       False   9.0
137474  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           40710353         385.032295             395.032295   0.001418        52      0.009037       False  10.0
137450  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1443761312         331.997016             341.997016   0.001368        52      0.007544       False  11.0
137459  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790312749       10115.848485           10125.848485   0.000041        52      0.006679       False  12.0
137476  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           45900667        1243.288641            1253.288641   0.000249        52      0.005041       False  13.0
137456  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27313228        1720.737113            1730.737113   0.000106        52      0.002960       False  14.0
137465  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27431145         967.602899             977.602899   0.000121        52      0.001900       False  15.0
2018-04-30 19:20:50,101 - memory_profile6_log - INFO - 

2018-04-30 19:20:50,102 - memory_profile6_log - INFO - Len of model_transform: 236444
2018-04-30 19:20:50,104 - memory_profile6_log - INFO - Len of df_dt: 1929359
2018-04-30 19:20:50,105 - memory_profile6_log - INFO - Total train time: 47.557s
2018-04-30 19:20:50,105 - memory_profile6_log - INFO - memory left before cleaning: 74.200 percent memory...
2018-04-30 19:20:50,107 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-30 19:20:50,108 - memory_profile6_log - INFO - deleting df_dut...
2018-04-30 19:20:50,109 - memory_profile6_log - INFO - deleting df_dt...
2018-04-30 19:20:50,111 - memory_profile6_log - INFO - deleting df_input...
2018-04-30 19:20:50,190 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-30 19:20:50,191 - memory_profile6_log - INFO - deleting df_current...
2018-04-30 19:20:50,194 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-30 19:20:50,206 - memory_profile6_log - INFO - deleting model_fit...
2018-04-30 19:20:50,209 - memory_profile6_log - INFO - deleting result...
2018-04-30 19:20:50,226 - memory_profile6_log - INFO - deleting model_transform...
2018-04-30 19:20:50,230 - memory_profile6_log - INFO - memory left after cleaning: 73.600 percent memory...
2018-04-30 19:20:50,230 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-30 19:20:50,233 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-30 19:20:50,236 - memory_profile6_log - INFO - Saving main Transform model to Google DataStore...
2018-04-30 19:20:50,236 - memory_profile6_log - INFO - Saving total data: 236444
2018-04-30 19:20:50,436 - memory_profile6_log - INFO - Saving fitted_models as history to Google DataStore...
2018-04-30 19:20:50,499 - memory_profile6_log - INFO - Saving total data: 253995
2018-04-30 19:20:50,525 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 10
2018-04-30 19:20:50,539 - memory_profile6_log - INFO - processing batch-0
2018-04-30 19:20:50,752 - memory_profile6_log - INFO - processing batch-1
2018-04-30 19:20:50,959 - memory_profile6_log - INFO - processing batch-2
2018-04-30 19:20:51,190 - memory_profile6_log - INFO - processing batch-3
2018-04-30 19:20:51,418 - memory_profile6_log - INFO - processing batch-4
2018-04-30 19:20:51,605 - memory_profile6_log - INFO - processing batch-5
2018-04-30 19:20:51,832 - memory_profile6_log - INFO - processing batch-6
2018-04-30 19:20:52,030 - memory_profile6_log - INFO - processing batch-7
2018-04-30 19:20:52,257 - memory_profile6_log - INFO - processing batch-8
2018-04-30 19:20:52,493 - memory_profile6_log - INFO - processing batch-9
2018-04-30 19:20:52,707 - memory_profile6_log - INFO - deleting X_split...
2018-04-30 19:20:52,710 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-04-30 19:20:52,710 - memory_profile6_log - INFO - deleting BR...
2018-04-30 19:20:52,724 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 19:20:52,724 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 19:20:52,726 - memory_profile6_log - INFO - ================================================

2018-04-30 19:20:52,726 - memory_profile6_log - INFO -    113    930.4 MiB    930.4 MiB   @profile

2018-04-30 19:20:52,726 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-30 19:20:52,727 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-30 19:20:52,727 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-30 19:20:52,729 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-30 19:20:52,730 - memory_profile6_log - INFO -    118                                      saveto="datastore", fitted_models_hist=None):

2018-04-30 19:20:52,730 - memory_profile6_log - INFO -    119                                 """

2018-04-30 19:20:52,730 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-30 19:20:52,732 - memory_profile6_log - INFO -    121                                 """

2018-04-30 19:20:52,733 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-30 19:20:52,733 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-30 19:20:52,734 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-30 19:20:52,734 - memory_profile6_log - INFO -    125    930.4 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-30 19:20:52,734 - memory_profile6_log - INFO -    126    938.6 MiB      8.1 MiB       df_dut = df_input.copy(deep=True)

2018-04-30 19:20:52,736 - memory_profile6_log - INFO -    127    938.6 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 19:20:52,736 - memory_profile6_log - INFO -    128                             

2018-04-30 19:20:52,742 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-30 19:20:52,742 - memory_profile6_log - INFO -    130    999.3 MiB     60.7 MiB       df_dt = df_current.copy(deep=True)

2018-04-30 19:20:52,743 - memory_profile6_log - INFO -    131    999.3 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 19:20:52,743 - memory_profile6_log - INFO -    132                             

2018-04-30 19:20:52,744 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-30 19:20:52,744 - memory_profile6_log - INFO -    134    999.3 MiB      0.0 MiB       t0 = time.time()

2018-04-30 19:20:52,746 - memory_profile6_log - INFO -    135    999.3 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-30 19:20:52,746 - memory_profile6_log - INFO -    136    999.3 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-30 19:20:52,746 - memory_profile6_log - INFO -    137    999.3 MiB      0.0 MiB       logger.info("apply on: %d total history...)", len(df_hist))

2018-04-30 19:20:52,747 - memory_profile6_log - INFO -    138                             

2018-04-30 19:20:52,747 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-30 19:20:52,749 - memory_profile6_log - INFO -    140    999.3 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-30 19:20:52,750 - memory_profile6_log - INFO -    141                             

2018-04-30 19:20:52,750 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-30 19:20:52,750 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-30 19:20:52,750 - memory_profile6_log - INFO -    144   1002.2 MiB      2.9 MiB       NB = BR.processX(df_dut)

2018-04-30 19:20:52,750 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-30 19:20:52,752 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-30 19:20:52,752 - memory_profile6_log - INFO -    147   1012.2 MiB      9.9 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-30 19:20:52,756 - memory_profile6_log - INFO -    148                                 """

2018-04-30 19:20:52,756 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-30 19:20:52,756 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-30 19:20:52,757 - memory_profile6_log - INFO -    151                                 """

2018-04-30 19:20:52,757 - memory_profile6_log - INFO -    152   1012.2 MiB      0.0 MiB       fitby_sigmant = False

2018-04-30 19:20:52,759 - memory_profile6_log - INFO -    153   1012.2 MiB      0.0 MiB       uniques_fit_hist = None

2018-04-30 19:20:52,759 - memory_profile6_log - INFO -    154   1012.2 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-30 19:20:52,759 - memory_profile6_log - INFO -    155   1012.2 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-30 19:20:52,759 - memory_profile6_log - INFO -    156   1021.9 MiB      9.7 MiB                            'is_general']]

2018-04-30 19:20:52,760 - memory_profile6_log - INFO -    157                                 # get sigma_Nt from fitted_models_hist

2018-04-30 19:20:52,760 - memory_profile6_log - INFO -    158   1021.9 MiB      0.0 MiB       if fitby_sigmant:

2018-04-30 19:20:52,762 - memory_profile6_log - INFO -    159                                     uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-04-30 19:20:52,762 - memory_profile6_log - INFO -    160                                     logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-04-30 19:20:52,763 - memory_profile6_log - INFO -    161                                     uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-04-30 19:20:52,763 - memory_profile6_log - INFO -    162                                     logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-04-30 19:20:52,763 - memory_profile6_log - INFO -    163                             

2018-04-30 19:20:52,763 - memory_profile6_log - INFO -    164                                 # begin fit

2018-04-30 19:20:52,769 - memory_profile6_log - INFO -    165   1021.9 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-30 19:20:52,769 - memory_profile6_log - INFO -    166   1021.9 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-30 19:20:52,769 - memory_profile6_log - INFO -    167   1047.3 MiB     25.4 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-04-30 19:20:52,770 - memory_profile6_log - INFO -    168   1047.3 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-30 19:20:52,770 - memory_profile6_log - INFO -    169   1047.3 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-30 19:20:52,773 - memory_profile6_log - INFO -    170                                 # print model_fit[['num_x', 'num_x', 'sigma_Nt', 'date_all_click']].loc[model_fit['user_id']=="1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157"].head(5)

2018-04-30 19:20:52,773 - memory_profile6_log - INFO -    171                             

2018-04-30 19:20:52,773 - memory_profile6_log - INFO -    172                                 # ~~ and Transform ~~

2018-04-30 19:20:52,775 - memory_profile6_log - INFO -    173                                 #   handling current news interest == current date

2018-04-30 19:20:52,776 - memory_profile6_log - INFO -    174   1047.3 MiB      0.0 MiB       if df_dt.empty:

2018-04-30 19:20:52,776 - memory_profile6_log - INFO -    175                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-30 19:20:52,776 - memory_profile6_log - INFO -    176                                     return None

2018-04-30 19:20:52,778 - memory_profile6_log - INFO -    177   1053.1 MiB      5.8 MiB       NB = BR.processX(df_dt)

2018-04-30 19:20:52,782 - memory_profile6_log - INFO -    178   1143.8 MiB     90.7 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-30 19:20:52,782 - memory_profile6_log - INFO -    179                             

2018-04-30 19:20:52,782 - memory_profile6_log - INFO -    180   1143.8 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-30 19:20:52,783 - memory_profile6_log - INFO -    181   1209.6 MiB     65.7 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-30 19:20:52,783 - memory_profile6_log - INFO -    182                             

2018-04-30 19:20:52,785 - memory_profile6_log - INFO -    183                                 # print "model_fit dtypes:\n", model_fit.dtypes

2018-04-30 19:20:52,786 - memory_profile6_log - INFO -    184                                 # print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-04-30 19:20:52,786 - memory_profile6_log - INFO -    185   1209.6 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-30 19:20:52,788 - memory_profile6_log - INFO -    186   1209.6 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-30 19:20:52,788 - memory_profile6_log - INFO -    187   1209.6 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-04-30 19:20:52,789 - memory_profile6_log - INFO -    188   1209.6 MiB      0.0 MiB                                                                                           "topic_id", "user_id",

2018-04-30 19:20:52,795 - memory_profile6_log - INFO -    189   1217.3 MiB      7.8 MiB                                                                                           "sigma_Nt"]],

2018-04-30 19:20:52,796 - memory_profile6_log - INFO -    190   1225.6 MiB      8.3 MiB                                                     verbose=False)

2018-04-30 19:20:52,796 - memory_profile6_log - INFO -    191                             

2018-04-30 19:20:52,796 - memory_profile6_log - INFO -    192                                 # ~~~ filter is general and specific topic ~~~

2018-04-30 19:20:52,798 - memory_profile6_log - INFO -    193                                 # the idea is just we need to rerank every topic according

2018-04-30 19:20:52,799 - memory_profile6_log - INFO -    194                                 #    to user_id and and is_general by p0_posterior

2018-04-30 19:20:52,799 - memory_profile6_log - INFO -    195   1225.6 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-30 19:20:52,801 - memory_profile6_log - INFO -    196   1227.5 MiB      1.9 MiB                                     'is_general']].groupby(['topic_id',

2018-04-30 19:20:52,802 - memory_profile6_log - INFO -    197   1225.6 MiB     -1.9 MiB                                                             'is_general']

2018-04-30 19:20:52,805 - memory_profile6_log - INFO -    198                                                                                      ).size().to_frame().reset_index()

2018-04-30 19:20:52,806 - memory_profile6_log - INFO -    199                             

2018-04-30 19:20:52,808 - memory_profile6_log - INFO -    200                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-30 19:20:52,809 - memory_profile6_log - INFO -    201                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-30 19:20:52,809 - memory_profile6_log - INFO -    202   1225.7 MiB      0.1 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-30 19:20:52,811 - memory_profile6_log - INFO -    203                             

2018-04-30 19:20:52,812 - memory_profile6_log - INFO -    204                                 # ~ start by provide rank for each topic type ~

2018-04-30 19:20:52,812 - memory_profile6_log - INFO -    205   1237.0 MiB     11.2 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-30 19:20:52,812 - memory_profile6_log - INFO -    206   1242.8 MiB      5.8 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-30 19:20:52,812 - memory_profile6_log - INFO -    207                             

2018-04-30 19:20:52,813 - memory_profile6_log - INFO -    208                                 # ~ set threshold to filter output

2018-04-30 19:20:52,815 - memory_profile6_log - INFO -    209   1242.8 MiB      0.0 MiB       if threshold > 0:

2018-04-30 19:20:52,815 - memory_profile6_log - INFO -    210   1242.8 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-30 19:20:52,815 - memory_profile6_log - INFO -    211   1242.8 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-30 19:20:52,816 - memory_profile6_log - INFO -    212   1242.5 MiB     -0.3 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-30 19:20:52,816 - memory_profile6_log - INFO -    213                             

2018-04-30 19:20:52,816 - memory_profile6_log - INFO -    214   1242.5 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-30 19:20:52,822 - memory_profile6_log - INFO -    215   1242.6 MiB      0.1 MiB       print model_transform[model_transform['user_id']=='1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157']

2018-04-30 19:20:52,822 - memory_profile6_log - INFO -    216   1242.6 MiB      0.0 MiB       logger.info("Len of model_transform: %d", len(model_transform))

2018-04-30 19:20:52,823 - memory_profile6_log - INFO -    217   1242.6 MiB      0.0 MiB       logger.info("Len of df_dt: %d", len(df_dt))

2018-04-30 19:20:52,825 - memory_profile6_log - INFO -    218   1242.6 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-30 19:20:52,825 - memory_profile6_log - INFO -    219                             

2018-04-30 19:20:52,826 - memory_profile6_log - INFO -    220   1242.6 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-30 19:20:52,826 - memory_profile6_log - INFO -    221                             

2018-04-30 19:20:52,828 - memory_profile6_log - INFO -    222   1242.6 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-30 19:20:52,828 - memory_profile6_log - INFO -    223   1242.6 MiB      0.0 MiB       del df_dut

2018-04-30 19:20:52,828 - memory_profile6_log - INFO -    224   1242.6 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-30 19:20:52,832 - memory_profile6_log - INFO -    225   1242.6 MiB      0.0 MiB       del df_dt

2018-04-30 19:20:52,834 - memory_profile6_log - INFO -    226   1242.6 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-30 19:20:52,835 - memory_profile6_log - INFO -    227   1242.6 MiB      0.0 MiB       del df_input

2018-04-30 19:20:52,835 - memory_profile6_log - INFO -    228   1242.6 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-30 19:20:52,835 - memory_profile6_log - INFO -    229   1167.1 MiB    -75.4 MiB       del df_input_X

2018-04-30 19:20:52,836 - memory_profile6_log - INFO -    230   1167.1 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-30 19:20:52,836 - memory_profile6_log - INFO -    231   1167.1 MiB      0.0 MiB       del df_current

2018-04-30 19:20:52,836 - memory_profile6_log - INFO -    232   1167.1 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-30 19:20:52,838 - memory_profile6_log - INFO -    233   1167.1 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-30 19:20:52,838 - memory_profile6_log - INFO -    234   1167.1 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-30 19:20:52,839 - memory_profile6_log - INFO -    235   1145.8 MiB    -21.3 MiB       del model_fit

2018-04-30 19:20:52,841 - memory_profile6_log - INFO -    236   1145.8 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-30 19:20:52,841 - memory_profile6_log - INFO -    237   1145.8 MiB      0.0 MiB       del result

2018-04-30 19:20:52,842 - memory_profile6_log - INFO -    238   1145.8 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-30 19:20:52,845 - memory_profile6_log - INFO -    239                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 19:20:52,845 - memory_profile6_log - INFO -    240                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 19:20:52,845 - memory_profile6_log - INFO -    241                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 19:20:52,848 - memory_profile6_log - INFO -    242   1145.8 MiB      0.0 MiB       if savetrain:

2018-04-30 19:20:52,849 - memory_profile6_log - INFO -    243   1151.2 MiB      5.4 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-30 19:20:52,849 - memory_profile6_log - INFO -    244   1151.2 MiB      0.0 MiB           del model_transform

2018-04-30 19:20:52,849 - memory_profile6_log - INFO -    245   1151.2 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-30 19:20:52,851 - memory_profile6_log - INFO -    246   1151.2 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-30 19:20:52,851 - memory_profile6_log - INFO -    247                             

2018-04-30 19:20:52,851 - memory_profile6_log - INFO -    248   1151.2 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-30 19:20:52,852 - memory_profile6_log - INFO -    249                                     # ~ Place your code to save the training model here ~

2018-04-30 19:20:52,852 - memory_profile6_log - INFO -    250   1151.2 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-30 19:20:52,852 - memory_profile6_log - INFO -    251   1151.2 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-30 19:20:52,857 - memory_profile6_log - INFO -    252   1151.2 MiB      0.0 MiB               if multproc:

2018-04-30 19:20:52,857 - memory_profile6_log - INFO -    253                                             # ~ save transform models ~

2018-04-30 19:20:52,858 - memory_profile6_log - INFO -    254   1151.2 MiB      0.0 MiB                   logger.info("Saving main Transform model to Google DataStore...")

2018-04-30 19:20:52,858 - memory_profile6_log - INFO -    255   1151.2 MiB      0.0 MiB                   logger.info("Saving total data: %d", len(model_transformsv))

2018-04-30 19:20:52,858 - memory_profile6_log - INFO -    256   1131.3 MiB    -19.9 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-30 19:20:52,858 - memory_profile6_log - INFO -    257                             

2018-04-30 19:20:52,859 - memory_profile6_log - INFO -    258                                             # ~ save fitted models ~

2018-04-30 19:20:52,859 - memory_profile6_log - INFO -    259   1131.3 MiB      0.0 MiB                   logger.info("Saving fitted_models as history to Google DataStore...")

2018-04-30 19:20:52,859 - memory_profile6_log - INFO -    260   1131.5 MiB      0.2 MiB                   save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-30 19:20:52,861 - memory_profile6_log - INFO -    261   1146.3 MiB     14.8 MiB                   fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-30 19:20:52,861 - memory_profile6_log - INFO -    262   1146.3 MiB      0.0 MiB                   logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-04-30 19:20:52,861 - memory_profile6_log - INFO -    263   1157.3 MiB     10.9 MiB                   X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-30 19:20:52,861 - memory_profile6_log - INFO -    264   1157.3 MiB      0.0 MiB                   logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-30 19:20:52,862 - memory_profile6_log - INFO -    265   1158.9 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-30 19:20:52,862 - memory_profile6_log - INFO -    266   1158.9 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-30 19:20:52,864 - memory_profile6_log - INFO -    267   1158.9 MiB      1.6 MiB                       mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-30 19:20:52,865 - memory_profile6_log - INFO -    268                             

2018-04-30 19:20:52,865 - memory_profile6_log - INFO -    269   1158.9 MiB      0.0 MiB                   del X_split

2018-04-30 19:20:52,868 - memory_profile6_log - INFO -    270   1158.9 MiB      0.0 MiB                   logger.info("deleting X_split...")

2018-04-30 19:20:52,868 - memory_profile6_log - INFO -    271   1158.9 MiB      0.0 MiB                   del save_sigma_nt

2018-04-30 19:20:52,869 - memory_profile6_log - INFO -    272   1158.9 MiB      0.0 MiB                   logger.info("deleting save_sigma_nt...")

2018-04-30 19:20:52,869 - memory_profile6_log - INFO -    273                             

2018-04-30 19:20:52,872 - memory_profile6_log - INFO -    274   1158.9 MiB      0.0 MiB                   del BR

2018-04-30 19:20:52,874 - memory_profile6_log - INFO -    275   1158.9 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-30 19:20:52,875 - memory_profile6_log - INFO -    276                             

2018-04-30 19:20:52,877 - memory_profile6_log - INFO -    277                                     elif str(saveto).lower() == "elastic":

2018-04-30 19:20:52,877 - memory_profile6_log - INFO -    278                                         logger.info("Using ElasticSearch as storage...")

2018-04-30 19:20:52,877 - memory_profile6_log - INFO -    279                                         logging.info("Saving main Transform model to Elasticsearch...")

2018-04-30 19:20:52,880 - memory_profile6_log - INFO -    280                                         mh.saveElasticS(model_transformsv)

2018-04-30 19:20:52,881 - memory_profile6_log - INFO -    281                             

2018-04-30 19:20:52,881 - memory_profile6_log - INFO -    282                                     # need save sigma_nt for daily train

2018-04-30 19:20:52,881 - memory_profile6_log - INFO -    283                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-30 19:20:52,881 - memory_profile6_log - INFO -    284                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-30 19:20:52,881 - memory_profile6_log - INFO -    285   1158.9 MiB      0.0 MiB           if start_date and end_date:

2018-04-30 19:20:52,882 - memory_profile6_log - INFO -    286                                         if not fitby_sigmant:

2018-04-30 19:20:52,882 - memory_profile6_log - INFO -    287                                             logging.info("Saving sigma Nt...")

2018-04-30 19:20:52,884 - memory_profile6_log - INFO -    288                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-30 19:20:52,884 - memory_profile6_log - INFO -    289                                             save_sigma_nt['start_date'] = start_date

2018-04-30 19:20:52,884 - memory_profile6_log - INFO -    290                                             save_sigma_nt['end_date'] = end_date

2018-04-30 19:20:52,884 - memory_profile6_log - INFO -    291                                             print save_sigma_nt.head(5)

2018-04-30 19:20:52,884 - memory_profile6_log - INFO -    292                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-30 19:20:52,884 - memory_profile6_log - INFO -    293                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-30 19:20:52,885 - memory_profile6_log - INFO -    294   1158.9 MiB      0.0 MiB       return

2018-04-30 19:20:52,885 - memory_profile6_log - INFO - 


2018-04-30 19:20:52,885 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-30 19:42:35,174 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-30 19:42:35,177 - memory_profile6_log - INFO - date_generated: 
2018-04-30 19:42:35,180 - memory_profile6_log - INFO -  
2018-04-30 19:42:35,180 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-30 19:42:35,180 - memory_profile6_log - INFO - 

2018-04-30 19:42:35,180 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-30 19:42:35,180 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-30 19:42:35,180 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-30 19:42:35,385 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-30 19:42:35,394 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-30 19:43:42,936 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-30 19:43:42,950 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-30 19:43:42,996 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-30 19:43:43,005 - memory_profile6_log - INFO - Appending history data...
2018-04-30 19:43:43,006 - memory_profile6_log - INFO - processing batch-0
2018-04-30 19:43:43,009 - memory_profile6_log - INFO - creating list history data...
2018-04-30 19:43:43,095 - memory_profile6_log - INFO - call history data...
2018-04-30 19:44:25,898 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 19:44:27,092 - memory_profile6_log - INFO - processing batch-1
2018-04-30 19:44:27,101 - memory_profile6_log - INFO - creating list history data...
2018-04-30 19:44:27,154 - memory_profile6_log - INFO - call history data...
2018-04-30 19:45:11,857 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 19:45:13,105 - memory_profile6_log - INFO - processing batch-2
2018-04-30 19:45:13,109 - memory_profile6_log - INFO - creating list history data...
2018-04-30 19:45:13,144 - memory_profile6_log - INFO - call history data...
2018-04-30 19:45:54,338 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 19:45:55,585 - memory_profile6_log - INFO - processing batch-3
2018-04-30 19:45:55,591 - memory_profile6_log - INFO - creating list history data...
2018-04-30 19:45:55,638 - memory_profile6_log - INFO - call history data...
2018-04-30 19:46:37,568 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 19:46:38,809 - memory_profile6_log - INFO - processing batch-4
2018-04-30 19:46:38,818 - memory_profile6_log - INFO - creating list history data...
2018-04-30 19:46:38,864 - memory_profile6_log - INFO - call history data...
2018-04-30 19:47:21,612 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 19:47:22,910 - memory_profile6_log - INFO - Appending training data...
2018-04-30 19:47:22,914 - memory_profile6_log - INFO - len datalist: 1
2018-04-30 19:47:22,915 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-30 19:47:22,921 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 19:47:22,931 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 19:47:22,934 - memory_profile6_log - INFO - ================================================

2018-04-30 19:47:22,934 - memory_profile6_log - INFO -    320     87.4 MiB     87.4 MiB   @profile

2018-04-30 19:47:22,937 - memory_profile6_log - INFO -    321                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-30 19:47:22,938 - memory_profile6_log - INFO -    322     87.4 MiB      0.0 MiB       bq_client = client

2018-04-30 19:47:22,940 - memory_profile6_log - INFO -    323     87.4 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 19:47:22,944 - memory_profile6_log - INFO -    324                             

2018-04-30 19:47:22,946 - memory_profile6_log - INFO -    325     87.4 MiB      0.0 MiB       datalist = []

2018-04-30 19:47:22,947 - memory_profile6_log - INFO -    326     87.4 MiB      0.0 MiB       datalist_hist = []

2018-04-30 19:47:22,948 - memory_profile6_log - INFO -    327                             

2018-04-30 19:47:22,950 - memory_profile6_log - INFO -    328     87.4 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-30 19:47:22,951 - memory_profile6_log - INFO -    329    446.1 MiB      0.0 MiB       for ndate in date_generated:

2018-04-30 19:47:22,953 - memory_profile6_log - INFO -    330    340.3 MiB    252.9 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-30 19:47:22,956 - memory_profile6_log - INFO -    331    340.3 MiB      0.0 MiB           if tframe is not None:

2018-04-30 19:47:22,957 - memory_profile6_log - INFO -    332    340.3 MiB      0.0 MiB               if not tframe.empty:

2018-04-30 19:47:22,957 - memory_profile6_log - INFO -    333    347.8 MiB      7.5 MiB                   X_split = np.array_split(tframe, 5)

2018-04-30 19:47:22,959 - memory_profile6_log - INFO -    334    347.8 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-30 19:47:22,960 - memory_profile6_log - INFO -    335    347.8 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-30 19:47:22,960 - memory_profile6_log - INFO -    336    446.1 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-30 19:47:22,961 - memory_profile6_log - INFO -    337                                                 # ~ loading history

2018-04-30 19:47:22,963 - memory_profile6_log - INFO -    338                                                 """

2018-04-30 19:47:22,963 - memory_profile6_log - INFO -    339                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-30 19:47:22,963 - memory_profile6_log - INFO -    340                                                 """

2018-04-30 19:47:22,964 - memory_profile6_log - INFO -    341    429.3 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-30 19:47:22,966 - memory_profile6_log - INFO -    342                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-30 19:47:22,967 - memory_profile6_log - INFO -    343    429.3 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-30 19:47:22,967 - memory_profile6_log - INFO -    344                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-30 19:47:22,967 - memory_profile6_log - INFO -    345    429.7 MiB      4.1 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-30 19:47:22,969 - memory_profile6_log - INFO -    346                             

2018-04-30 19:47:22,970 - memory_profile6_log - INFO -    347    429.7 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-30 19:47:22,970 - memory_profile6_log - INFO -    348    475.3 MiB    256.3 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-30 19:47:22,971 - memory_profile6_log - INFO -    349                             

2018-04-30 19:47:22,973 - memory_profile6_log - INFO -    350                                                 # me = os.getpid()

2018-04-30 19:47:22,977 - memory_profile6_log - INFO -    351                                                 # kill_proc_tree(me)

2018-04-30 19:47:22,979 - memory_profile6_log - INFO -    352                             

2018-04-30 19:47:22,980 - memory_profile6_log - INFO -    353    475.3 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-30 19:47:22,980 - memory_profile6_log - INFO -    354    476.4 MiB     -1.9 MiB                       for m in h_frame:

2018-04-30 19:47:22,982 - memory_profile6_log - INFO -    355    476.4 MiB     -1.9 MiB                           if m is not None:

2018-04-30 19:47:22,983 - memory_profile6_log - INFO -    356    476.4 MiB     -1.9 MiB                               if len(m) > 0:

2018-04-30 19:47:22,983 - memory_profile6_log - INFO -    357    476.4 MiB      2.1 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-30 19:47:22,984 - memory_profile6_log - INFO -    358    446.1 MiB   -166.1 MiB                       del h_frame

2018-04-30 19:47:22,986 - memory_profile6_log - INFO -    359    446.1 MiB      0.0 MiB                       del lhistory

2018-04-30 19:47:22,986 - memory_profile6_log - INFO -    360                             

2018-04-30 19:47:22,987 - memory_profile6_log - INFO -    361    446.1 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-30 19:47:22,989 - memory_profile6_log - INFO -    362    446.1 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-30 19:47:22,992 - memory_profile6_log - INFO -    363                                     else: 

2018-04-30 19:47:22,993 - memory_profile6_log - INFO -    364                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-30 19:47:22,994 - memory_profile6_log - INFO -    365    446.1 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-30 19:47:22,996 - memory_profile6_log - INFO -    366    446.1 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-30 19:47:22,996 - memory_profile6_log - INFO -    367                             

2018-04-30 19:47:22,997 - memory_profile6_log - INFO -    368    446.1 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-30 19:47:22,999 - memory_profile6_log - INFO - 


2018-04-30 19:47:24,153 - memory_profile6_log - INFO - size of big_frame_hist: 69.87 MB
2018-04-30 19:47:24,234 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-30 19:47:24,237 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-04-30 19:53:18,177 - memory_profile6_log - INFO - size of df: 474.10 MB
2018-04-30 19:53:18,188 - memory_profile6_log - INFO - getting total: 1936921 training data(current date interest)
2018-04-30 19:53:18,742 - memory_profile6_log - INFO - size of current_frame: 488.88 MB
2018-04-30 19:53:18,749 - memory_profile6_log - INFO - loading time of: 2190916 total genuine-current interest data ~ take 643.410s
2018-04-30 19:53:18,769 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 19:53:18,769 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 19:53:18,770 - memory_profile6_log - INFO - ================================================

2018-04-30 19:53:18,772 - memory_profile6_log - INFO -    370     87.2 MiB     87.2 MiB   @profile

2018-04-30 19:53:18,773 - memory_profile6_log - INFO -    371                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-30 19:53:18,773 - memory_profile6_log - INFO -    372     87.4 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-30 19:53:18,773 - memory_profile6_log - INFO -    373     87.4 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 19:53:18,775 - memory_profile6_log - INFO -    374                             

2018-04-30 19:53:18,776 - memory_profile6_log - INFO -    375                                 # ~~~ Begin collecting data ~~~

2018-04-30 19:53:18,776 - memory_profile6_log - INFO -    376     87.4 MiB      0.0 MiB       t0 = time.time()

2018-04-30 19:53:18,776 - memory_profile6_log - INFO -    377    440.1 MiB    352.7 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-30 19:53:18,778 - memory_profile6_log - INFO -    378    440.1 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-30 19:53:18,778 - memory_profile6_log - INFO -    379                                     logger.info("Training cannot be empty..")

2018-04-30 19:53:18,779 - memory_profile6_log - INFO -    380                                     return False

2018-04-30 19:53:18,779 - memory_profile6_log - INFO -    381    456.2 MiB     16.1 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-30 19:53:18,779 - memory_profile6_log - INFO -    382                                 # print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-30 19:53:18,779 - memory_profile6_log - INFO -    383    456.2 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-30 19:53:18,780 - memory_profile6_log - INFO -    384                             

2018-04-30 19:53:18,780 - memory_profile6_log - INFO -    385    462.4 MiB      6.2 MiB       big_frame = pd.concat(datalist)

2018-04-30 19:53:18,782 - memory_profile6_log - INFO -    386    462.4 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-30 19:53:18,782 - memory_profile6_log - INFO -    387    456.5 MiB     -5.8 MiB       del datalist

2018-04-30 19:53:18,782 - memory_profile6_log - INFO -    388                             

2018-04-30 19:53:18,786 - memory_profile6_log - INFO -    389    456.5 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 19:53:18,788 - memory_profile6_log - INFO -    390                             

2018-04-30 19:53:18,789 - memory_profile6_log - INFO -    391                                 # ~ get current news interest ~

2018-04-30 19:53:18,789 - memory_profile6_log - INFO -    392    456.5 MiB      0.0 MiB       if not cd:

2018-04-30 19:53:18,789 - memory_profile6_log - INFO -    393    456.5 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-04-30 19:53:18,790 - memory_profile6_log - INFO -    394    924.8 MiB    468.2 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-30 19:53:18,792 - memory_profile6_log - INFO -    395    924.8 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-30 19:53:18,792 - memory_profile6_log - INFO -    396                                 else:

2018-04-30 19:53:18,792 - memory_profile6_log - INFO -    397                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-30 19:53:18,793 - memory_profile6_log - INFO -    398                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-30 19:53:18,793 - memory_profile6_log - INFO -    399                             

2018-04-30 19:53:18,793 - memory_profile6_log - INFO -    400                                     # safe handling of query parameter

2018-04-30 19:53:18,795 - memory_profile6_log - INFO -    401                                     query_params = [

2018-04-30 19:53:18,795 - memory_profile6_log - INFO -    402                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-30 19:53:18,796 - memory_profile6_log - INFO -    403                                     ]

2018-04-30 19:53:18,796 - memory_profile6_log - INFO -    404                             

2018-04-30 19:53:18,796 - memory_profile6_log - INFO -    405                                     job_config.query_parameters = query_params

2018-04-30 19:53:18,796 - memory_profile6_log - INFO -    406                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-30 19:53:18,796 - memory_profile6_log - INFO -    407                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-30 19:53:18,798 - memory_profile6_log - INFO -    408                             

2018-04-30 19:53:18,798 - memory_profile6_log - INFO -    409    939.6 MiB     14.8 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-30 19:53:18,802 - memory_profile6_log - INFO -    410    939.6 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-30 19:53:18,802 - memory_profile6_log - INFO -    411    939.6 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-30 19:53:18,805 - memory_profile6_log - INFO -    412    939.6 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-30 19:53:18,805 - memory_profile6_log - INFO -    413    939.6 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-30 19:53:18,805 - memory_profile6_log - INFO -    414    939.6 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-30 19:53:18,805 - memory_profile6_log - INFO -    415                             

2018-04-30 19:53:18,806 - memory_profile6_log - INFO -    416    939.6 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-30 19:53:18,806 - memory_profile6_log - INFO - 


2018-04-30 19:53:18,811 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-30 19:53:18,946 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-30 19:53:18,950 - memory_profile6_log - INFO - transform on: 1936921 total current data(D(t))
2018-04-30 19:53:18,951 - memory_profile6_log - INFO - apply on: 253995 total history...)
2018-04-30 19:53:19,829 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-30 19:53:19,832 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-30 19:53:23,391 - memory_profile6_log - INFO - Len of fitted_models on main class: 253995
2018-04-30 19:53:23,394 - memory_profile6_log - INFO - 

2018-04-30 19:53:23,395 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-30 19:53:23,397 - memory_profile6_log - INFO - 

2018-04-30 19:53:23,398 - memory_profile6_log - INFO - fitted_model_hist:

2018-04-30 19:53:23,398 - memory_profile6_log - INFO - 

2018-04-30 19:53:23,546 - memory_profile6_log - INFO -      pt_posterior_x_Nt           topic_id                                            user_id  sigma_Nt
25          294.636364           22551890  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
117          15.991521           22553543  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
210          83.719466           22601470  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
252         430.739355           22615029  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
121         516.753870           27313197  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
86          860.368557           27313228  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
252          84.916311           27431099  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
148         483.801449           27431145  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
254         426.610863           27433171  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
147        2528.962121           27434611  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
48          157.761342           36060446  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
271         187.225463           39062368  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
250          94.876510           39301645  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
121         209.556183           40281010  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
172         445.097333           40410447  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
158         192.516148           40710353  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
314         984.728614           43444400  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
42          621.644320           45900667  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
9           135.178376           47085613  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
195        7418.288889           47134121  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
209        1589.633333           51484995  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
218        1470.585903          403246831  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
82         2112.803797         1014424906  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
244         940.346479         1019508535  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
125        2073.434783         1036923500  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
183         951.062678         1046474724  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
275         858.156812         1060869149  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
99         1346.060484         1060896301  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
334        1420.523404         1068590702  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
299         748.482063         1164316760  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
319         165.998508         1443761312  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
336         724.914224  27431110790312055  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
297        5057.924242  27431110790312749  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
42          702.785263  27431110790313167  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
7          1094.501639  27431110790313263  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
232         738.546460  27431110790314020  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
209         655.840864  27431110790314045  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
19          194.422248  27431110790314052  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
2018-04-30 19:53:23,562 - memory_profile6_log - INFO - 

2018-04-30 19:53:23,898 - memory_profile6_log - INFO - len of fitted models after concat: 28268
2018-04-30 19:53:23,900 - memory_profile6_log - INFO - 

2018-04-30 19:53:23,901 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-30 19:53:23,903 - memory_profile6_log - INFO - 

2018-04-30 20:00:25,019 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-30 20:00:25,022 - memory_profile6_log - INFO - date_generated: 
2018-04-30 20:00:25,022 - memory_profile6_log - INFO -  
2018-04-30 20:00:25,022 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-30 20:00:25,022 - memory_profile6_log - INFO - 

2018-04-30 20:00:25,022 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-30 20:00:25,023 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-30 20:00:25,023 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-30 20:00:25,154 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-30 20:00:25,171 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-30 20:01:34,328 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-30 20:01:34,342 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-30 20:01:34,368 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-30 20:01:34,384 - memory_profile6_log - INFO - Appending history data...
2018-04-30 20:01:34,384 - memory_profile6_log - INFO - processing batch-0
2018-04-30 20:01:34,385 - memory_profile6_log - INFO - creating list history data...
2018-04-30 20:01:34,464 - memory_profile6_log - INFO - call history data...
2018-04-30 20:02:16,911 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 20:02:18,193 - memory_profile6_log - INFO - processing batch-1
2018-04-30 20:02:18,194 - memory_profile6_log - INFO - creating list history data...
2018-04-30 20:02:18,243 - memory_profile6_log - INFO - call history data...
2018-04-30 20:02:59,786 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 20:03:01,066 - memory_profile6_log - INFO - processing batch-2
2018-04-30 20:03:01,078 - memory_profile6_log - INFO - creating list history data...
2018-04-30 20:03:01,128 - memory_profile6_log - INFO - call history data...
2018-04-30 20:03:41,805 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 20:03:43,170 - memory_profile6_log - INFO - processing batch-3
2018-04-30 20:03:43,171 - memory_profile6_log - INFO - creating list history data...
2018-04-30 20:03:43,223 - memory_profile6_log - INFO - call history data...
2018-04-30 20:04:24,941 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 20:04:26,401 - memory_profile6_log - INFO - processing batch-4
2018-04-30 20:04:26,404 - memory_profile6_log - INFO - creating list history data...
2018-04-30 20:04:26,463 - memory_profile6_log - INFO - call history data...
2018-04-30 20:05:08,282 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 20:05:09,480 - memory_profile6_log - INFO - Appending training data...
2018-04-30 20:05:09,496 - memory_profile6_log - INFO - len datalist: 1
2018-04-30 20:05:09,499 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-30 20:05:09,500 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 20:05:09,507 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 20:05:09,509 - memory_profile6_log - INFO - ================================================

2018-04-30 20:05:09,509 - memory_profile6_log - INFO -    320     87.4 MiB     87.4 MiB   @profile

2018-04-30 20:05:09,513 - memory_profile6_log - INFO -    321                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-30 20:05:09,515 - memory_profile6_log - INFO -    322     87.4 MiB      0.0 MiB       bq_client = client

2018-04-30 20:05:09,516 - memory_profile6_log - INFO -    323     87.4 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 20:05:09,517 - memory_profile6_log - INFO -    324                             

2018-04-30 20:05:09,519 - memory_profile6_log - INFO -    325     87.4 MiB      0.0 MiB       datalist = []

2018-04-30 20:05:09,519 - memory_profile6_log - INFO -    326     87.4 MiB      0.0 MiB       datalist_hist = []

2018-04-30 20:05:09,520 - memory_profile6_log - INFO -    327                             

2018-04-30 20:05:09,522 - memory_profile6_log - INFO -    328     87.4 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-30 20:05:09,523 - memory_profile6_log - INFO -    329    445.2 MiB      0.0 MiB       for ndate in date_generated:

2018-04-30 20:05:09,523 - memory_profile6_log - INFO -    330    340.0 MiB    252.6 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-30 20:05:09,525 - memory_profile6_log - INFO -    331    340.0 MiB      0.0 MiB           if tframe is not None:

2018-04-30 20:05:09,525 - memory_profile6_log - INFO -    332    340.0 MiB      0.0 MiB               if not tframe.empty:

2018-04-30 20:05:09,526 - memory_profile6_log - INFO -    333    347.5 MiB      7.5 MiB                   X_split = np.array_split(tframe, 5)

2018-04-30 20:05:09,530 - memory_profile6_log - INFO -    334    347.5 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-30 20:05:09,532 - memory_profile6_log - INFO -    335    347.5 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-30 20:05:09,532 - memory_profile6_log - INFO -    336    445.2 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-30 20:05:09,533 - memory_profile6_log - INFO -    337                                                 # ~ loading history

2018-04-30 20:05:09,535 - memory_profile6_log - INFO -    338                                                 """

2018-04-30 20:05:09,536 - memory_profile6_log - INFO -    339                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-30 20:05:09,536 - memory_profile6_log - INFO -    340                                                 """

2018-04-30 20:05:09,536 - memory_profile6_log - INFO -    341    428.9 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-30 20:05:09,538 - memory_profile6_log - INFO -    342                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-30 20:05:09,538 - memory_profile6_log - INFO -    343    428.9 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-30 20:05:09,539 - memory_profile6_log - INFO -    344                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-30 20:05:09,539 - memory_profile6_log - INFO -    345    429.5 MiB      3.7 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-30 20:05:09,540 - memory_profile6_log - INFO -    346                             

2018-04-30 20:05:09,540 - memory_profile6_log - INFO -    347    429.5 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-30 20:05:09,542 - memory_profile6_log - INFO -    348    476.2 MiB    256.4 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-30 20:05:09,545 - memory_profile6_log - INFO -    349                             

2018-04-30 20:05:09,546 - memory_profile6_log - INFO -    350                                                 # me = os.getpid()

2018-04-30 20:05:09,548 - memory_profile6_log - INFO -    351                                                 # kill_proc_tree(me)

2018-04-30 20:05:09,549 - memory_profile6_log - INFO -    352                             

2018-04-30 20:05:09,549 - memory_profile6_log - INFO -    353    476.2 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-30 20:05:09,551 - memory_profile6_log - INFO -    354    477.2 MiB     -2.2 MiB                       for m in h_frame:

2018-04-30 20:05:09,551 - memory_profile6_log - INFO -    355    477.2 MiB     -2.2 MiB                           if m is not None:

2018-04-30 20:05:09,552 - memory_profile6_log - INFO -    356    477.2 MiB     -2.2 MiB                               if len(m) > 0:

2018-04-30 20:05:09,552 - memory_profile6_log - INFO -    357    477.2 MiB      0.8 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-30 20:05:09,552 - memory_profile6_log - INFO -    358    445.2 MiB   -165.5 MiB                       del h_frame

2018-04-30 20:05:09,553 - memory_profile6_log - INFO -    359    445.2 MiB      0.0 MiB                       del lhistory

2018-04-30 20:05:09,555 - memory_profile6_log - INFO -    360                             

2018-04-30 20:05:09,555 - memory_profile6_log - INFO -    361    445.2 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-30 20:05:09,555 - memory_profile6_log - INFO -    362    445.2 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-30 20:05:09,556 - memory_profile6_log - INFO -    363                                     else: 

2018-04-30 20:05:09,556 - memory_profile6_log - INFO -    364                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-30 20:05:09,558 - memory_profile6_log - INFO -    365    445.2 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-30 20:05:09,559 - memory_profile6_log - INFO -    366    445.2 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-30 20:05:09,565 - memory_profile6_log - INFO -    367                             

2018-04-30 20:05:09,565 - memory_profile6_log - INFO -    368    445.2 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-30 20:05:09,568 - memory_profile6_log - INFO - 


2018-04-30 20:05:10,770 - memory_profile6_log - INFO - size of big_frame_hist: 69.87 MB
2018-04-30 20:05:10,848 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-30 20:05:10,864 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-04-30 20:10:41,500 - memory_profile6_log - INFO - size of df: 474.10 MB
2018-04-30 20:10:41,509 - memory_profile6_log - INFO - getting total: 1936921 training data(current date interest)
2018-04-30 20:10:42,040 - memory_profile6_log - INFO - size of current_frame: 488.88 MB
2018-04-30 20:10:42,058 - memory_profile6_log - INFO - loading time of: 2190916 total genuine-current interest data ~ take 616.921s
2018-04-30 20:10:42,065 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 20:10:42,076 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 20:10:42,078 - memory_profile6_log - INFO - ================================================

2018-04-30 20:10:42,079 - memory_profile6_log - INFO -    370     87.3 MiB     87.3 MiB   @profile

2018-04-30 20:10:42,081 - memory_profile6_log - INFO -    371                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-30 20:10:42,082 - memory_profile6_log - INFO -    372     87.4 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-30 20:10:42,082 - memory_profile6_log - INFO -    373     87.4 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 20:10:42,082 - memory_profile6_log - INFO -    374                             

2018-04-30 20:10:42,085 - memory_profile6_log - INFO -    375                                 # ~~~ Begin collecting data ~~~

2018-04-30 20:10:42,085 - memory_profile6_log - INFO -    376     87.4 MiB      0.0 MiB       t0 = time.time()

2018-04-30 20:10:42,085 - memory_profile6_log - INFO -    377    439.7 MiB    352.2 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-30 20:10:42,085 - memory_profile6_log - INFO -    378    439.7 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-30 20:10:42,086 - memory_profile6_log - INFO -    379                                     logger.info("Training cannot be empty..")

2018-04-30 20:10:42,086 - memory_profile6_log - INFO -    380                                     return False

2018-04-30 20:10:42,088 - memory_profile6_log - INFO -    381    456.0 MiB     16.4 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-30 20:10:42,088 - memory_profile6_log - INFO -    382                                 # print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-30 20:10:42,088 - memory_profile6_log - INFO -    383    456.0 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-30 20:10:42,089 - memory_profile6_log - INFO -    384                             

2018-04-30 20:10:42,094 - memory_profile6_log - INFO -    385    462.1 MiB      6.1 MiB       big_frame = pd.concat(datalist)

2018-04-30 20:10:42,095 - memory_profile6_log - INFO -    386    462.1 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-30 20:10:42,095 - memory_profile6_log - INFO -    387    454.7 MiB     -7.4 MiB       del datalist

2018-04-30 20:10:42,096 - memory_profile6_log - INFO -    388                             

2018-04-30 20:10:42,098 - memory_profile6_log - INFO -    389    454.7 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 20:10:42,098 - memory_profile6_log - INFO -    390                             

2018-04-30 20:10:42,098 - memory_profile6_log - INFO -    391                                 # ~ get current news interest ~

2018-04-30 20:10:42,099 - memory_profile6_log - INFO -    392    454.7 MiB      0.0 MiB       if not cd:

2018-04-30 20:10:42,101 - memory_profile6_log - INFO -    393    454.7 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-04-30 20:10:42,101 - memory_profile6_log - INFO -    394    923.4 MiB    468.7 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-30 20:10:42,101 - memory_profile6_log - INFO -    395    923.4 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-30 20:10:42,101 - memory_profile6_log - INFO -    396                                 else:

2018-04-30 20:10:42,101 - memory_profile6_log - INFO -    397                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-30 20:10:42,102 - memory_profile6_log - INFO -    398                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-30 20:10:42,102 - memory_profile6_log - INFO -    399                             

2018-04-30 20:10:42,104 - memory_profile6_log - INFO -    400                                     # safe handling of query parameter

2018-04-30 20:10:42,104 - memory_profile6_log - INFO -    401                                     query_params = [

2018-04-30 20:10:42,105 - memory_profile6_log - INFO -    402                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-30 20:10:42,105 - memory_profile6_log - INFO -    403                                     ]

2018-04-30 20:10:42,105 - memory_profile6_log - INFO -    404                             

2018-04-30 20:10:42,108 - memory_profile6_log - INFO -    405                                     job_config.query_parameters = query_params

2018-04-30 20:10:42,109 - memory_profile6_log - INFO -    406                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-30 20:10:42,111 - memory_profile6_log - INFO -    407                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-30 20:10:42,111 - memory_profile6_log - INFO -    408                             

2018-04-30 20:10:42,111 - memory_profile6_log - INFO -    409    938.2 MiB     14.8 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-30 20:10:42,112 - memory_profile6_log - INFO -    410    938.2 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-30 20:10:42,112 - memory_profile6_log - INFO -    411    938.3 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-30 20:10:42,114 - memory_profile6_log - INFO -    412    938.3 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-30 20:10:42,115 - memory_profile6_log - INFO -    413    938.3 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-30 20:10:42,115 - memory_profile6_log - INFO -    414    938.3 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-30 20:10:42,115 - memory_profile6_log - INFO -    415                             

2018-04-30 20:10:42,115 - memory_profile6_log - INFO -    416    938.3 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-30 20:10:42,117 - memory_profile6_log - INFO - 


2018-04-30 20:10:42,121 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-30 20:10:42,233 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-30 20:10:42,249 - memory_profile6_log - INFO - transform on: 1936921 total current data(D(t))
2018-04-30 20:10:42,252 - memory_profile6_log - INFO - apply on: 253995 total history...)
2018-04-30 20:10:43,214 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-30 20:10:43,217 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-30 20:10:46,772 - memory_profile6_log - INFO - Len of fitted_models on main class: 253995
2018-04-30 20:10:46,773 - memory_profile6_log - INFO - 

2018-04-30 20:10:46,773 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-30 20:10:46,775 - memory_profile6_log - INFO - 

2018-04-30 20:10:46,776 - memory_profile6_log - INFO - fitted_model_hist:

2018-04-30 20:10:46,776 - memory_profile6_log - INFO - 

2018-04-30 20:10:46,982 - memory_profile6_log - INFO -      pt_posterior_x_Nt           topic_id                                            user_id  sigma_Nt
67          294.636364           22551890  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
85           15.991521           22553543  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
198          83.719466           22601470  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
185         430.739355           22615029  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
16          516.753870           27313197  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
114         860.368557           27313228  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
120          84.916311           27431099  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
60          483.801449           27431145  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
64          426.610863           27433171  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
119        2528.962121           27434611  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
297         157.761342           36060446  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
255         187.225463           39062368  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
98           94.876510           39301645  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
40          209.556183           40281010  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
226         445.097333           40410447  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
232         192.516148           40710353  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
305         984.728614           43444400  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
107         621.644320           45900667  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
76          135.178376           47085613  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
231        7418.288889           47134121  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
5          1589.633333           51484995  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
244        1470.585903          403246831  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
103        2112.803797         1014424906  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
227         940.346479         1019508535  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
195        2073.434783         1036923500  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
251         951.062678         1046474724  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
19          858.156812         1060869149  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
100        1346.060484         1060896301  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
338        1420.523404         1068590702  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
260         748.482063         1164316760  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
327         165.998508         1443761312  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
300         724.914224  27431110790312055  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
27         5057.924242  27431110790312749  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
248         702.785263  27431110790313167  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
235        1094.501639  27431110790313263  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
208         738.546460  27431110790314020  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
163         655.840864  27431110790314045  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
192         194.422248  27431110790314052  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
2018-04-30 20:10:46,986 - memory_profile6_log - INFO - 

2018-04-30 20:10:47,246 - memory_profile6_log - INFO - len of fitted models after concat: 28268
2018-04-30 20:10:47,256 - memory_profile6_log - INFO - 

2018-04-30 20:10:47,329 - memory_profile6_log - INFO -                                                  user_id  sigma_Nt
0                   107e02cd-54af-40fd-a602-e6105250ae8c    1175.0
1      1610c8e6b6466b-05a6ac1ba8c1b8-4323461-100200-1...    9555.0
2      1610c8f07a4239-0d359d8c177cf2-32607400-100200-...     198.0
3      1610c92e11ec4a-0dcda7bb5f421e-4323461-100200-1...   33500.0
4      1610c93bf224cd-0eff64e94-504b7d3c-13c680-1610c...     448.0
5      1610c9581a66e6-09411fca5fe1ec-4323461-100200-1...     312.0
6      1610c9932825c9-08985fd9641f06-72226750-1fa400-...      16.0
7      1610c9d0a56a5-02f78376aa7daa-4323461-100200-16...     154.0
8      1610c9d81f3c1-0d3e8d10313ab4-32677403-fa000-16...    8260.0
9      1610ca005ca277-0a0c0d3fda88f4-5768397b-100200-...      50.0
10     1610ca03be0179-09054d53bc9fe2-4323461-100200-1...     272.0
11     1610ca7cec566b-0ee3a59b262c89-4323461-100200-1...     130.0
12     1610ca98d49149-0e61ac652d8ef-4323461-100200-16...   60759.0
13     1610cb0499043a-0b3a08a39f2295-393d5f0e-15f900-...      64.0
14     1610cb3a8c1348-0c7cec1541e7ac-4323461-100200-1...     592.0
15     1610cb732313c-0c6252e2dec4ba-4323461-100200-16...     187.0
16     1610cc083853e4-0f8b41aa95b129-3f616948-fa000-1...     378.0
17     1610cc0ddda72a-0b108e976594d7-4323461-100200-1...    2107.0
18     1610cc7ba85492-0b4758aa5299bd-4323461-100200-1...     425.0
19     1610ccf527677-05e02374e03854-796f1035-ff000-16...     714.0
20     1610ccff3ee25b-0133d5164c6939-4323461-100200-1...   14307.0
21     1610ce362fca9-0e4d4c092c7c7c-1e271709-ff000-16...      25.0
22     1610ce437a6236-02615b62932a02-4323461-100200-1...    2268.0
23     1610cfd0b6889-0a7734e8f16da-32607400-13c680-16...    3168.0
24     1610cffb117c1-0d4678545a8eb88-173a7640-ff000-1...    1269.0
25     1610d0565bd3d2-04b54ea1af81c1-4323461-1fa400-1...     420.0
26     1610d08ca457bc-03b7b3aa063da1-b7a103e-100200-1...     464.0
27     1610d0a8acb1f7-083495b4f03816-1c43142b-fa000-1...      16.0
28     1610d0afdea6a-0059db2b18b865-5768397b-f0000-16...     180.0
29     1610d0db1aaba-0637ca9a9a8404-1f184a35-100200-1...      25.0
...                                                  ...       ...
28238  162c100970016a-090ba6687-3700692c-38400-162c10...    2236.0
28239  162c1084aeb174-06a28149c-1c415461-3cd20-162c10...    1218.0
28240  162c1162c8149-0ebd9adb6-24504816-38400-162c116...    4644.0
28241  162c1ff18e862-0d4c66245fd0c4-5e6f6b3c-38400-16...    4947.0
28242  162c4df08f47-065a92c88-25594117-38400-162c4df0...    1044.0
28243  162c4dfa67a1b-0d65ad911-152e7c3e-348f0-162c4df...    4056.0
28244  162c4e85b5556-0876fabd1-1c445461-5020a.72f0539...    1394.0
28245  162c4fa98dedc-08782872a-a26765c-38400-162c4fa9...    4920.0
28246  162c4fd089237-074e64e14-56001241-38400-162c4fd...     864.0
28247  162c5008eb08f-0a61ac61d-662f3a45-38400-162c500...    2200.0
28248  162c504535358-0d6cc9276-19723e24-38400-162c504...     989.0
28249  162c50514d60-0973822ce-136f777b-38400-162c5051...    1776.0
28250  162c5153d5212f-0890790ef-7c797f46-38400-162c51...      64.0
28251  162c5175279c0-03a5e0a86-5004076e-38400-162c517...      64.0
28252  162c520e546f7-052008b0f-57116e2f-34080-162c520...      25.0
28253               3f2b5c6a-6f1d-411d-a76f-71917c791390      25.0
28254               49664d27-3543-4273-ac9a-7eae1761dbe7     726.0
28255               4dc07eb0-c66b-4286-bf28-b4a616de5f20      25.0
28256               53df2fa3-1668-4359-ab86-b6d1c8b2dd8e    1690.0
28257               5e182929-1365-4734-9505-a88e875e921f       1.0
28258               61db6976-23c8-4ad2-b3fe-38e5cc433395      80.0
28259               67daf23a-7e90-42fd-a72f-fb2b223bf811      16.0
28260               748e839d-41eb-4112-a16d-05aadefb12f2   12540.0
28261               7608728d-c385-4fbf-b7ca-a8b14b89487c     576.0
28262               7c856fb2-2a4d-45a6-923e-278ad25fd175      81.0
28263               95a1c6c6-b223-424a-92a4-0f66da0524f7     128.0
28264               9c21818e-b621-49cb-8358-5e2e0b85e997      50.0
28265               d5d1f35b-494e-4a87-9bb3-52bc7d0504d6     130.0
28266               e5ca5fa0-a122-4ca7-928e-006d6c4b5f81    1485.0
28267               edc501b3-014c-42be-a687-e68121c0f52b    1058.0

[28268 rows x 2 columns]
2018-04-30 20:10:47,345 - memory_profile6_log - INFO - 

2018-04-30 20:10:47,348 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-30 20:10:47,349 - memory_profile6_log - INFO - 

2018-04-30 20:33:10,361 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-30 20:33:10,365 - memory_profile6_log - INFO - date_generated: 
2018-04-30 20:33:10,368 - memory_profile6_log - INFO -  
2018-04-30 20:33:10,368 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-30 20:33:10,368 - memory_profile6_log - INFO - 

2018-04-30 20:33:10,368 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-30 20:33:10,369 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-30 20:33:10,369 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-30 20:33:10,545 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-30 20:33:10,561 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-30 20:34:19,282 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-30 20:34:19,283 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-30 20:34:19,328 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-30 20:34:19,331 - memory_profile6_log - INFO - Appending history data...
2018-04-30 20:34:19,332 - memory_profile6_log - INFO - processing batch-0
2018-04-30 20:34:19,334 - memory_profile6_log - INFO - creating list history data...
2018-04-30 20:34:19,415 - memory_profile6_log - INFO - call history data...
2018-04-30 20:35:04,665 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 20:35:05,921 - memory_profile6_log - INFO - processing batch-1
2018-04-30 20:35:05,927 - memory_profile6_log - INFO - creating list history data...
2018-04-30 20:35:05,973 - memory_profile6_log - INFO - call history data...
2018-04-30 20:35:48,411 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 20:35:49,677 - memory_profile6_log - INFO - processing batch-2
2018-04-30 20:35:49,680 - memory_profile6_log - INFO - creating list history data...
2018-04-30 20:35:49,740 - memory_profile6_log - INFO - call history data...
2018-04-30 20:36:28,194 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 20:36:29,513 - memory_profile6_log - INFO - processing batch-3
2018-04-30 20:36:29,515 - memory_profile6_log - INFO - creating list history data...
2018-04-30 20:36:29,569 - memory_profile6_log - INFO - call history data...
2018-04-30 20:37:11,987 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 20:37:13,335 - memory_profile6_log - INFO - processing batch-4
2018-04-30 20:37:13,335 - memory_profile6_log - INFO - creating list history data...
2018-04-30 20:37:13,381 - memory_profile6_log - INFO - call history data...
2018-04-30 20:37:56,500 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-30 20:37:57,953 - memory_profile6_log - INFO - Appending training data...
2018-04-30 20:37:57,954 - memory_profile6_log - INFO - len datalist: 1
2018-04-30 20:37:57,957 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-30 20:37:57,963 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 20:37:57,967 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 20:37:57,970 - memory_profile6_log - INFO - ================================================

2018-04-30 20:37:57,970 - memory_profile6_log - INFO -    320     86.8 MiB     86.8 MiB   @profile

2018-04-30 20:37:57,973 - memory_profile6_log - INFO -    321                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-30 20:37:57,973 - memory_profile6_log - INFO -    322     86.8 MiB      0.0 MiB       bq_client = client

2018-04-30 20:37:57,974 - memory_profile6_log - INFO -    323     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 20:37:57,977 - memory_profile6_log - INFO -    324                             

2018-04-30 20:37:57,979 - memory_profile6_log - INFO -    325     86.8 MiB      0.0 MiB       datalist = []

2018-04-30 20:37:57,982 - memory_profile6_log - INFO -    326     86.8 MiB      0.0 MiB       datalist_hist = []

2018-04-30 20:37:57,983 - memory_profile6_log - INFO -    327                             

2018-04-30 20:37:57,984 - memory_profile6_log - INFO -    328     86.8 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-30 20:37:57,986 - memory_profile6_log - INFO -    329    445.5 MiB      0.0 MiB       for ndate in date_generated:

2018-04-30 20:37:57,986 - memory_profile6_log - INFO -    330    339.8 MiB    253.0 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-30 20:37:57,987 - memory_profile6_log - INFO -    331    339.8 MiB      0.0 MiB           if tframe is not None:

2018-04-30 20:37:57,989 - memory_profile6_log - INFO -    332    339.8 MiB      0.0 MiB               if not tframe.empty:

2018-04-30 20:37:57,992 - memory_profile6_log - INFO -    333    346.7 MiB      7.0 MiB                   X_split = np.array_split(tframe, 5)

2018-04-30 20:37:57,993 - memory_profile6_log - INFO -    334    346.7 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-30 20:37:57,993 - memory_profile6_log - INFO -    335    346.7 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-30 20:37:57,994 - memory_profile6_log - INFO -    336    445.5 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-30 20:37:57,994 - memory_profile6_log - INFO -    337                                                 # ~ loading history

2018-04-30 20:37:57,996 - memory_profile6_log - INFO -    338                                                 """

2018-04-30 20:37:57,996 - memory_profile6_log - INFO -    339                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-30 20:37:57,996 - memory_profile6_log - INFO -    340                                                 """

2018-04-30 20:37:57,997 - memory_profile6_log - INFO -    341    428.1 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-30 20:37:57,999 - memory_profile6_log - INFO -    342                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-30 20:37:58,000 - memory_profile6_log - INFO -    343    428.1 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-30 20:37:58,000 - memory_profile6_log - INFO -    344                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-30 20:37:58,000 - memory_profile6_log - INFO -    345    429.0 MiB      4.8 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-30 20:37:58,002 - memory_profile6_log - INFO -    346                             

2018-04-30 20:37:58,006 - memory_profile6_log - INFO -    347    429.0 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-30 20:37:58,007 - memory_profile6_log - INFO -    348    475.2 MiB    252.4 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-30 20:37:58,009 - memory_profile6_log - INFO -    349                             

2018-04-30 20:37:58,009 - memory_profile6_log - INFO -    350                                                 # me = os.getpid()

2018-04-30 20:37:58,010 - memory_profile6_log - INFO -    351                                                 # kill_proc_tree(me)

2018-04-30 20:37:58,012 - memory_profile6_log - INFO -    352                             

2018-04-30 20:37:58,013 - memory_profile6_log - INFO -    353    475.2 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-30 20:37:58,013 - memory_profile6_log - INFO -    354    476.4 MiB     -0.8 MiB                       for m in h_frame:

2018-04-30 20:37:58,013 - memory_profile6_log - INFO -    355    476.3 MiB     -0.8 MiB                           if m is not None:

2018-04-30 20:37:58,016 - memory_profile6_log - INFO -    356    476.3 MiB     -0.8 MiB                               if len(m) > 0:

2018-04-30 20:37:58,017 - memory_profile6_log - INFO -    357    476.4 MiB      3.7 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-30 20:37:58,019 - memory_profile6_log - INFO -    358    445.5 MiB   -163.0 MiB                       del h_frame

2018-04-30 20:37:58,019 - memory_profile6_log - INFO -    359    445.5 MiB      0.0 MiB                       del lhistory

2018-04-30 20:37:58,020 - memory_profile6_log - INFO -    360                             

2018-04-30 20:37:58,020 - memory_profile6_log - INFO -    361    445.5 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-30 20:37:58,022 - memory_profile6_log - INFO -    362    445.5 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-30 20:37:58,023 - memory_profile6_log - INFO -    363                                     else: 

2018-04-30 20:37:58,023 - memory_profile6_log - INFO -    364                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-30 20:37:58,025 - memory_profile6_log - INFO -    365    445.5 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-30 20:37:58,028 - memory_profile6_log - INFO -    366    445.5 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-30 20:37:58,029 - memory_profile6_log - INFO -    367                             

2018-04-30 20:37:58,029 - memory_profile6_log - INFO -    368    445.5 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-30 20:37:58,030 - memory_profile6_log - INFO - 


2018-04-30 20:37:59,338 - memory_profile6_log - INFO - size of big_frame_hist: 69.87 MB
2018-04-30 20:37:59,436 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-30 20:37:59,444 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-04-30 20:44:12,403 - memory_profile6_log - INFO - size of df: 486.29 MB
2018-04-30 20:44:12,414 - memory_profile6_log - INFO - getting total: 1985188 training data(current date interest)
2018-04-30 20:44:12,992 - memory_profile6_log - INFO - size of current_frame: 501.44 MB
2018-04-30 20:44:12,996 - memory_profile6_log - INFO - loading time of: 2239183 total genuine-current interest data ~ take 662.486s
2018-04-30 20:44:13,016 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 20:44:13,016 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 20:44:13,016 - memory_profile6_log - INFO - ================================================

2018-04-30 20:44:13,017 - memory_profile6_log - INFO -    370     86.6 MiB     86.6 MiB   @profile

2018-04-30 20:44:13,017 - memory_profile6_log - INFO -    371                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-30 20:44:13,020 - memory_profile6_log - INFO -    372     86.8 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-30 20:44:13,020 - memory_profile6_log - INFO -    373     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-30 20:44:13,023 - memory_profile6_log - INFO -    374                             

2018-04-30 20:44:13,025 - memory_profile6_log - INFO -    375                                 # ~~~ Begin collecting data ~~~

2018-04-30 20:44:13,026 - memory_profile6_log - INFO -    376     86.8 MiB      0.0 MiB       t0 = time.time()

2018-04-30 20:44:13,026 - memory_profile6_log - INFO -    377    441.5 MiB    354.8 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-30 20:44:13,028 - memory_profile6_log - INFO -    378    441.5 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-30 20:44:13,029 - memory_profile6_log - INFO -    379                                     logger.info("Training cannot be empty..")

2018-04-30 20:44:13,029 - memory_profile6_log - INFO -    380                                     return False

2018-04-30 20:44:13,029 - memory_profile6_log - INFO -    381    456.3 MiB     14.7 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-30 20:44:13,030 - memory_profile6_log - INFO -    382                                 # print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-30 20:44:13,030 - memory_profile6_log - INFO -    383    456.3 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-30 20:44:13,032 - memory_profile6_log - INFO -    384                             

2018-04-30 20:44:13,032 - memory_profile6_log - INFO -    385    462.3 MiB      6.1 MiB       big_frame = pd.concat(datalist)

2018-04-30 20:44:13,032 - memory_profile6_log - INFO -    386    462.3 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-30 20:44:13,032 - memory_profile6_log - INFO -    387    456.5 MiB     -5.8 MiB       del datalist

2018-04-30 20:44:13,032 - memory_profile6_log - INFO -    388                             

2018-04-30 20:44:13,033 - memory_profile6_log - INFO -    389    456.5 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 20:44:13,033 - memory_profile6_log - INFO -    390                             

2018-04-30 20:44:13,035 - memory_profile6_log - INFO -    391                                 # ~ get current news interest ~

2018-04-30 20:44:13,036 - memory_profile6_log - INFO -    392    456.5 MiB      0.0 MiB       if not cd:

2018-04-30 20:44:13,036 - memory_profile6_log - INFO -    393    456.5 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-04-30 20:44:13,038 - memory_profile6_log - INFO -    394    934.1 MiB    477.6 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-30 20:44:13,038 - memory_profile6_log - INFO -    395    934.1 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-30 20:44:13,042 - memory_profile6_log - INFO -    396                                 else:

2018-04-30 20:44:13,043 - memory_profile6_log - INFO -    397                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-30 20:44:13,045 - memory_profile6_log - INFO -    398                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-30 20:44:13,046 - memory_profile6_log - INFO -    399                             

2018-04-30 20:44:13,046 - memory_profile6_log - INFO -    400                                     # safe handling of query parameter

2018-04-30 20:44:13,046 - memory_profile6_log - INFO -    401                                     query_params = [

2018-04-30 20:44:13,046 - memory_profile6_log - INFO -    402                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-30 20:44:13,048 - memory_profile6_log - INFO -    403                                     ]

2018-04-30 20:44:13,048 - memory_profile6_log - INFO -    404                             

2018-04-30 20:44:13,048 - memory_profile6_log - INFO -    405                                     job_config.query_parameters = query_params

2018-04-30 20:44:13,049 - memory_profile6_log - INFO -    406                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-30 20:44:13,049 - memory_profile6_log - INFO -    407                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-30 20:44:13,049 - memory_profile6_log - INFO -    408                             

2018-04-30 20:44:13,049 - memory_profile6_log - INFO -    409    949.2 MiB     15.2 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-30 20:44:13,049 - memory_profile6_log - INFO -    410    949.2 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-30 20:44:13,049 - memory_profile6_log - INFO -    411    949.3 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-30 20:44:13,051 - memory_profile6_log - INFO -    412    949.3 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-30 20:44:13,052 - memory_profile6_log - INFO -    413    949.3 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-30 20:44:13,053 - memory_profile6_log - INFO -    414    949.3 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-30 20:44:13,053 - memory_profile6_log - INFO -    415                             

2018-04-30 20:44:13,059 - memory_profile6_log - INFO -    416    949.3 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-30 20:44:13,059 - memory_profile6_log - INFO - 


2018-04-30 20:44:13,065 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-30 20:44:13,211 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-30 20:44:13,214 - memory_profile6_log - INFO - transform on: 1985188 total current data(D(t))
2018-04-30 20:44:13,216 - memory_profile6_log - INFO - apply on: 253995 total history...)
2018-04-30 20:44:13,463 - memory_profile6_log - INFO - len of uniques_fit_hist:253995
2018-04-30 20:44:13,651 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:28268
2018-04-30 20:44:14,121 - memory_profile6_log - INFO - self.sum_all_nt before concate:

2018-04-30 20:44:14,153 - memory_profile6_log - INFO -                                                  user_id  sigma_Nt
15408  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
2018-04-30 20:44:14,157 - memory_profile6_log - INFO - 

2018-04-30 20:44:14,217 - memory_profile6_log - INFO - self.sum_all_nt after concate:

2018-04-30 20:44:14,236 - memory_profile6_log - INFO -                                                  user_id  sigma_Nt
15408  1616f009d96b1-0285d8288a5bce-70217860-38400-16...       104
2018-04-30 20:44:14,247 - memory_profile6_log - INFO - 

2018-04-30 20:44:14,467 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-30 20:44:14,479 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-30 20:44:18,207 - memory_profile6_log - INFO - Len of fitted_models on main class: 253995
2018-04-30 20:44:18,209 - memory_profile6_log - INFO - 

2018-04-30 20:44:18,210 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-30 20:44:18,211 - memory_profile6_log - INFO - 

2018-04-30 20:44:18,213 - memory_profile6_log - INFO - fitted_model_hist:

2018-04-30 20:44:18,213 - memory_profile6_log - INFO - 

2018-04-30 20:44:18,358 - memory_profile6_log - INFO -      pt_posterior_x_Nt           topic_id                                            user_id  sigma_Nt
30          294.636364           22551890  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
117          15.991521           22553543  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
190          83.719466           22601470  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
290         430.739355           22615029  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
44          516.753870           27313197  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
178         860.368557           27313228  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
225          84.916311           27431099  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
290         483.801449           27431145  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
228         426.610863           27433171  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
201        2528.962121           27434611  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
58          157.761342           36060446  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
141         187.225463           39062368  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
159          94.876510           39301645  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
181         209.556183           40281010  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
227         445.097333           40410447  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
189         192.516148           40710353  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
311         984.728614           43444400  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
69          621.644320           45900667  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
137         135.178376           47085613  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
142        7418.288889           47134121  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
122        1589.633333           51484995  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
30         1470.585903          403246831  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
128        2112.803797         1014424906  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
61          940.346479         1019508535  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
67         2073.434783         1036923500  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
117         951.062678         1046474724  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
199         858.156812         1060869149  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
104        1346.060484         1060896301  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
330        1420.523404         1068590702  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
65          748.482063         1164316760  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
312         165.998508         1443761312  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
301         724.914224  27431110790312055  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
146        5057.924242  27431110790312749  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
113         702.785263  27431110790313167  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
293        1094.501639  27431110790313263  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
237         738.546460  27431110790314020  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
128         655.840864  27431110790314045  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
176         194.422248  27431110790314052  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
2018-04-30 20:44:18,375 - memory_profile6_log - INFO - 

2018-04-30 20:44:18,424 - memory_profile6_log - INFO - len of fitted models after concat: 507990
2018-04-30 20:44:18,430 - memory_profile6_log - INFO - 

2018-04-30 20:44:18,433 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-30 20:44:18,434 - memory_profile6_log - INFO - 

2018-04-30 20:44:18,802 - memory_profile6_log - INFO - fitted_models after cobine:

2018-04-30 20:44:18,805 - memory_profile6_log - INFO - 

2018-04-30 20:44:18,971 - memory_profile6_log - INFO -      pt_posterior_x_Nt           topic_id                                            user_id  sigma_Nt
30          294.636364           22551890  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
117          15.991521           22553543  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
190          83.719466           22601470  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
290         430.739355           22615029  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
44          516.753870           27313197  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
178         860.368557           27313228  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
225          84.916311           27431099  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
290         483.801449           27431145  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
228         426.610863           27433171  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
201        2528.962121           27434611  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
58          157.761342           36060446  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
141         187.225463           39062368  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
159          94.876510           39301645  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
181         209.556183           40281010  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
227         445.097333           40410447  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
189         192.516148           40710353  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
311         984.728614           43444400  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
69          621.644320           45900667  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
137         135.178376           47085613  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
142        7418.288889           47134121  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
122        1589.633333           51484995  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
30         1470.585903          403246831  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
128        2112.803797         1014424906  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
61          940.346479         1019508535  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
67         2073.434783         1036923500  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
117         951.062678         1046474724  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
199         858.156812         1060869149  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
104        1346.060484         1060896301  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
330        1420.523404         1068590702  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
65          748.482063         1164316760  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
312         165.998508         1443761312  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
301         724.914224  27431110790312055  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
146        5057.924242  27431110790312749  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
113         702.785263  27431110790313167  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
293        1094.501639  27431110790313263  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
237         738.546460  27431110790314020  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
128         655.840864  27431110790314045  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
176         194.422248  27431110790314052  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        52
2018-04-30 20:44:18,973 - memory_profile6_log - INFO - 

2018-04-30 20:44:18,974 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 253995
2018-04-30 20:44:18,976 - memory_profile6_log - INFO - 

2018-04-30 20:45:06,155 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-30 20:45:06,346 - memory_profile6_log - INFO -                                                   user_id           topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt  p0_posterior  is_general  rank
137453  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22601470         167.438933             177.438933   0.061244       104      0.095325        True   1.0
137457  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27431099         169.832621             179.832621   0.049881       104      0.078686        True   2.0
137452  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22553543          31.983042              41.983042   0.134746       104      0.049623        True   3.0
137451  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22551890         589.272727             599.272727   0.001479       104      0.007773        True   4.0
137454  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22615029         861.478710             871.478710   0.000142       104      0.001085        True   5.0
137477  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           47085613         270.356752             280.356752   0.050830       104      0.125005       False   1.0
137458  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790312055        1449.828447            1459.828447   0.006643       104      0.085070       False   2.0
137468  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           36060446         315.522684             325.522684   0.025271       104      0.072161       False   3.0
137455  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27313197        1033.507740            1043.507740   0.005243       104      0.047996       False   4.0
137467  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27434611        5057.924242            5067.924242   0.000736       104      0.032704       False   5.0
137470  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           39301645         189.753020             199.753020   0.013931       104      0.024411       False   6.0
137478  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           47134121       14836.577778           14846.577778   0.000124       104      0.016119       False   7.0
137466  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27433171         853.221725             863.221725   0.001503       104      0.011379       False   8.0
137473  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           40410447         890.194667             900.194667   0.000836       104      0.006602       False   9.0
137474  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           40710353         385.032295             395.032295   0.001420       104      0.004921       False  10.0
137450  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1443761312         331.997016             341.997016   0.001364       104      0.004092       False  11.0
137476  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           45900667        1243.288641            1253.288641   0.000320       104      0.003515       False  12.0
137459  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790312749       10115.848485           10125.848485   0.000030       104      0.002663       False  13.0
137456  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27313228        1720.737113            1730.737113   0.000112       104      0.001699       False  14.0
137465  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27431145         967.602899             977.602899   0.000154       104      0.001325       False  15.0
2018-04-30 20:45:06,364 - memory_profile6_log - INFO - 

2018-04-30 20:45:06,364 - memory_profile6_log - INFO - Len of model_transform: 236474
2018-04-30 20:45:06,365 - memory_profile6_log - INFO - Len of df_dt: 1985188
2018-04-30 20:45:06,367 - memory_profile6_log - INFO - Total train time: 52.974s
2018-04-30 20:45:06,368 - memory_profile6_log - INFO - memory left before cleaning: 74.200 percent memory...
2018-04-30 20:45:06,368 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-30 20:45:06,369 - memory_profile6_log - INFO - deleting df_dut...
2018-04-30 20:45:06,371 - memory_profile6_log - INFO - deleting df_dt...
2018-04-30 20:45:06,371 - memory_profile6_log - INFO - deleting df_input...
2018-04-30 20:45:06,437 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-30 20:45:06,448 - memory_profile6_log - INFO - deleting df_current...
2018-04-30 20:45:06,450 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-30 20:45:06,453 - memory_profile6_log - INFO - deleting model_fit...
2018-04-30 20:45:06,467 - memory_profile6_log - INFO - deleting result...
2018-04-30 20:45:06,486 - memory_profile6_log - INFO - deleting model_transform...
2018-04-30 20:45:06,490 - memory_profile6_log - INFO - memory left after cleaning: 73.600 percent memory...
2018-04-30 20:45:06,490 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-30 20:45:06,493 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-30 20:45:06,494 - memory_profile6_log - INFO - Saving main Transform model to Google DataStore...
2018-04-30 20:45:06,496 - memory_profile6_log - INFO - Saving total data: 236474
2018-04-30 20:45:06,684 - memory_profile6_log - INFO - Saving fitted_models as history to Google DataStore...
2018-04-30 20:45:06,736 - memory_profile6_log - INFO - Saving total data: 253995
2018-04-30 20:45:06,769 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 10
2018-04-30 20:45:06,772 - memory_profile6_log - INFO - processing batch-0
2018-04-30 20:45:07,121 - memory_profile6_log - INFO - processing batch-1
2018-04-30 20:45:07,315 - memory_profile6_log - INFO - processing batch-2
2018-04-30 20:45:07,519 - memory_profile6_log - INFO - processing batch-3
2018-04-30 20:45:07,711 - memory_profile6_log - INFO - processing batch-4
2018-04-30 20:45:07,901 - memory_profile6_log - INFO - processing batch-5
2018-04-30 20:45:08,096 - memory_profile6_log - INFO - processing batch-6
2018-04-30 20:45:08,292 - memory_profile6_log - INFO - processing batch-7
2018-04-30 20:45:08,513 - memory_profile6_log - INFO - processing batch-8
2018-04-30 20:45:08,688 - memory_profile6_log - INFO - processing batch-9
2018-04-30 20:45:08,884 - memory_profile6_log - INFO - deleting X_split...
2018-04-30 20:45:08,885 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-04-30 20:45:08,887 - memory_profile6_log - INFO - deleting BR...
2018-04-30 20:45:08,897 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-30 20:45:08,898 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-30 20:45:08,898 - memory_profile6_log - INFO - ================================================

2018-04-30 20:45:08,898 - memory_profile6_log - INFO -    113    943.8 MiB    943.8 MiB   @profile

2018-04-30 20:45:08,901 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-30 20:45:08,901 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-30 20:45:08,901 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-30 20:45:08,904 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-30 20:45:08,904 - memory_profile6_log - INFO -    118                                      saveto="datastore", fitted_models_hist=None):

2018-04-30 20:45:08,905 - memory_profile6_log - INFO -    119                                 """

2018-04-30 20:45:08,905 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-30 20:45:08,907 - memory_profile6_log - INFO -    121                                 """

2018-04-30 20:45:08,907 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-30 20:45:08,907 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-30 20:45:08,907 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-30 20:45:08,907 - memory_profile6_log - INFO -    125    943.8 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-30 20:45:08,907 - memory_profile6_log - INFO -    126    951.7 MiB      7.9 MiB       df_dut = df_input.copy(deep=True)

2018-04-30 20:45:08,908 - memory_profile6_log - INFO -    127    951.7 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 20:45:08,908 - memory_profile6_log - INFO -    128                             

2018-04-30 20:45:08,908 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-30 20:45:08,910 - memory_profile6_log - INFO -    130   1014.2 MiB     62.5 MiB       df_dt = df_current.copy(deep=True)

2018-04-30 20:45:08,910 - memory_profile6_log - INFO -    131   1014.2 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-30 20:45:08,910 - memory_profile6_log - INFO -    132                             

2018-04-30 20:45:08,910 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-30 20:45:08,911 - memory_profile6_log - INFO -    134   1014.2 MiB      0.0 MiB       t0 = time.time()

2018-04-30 20:45:08,911 - memory_profile6_log - INFO -    135   1014.2 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-30 20:45:08,911 - memory_profile6_log - INFO -    136   1014.2 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-30 20:45:08,911 - memory_profile6_log - INFO -    137   1014.2 MiB      0.0 MiB       logger.info("apply on: %d total history...)", len(df_hist))

2018-04-30 20:45:08,911 - memory_profile6_log - INFO -    138                             

2018-04-30 20:45:08,913 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-30 20:45:08,913 - memory_profile6_log - INFO -    140   1014.2 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-30 20:45:08,914 - memory_profile6_log - INFO -    141                             

2018-04-30 20:45:08,914 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-30 20:45:08,914 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-30 20:45:08,921 - memory_profile6_log - INFO -    144   1015.6 MiB      1.4 MiB       NB = BR.processX(df_dut)

2018-04-30 20:45:08,921 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-30 20:45:08,924 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-30 20:45:08,924 - memory_profile6_log - INFO -    147   1024.9 MiB      9.3 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-30 20:45:08,924 - memory_profile6_log - INFO -    148                                 """

2018-04-30 20:45:08,926 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-30 20:45:08,927 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-30 20:45:08,927 - memory_profile6_log - INFO -    151                                 """

2018-04-30 20:45:08,927 - memory_profile6_log - INFO -    152   1024.9 MiB      0.0 MiB       fitby_sigmant = False

2018-04-30 20:45:08,928 - memory_profile6_log - INFO -    153   1024.9 MiB      0.0 MiB       uniques_fit_hist = None

2018-04-30 20:45:08,928 - memory_profile6_log - INFO -    154   1024.9 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-30 20:45:08,930 - memory_profile6_log - INFO -    155   1024.9 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-30 20:45:08,930 - memory_profile6_log - INFO -    156   1034.6 MiB      9.7 MiB                            'is_general']]

2018-04-30 20:45:08,930 - memory_profile6_log - INFO -    157                                                      

2018-04-30 20:45:08,931 - memory_profile6_log - INFO -    158                                 # get sigma_Nt from fitted_models_hist

2018-04-30 20:45:08,937 - memory_profile6_log - INFO -    159   1038.5 MiB      3.9 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-04-30 20:45:08,937 - memory_profile6_log - INFO -    160   1038.5 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-04-30 20:45:08,938 - memory_profile6_log - INFO -    161   1035.9 MiB     -2.6 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-04-30 20:45:08,940 - memory_profile6_log - INFO -    162   1035.9 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-04-30 20:45:08,940 - memory_profile6_log - INFO -    163                             

2018-04-30 20:45:08,940 - memory_profile6_log - INFO -    164                                 # begin fit

2018-04-30 20:45:08,940 - memory_profile6_log - INFO -    165   1035.9 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-30 20:45:08,941 - memory_profile6_log - INFO -    166   1035.9 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-30 20:45:08,941 - memory_profile6_log - INFO -    167   1063.5 MiB     27.6 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-04-30 20:45:08,943 - memory_profile6_log - INFO -    168   1063.5 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-30 20:45:08,943 - memory_profile6_log - INFO -    169   1063.5 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-30 20:45:08,944 - memory_profile6_log - INFO -    170                                 # print model_fit[['num_x', 'num_x', 'sigma_Nt', 'date_all_click']].loc[model_fit['user_id']=="1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157"].head(5)

2018-04-30 20:45:08,944 - memory_profile6_log - INFO -    171                             

2018-04-30 20:45:08,944 - memory_profile6_log - INFO -    172                                 # ~~ and Transform ~~

2018-04-30 20:45:08,944 - memory_profile6_log - INFO -    173                                 #   handling current news interest == current date

2018-04-30 20:45:08,946 - memory_profile6_log - INFO -    174   1063.5 MiB      0.0 MiB       if df_dt.empty:

2018-04-30 20:45:08,947 - memory_profile6_log - INFO -    175                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-30 20:45:08,947 - memory_profile6_log - INFO -    176                                     return None

2018-04-30 20:45:08,947 - memory_profile6_log - INFO -    177   1069.6 MiB      6.1 MiB       NB = BR.processX(df_dt)

2018-04-30 20:45:08,947 - memory_profile6_log - INFO -    178   1162.7 MiB     93.1 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-30 20:45:08,953 - memory_profile6_log - INFO -    179                             

2018-04-30 20:45:08,953 - memory_profile6_log - INFO -    180   1162.7 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-30 20:45:08,956 - memory_profile6_log - INFO -    181   1230.7 MiB     67.9 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-30 20:45:08,957 - memory_profile6_log - INFO -    182                             

2018-04-30 20:45:08,957 - memory_profile6_log - INFO -    183                                 # print "model_fit dtypes:\n", model_fit.dtypes

2018-04-30 20:45:08,959 - memory_profile6_log - INFO -    184                                 # print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-04-30 20:45:08,959 - memory_profile6_log - INFO -    185   1230.7 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-30 20:45:08,960 - memory_profile6_log - INFO -    186   1230.7 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-30 20:45:08,960 - memory_profile6_log - INFO -    187   1230.7 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-04-30 20:45:08,960 - memory_profile6_log - INFO -    188   1230.7 MiB      0.0 MiB                                                                                           "topic_id", "user_id",

2018-04-30 20:45:08,961 - memory_profile6_log - INFO -    189   1238.4 MiB      7.8 MiB                                                                                           "sigma_Nt"]],

2018-04-30 20:45:08,961 - memory_profile6_log - INFO -    190   1244.5 MiB      6.1 MiB                                                     verbose=False)

2018-04-30 20:45:08,961 - memory_profile6_log - INFO -    191                             

2018-04-30 20:45:08,963 - memory_profile6_log - INFO -    192                                 # ~~~ filter is general and specific topic ~~~

2018-04-30 20:45:08,963 - memory_profile6_log - INFO -    193                                 # the idea is just we need to rerank every topic according

2018-04-30 20:45:08,963 - memory_profile6_log - INFO -    194                                 #    to user_id and and is_general by p0_posterior

2018-04-30 20:45:08,964 - memory_profile6_log - INFO -    195   1244.5 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-30 20:45:08,969 - memory_profile6_log - INFO -    196   1246.5 MiB      1.9 MiB                                     'is_general']].groupby(['topic_id',

2018-04-30 20:45:08,969 - memory_profile6_log - INFO -    197   1244.6 MiB     -1.9 MiB                                                             'is_general']

2018-04-30 20:45:08,970 - memory_profile6_log - INFO -    198                                                                                      ).size().to_frame().reset_index()

2018-04-30 20:45:08,970 - memory_profile6_log - INFO -    199                             

2018-04-30 20:45:08,971 - memory_profile6_log - INFO -    200                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-30 20:45:08,973 - memory_profile6_log - INFO -    201                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-30 20:45:08,973 - memory_profile6_log - INFO -    202   1244.6 MiB      0.1 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-30 20:45:08,973 - memory_profile6_log - INFO -    203                             

2018-04-30 20:45:08,973 - memory_profile6_log - INFO -    204                                 # ~ start by provide rank for each topic type ~

2018-04-30 20:45:08,974 - memory_profile6_log - INFO -    205   1247.3 MiB      2.6 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-30 20:45:08,974 - memory_profile6_log - INFO -    206   1253.1 MiB      5.8 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-30 20:45:08,974 - memory_profile6_log - INFO -    207                             

2018-04-30 20:45:08,976 - memory_profile6_log - INFO -    208                                 # ~ set threshold to filter output

2018-04-30 20:45:08,976 - memory_profile6_log - INFO -    209   1253.1 MiB      0.0 MiB       if threshold > 0:

2018-04-30 20:45:08,976 - memory_profile6_log - INFO -    210   1253.1 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-30 20:45:08,976 - memory_profile6_log - INFO -    211   1253.1 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-30 20:45:08,976 - memory_profile6_log - INFO -    212   1252.7 MiB     -0.4 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-30 20:45:08,977 - memory_profile6_log - INFO -    213                             

2018-04-30 20:45:08,977 - memory_profile6_log - INFO -    214   1252.7 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-30 20:45:08,977 - memory_profile6_log - INFO -    215   1252.8 MiB      0.1 MiB       print model_transform[model_transform['user_id']=='1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157']

2018-04-30 20:45:08,979 - memory_profile6_log - INFO -    216   1252.8 MiB      0.0 MiB       logger.info("Len of model_transform: %d", len(model_transform))

2018-04-30 20:45:08,979 - memory_profile6_log - INFO -    217   1252.8 MiB      0.0 MiB       logger.info("Len of df_dt: %d", len(df_dt))

2018-04-30 20:45:08,980 - memory_profile6_log - INFO -    218   1252.8 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-30 20:45:08,980 - memory_profile6_log - INFO -    219                             

2018-04-30 20:45:08,980 - memory_profile6_log - INFO -    220   1252.8 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-30 20:45:08,986 - memory_profile6_log - INFO -    221                             

2018-04-30 20:45:08,986 - memory_profile6_log - INFO -    222   1252.8 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-30 20:45:08,987 - memory_profile6_log - INFO -    223   1252.8 MiB      0.0 MiB       del df_dut

2018-04-30 20:45:08,989 - memory_profile6_log - INFO -    224   1252.8 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-30 20:45:08,989 - memory_profile6_log - INFO -    225   1252.8 MiB      0.0 MiB       del df_dt

2018-04-30 20:45:08,989 - memory_profile6_log - INFO -    226   1252.8 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-30 20:45:08,990 - memory_profile6_log - INFO -    227   1252.8 MiB      0.0 MiB       del df_input

2018-04-30 20:45:08,990 - memory_profile6_log - INFO -    228   1252.8 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-30 20:45:08,990 - memory_profile6_log - INFO -    229   1175.2 MiB    -77.6 MiB       del df_input_X

2018-04-30 20:45:08,990 - memory_profile6_log - INFO -    230   1175.2 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-30 20:45:08,992 - memory_profile6_log - INFO -    231   1175.2 MiB      0.0 MiB       del df_current

2018-04-30 20:45:08,992 - memory_profile6_log - INFO -    232   1175.2 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-30 20:45:08,993 - memory_profile6_log - INFO -    233   1175.2 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-30 20:45:08,993 - memory_profile6_log - INFO -    234   1175.2 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-30 20:45:08,993 - memory_profile6_log - INFO -    235   1153.8 MiB    -21.3 MiB       del model_fit

2018-04-30 20:45:08,993 - memory_profile6_log - INFO -    236   1153.8 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-30 20:45:08,993 - memory_profile6_log - INFO -    237   1153.8 MiB      0.0 MiB       del result

2018-04-30 20:45:08,994 - memory_profile6_log - INFO -    238   1153.8 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-30 20:45:08,994 - memory_profile6_log - INFO -    239                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 20:45:08,994 - memory_profile6_log - INFO -    240                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 20:45:08,996 - memory_profile6_log - INFO -    241                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-30 20:45:08,996 - memory_profile6_log - INFO -    242   1153.8 MiB      0.0 MiB       if savetrain:

2018-04-30 20:45:08,996 - memory_profile6_log - INFO -    243   1159.3 MiB      5.4 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-30 20:45:09,000 - memory_profile6_log - INFO -    244   1159.3 MiB      0.0 MiB           del model_transform

2018-04-30 20:45:09,003 - memory_profile6_log - INFO -    245   1159.3 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-30 20:45:09,003 - memory_profile6_log - INFO -    246   1159.3 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-30 20:45:09,005 - memory_profile6_log - INFO -    247                             

2018-04-30 20:45:09,006 - memory_profile6_log - INFO -    248   1159.3 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-30 20:45:09,006 - memory_profile6_log - INFO -    249                                     # ~ Place your code to save the training model here ~

2018-04-30 20:45:09,006 - memory_profile6_log - INFO -    250   1159.3 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-30 20:45:09,006 - memory_profile6_log - INFO -    251   1159.3 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-30 20:45:09,009 - memory_profile6_log - INFO -    252   1159.3 MiB      0.0 MiB               if multproc:

2018-04-30 20:45:09,009 - memory_profile6_log - INFO -    253                                             # ~ save transform models ~

2018-04-30 20:45:09,009 - memory_profile6_log - INFO -    254   1159.3 MiB      0.0 MiB                   logger.info("Saving main Transform model to Google DataStore...")

2018-04-30 20:45:09,009 - memory_profile6_log - INFO -    255   1159.3 MiB      0.0 MiB                   logger.info("Saving total data: %d", len(model_transformsv))

2018-04-30 20:45:09,009 - memory_profile6_log - INFO -    256   1139.6 MiB    -19.6 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-30 20:45:09,010 - memory_profile6_log - INFO -    257                             

2018-04-30 20:45:09,010 - memory_profile6_log - INFO -    258                                             # ~ save fitted models ~

2018-04-30 20:45:09,012 - memory_profile6_log - INFO -    259   1139.6 MiB      0.0 MiB                   logger.info("Saving fitted_models as history to Google DataStore...")

2018-04-30 20:45:09,012 - memory_profile6_log - INFO -    260   1139.8 MiB      0.2 MiB                   save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-30 20:45:09,013 - memory_profile6_log - INFO -    261   1154.6 MiB     14.7 MiB                   fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-30 20:45:09,016 - memory_profile6_log - INFO -    262   1154.6 MiB      0.0 MiB                   logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-04-30 20:45:09,019 - memory_profile6_log - INFO -    263   1165.6 MiB     11.1 MiB                   X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-30 20:45:09,019 - memory_profile6_log - INFO -    264   1165.6 MiB      0.0 MiB                   logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-30 20:45:09,020 - memory_profile6_log - INFO -    265   1167.5 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-30 20:45:09,020 - memory_profile6_log - INFO -    266   1167.5 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-30 20:45:09,022 - memory_profile6_log - INFO -    267   1167.5 MiB      1.9 MiB                       mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-30 20:45:09,022 - memory_profile6_log - INFO -    268                             

2018-04-30 20:45:09,023 - memory_profile6_log - INFO -    269   1167.5 MiB      0.0 MiB                   del X_split

2018-04-30 20:45:09,023 - memory_profile6_log - INFO -    270   1167.5 MiB      0.0 MiB                   logger.info("deleting X_split...")

2018-04-30 20:45:09,023 - memory_profile6_log - INFO -    271   1167.5 MiB      0.0 MiB                   del save_sigma_nt

2018-04-30 20:45:09,023 - memory_profile6_log - INFO -    272   1167.5 MiB      0.0 MiB                   logger.info("deleting save_sigma_nt...")

2018-04-30 20:45:09,025 - memory_profile6_log - INFO -    273                             

2018-04-30 20:45:09,025 - memory_profile6_log - INFO -    274   1167.5 MiB      0.0 MiB                   del BR

2018-04-30 20:45:09,025 - memory_profile6_log - INFO -    275   1167.5 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-30 20:45:09,026 - memory_profile6_log - INFO -    276                             

2018-04-30 20:45:09,026 - memory_profile6_log - INFO -    277                                     elif str(saveto).lower() == "elastic":

2018-04-30 20:45:09,026 - memory_profile6_log - INFO -    278                                         logger.info("Using ElasticSearch as storage...")

2018-04-30 20:45:09,026 - memory_profile6_log - INFO -    279                                         logging.info("Saving main Transform model to Elasticsearch...")

2018-04-30 20:45:09,026 - memory_profile6_log - INFO -    280                                         mh.saveElasticS(model_transformsv)

2018-04-30 20:45:09,028 - memory_profile6_log - INFO -    281                             

2018-04-30 20:45:09,028 - memory_profile6_log - INFO -    282                                     # need save sigma_nt for daily train

2018-04-30 20:45:09,029 - memory_profile6_log - INFO -    283                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-30 20:45:09,033 - memory_profile6_log - INFO -    284                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-30 20:45:09,033 - memory_profile6_log - INFO -    285   1167.5 MiB      0.0 MiB           if start_date and end_date:

2018-04-30 20:45:09,036 - memory_profile6_log - INFO -    286                                         if not fitby_sigmant:

2018-04-30 20:45:09,036 - memory_profile6_log - INFO -    287                                             logging.info("Saving sigma Nt...")

2018-04-30 20:45:09,036 - memory_profile6_log - INFO -    288                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-30 20:45:09,038 - memory_profile6_log - INFO -    289                                             save_sigma_nt['start_date'] = start_date

2018-04-30 20:45:09,038 - memory_profile6_log - INFO -    290                                             save_sigma_nt['end_date'] = end_date

2018-04-30 20:45:09,039 - memory_profile6_log - INFO -    291                                             print save_sigma_nt.head(5)

2018-04-30 20:45:09,039 - memory_profile6_log - INFO -    292                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-30 20:45:09,039 - memory_profile6_log - INFO -    293                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-30 20:45:09,040 - memory_profile6_log - INFO -    294   1167.5 MiB      0.0 MiB       return

2018-04-30 20:45:09,040 - memory_profile6_log - INFO - 


2018-04-30 20:45:09,040 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-05-02 08:35:34,961 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 08:35:34,977 - memory_profile6_log - INFO - date_generated: 
2018-05-02 08:35:34,979 - memory_profile6_log - INFO -  
2018-05-02 08:35:34,980 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 8, 35, 34, 976000)]
2018-05-02 08:35:34,980 - memory_profile6_log - INFO - 

2018-05-02 08:35:34,997 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 08:35:34,999 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 08:35:35,000 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 08:35:35,157 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 08:35:35,161 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 08:37:07,688 - memory_profile6_log - INFO - size of df: 82.28 MB
2018-05-02 08:37:07,690 - memory_profile6_log - INFO - getting total: 324874 training data(genuine interest) for date: 2018-05-01
2018-05-02 08:37:07,736 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-05-02 08:37:07,737 - memory_profile6_log - INFO - Appending history data...
2018-05-02 08:37:07,739 - memory_profile6_log - INFO - processing batch-0
2018-05-02 08:37:07,740 - memory_profile6_log - INFO - creating list history data...
2018-05-02 08:37:07,844 - memory_profile6_log - INFO - call history data...
2018-05-02 08:37:55,365 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 08:37:56,601 - memory_profile6_log - INFO - processing batch-1
2018-05-02 08:37:56,602 - memory_profile6_log - INFO - creating list history data...
2018-05-02 08:37:56,674 - memory_profile6_log - INFO - call history data...
2018-05-02 08:38:40,180 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 08:38:41,171 - memory_profile6_log - INFO - processing batch-2
2018-05-02 08:38:41,171 - memory_profile6_log - INFO - creating list history data...
2018-05-02 08:38:41,237 - memory_profile6_log - INFO - call history data...
2018-05-02 08:39:25,796 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 08:39:26,803 - memory_profile6_log - INFO - processing batch-3
2018-05-02 08:39:26,805 - memory_profile6_log - INFO - creating list history data...
2018-05-02 08:39:26,869 - memory_profile6_log - INFO - call history data...
2018-05-02 08:40:08,072 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 08:40:08,760 - memory_profile6_log - INFO - processing batch-4
2018-05-02 08:40:08,763 - memory_profile6_log - INFO - creating list history data...
2018-05-02 08:40:08,823 - memory_profile6_log - INFO - call history data...
2018-05-02 08:40:56,214 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 08:40:56,604 - memory_profile6_log - INFO - Appending training data...
2018-05-02 08:40:56,605 - memory_profile6_log - INFO - len datalist: 1
2018-05-02 08:40:56,605 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-05-02 08:40:56,619 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 08:40:56,621 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 08:40:56,621 - memory_profile6_log - INFO - ================================================

2018-05-02 08:40:56,624 - memory_profile6_log - INFO -    319     87.1 MiB     87.1 MiB   @profile

2018-05-02 08:40:56,625 - memory_profile6_log - INFO -    320                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-05-02 08:40:56,627 - memory_profile6_log - INFO -    321     87.1 MiB      0.0 MiB       bq_client = client

2018-05-02 08:40:56,628 - memory_profile6_log - INFO -    322     87.1 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 08:40:56,630 - memory_profile6_log - INFO -    323                             

2018-05-02 08:40:56,631 - memory_profile6_log - INFO -    324     87.1 MiB      0.0 MiB       datalist = []

2018-05-02 08:40:56,631 - memory_profile6_log - INFO -    325     87.1 MiB      0.0 MiB       datalist_hist = []

2018-05-02 08:40:56,631 - memory_profile6_log - INFO -    326                             

2018-05-02 08:40:56,632 - memory_profile6_log - INFO -    327     87.1 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-05-02 08:40:56,635 - memory_profile6_log - INFO -    328    402.9 MiB      0.0 MiB       for ndate in date_generated:

2018-05-02 08:40:56,638 - memory_profile6_log - INFO -    329    366.1 MiB    279.0 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-05-02 08:40:56,640 - memory_profile6_log - INFO -    330    366.1 MiB      0.0 MiB           if tframe is not None:

2018-05-02 08:40:56,641 - memory_profile6_log - INFO -    331    366.1 MiB      0.0 MiB               if not tframe.empty:

2018-05-02 08:40:56,641 - memory_profile6_log - INFO -    332    376.3 MiB     10.1 MiB                   X_split = np.array_split(tframe, 5)

2018-05-02 08:40:56,642 - memory_profile6_log - INFO -    333    376.3 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 08:40:56,644 - memory_profile6_log - INFO -    334    376.3 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-05-02 08:40:56,644 - memory_profile6_log - INFO -    335    403.4 MiB     -1.0 MiB                   for ix in range(len(X_split)):

2018-05-02 08:40:56,648 - memory_profile6_log - INFO -    336                                                 # ~ loading history

2018-05-02 08:40:56,648 - memory_profile6_log - INFO -    337                                                 """

2018-05-02 08:40:56,651 - memory_profile6_log - INFO -    338                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-05-02 08:40:56,651 - memory_profile6_log - INFO -    339                                                 """

2018-05-02 08:40:56,651 - memory_profile6_log - INFO -    340    403.4 MiB     -0.5 MiB                       logger.info("processing batch-%d", ix)

2018-05-02 08:40:56,653 - memory_profile6_log - INFO -    341                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-05-02 08:40:56,654 - memory_profile6_log - INFO -    342    403.4 MiB     -0.5 MiB                       logger.info("creating list history data...")

2018-05-02 08:40:56,654 - memory_profile6_log - INFO -    343                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-05-02 08:40:56,655 - memory_profile6_log - INFO -    344    403.4 MiB      3.3 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 08:40:56,658 - memory_profile6_log - INFO -    345                             

2018-05-02 08:40:56,660 - memory_profile6_log - INFO -    346    403.4 MiB     -0.5 MiB                       logger.info("call history data...")

2018-05-02 08:40:56,661 - memory_profile6_log - INFO -    347    422.0 MiB     65.6 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-05-02 08:40:56,661 - memory_profile6_log - INFO -    348                             

2018-05-02 08:40:56,663 - memory_profile6_log - INFO -    349                                                 # me = os.getpid()

2018-05-02 08:40:56,664 - memory_profile6_log - INFO -    350                                                 # kill_proc_tree(me)

2018-05-02 08:40:56,664 - memory_profile6_log - INFO -    351                             

2018-05-02 08:40:56,664 - memory_profile6_log - INFO -    352    422.0 MiB    -68.5 MiB                       logger.info("done collecting history data, appending now...")

2018-05-02 08:40:56,665 - memory_profile6_log - INFO -    353    422.7 MiB -10731.0 MiB                       for m in h_frame:

2018-05-02 08:40:56,667 - memory_profile6_log - INFO -    354    422.7 MiB -10663.2 MiB                           if m is not None:

2018-05-02 08:40:56,671 - memory_profile6_log - INFO -    355    422.7 MiB -10660.9 MiB                               if len(m) > 0:

2018-05-02 08:40:56,673 - memory_profile6_log - INFO -    356    422.7 MiB  -8644.6 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-05-02 08:40:56,674 - memory_profile6_log - INFO -    357    403.4 MiB   -112.1 MiB                       del h_frame

2018-05-02 08:40:56,676 - memory_profile6_log - INFO -    358    403.4 MiB     -4.3 MiB                       del lhistory

2018-05-02 08:40:56,677 - memory_profile6_log - INFO -    359                             

2018-05-02 08:40:56,677 - memory_profile6_log - INFO -    360    402.9 MiB     -0.5 MiB                   logger.info("Appending training data...")

2018-05-02 08:40:56,678 - memory_profile6_log - INFO -    361    402.9 MiB      0.0 MiB                   datalist.append(tframe)

2018-05-02 08:40:56,680 - memory_profile6_log - INFO -    362                                     else: 

2018-05-02 08:40:56,683 - memory_profile6_log - INFO -    363                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-05-02 08:40:56,684 - memory_profile6_log - INFO -    364    402.9 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-05-02 08:40:56,686 - memory_profile6_log - INFO -    365    402.9 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-05-02 08:40:56,687 - memory_profile6_log - INFO -    366                             

2018-05-02 08:40:56,688 - memory_profile6_log - INFO -    367    402.9 MiB      0.0 MiB       return datalist, datalist_hist

2018-05-02 08:40:56,690 - memory_profile6_log - INFO - 


2018-05-02 08:40:57,723 - memory_profile6_log - INFO - size of big_frame_hist: 27.00 MB
2018-05-02 08:40:57,806 - memory_profile6_log - INFO - size of big_frame: 82.28 MB
2018-05-02 08:40:57,822 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-05-02 08:43:33,779 - memory_profile6_log - INFO - size of df: 198.61 MB
2018-05-02 08:43:33,780 - memory_profile6_log - INFO - getting total: 812942 training data(current date interest)
2018-05-02 08:43:33,976 - memory_profile6_log - INFO - size of current_frame: 204.81 MB
2018-05-02 08:43:33,977 - memory_profile6_log - INFO - loading time of: 1137816 total genuine-current interest data ~ take 478.851s
2018-05-02 08:43:33,989 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 08:43:33,990 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 08:43:33,990 - memory_profile6_log - INFO - ================================================

2018-05-02 08:43:33,990 - memory_profile6_log - INFO -    369     87.0 MiB     87.0 MiB   @profile

2018-05-02 08:43:33,992 - memory_profile6_log - INFO -    370                             def preprocess(cpu, cd, query_fit, date_generated):

2018-05-02 08:43:33,993 - memory_profile6_log - INFO -    371     87.1 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-05-02 08:43:33,993 - memory_profile6_log - INFO -    372     87.1 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 08:43:33,993 - memory_profile6_log - INFO -    373                             

2018-05-02 08:43:33,993 - memory_profile6_log - INFO -    374                                 # ~~~ Begin collecting data ~~~

2018-05-02 08:43:33,994 - memory_profile6_log - INFO -    375     87.1 MiB      0.0 MiB       t0 = time.time()

2018-05-02 08:43:33,994 - memory_profile6_log - INFO -    376    378.1 MiB    291.0 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-05-02 08:43:33,997 - memory_profile6_log - INFO -    377    378.1 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-05-02 08:43:34,000 - memory_profile6_log - INFO -    378                                     logger.info("Training cannot be empty..")

2018-05-02 08:43:34,000 - memory_profile6_log - INFO -    379                                     return False

2018-05-02 08:43:34,002 - memory_profile6_log - INFO -    380    385.8 MiB      7.7 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-05-02 08:43:34,002 - memory_profile6_log - INFO -    381                                 # print "big_frame_hist:\n", big_frame_hist.head(20)

2018-05-02 08:43:34,003 - memory_profile6_log - INFO -    382    385.8 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-05-02 08:43:34,003 - memory_profile6_log - INFO -    383                             

2018-05-02 08:43:34,003 - memory_profile6_log - INFO -    384    393.5 MiB      7.8 MiB       big_frame = pd.concat(datalist)

2018-05-02 08:43:34,005 - memory_profile6_log - INFO -    385    393.5 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-05-02 08:43:34,006 - memory_profile6_log - INFO -    386    386.1 MiB     -7.4 MiB       del datalist

2018-05-02 08:43:34,006 - memory_profile6_log - INFO -    387                             

2018-05-02 08:43:34,009 - memory_profile6_log - INFO -    388    386.1 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 08:43:34,010 - memory_profile6_log - INFO -    389                             

2018-05-02 08:43:34,012 - memory_profile6_log - INFO -    390                                 # ~ get current news interest ~

2018-05-02 08:43:34,012 - memory_profile6_log - INFO -    391    386.1 MiB      0.0 MiB       if not cd:

2018-05-02 08:43:34,013 - memory_profile6_log - INFO -    392    386.1 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-05-02 08:43:34,013 - memory_profile6_log - INFO -    393    605.1 MiB    219.0 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-05-02 08:43:34,013 - memory_profile6_log - INFO -    394    605.1 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-05-02 08:43:34,015 - memory_profile6_log - INFO -    395                                 else:

2018-05-02 08:43:34,015 - memory_profile6_log - INFO -    396                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-05-02 08:43:34,016 - memory_profile6_log - INFO -    397                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-05-02 08:43:34,016 - memory_profile6_log - INFO -    398                             

2018-05-02 08:43:34,016 - memory_profile6_log - INFO -    399                                     # safe handling of query parameter

2018-05-02 08:43:34,016 - memory_profile6_log - INFO -    400                                     query_params = [

2018-05-02 08:43:34,016 - memory_profile6_log - INFO -    401                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-05-02 08:43:34,016 - memory_profile6_log - INFO -    402                                     ]

2018-05-02 08:43:34,020 - memory_profile6_log - INFO -    403                             

2018-05-02 08:43:34,025 - memory_profile6_log - INFO -    404                                     job_config.query_parameters = query_params

2018-05-02 08:43:34,025 - memory_profile6_log - INFO -    405                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-05-02 08:43:34,026 - memory_profile6_log - INFO -    406                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-05-02 08:43:34,026 - memory_profile6_log - INFO -    407                             

2018-05-02 08:43:34,026 - memory_profile6_log - INFO -    408    611.4 MiB      6.2 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-05-02 08:43:34,026 - memory_profile6_log - INFO -    409    611.4 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-05-02 08:43:34,028 - memory_profile6_log - INFO -    410    611.4 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-05-02 08:43:34,029 - memory_profile6_log - INFO -    411    611.4 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-05-02 08:43:34,029 - memory_profile6_log - INFO -    412    611.4 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 08:43:34,032 - memory_profile6_log - INFO -    413    611.4 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-05-02 08:43:34,033 - memory_profile6_log - INFO -    414                             

2018-05-02 08:43:34,035 - memory_profile6_log - INFO -    415    611.4 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-05-02 08:43:34,035 - memory_profile6_log - INFO - 


2018-05-02 08:43:34,039 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-05-02 08:43:34,099 - memory_profile6_log - INFO - train on: 324874 total genuine interest data(D(u, t))
2018-05-02 08:43:34,101 - memory_profile6_log - INFO - transform on: 812942 total current data(D(t))
2018-05-02 08:43:34,101 - memory_profile6_log - INFO - apply on: 99031 total history...)
2018-05-02 08:43:34,405 - memory_profile6_log - INFO - len of uniques_fit_hist:99031
2018-05-02 08:43:34,464 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:28983
2018-05-02 08:43:35,032 - memory_profile6_log - INFO - self.sum_all_nt before concate:

2018-05-02 08:43:35,076 - memory_profile6_log - INFO -                                                  user_id  sigma_Nt
15052  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        27
2018-05-02 08:43:35,078 - memory_profile6_log - INFO - 

2018-05-02 08:43:35,145 - memory_profile6_log - INFO - self.sum_all_nt after concate:

2018-05-02 08:43:35,168 - memory_profile6_log - INFO -                                                  user_id  sigma_Nt
15052  1616f009d96b1-0285d8288a5bce-70217860-38400-16...      1288
2018-05-02 08:43:35,171 - memory_profile6_log - INFO - 

2018-05-02 08:43:35,470 - memory_profile6_log - INFO - Len of model_fit: 324874
2018-05-02 08:43:35,473 - memory_profile6_log - INFO - Len of df_dut: 324874
2018-05-02 08:43:37,010 - memory_profile6_log - INFO - Len of fitted_models on main class: 324874
2018-05-02 08:43:37,012 - memory_profile6_log - INFO - 

2018-05-02 08:43:37,013 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-05-02 08:43:37,013 - memory_profile6_log - INFO - 

2018-05-02 08:43:37,015 - memory_profile6_log - INFO - fitted_model_hist:

2018-05-02 08:43:37,016 - memory_profile6_log - INFO - 

2018-05-02 08:43:37,072 - memory_profile6_log - INFO -      pt_posterior_x_Nt   topic_id                                            user_id
44           15.991521   22553543  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
157          83.719466   22601470  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
40          516.753870   27313197  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
185          84.916311   27431099  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
151          28.579819   33020303  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
194          94.876510   39301645  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
34           67.425244   41639090  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
43           15.283089  107151123  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
2018-05-02 08:43:37,072 - memory_profile6_log - INFO - 

2018-05-02 08:43:37,128 - memory_profile6_log - INFO - len of fitted models after concat: 423905
2018-05-02 08:43:37,128 - memory_profile6_log - INFO - 

2018-05-02 08:43:37,130 - memory_profile6_log - INFO - Recalculating fitted models...
2018-05-02 08:43:37,131 - memory_profile6_log - INFO - 

2018-05-02 08:43:37,411 - memory_profile6_log - INFO - fitted_models after cobine:

2018-05-02 08:43:37,413 - memory_profile6_log - INFO - 

2018-05-02 08:43:37,469 - memory_profile6_log - INFO -      pt_posterior_x_Nt   topic_id                                            user_id
44           15.991521   22553543  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
157          83.719466   22601470  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
40          516.753870   27313197  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
185          84.916311   27431099  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
151          28.579819   33020303  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
194          94.876510   39301645  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
34           67.425244   41639090  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
43           15.283089  107151123  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
2018-05-02 08:43:37,470 - memory_profile6_log - INFO - 

2018-05-02 08:43:37,470 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 324874
2018-05-02 08:43:37,473 - memory_profile6_log - INFO - 

2018-05-02 08:44:18,076 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-05-02 08:44:18,305 - memory_profile6_log - INFO -                                                   user_id           topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt  p0_posterior  is_general  rank
168616  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22601470         180.583539             190.583539   0.071150      1288      0.010447        True   1.0
168618  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27431099         188.011932             198.011932   0.064369      1288      0.009820        True   2.0
168615  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22553543          38.386972              48.386972   0.131969      1288      0.004920        True   3.0
168623  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           33020303          93.333873             103.333873   0.020059      1288      0.001597        True   4.0
168617  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27313197        1059.941030            1069.941030   0.000415      1288      0.000342        True   5.0
168624  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           39301645         351.528952             361.528952   0.017232      1288      0.004800       False   1.0
168612  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1149084290        3395.843537            3405.843537   0.000584      1288      0.001533       False   2.0
168611  1616f009d96b1-0285d8288a5bce-70217860-38400-16...          107151123         106.692904             116.692904   0.010699      1288      0.000962       False   3.0
168627  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           62323193         776.343701             786.343701   0.000997      1288      0.000604       False   4.0
168608  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1023751122         771.544049             781.544049   0.000999      1288      0.000602       False   5.0
168625  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           39825972          33.653947              43.653947   0.016376      1288      0.000551       False   6.0
168609  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1037574949          20.851671              30.851671   0.018079      1288      0.000430       False   7.0
168622  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790314285         102.376743             112.376743   0.004905      1288      0.000425       False   8.0
168614  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1161375937          56.687372              66.687372   0.006509      1288      0.000334       False   9.0
168610  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1038512181          71.312714              81.312714   0.004241      1288      0.000266       False  10.0
168613  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1157057159          66.108992              76.108992   0.003970      1288      0.000233       False  11.0
168620  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790313523        2655.260638            2665.260638   0.000067      1288      0.000138       False  12.0
168626  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           41639090        1067.804002            1077.804002   0.000145      1288      0.000120       False  13.0
168619  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790312980        5254.621053            5264.621053   0.000017      1288      0.000070       False  14.0
168621  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790314267         597.116029             607.116029   0.000051      1288      0.000024       False  15.0
2018-05-02 08:44:18,306 - memory_profile6_log - INFO - 

2018-05-02 08:44:18,308 - memory_profile6_log - INFO - Len of model_transform: 304803
2018-05-02 08:44:18,309 - memory_profile6_log - INFO - Len of df_dt: 812942
2018-05-02 08:44:18,311 - memory_profile6_log - INFO - Total train time: 44.012s
2018-05-02 08:44:18,312 - memory_profile6_log - INFO - memory left before cleaning: 83.900 percent memory...
2018-05-02 08:44:18,312 - memory_profile6_log - INFO - cleaning up some objects...
2018-05-02 08:44:18,313 - memory_profile6_log - INFO - deleting df_dut...
2018-05-02 08:44:18,318 - memory_profile6_log - INFO - deleting df_dt...
2018-05-02 08:44:18,319 - memory_profile6_log - INFO - deleting df_input...
2018-05-02 08:44:18,348 - memory_profile6_log - INFO - deleting df_input_X...
2018-05-02 08:44:18,349 - memory_profile6_log - INFO - deleting df_current...
2018-05-02 08:44:18,351 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-05-02 08:44:18,367 - memory_profile6_log - INFO - deleting model_fit...
2018-05-02 08:44:18,368 - memory_profile6_log - INFO - deleting result...
2018-05-02 08:44:18,391 - memory_profile6_log - INFO - deleting model_transform...
2018-05-02 08:44:18,391 - memory_profile6_log - INFO - memory left after cleaning: 83.600 percent memory...
2018-05-02 08:44:18,394 - memory_profile6_log - INFO - Begin saving trained data...
2018-05-02 08:44:18,394 - memory_profile6_log - INFO - Using ElasticSearch as storage...
2018-05-02 08:47:50,621 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 08:47:50,625 - memory_profile6_log - INFO - date_generated: 
2018-05-02 08:47:50,625 - memory_profile6_log - INFO -  
2018-05-02 08:47:50,627 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 8, 47, 50, 624000)]
2018-05-02 08:47:50,627 - memory_profile6_log - INFO - 

2018-05-02 08:47:50,628 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 08:47:50,628 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 08:47:50,628 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 08:47:50,769 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 08:47:50,773 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 08:49:19,516 - memory_profile6_log - INFO - size of df: 82.28 MB
2018-05-02 08:49:19,519 - memory_profile6_log - INFO - getting total: 324874 training data(genuine interest) for date: 2018-05-01
2018-05-02 08:49:19,568 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-05-02 08:49:19,569 - memory_profile6_log - INFO - Appending history data...
2018-05-02 08:49:19,575 - memory_profile6_log - INFO - processing batch-0
2018-05-02 08:49:19,575 - memory_profile6_log - INFO - creating list history data...
2018-05-02 08:49:19,673 - memory_profile6_log - INFO - call history data...
2018-05-02 08:50:42,966 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 08:50:44,069 - memory_profile6_log - INFO - processing batch-1
2018-05-02 08:50:44,071 - memory_profile6_log - INFO - creating list history data...
2018-05-02 08:50:44,130 - memory_profile6_log - INFO - call history data...
2018-05-02 08:52:22,371 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 08:52:23,460 - memory_profile6_log - INFO - processing batch-2
2018-05-02 08:52:23,460 - memory_profile6_log - INFO - creating list history data...
2018-05-02 08:52:23,528 - memory_profile6_log - INFO - call history data...
2018-05-02 08:53:50,828 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 08:53:51,845 - memory_profile6_log - INFO - processing batch-3
2018-05-02 08:53:51,845 - memory_profile6_log - INFO - creating list history data...
2018-05-02 08:53:51,905 - memory_profile6_log - INFO - call history data...
2018-05-02 08:55:12,937 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 08:55:13,622 - memory_profile6_log - INFO - processing batch-4
2018-05-02 08:55:13,625 - memory_profile6_log - INFO - creating list history data...
2018-05-02 08:55:13,686 - memory_profile6_log - INFO - call history data...
2018-05-02 08:56:33,713 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 08:56:34,130 - memory_profile6_log - INFO - Appending training data...
2018-05-02 08:56:34,131 - memory_profile6_log - INFO - len datalist: 1
2018-05-02 08:56:34,132 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-05-02 08:56:34,147 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 08:56:34,148 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 08:56:34,150 - memory_profile6_log - INFO - ================================================

2018-05-02 08:56:34,151 - memory_profile6_log - INFO -    343     87.1 MiB     87.1 MiB   @profile

2018-05-02 08:56:34,154 - memory_profile6_log - INFO -    344                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-05-02 08:56:34,155 - memory_profile6_log - INFO -    345     87.1 MiB      0.0 MiB       bq_client = client

2018-05-02 08:56:34,157 - memory_profile6_log - INFO -    346     87.1 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 08:56:34,157 - memory_profile6_log - INFO -    347                             

2018-05-02 08:56:34,158 - memory_profile6_log - INFO -    348     87.1 MiB      0.0 MiB       datalist = []

2018-05-02 08:56:34,161 - memory_profile6_log - INFO -    349     87.1 MiB      0.0 MiB       datalist_hist = []

2018-05-02 08:56:34,161 - memory_profile6_log - INFO -    350                             

2018-05-02 08:56:34,164 - memory_profile6_log - INFO -    351     87.1 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-05-02 08:56:34,164 - memory_profile6_log - INFO -    352    404.1 MiB      0.0 MiB       for ndate in date_generated:

2018-05-02 08:56:34,165 - memory_profile6_log - INFO -    353    365.8 MiB    278.7 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-05-02 08:56:34,167 - memory_profile6_log - INFO -    354    365.8 MiB      0.0 MiB           if tframe is not None:

2018-05-02 08:56:34,167 - memory_profile6_log - INFO -    355    365.8 MiB      0.0 MiB               if not tframe.empty:

2018-05-02 08:56:34,167 - memory_profile6_log - INFO -    356    375.9 MiB     10.1 MiB                   X_split = np.array_split(tframe, 5)

2018-05-02 08:56:34,171 - memory_profile6_log - INFO -    357    375.9 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 08:56:34,174 - memory_profile6_log - INFO -    358    375.9 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-05-02 08:56:34,174 - memory_profile6_log - INFO -    359    405.7 MiB     -3.5 MiB                   for ix in range(len(X_split)):

2018-05-02 08:56:34,174 - memory_profile6_log - INFO -    360                                                 # ~ loading history

2018-05-02 08:56:34,176 - memory_profile6_log - INFO -    361                                                 """

2018-05-02 08:56:34,177 - memory_profile6_log - INFO -    362                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-05-02 08:56:34,177 - memory_profile6_log - INFO -    363                                                 """

2018-05-02 08:56:34,178 - memory_profile6_log - INFO -    364    405.7 MiB     -1.9 MiB                       logger.info("processing batch-%d", ix)

2018-05-02 08:56:34,180 - memory_profile6_log - INFO -    365                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-05-02 08:56:34,184 - memory_profile6_log - INFO -    366    405.7 MiB     -1.9 MiB                       logger.info("creating list history data...")

2018-05-02 08:56:34,186 - memory_profile6_log - INFO -    367                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-05-02 08:56:34,187 - memory_profile6_log - INFO -    368    404.7 MiB      0.1 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 08:56:34,188 - memory_profile6_log - INFO -    369                             

2018-05-02 08:56:34,190 - memory_profile6_log - INFO -    370    404.7 MiB     -0.9 MiB                       logger.info("call history data...")

2018-05-02 08:56:34,190 - memory_profile6_log - INFO -    371    421.7 MiB     65.3 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-05-02 08:56:34,190 - memory_profile6_log - INFO -    372                             

2018-05-02 08:56:34,191 - memory_profile6_log - INFO -    373                                                 # me = os.getpid()

2018-05-02 08:56:34,196 - memory_profile6_log - INFO -    374                                                 # kill_proc_tree(me)

2018-05-02 08:56:34,196 - memory_profile6_log - INFO -    375                             

2018-05-02 08:56:34,197 - memory_profile6_log - INFO -    376    421.7 MiB    -64.4 MiB                       logger.info("done collecting history data, appending now...")

2018-05-02 08:56:34,197 - memory_profile6_log - INFO -    377    422.3 MiB -10040.6 MiB                       for m in h_frame:

2018-05-02 08:56:34,198 - memory_profile6_log - INFO -    378    422.3 MiB  -9976.7 MiB                           if m is not None:

2018-05-02 08:56:34,200 - memory_profile6_log - INFO -    379    422.3 MiB  -9976.7 MiB                               if len(m) > 0:

2018-05-02 08:56:34,200 - memory_profile6_log - INFO -    380    422.3 MiB  -8117.1 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-05-02 08:56:34,200 - memory_profile6_log - INFO -    381    405.7 MiB   -106.6 MiB                       del h_frame

2018-05-02 08:56:34,203 - memory_profile6_log - INFO -    382    405.7 MiB     -4.5 MiB                       del lhistory

2018-05-02 08:56:34,203 - memory_profile6_log - INFO -    383                             

2018-05-02 08:56:34,207 - memory_profile6_log - INFO -    384    404.1 MiB     -1.6 MiB                   logger.info("Appending training data...")

2018-05-02 08:56:34,209 - memory_profile6_log - INFO -    385    404.1 MiB      0.0 MiB                   datalist.append(tframe)

2018-05-02 08:56:34,210 - memory_profile6_log - INFO -    386                                     else: 

2018-05-02 08:56:34,210 - memory_profile6_log - INFO -    387                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-05-02 08:56:34,211 - memory_profile6_log - INFO -    388    404.1 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-05-02 08:56:34,213 - memory_profile6_log - INFO -    389    404.1 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-05-02 08:56:34,213 - memory_profile6_log - INFO -    390                             

2018-05-02 08:56:34,217 - memory_profile6_log - INFO -    391    404.1 MiB      0.0 MiB       return datalist, datalist_hist

2018-05-02 08:56:34,220 - memory_profile6_log - INFO - 


2018-05-02 08:56:35,186 - memory_profile6_log - INFO - size of big_frame_hist: 27.00 MB
2018-05-02 08:56:35,276 - memory_profile6_log - INFO - size of big_frame: 82.28 MB
2018-05-02 08:56:35,292 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-05-02 08:59:01,447 - memory_profile6_log - INFO - size of df: 198.61 MB
2018-05-02 08:59:01,448 - memory_profile6_log - INFO - getting total: 812942 training data(current date interest)
2018-05-02 08:59:01,650 - memory_profile6_log - INFO - size of current_frame: 204.81 MB
2018-05-02 08:59:01,651 - memory_profile6_log - INFO - loading time of: 1137816 total genuine-current interest data ~ take 670.905s
2018-05-02 08:59:01,663 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 08:59:01,663 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 08:59:01,664 - memory_profile6_log - INFO - ================================================

2018-05-02 08:59:01,665 - memory_profile6_log - INFO -    393     87.0 MiB     87.0 MiB   @profile

2018-05-02 08:59:01,665 - memory_profile6_log - INFO -    394                             def preprocess(cpu, cd, query_fit, date_generated):

2018-05-02 08:59:01,667 - memory_profile6_log - INFO -    395     87.1 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-05-02 08:59:01,667 - memory_profile6_log - INFO -    396     87.1 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 08:59:01,667 - memory_profile6_log - INFO -    397                             

2018-05-02 08:59:01,667 - memory_profile6_log - INFO -    398                                 # ~~~ Begin collecting data ~~~

2018-05-02 08:59:01,668 - memory_profile6_log - INFO -    399     87.1 MiB      0.0 MiB       t0 = time.time()

2018-05-02 08:59:01,673 - memory_profile6_log - INFO -    400    377.1 MiB    290.0 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-05-02 08:59:01,674 - memory_profile6_log - INFO -    401    377.1 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-05-02 08:59:01,674 - memory_profile6_log - INFO -    402                                     logger.info("Training cannot be empty..")

2018-05-02 08:59:01,674 - memory_profile6_log - INFO -    403                                     return False

2018-05-02 08:59:01,676 - memory_profile6_log - INFO -    404    385.2 MiB      8.1 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-05-02 08:59:01,676 - memory_profile6_log - INFO -    405                                 # print "big_frame_hist:\n", big_frame_hist.head(20)

2018-05-02 08:59:01,677 - memory_profile6_log - INFO -    406    385.2 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-05-02 08:59:01,677 - memory_profile6_log - INFO -    407                             

2018-05-02 08:59:01,677 - memory_profile6_log - INFO -    408    393.0 MiB      7.8 MiB       big_frame = pd.concat(datalist)

2018-05-02 08:59:01,678 - memory_profile6_log - INFO -    409    393.0 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-05-02 08:59:01,678 - memory_profile6_log - INFO -    410    385.5 MiB     -7.4 MiB       del datalist

2018-05-02 08:59:01,680 - memory_profile6_log - INFO -    411                             

2018-05-02 08:59:01,680 - memory_profile6_log - INFO -    412    385.5 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 08:59:01,680 - memory_profile6_log - INFO -    413                             

2018-05-02 08:59:01,680 - memory_profile6_log - INFO -    414                                 # ~ get current news interest ~

2018-05-02 08:59:01,680 - memory_profile6_log - INFO -    415    385.5 MiB      0.0 MiB       if not cd:

2018-05-02 08:59:01,684 - memory_profile6_log - INFO -    416    385.5 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-05-02 08:59:01,687 - memory_profile6_log - INFO -    417    603.6 MiB    218.0 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-05-02 08:59:01,687 - memory_profile6_log - INFO -    418    603.6 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-05-02 08:59:01,688 - memory_profile6_log - INFO -    419                                 else:

2018-05-02 08:59:01,688 - memory_profile6_log - INFO -    420                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-05-02 08:59:01,690 - memory_profile6_log - INFO -    421                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-05-02 08:59:01,690 - memory_profile6_log - INFO -    422                             

2018-05-02 08:59:01,690 - memory_profile6_log - INFO -    423                                     # safe handling of query parameter

2018-05-02 08:59:01,690 - memory_profile6_log - INFO -    424                                     query_params = [

2018-05-02 08:59:01,690 - memory_profile6_log - INFO -    425                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-05-02 08:59:01,691 - memory_profile6_log - INFO -    426                                     ]

2018-05-02 08:59:01,691 - memory_profile6_log - INFO -    427                             

2018-05-02 08:59:01,693 - memory_profile6_log - INFO -    428                                     job_config.query_parameters = query_params

2018-05-02 08:59:01,693 - memory_profile6_log - INFO -    429                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-05-02 08:59:01,696 - memory_profile6_log - INFO -    430                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-05-02 08:59:01,698 - memory_profile6_log - INFO -    431                             

2018-05-02 08:59:01,700 - memory_profile6_log - INFO -    432    609.8 MiB      6.2 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-05-02 08:59:01,700 - memory_profile6_log - INFO -    433    609.8 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-05-02 08:59:01,700 - memory_profile6_log - INFO -    434    609.8 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-05-02 08:59:01,700 - memory_profile6_log - INFO -    435    609.8 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-05-02 08:59:01,700 - memory_profile6_log - INFO -    436    609.8 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 08:59:01,701 - memory_profile6_log - INFO -    437    609.8 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-05-02 08:59:01,701 - memory_profile6_log - INFO -    438                             

2018-05-02 08:59:01,701 - memory_profile6_log - INFO -    439    609.8 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-05-02 08:59:01,703 - memory_profile6_log - INFO - 


2018-05-02 08:59:01,707 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-05-02 08:59:01,772 - memory_profile6_log - INFO - train on: 324874 total genuine interest data(D(u, t))
2018-05-02 08:59:01,773 - memory_profile6_log - INFO - transform on: 812942 total current data(D(t))
2018-05-02 08:59:01,776 - memory_profile6_log - INFO - apply on: 99031 total history...)
2018-05-02 08:59:02,102 - memory_profile6_log - INFO - len of uniques_fit_hist:99031
2018-05-02 08:59:02,171 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:28983
2018-05-02 08:59:02,795 - memory_profile6_log - INFO - self.sum_all_nt before concate:

2018-05-02 08:59:02,826 - memory_profile6_log - INFO -                                                  user_id  sigma_Nt
15052  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        27
2018-05-02 08:59:02,828 - memory_profile6_log - INFO - 

2018-05-02 08:59:02,890 - memory_profile6_log - INFO - self.sum_all_nt after concate:

2018-05-02 08:59:02,914 - memory_profile6_log - INFO -                                                  user_id  sigma_Nt
15052  1616f009d96b1-0285d8288a5bce-70217860-38400-16...      1288
2018-05-02 08:59:02,915 - memory_profile6_log - INFO - 

2018-05-02 08:59:03,197 - memory_profile6_log - INFO - Len of model_fit: 324874
2018-05-02 08:59:03,198 - memory_profile6_log - INFO - Len of df_dut: 324874
2018-05-02 08:59:04,838 - memory_profile6_log - INFO - Len of fitted_models on main class: 324874
2018-05-02 08:59:04,839 - memory_profile6_log - INFO - 

2018-05-02 08:59:04,841 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-05-02 08:59:04,842 - memory_profile6_log - INFO - 

2018-05-02 08:59:04,842 - memory_profile6_log - INFO - fitted_model_hist:

2018-05-02 08:59:04,844 - memory_profile6_log - INFO - 

2018-05-02 08:59:04,900 - memory_profile6_log - INFO -      pt_posterior_x_Nt   topic_id                                            user_id
99           15.991521   22553543  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
89           83.719466   22601470  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
55          516.753870   27313197  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
41           84.916311   27431099  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
179          28.579819   33020303  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
192          94.876510   39301645  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
32           67.425244   41639090  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
50           15.283089  107151123  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
2018-05-02 08:59:04,901 - memory_profile6_log - INFO - 

2018-05-02 08:59:04,953 - memory_profile6_log - INFO - len of fitted models after concat: 423905
2018-05-02 08:59:04,953 - memory_profile6_log - INFO - 

2018-05-02 08:59:04,954 - memory_profile6_log - INFO - Recalculating fitted models...
2018-05-02 08:59:04,956 - memory_profile6_log - INFO - 

2018-05-02 08:59:05,220 - memory_profile6_log - INFO - fitted_models after cobine:

2018-05-02 08:59:05,223 - memory_profile6_log - INFO - 

2018-05-02 08:59:05,278 - memory_profile6_log - INFO -      pt_posterior_x_Nt   topic_id                                            user_id
99           15.991521   22553543  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
89           83.719466   22601470  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
55          516.753870   27313197  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
41           84.916311   27431099  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
179          28.579819   33020303  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
192          94.876510   39301645  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
32           67.425244   41639090  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
50           15.283089  107151123  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
2018-05-02 08:59:05,279 - memory_profile6_log - INFO - 

2018-05-02 08:59:05,280 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 324874
2018-05-02 08:59:05,282 - memory_profile6_log - INFO - 

2018-05-02 08:59:47,621 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-05-02 08:59:47,878 - memory_profile6_log - INFO -                                                   user_id           topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt  p0_posterior  is_general  rank
168616  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22601470         180.583539             190.583539   0.071150      1288      0.010447        True   1.0
168618  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27431099         188.011932             198.011932   0.064369      1288      0.009820        True   2.0
168615  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22553543          38.386972              48.386972   0.131969      1288      0.004920        True   3.0
168623  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           33020303          93.333873             103.333873   0.020059      1288      0.001597        True   4.0
168617  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27313197        1059.941030            1069.941030   0.000415      1288      0.000342        True   5.0
168624  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           39301645         351.528952             361.528952   0.017232      1288      0.004800       False   1.0
168612  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1149084290        3395.843537            3405.843537   0.000584      1288      0.001533       False   2.0
168611  1616f009d96b1-0285d8288a5bce-70217860-38400-16...          107151123         106.692904             116.692904   0.010699      1288      0.000962       False   3.0
168627  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           62323193         776.343701             786.343701   0.000997      1288      0.000604       False   4.0
168608  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1023751122         771.544049             781.544049   0.000999      1288      0.000602       False   5.0
168625  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           39825972          33.653947              43.653947   0.016376      1288      0.000551       False   6.0
168609  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1037574949          20.851671              30.851671   0.018079      1288      0.000430       False   7.0
168622  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790314285         102.376743             112.376743   0.004905      1288      0.000425       False   8.0
168614  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1161375937          56.687372              66.687372   0.006509      1288      0.000334       False   9.0
168610  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1038512181          71.312714              81.312714   0.004241      1288      0.000266       False  10.0
168613  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1157057159          66.108992              76.108992   0.003970      1288      0.000233       False  11.0
168620  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790313523        2655.260638            2665.260638   0.000067      1288      0.000138       False  12.0
168626  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           41639090        1067.804002            1077.804002   0.000145      1288      0.000120       False  13.0
168619  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790312980        5254.621053            5264.621053   0.000017      1288      0.000070       False  14.0
168621  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790314267         597.116029             607.116029   0.000051      1288      0.000024       False  15.0
2018-05-02 08:59:47,878 - memory_profile6_log - INFO - 

2018-05-02 08:59:47,881 - memory_profile6_log - INFO - Len of model_transform: 304803
2018-05-02 08:59:47,881 - memory_profile6_log - INFO - Len of df_dt: 812942
2018-05-02 08:59:47,882 - memory_profile6_log - INFO - Total train time: 45.884s
2018-05-02 08:59:47,884 - memory_profile6_log - INFO - memory left before cleaning: 83.000 percent memory...
2018-05-02 08:59:47,885 - memory_profile6_log - INFO - cleaning up some objects...
2018-05-02 08:59:47,887 - memory_profile6_log - INFO - deleting df_dut...
2018-05-02 08:59:47,888 - memory_profile6_log - INFO - deleting df_dt...
2018-05-02 08:59:47,891 - memory_profile6_log - INFO - deleting df_input...
2018-05-02 08:59:47,917 - memory_profile6_log - INFO - deleting df_input_X...
2018-05-02 08:59:47,918 - memory_profile6_log - INFO - deleting df_current...
2018-05-02 08:59:47,921 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-05-02 08:59:47,936 - memory_profile6_log - INFO - deleting model_fit...
2018-05-02 08:59:47,937 - memory_profile6_log - INFO - deleting result...
2018-05-02 08:59:47,963 - memory_profile6_log - INFO - deleting model_transform...
2018-05-02 08:59:47,964 - memory_profile6_log - INFO - memory left after cleaning: 82.700 percent memory...
2018-05-02 08:59:47,966 - memory_profile6_log - INFO - Begin saving trained data...
2018-05-02 08:59:47,967 - memory_profile6_log - INFO - Using ElasticSearch as storage...
2018-05-02 08:59:47,990 - memory_profile6_log - INFO - Saving total data: 304803
2018-05-02 08:59:47,992 - memory_profile6_log - INFO - Len of X_split for batch save model_transformsv: 5
2018-05-02 08:59:47,993 - memory_profile6_log - INFO - processing batch-0
2018-05-02 09:00:43,180 - memory_profile6_log - INFO - processing batch-1
2018-05-02 09:01:29,780 - memory_profile6_log - INFO - processing batch-2
2018-05-02 09:02:27,138 - memory_profile6_log - INFO - processing batch-3
2018-05-02 09:03:43,719 - memory_profile6_log - INFO - processing batch-4
2018-05-02 09:04:35,940 - memory_profile6_log - INFO - Saving fitted_models as history to Elasticsearch...
2018-05-02 09:04:36,036 - memory_profile6_log - INFO - Saving total data: 324874
2018-05-02 09:04:36,038 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 5
2018-05-02 09:04:36,039 - memory_profile6_log - INFO - processing batch-0
2018-05-02 09:05:42,667 - memory_profile6_log - INFO - processing batch-1
2018-05-02 09:06:44,168 - memory_profile6_log - INFO - processing batch-2
2018-05-02 09:07:36,063 - memory_profile6_log - INFO - processing batch-3
2018-05-02 09:08:32,947 - memory_profile6_log - INFO - processing batch-4
2018-05-02 09:09:47,282 - memory_profile6_log - INFO - deleting BR...
2018-05-02 09:09:47,285 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-05-02 09:09:47,292 - memory_profile6_log - INFO - deleting fitted_models_sigmant...
2018-05-02 09:09:47,306 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 09:09:47,306 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 09:09:47,309 - memory_profile6_log - INFO - ================================================

2018-05-02 09:09:47,309 - memory_profile6_log - INFO -    113    609.8 MiB    609.8 MiB   @profile

2018-05-02 09:09:47,309 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-05-02 09:09:47,312 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-05-02 09:09:47,312 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-05-02 09:09:47,313 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-05-02 09:09:47,315 - memory_profile6_log - INFO -    118                                      saveto="datastore", fitted_models_hist=None):

2018-05-02 09:09:47,315 - memory_profile6_log - INFO -    119                                 """

2018-05-02 09:09:47,315 - memory_profile6_log - INFO -    120                                     Main Process

2018-05-02 09:09:47,315 - memory_profile6_log - INFO -    121                                 """

2018-05-02 09:09:47,315 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-05-02 09:09:47,316 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-05-02 09:09:47,316 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-05-02 09:09:47,318 - memory_profile6_log - INFO -    125    609.8 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-05-02 09:09:47,318 - memory_profile6_log - INFO -    126    619.9 MiB     10.1 MiB       df_dut = df_input.copy(deep=True)

2018-05-02 09:09:47,319 - memory_profile6_log - INFO -    127    619.9 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 09:09:47,319 - memory_profile6_log - INFO -    128                             

2018-05-02 09:09:47,319 - memory_profile6_log - INFO -    129                                 # D(t)

2018-05-02 09:09:47,319 - memory_profile6_log - INFO -    130    645.5 MiB     25.6 MiB       df_dt = df_current.copy(deep=True)

2018-05-02 09:09:47,319 - memory_profile6_log - INFO -    131    645.5 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 09:09:47,325 - memory_profile6_log - INFO -    132                             

2018-05-02 09:09:47,325 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-05-02 09:09:47,325 - memory_profile6_log - INFO -    134    645.5 MiB      0.0 MiB       t0 = time.time()

2018-05-02 09:09:47,326 - memory_profile6_log - INFO -    135    645.5 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-05-02 09:09:47,326 - memory_profile6_log - INFO -    136    645.5 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-05-02 09:09:47,328 - memory_profile6_log - INFO -    137    645.5 MiB      0.0 MiB       logger.info("apply on: %d total history...)", len(df_hist))

2018-05-02 09:09:47,328 - memory_profile6_log - INFO -    138                             

2018-05-02 09:09:47,328 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-05-02 09:09:47,328 - memory_profile6_log - INFO -    140    645.5 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-05-02 09:09:47,328 - memory_profile6_log - INFO -    141                             

2018-05-02 09:09:47,329 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-05-02 09:09:47,331 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-05-02 09:09:47,331 - memory_profile6_log - INFO -    144    647.6 MiB      2.1 MiB       NB = BR.processX(df_dut)

2018-05-02 09:09:47,332 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-05-02 09:09:47,332 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-05-02 09:09:47,332 - memory_profile6_log - INFO -    147    660.1 MiB     12.5 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-05-02 09:09:47,336 - memory_profile6_log - INFO -    148                                 """

2018-05-02 09:09:47,338 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-05-02 09:09:47,338 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-05-02 09:09:47,338 - memory_profile6_log - INFO -    151                                 """

2018-05-02 09:09:47,339 - memory_profile6_log - INFO -    152    660.1 MiB      0.0 MiB       fitby_sigmant = False

2018-05-02 09:09:47,339 - memory_profile6_log - INFO -    153    660.1 MiB      0.0 MiB       uniques_fit_hist = None

2018-05-02 09:09:47,341 - memory_profile6_log - INFO -    154    660.1 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-05-02 09:09:47,341 - memory_profile6_log - INFO -    155    660.1 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-05-02 09:09:47,342 - memory_profile6_log - INFO -    156    672.5 MiB     12.4 MiB                            'is_general']]

2018-05-02 09:09:47,342 - memory_profile6_log - INFO -    157                             

2018-05-02 09:09:47,342 - memory_profile6_log - INFO -    158                                 # get sigma_Nt from fitted_models_hist

2018-05-02 09:09:47,342 - memory_profile6_log - INFO -    159    674.8 MiB      2.3 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-05-02 09:09:47,344 - memory_profile6_log - INFO -    160    674.8 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-05-02 09:09:47,344 - memory_profile6_log - INFO -    161    676.3 MiB      1.5 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-05-02 09:09:47,344 - memory_profile6_log - INFO -    162    676.3 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-05-02 09:09:47,348 - memory_profile6_log - INFO -    163                             

2018-05-02 09:09:47,348 - memory_profile6_log - INFO -    164                                 # begin fit

2018-05-02 09:09:47,348 - memory_profile6_log - INFO -    165    676.3 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-05-02 09:09:47,349 - memory_profile6_log - INFO -    166    676.3 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-05-02 09:09:47,349 - memory_profile6_log - INFO -    167    710.0 MiB     33.7 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-05-02 09:09:47,351 - memory_profile6_log - INFO -    168    710.0 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-05-02 09:09:47,351 - memory_profile6_log - INFO -    169    710.0 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-05-02 09:09:47,351 - memory_profile6_log - INFO -    170                                 # print model_fit[['num_x', 'num_x', 'sigma_Nt', 'date_all_click']].loc[model_fit['user_id']=="1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157"].head(5)

2018-05-02 09:09:47,352 - memory_profile6_log - INFO -    171                             

2018-05-02 09:09:47,352 - memory_profile6_log - INFO -    172                                 # ~~ and Transform ~~

2018-05-02 09:09:47,354 - memory_profile6_log - INFO -    173                                 #   handling current news interest == current date

2018-05-02 09:09:47,354 - memory_profile6_log - INFO -    174    710.0 MiB      0.0 MiB       if df_dt.empty:

2018-05-02 09:09:47,355 - memory_profile6_log - INFO -    175                                     print "Cek your df_dt, cannot be emtpy!!"

2018-05-02 09:09:47,355 - memory_profile6_log - INFO -    176                                     return None

2018-05-02 09:09:47,355 - memory_profile6_log - INFO -    177    711.5 MiB      1.5 MiB       NB = BR.processX(df_dt)

2018-05-02 09:09:47,355 - memory_profile6_log - INFO -    178    748.0 MiB     36.5 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-05-02 09:09:47,359 - memory_profile6_log - INFO -    179                             

2018-05-02 09:09:47,361 - memory_profile6_log - INFO -    180    748.0 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-05-02 09:09:47,361 - memory_profile6_log - INFO -    181    766.6 MiB     18.6 MiB                            'num_x', 'num_y', 'is_general']]

2018-05-02 09:09:47,361 - memory_profile6_log - INFO -    182                             

2018-05-02 09:09:47,362 - memory_profile6_log - INFO -    183                                 # print "model_fit dtypes:\n", model_fit.dtypes

2018-05-02 09:09:47,362 - memory_profile6_log - INFO -    184                                 # print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-05-02 09:09:47,364 - memory_profile6_log - INFO -    185    766.6 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-05-02 09:09:47,365 - memory_profile6_log - INFO -    186    766.6 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-05-02 09:09:47,365 - memory_profile6_log - INFO -    187    766.6 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-05-02 09:09:47,365 - memory_profile6_log - INFO -    188    768.1 MiB      1.5 MiB                                                                                           "topic_id", "user_id"]],

2018-05-02 09:09:47,367 - memory_profile6_log - INFO -    189    793.3 MiB     25.1 MiB                                                     verbose=False)

2018-05-02 09:09:47,368 - memory_profile6_log - INFO -    190                             

2018-05-02 09:09:47,368 - memory_profile6_log - INFO -    191                                 # ~~~ filter is general and specific topic ~~~

2018-05-02 09:09:47,372 - memory_profile6_log - INFO -    192                                 # the idea is just we need to rerank every topic according

2018-05-02 09:09:47,372 - memory_profile6_log - INFO -    193                                 #    to user_id and and is_general by p0_posterior

2018-05-02 09:09:47,374 - memory_profile6_log - INFO -    194    793.3 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-05-02 09:09:47,375 - memory_profile6_log - INFO -    195    795.8 MiB      2.5 MiB                                     'is_general']].groupby(['topic_id',

2018-05-02 09:09:47,375 - memory_profile6_log - INFO -    196    793.3 MiB     -2.5 MiB                                                             'is_general']

2018-05-02 09:09:47,375 - memory_profile6_log - INFO -    197                                                                                      ).size().to_frame().reset_index()

2018-05-02 09:09:47,378 - memory_profile6_log - INFO -    198                             

2018-05-02 09:09:47,378 - memory_profile6_log - INFO -    199                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-05-02 09:09:47,378 - memory_profile6_log - INFO -    200                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-05-02 09:09:47,378 - memory_profile6_log - INFO -    201    793.3 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-05-02 09:09:47,380 - memory_profile6_log - INFO -    202                             

2018-05-02 09:09:47,380 - memory_profile6_log - INFO -    203                                 # ~ start by provide rank for each topic type ~

2018-05-02 09:09:47,382 - memory_profile6_log - INFO -    204    810.8 MiB     17.4 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-05-02 09:09:47,384 - memory_profile6_log - INFO -    205    813.7 MiB      2.9 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-05-02 09:09:47,384 - memory_profile6_log - INFO -    206                             

2018-05-02 09:09:47,385 - memory_profile6_log - INFO -    207                                 # ~ set threshold to filter output

2018-05-02 09:09:47,387 - memory_profile6_log - INFO -    208    813.7 MiB      0.0 MiB       if threshold > 0:

2018-05-02 09:09:47,387 - memory_profile6_log - INFO -    209    813.7 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-05-02 09:09:47,387 - memory_profile6_log - INFO -    210    813.7 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-05-02 09:09:47,388 - memory_profile6_log - INFO -    211    812.7 MiB     -1.0 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-05-02 09:09:47,388 - memory_profile6_log - INFO -    212                             

2018-05-02 09:09:47,388 - memory_profile6_log - INFO -    213    812.7 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 09:09:47,388 - memory_profile6_log - INFO -    214    812.7 MiB      0.1 MiB       print model_transform[model_transform['user_id']=='1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157']

2018-05-02 09:09:47,388 - memory_profile6_log - INFO -    215    812.7 MiB      0.0 MiB       logger.info("Len of model_transform: %d", len(model_transform))

2018-05-02 09:09:47,390 - memory_profile6_log - INFO -    216    812.7 MiB      0.0 MiB       logger.info("Len of df_dt: %d", len(df_dt))

2018-05-02 09:09:47,390 - memory_profile6_log - INFO -    217    812.7 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-05-02 09:09:47,391 - memory_profile6_log - INFO -    218                             

2018-05-02 09:09:47,391 - memory_profile6_log - INFO -    219    812.7 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-05-02 09:09:47,391 - memory_profile6_log - INFO -    220                             

2018-05-02 09:09:47,391 - memory_profile6_log - INFO -    221    812.7 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-05-02 09:09:47,395 - memory_profile6_log - INFO -    222    812.7 MiB      0.0 MiB       del df_dut

2018-05-02 09:09:47,395 - memory_profile6_log - INFO -    223    812.7 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-05-02 09:09:47,397 - memory_profile6_log - INFO -    224    812.7 MiB      0.0 MiB       del df_dt

2018-05-02 09:09:47,397 - memory_profile6_log - INFO -    225    812.7 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-05-02 09:09:47,397 - memory_profile6_log - INFO -    226    812.7 MiB      0.0 MiB       del df_input

2018-05-02 09:09:47,398 - memory_profile6_log - INFO -    227    812.7 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-05-02 09:09:47,398 - memory_profile6_log - INFO -    228    781.7 MiB    -31.0 MiB       del df_input_X

2018-05-02 09:09:47,398 - memory_profile6_log - INFO -    229    781.7 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-05-02 09:09:47,398 - memory_profile6_log - INFO -    230    781.7 MiB      0.0 MiB       del df_current

2018-05-02 09:09:47,400 - memory_profile6_log - INFO -    231    781.7 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-05-02 09:09:47,401 - memory_profile6_log - INFO -    232    781.7 MiB      0.0 MiB       del map_topic_isgeneral

2018-05-02 09:09:47,401 - memory_profile6_log - INFO -    233    781.7 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-05-02 09:09:47,401 - memory_profile6_log - INFO -    234    754.4 MiB    -27.3 MiB       del model_fit

2018-05-02 09:09:47,403 - memory_profile6_log - INFO -    235    754.4 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-05-02 09:09:47,403 - memory_profile6_log - INFO -    236    754.4 MiB      0.0 MiB       del result

2018-05-02 09:09:47,403 - memory_profile6_log - INFO -    237    754.4 MiB      0.0 MiB       logger.info("deleting result...")

2018-05-02 09:09:47,407 - memory_profile6_log - INFO -    238                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 09:09:47,407 - memory_profile6_log - INFO -    239                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 09:09:47,407 - memory_profile6_log - INFO -    240                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 09:09:47,407 - memory_profile6_log - INFO -    241    754.4 MiB      0.0 MiB       if savetrain:

2018-05-02 09:09:47,407 - memory_profile6_log - INFO -    242    761.4 MiB      7.0 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-05-02 09:09:47,408 - memory_profile6_log - INFO -    243    761.4 MiB      0.0 MiB           del model_transform

2018-05-02 09:09:47,410 - memory_profile6_log - INFO -    244    761.4 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-05-02 09:09:47,411 - memory_profile6_log - INFO -    245    761.4 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-05-02 09:09:47,411 - memory_profile6_log - INFO -    246                             

2018-05-02 09:09:47,411 - memory_profile6_log - INFO -    247    761.4 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-05-02 09:09:47,411 - memory_profile6_log - INFO -    248                                     # ~ Place your code to save the training model here ~

2018-05-02 09:09:47,413 - memory_profile6_log - INFO -    249    761.4 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-05-02 09:09:47,413 - memory_profile6_log - INFO -    250                                         logger.info("Using google datastore as storage...")

2018-05-02 09:09:47,413 - memory_profile6_log - INFO -    251                                         if multproc:

2018-05-02 09:09:47,414 - memory_profile6_log - INFO -    252                                             # ~ save transform models ~

2018-05-02 09:09:47,414 - memory_profile6_log - INFO -    253                                             logger.info("Saving main Transform model to Google DataStore...")

2018-05-02 09:09:47,417 - memory_profile6_log - INFO -    254                                             logger.info("Saving total data: %d", len(model_transformsv))

2018-05-02 09:09:47,418 - memory_profile6_log - INFO -    255                                             mh.saveDataStorePutMulti(model_transformsv)

2018-05-02 09:09:47,418 - memory_profile6_log - INFO -    256                             

2018-05-02 09:09:47,420 - memory_profile6_log - INFO -    257                                             # ~ save fitted models ~

2018-05-02 09:09:47,421 - memory_profile6_log - INFO -    258                                             logger.info("Saving fitted_models as history to Google DataStore...")

2018-05-02 09:09:47,421 - memory_profile6_log - INFO -    259                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 09:09:47,421 - memory_profile6_log - INFO -    260                                             fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-05-02 09:09:47,421 - memory_profile6_log - INFO -    261                                             logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-05-02 09:09:47,421 - memory_profile6_log - INFO -    262                                             X_split = np.array_split(fitted_models_sigmant, 10)

2018-05-02 09:09:47,423 - memory_profile6_log - INFO -    263                                             logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-05-02 09:09:47,423 - memory_profile6_log - INFO -    264                                             for ix in range(len(X_split)):

2018-05-02 09:09:47,424 - memory_profile6_log - INFO -    265                                                 logger.info("processing batch-%d", ix)

2018-05-02 09:09:47,424 - memory_profile6_log - INFO -    266                                                 mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-05-02 09:09:47,424 - memory_profile6_log - INFO -    267                             

2018-05-02 09:09:47,424 - memory_profile6_log - INFO -    268                                             del X_split

2018-05-02 09:09:47,424 - memory_profile6_log - INFO -    269                                             logger.info("deleting X_split...")

2018-05-02 09:09:47,426 - memory_profile6_log - INFO -    270                                             del save_sigma_nt

2018-05-02 09:09:47,426 - memory_profile6_log - INFO -    271                                             logger.info("deleting save_sigma_nt...")

2018-05-02 09:09:47,430 - memory_profile6_log - INFO -    272                             

2018-05-02 09:09:47,430 - memory_profile6_log - INFO -    273                                             del BR

2018-05-02 09:09:47,430 - memory_profile6_log - INFO -    274                                             logger.info("deleting BR...")

2018-05-02 09:09:47,430 - memory_profile6_log - INFO -    275                             

2018-05-02 09:09:47,431 - memory_profile6_log - INFO -    276    761.4 MiB      0.0 MiB           elif str(saveto).lower() == "elastic":

2018-05-02 09:09:47,431 - memory_profile6_log - INFO -    277    761.4 MiB      0.0 MiB               logger.info("Using ElasticSearch as storage...")

2018-05-02 09:09:47,431 - memory_profile6_log - INFO -    278    761.4 MiB      0.0 MiB               logging.info("Saving main Transform model to Elasticsearch...")

2018-05-02 09:09:47,433 - memory_profile6_log - INFO -    279    767.1 MiB      5.7 MiB               X_split = np.array_split(model_transformsv, 5)

2018-05-02 09:09:47,433 - memory_profile6_log - INFO -    280    767.1 MiB      0.0 MiB               logger.info("Saving total data: %d", len(model_transformsv))

2018-05-02 09:09:47,434 - memory_profile6_log - INFO -    281    767.1 MiB      0.0 MiB               logger.info("Len of X_split for batch save model_transformsv: %d", len(X_split))

2018-05-02 09:09:47,434 - memory_profile6_log - INFO -    282    767.1 MiB      0.0 MiB               for ix in range(len(X_split)):

2018-05-02 09:09:47,434 - memory_profile6_log - INFO -    283    767.1 MiB   -370.4 MiB                   logger.info("processing batch-%d", ix)

2018-05-02 09:09:47,434 - memory_profile6_log - INFO -    284    676.0 MiB   -461.5 MiB                   mh.saveElasticS(X_split[ix])

2018-05-02 09:09:47,436 - memory_profile6_log - INFO -    285    667.0 MiB   -100.1 MiB               del X_split

2018-05-02 09:09:47,436 - memory_profile6_log - INFO -    286                             

2018-05-02 09:09:47,436 - memory_profile6_log - INFO -    287    667.0 MiB      0.0 MiB               logger.info("Saving fitted_models as history to Elasticsearch...")

2018-05-02 09:09:47,437 - memory_profile6_log - INFO -    288    667.0 MiB      0.0 MiB               save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 09:09:47,437 - memory_profile6_log - INFO -    289    685.7 MiB     18.7 MiB               fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-05-02 09:09:47,437 - memory_profile6_log - INFO -    290    697.6 MiB     11.9 MiB               X_split = np.array_split(fitted_models_sigmant, 5)

2018-05-02 09:09:47,437 - memory_profile6_log - INFO -    291    697.6 MiB      0.0 MiB               logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-05-02 09:09:47,437 - memory_profile6_log - INFO -    292    697.6 MiB      0.0 MiB               logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-05-02 09:09:47,443 - memory_profile6_log - INFO -    293    697.6 MiB      0.0 MiB               for ix in range(len(X_split)):

2018-05-02 09:09:47,443 - memory_profile6_log - INFO -    294    697.6 MiB   -118.1 MiB                   logger.info("processing batch-%d", ix)

2018-05-02 09:09:47,444 - memory_profile6_log - INFO -    295    670.4 MiB   -145.3 MiB                   mh.saveElasticS(X_split[ix], esindex_name="transform_hist_index",  estype_name='transform_hist_type')

2018-05-02 09:09:47,444 - memory_profile6_log - INFO -    296                             

2018-05-02 09:09:47,446 - memory_profile6_log - INFO -    297    654.5 MiB    -43.1 MiB               del X_split

2018-05-02 09:09:47,446 - memory_profile6_log - INFO -    298    654.5 MiB      0.0 MiB               del BR

2018-05-02 09:09:47,447 - memory_profile6_log - INFO -    299    654.5 MiB      0.0 MiB               logger.info("deleting BR...")

2018-05-02 09:09:47,447 - memory_profile6_log - INFO -    300    654.5 MiB      0.0 MiB               del save_sigma_nt

2018-05-02 09:09:47,447 - memory_profile6_log - INFO -    301    654.5 MiB      0.0 MiB               logger.info("deleting save_sigma_nt...")

2018-05-02 09:09:47,447 - memory_profile6_log - INFO -    302    637.1 MiB    -17.4 MiB               del fitted_models_sigmant

2018-05-02 09:09:47,447 - memory_profile6_log - INFO -    303    637.1 MiB      0.0 MiB               logger.info("deleting fitted_models_sigmant...")

2018-05-02 09:09:47,447 - memory_profile6_log - INFO -    304                             

2018-05-02 09:09:47,448 - memory_profile6_log - INFO -    305                                     # need save sigma_nt for daily train

2018-05-02 09:09:47,448 - memory_profile6_log - INFO -    306                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-05-02 09:09:47,450 - memory_profile6_log - INFO -    307                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-05-02 09:09:47,450 - memory_profile6_log - INFO -    308    637.1 MiB      0.0 MiB           if start_date and end_date:

2018-05-02 09:09:47,450 - memory_profile6_log - INFO -    309                                         if not fitby_sigmant:

2018-05-02 09:09:47,454 - memory_profile6_log - INFO -    310                                             logging.info("Saving sigma Nt...")

2018-05-02 09:09:47,454 - memory_profile6_log - INFO -    311                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 09:09:47,456 - memory_profile6_log - INFO -    312                                             save_sigma_nt['start_date'] = start_date

2018-05-02 09:09:47,457 - memory_profile6_log - INFO -    313                                             save_sigma_nt['end_date'] = end_date

2018-05-02 09:09:47,457 - memory_profile6_log - INFO -    314                                             print save_sigma_nt.head(5)

2018-05-02 09:09:47,457 - memory_profile6_log - INFO -    315                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-05-02 09:09:47,457 - memory_profile6_log - INFO -    316                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-05-02 09:09:47,457 - memory_profile6_log - INFO -    317    637.1 MiB      0.0 MiB       return

2018-05-02 09:09:47,459 - memory_profile6_log - INFO - 


2018-05-02 09:09:47,459 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-05-02 10:33:20,848 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 10:33:20,851 - memory_profile6_log - INFO - date_generated: 
2018-05-02 10:33:20,852 - memory_profile6_log - INFO -  
2018-05-02 10:33:20,852 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 10, 33, 20, 850000)]
2018-05-02 10:33:20,854 - memory_profile6_log - INFO - 

2018-05-02 10:33:20,854 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 10:33:20,855 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 10:33:20,855 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 10:33:21,000 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 10:33:21,005 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 10:34:46,084 - memory_profile6_log - INFO - size of df: 82.28 MB
2018-05-02 10:34:46,085 - memory_profile6_log - INFO - getting total: 324874 training data(genuine interest) for date: 2018-05-01
2018-05-02 10:34:46,131 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-05-02 10:34:46,132 - memory_profile6_log - INFO - Appending history data...
2018-05-02 10:34:46,134 - memory_profile6_log - INFO - processing batch-0
2018-05-02 10:34:46,134 - memory_profile6_log - INFO - creating list history data...
2018-05-02 10:34:46,190 - memory_profile6_log - INFO - call history data...
2018-05-02 10:35:23,138 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 10:35:23,746 - memory_profile6_log - INFO - processing batch-1
2018-05-02 10:35:23,747 - memory_profile6_log - INFO - creating list history data...
2018-05-02 10:35:23,755 - memory_profile6_log - INFO - call history data...
2018-05-02 10:36:10,177 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 10:36:10,801 - memory_profile6_log - INFO - processing batch-2
2018-05-02 10:36:10,802 - memory_profile6_log - INFO - creating list history data...
2018-05-02 10:36:10,811 - memory_profile6_log - INFO - call history data...
2018-05-02 10:36:46,634 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 10:36:46,782 - memory_profile6_log - INFO - processing batch-3
2018-05-02 10:36:46,785 - memory_profile6_log - INFO - creating list history data...
2018-05-02 10:36:46,792 - memory_profile6_log - INFO - call history data...
2018-05-02 10:37:23,463 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 10:37:23,834 - memory_profile6_log - INFO - processing batch-4
2018-05-02 10:37:23,835 - memory_profile6_log - INFO - creating list history data...
2018-05-02 10:37:23,844 - memory_profile6_log - INFO - call history data...
2018-05-02 10:38:09,069 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 10:38:09,299 - memory_profile6_log - INFO - Appending training data...
2018-05-02 10:38:09,301 - memory_profile6_log - INFO - len datalist: 1
2018-05-02 10:38:09,302 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-05-02 10:38:09,303 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 10:38:09,305 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 10:38:09,305 - memory_profile6_log - INFO - ================================================

2018-05-02 10:38:09,306 - memory_profile6_log - INFO -    349     86.8 MiB     86.8 MiB   @profile

2018-05-02 10:38:09,309 - memory_profile6_log - INFO -    350                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-05-02 10:38:09,311 - memory_profile6_log - INFO -    351     86.8 MiB      0.0 MiB       bq_client = client

2018-05-02 10:38:09,312 - memory_profile6_log - INFO -    352     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 10:38:09,313 - memory_profile6_log - INFO -    353                             

2018-05-02 10:38:09,315 - memory_profile6_log - INFO -    354     86.8 MiB      0.0 MiB       datalist = []

2018-05-02 10:38:09,315 - memory_profile6_log - INFO -    355     86.8 MiB      0.0 MiB       datalist_hist = []

2018-05-02 10:38:09,316 - memory_profile6_log - INFO -    356                             

2018-05-02 10:38:09,318 - memory_profile6_log - INFO -    357     86.8 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-05-02 10:38:09,321 - memory_profile6_log - INFO -    358    377.9 MiB      0.0 MiB       for ndate in date_generated:

2018-05-02 10:38:09,322 - memory_profile6_log - INFO -    359    366.6 MiB    279.8 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-05-02 10:38:09,322 - memory_profile6_log - INFO -    360    366.6 MiB      0.0 MiB           if tframe is not None:

2018-05-02 10:38:09,323 - memory_profile6_log - INFO -    361    366.6 MiB      0.0 MiB               if not tframe.empty:

2018-05-02 10:38:09,325 - memory_profile6_log - INFO -    362    375.9 MiB      9.3 MiB                   X_split = np.array_split(tframe, 5)

2018-05-02 10:38:09,325 - memory_profile6_log - INFO -    363    375.9 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 10:38:09,326 - memory_profile6_log - INFO -    364    375.9 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-05-02 10:38:09,326 - memory_profile6_log - INFO -    365    377.9 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-05-02 10:38:09,328 - memory_profile6_log - INFO -    366                                                 # ~ loading history

2018-05-02 10:38:09,331 - memory_profile6_log - INFO -    367                                                 """

2018-05-02 10:38:09,332 - memory_profile6_log - INFO -    368                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-05-02 10:38:09,334 - memory_profile6_log - INFO -    369                                                 """

2018-05-02 10:38:09,335 - memory_profile6_log - INFO -    370    377.8 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-05-02 10:38:09,335 - memory_profile6_log - INFO -    371                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-05-02 10:38:09,336 - memory_profile6_log - INFO -    372    377.8 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-05-02 10:38:09,338 - memory_profile6_log - INFO -    373    377.8 MiB      0.5 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-05-02 10:38:09,338 - memory_profile6_log - INFO -    374                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 10:38:09,339 - memory_profile6_log - INFO -    375                             

2018-05-02 10:38:09,342 - memory_profile6_log - INFO -    376    377.8 MiB      0.0 MiB                       logger.info("call history data...")

2018-05-02 10:38:09,345 - memory_profile6_log - INFO -    377    377.9 MiB      1.3 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-05-02 10:38:09,346 - memory_profile6_log - INFO -    378                             

2018-05-02 10:38:09,348 - memory_profile6_log - INFO -    379                                                 # me = os.getpid()

2018-05-02 10:38:09,348 - memory_profile6_log - INFO -    380                                                 # kill_proc_tree(me)

2018-05-02 10:38:09,349 - memory_profile6_log - INFO -    381                             

2018-05-02 10:38:09,351 - memory_profile6_log - INFO -    382    377.9 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-05-02 10:38:09,354 - memory_profile6_log - INFO -    383    377.9 MiB      0.0 MiB                       for m in h_frame:

2018-05-02 10:38:09,355 - memory_profile6_log - INFO -    384    377.9 MiB      0.0 MiB                           if m is not None:

2018-05-02 10:38:09,358 - memory_profile6_log - INFO -    385    377.9 MiB      0.0 MiB                               if len(m) > 0:

2018-05-02 10:38:09,359 - memory_profile6_log - INFO -    386    377.9 MiB      0.2 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-05-02 10:38:09,361 - memory_profile6_log - INFO -    387    377.9 MiB      0.0 MiB                       del h_frame

2018-05-02 10:38:09,364 - memory_profile6_log - INFO -    388    377.9 MiB      0.0 MiB                       del lhistory

2018-05-02 10:38:09,367 - memory_profile6_log - INFO -    389                             

2018-05-02 10:38:09,368 - memory_profile6_log - INFO -    390    377.9 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-05-02 10:38:09,369 - memory_profile6_log - INFO -    391    377.9 MiB      0.0 MiB                   datalist.append(tframe)

2018-05-02 10:38:09,369 - memory_profile6_log - INFO -    392                                     else: 

2018-05-02 10:38:09,371 - memory_profile6_log - INFO -    393                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-05-02 10:38:09,374 - memory_profile6_log - INFO -    394    377.9 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-05-02 10:38:09,375 - memory_profile6_log - INFO -    395    377.9 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-05-02 10:38:09,375 - memory_profile6_log - INFO -    396                             

2018-05-02 10:38:09,377 - memory_profile6_log - INFO -    397    377.9 MiB      0.0 MiB       return datalist, datalist_hist

2018-05-02 10:38:09,378 - memory_profile6_log - INFO - 


2018-05-02 10:38:10,003 - memory_profile6_log - INFO - size of big_frame_hist: 317.73 KB
2018-05-02 10:38:10,101 - memory_profile6_log - INFO - size of big_frame: 82.28 MB
2018-05-02 10:38:10,118 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-05-02 10:38:14,430 - memory_profile6_log - INFO - size of df: 25.22 KB
2018-05-02 10:38:14,430 - memory_profile6_log - INFO - getting total: 100 training data(current date interest)
2018-05-02 10:38:14,440 - memory_profile6_log - INFO - size of current_frame: 26.00 KB
2018-05-02 10:38:14,441 - memory_profile6_log - INFO - loading time of: 324974 total genuine-current interest data ~ take 293.463s
2018-05-02 10:38:14,447 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 10:38:14,447 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 10:38:14,448 - memory_profile6_log - INFO - ================================================

2018-05-02 10:38:14,450 - memory_profile6_log - INFO -    399     86.6 MiB     86.6 MiB   @profile

2018-05-02 10:38:14,453 - memory_profile6_log - INFO -    400                             def preprocess(cpu, cd, query_fit, date_generated):

2018-05-02 10:38:14,454 - memory_profile6_log - INFO -    401     86.8 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-05-02 10:38:14,454 - memory_profile6_log - INFO -    402     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 10:38:14,456 - memory_profile6_log - INFO -    403                             

2018-05-02 10:38:14,457 - memory_profile6_log - INFO -    404                                 # ~~~ Begin collecting data ~~~

2018-05-02 10:38:14,457 - memory_profile6_log - INFO -    405     86.8 MiB      0.0 MiB       t0 = time.time()

2018-05-02 10:38:14,460 - memory_profile6_log - INFO -    406    377.9 MiB    291.1 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-05-02 10:38:14,460 - memory_profile6_log - INFO -    407    377.9 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-05-02 10:38:14,461 - memory_profile6_log - INFO -    408                                     logger.info("Training cannot be empty..")

2018-05-02 10:38:14,461 - memory_profile6_log - INFO -    409                                     return False

2018-05-02 10:38:14,463 - memory_profile6_log - INFO -    410    378.7 MiB      0.8 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-05-02 10:38:14,463 - memory_profile6_log - INFO -    411                                 # print "big_frame_hist:\n", big_frame_hist.head(20)

2018-05-02 10:38:14,463 - memory_profile6_log - INFO -    412    378.7 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-05-02 10:38:14,463 - memory_profile6_log - INFO -    413                             

2018-05-02 10:38:14,463 - memory_profile6_log - INFO -    414    386.1 MiB      7.4 MiB       big_frame = pd.concat(datalist)

2018-05-02 10:38:14,463 - memory_profile6_log - INFO -    415    386.1 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-05-02 10:38:14,464 - memory_profile6_log - INFO -    416    378.7 MiB     -7.4 MiB       del datalist

2018-05-02 10:38:14,464 - memory_profile6_log - INFO -    417                             

2018-05-02 10:38:14,464 - memory_profile6_log - INFO -    418    378.7 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 10:38:14,464 - memory_profile6_log - INFO -    419                             

2018-05-02 10:38:14,466 - memory_profile6_log - INFO -    420                                 # ~ get current news interest ~

2018-05-02 10:38:14,466 - memory_profile6_log - INFO -    421    378.7 MiB      0.0 MiB       if not cd:

2018-05-02 10:38:14,467 - memory_profile6_log - INFO -    422    378.7 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-05-02 10:38:14,467 - memory_profile6_log - INFO -    423    379.9 MiB      1.2 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-05-02 10:38:14,467 - memory_profile6_log - INFO -    424    379.9 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-05-02 10:38:14,467 - memory_profile6_log - INFO -    425                                 else:

2018-05-02 10:38:14,473 - memory_profile6_log - INFO -    426                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-05-02 10:38:14,473 - memory_profile6_log - INFO -    427                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-05-02 10:38:14,476 - memory_profile6_log - INFO -    428                             

2018-05-02 10:38:14,476 - memory_profile6_log - INFO -    429                                     # safe handling of query parameter

2018-05-02 10:38:14,477 - memory_profile6_log - INFO -    430                                     query_params = [

2018-05-02 10:38:14,479 - memory_profile6_log - INFO -    431                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-05-02 10:38:14,479 - memory_profile6_log - INFO -    432                                     ]

2018-05-02 10:38:14,480 - memory_profile6_log - INFO -    433                             

2018-05-02 10:38:14,480 - memory_profile6_log - INFO -    434                                     job_config.query_parameters = query_params

2018-05-02 10:38:14,480 - memory_profile6_log - INFO -    435                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-05-02 10:38:14,480 - memory_profile6_log - INFO -    436                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-05-02 10:38:14,480 - memory_profile6_log - INFO -    437                             

2018-05-02 10:38:14,484 - memory_profile6_log - INFO -    438    379.9 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-05-02 10:38:14,484 - memory_profile6_log - INFO -    439    379.9 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-05-02 10:38:14,487 - memory_profile6_log - INFO -    440    379.9 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-05-02 10:38:14,487 - memory_profile6_log - INFO -    441    379.9 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-05-02 10:38:14,489 - memory_profile6_log - INFO -    442    379.9 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 10:38:14,489 - memory_profile6_log - INFO -    443    379.9 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-05-02 10:38:14,490 - memory_profile6_log - INFO -    444                             

2018-05-02 10:38:14,490 - memory_profile6_log - INFO -    445    379.9 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-05-02 10:38:14,490 - memory_profile6_log - INFO - 


2018-05-02 10:38:14,496 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-05-02 10:38:14,517 - memory_profile6_log - INFO - train on: 324874 total genuine interest data(D(u, t))
2018-05-02 10:38:14,519 - memory_profile6_log - INFO - transform on: 100 total current data(D(t))
2018-05-02 10:38:14,520 - memory_profile6_log - INFO - apply on: 1137 total history...)
2018-05-02 10:38:14,819 - memory_profile6_log - INFO - len of uniques_fit_hist:1137
2018-05-02 10:38:14,825 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:1086
2018-05-02 10:38:15,367 - memory_profile6_log - INFO - self.sum_all_nt before concate:

2018-05-02 10:38:15,400 - memory_profile6_log - INFO -                                                  user_id  sigma_Nt
15052  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        27
2018-05-02 10:38:15,401 - memory_profile6_log - INFO - 

2018-05-02 10:38:15,450 - memory_profile6_log - INFO - self.sum_all_nt after concate:

2018-05-02 10:38:15,474 - memory_profile6_log - INFO -                                                  user_id  sigma_Nt
15052  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        27
2018-05-02 10:38:15,476 - memory_profile6_log - INFO - 

2018-05-02 10:38:15,730 - memory_profile6_log - INFO - Len of model_fit: 324874
2018-05-02 10:38:15,732 - memory_profile6_log - INFO - Len of df_dut: 324874
2018-05-02 10:38:16,203 - memory_profile6_log - INFO - Len of fitted_models on main class: 324874
2018-05-02 10:38:16,203 - memory_profile6_log - INFO - 

2018-05-02 10:38:16,204 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-05-02 10:38:16,206 - memory_profile6_log - INFO - 

2018-05-02 10:38:16,207 - memory_profile6_log - INFO - fitted_model_hist:

2018-05-02 10:38:16,207 - memory_profile6_log - INFO - 

2018-05-02 10:38:16,219 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 10:38:16,220 - memory_profile6_log - INFO - 

2018-05-02 10:38:16,262 - memory_profile6_log - INFO - len of fitted models after concat: 326011
2018-05-02 10:38:16,263 - memory_profile6_log - INFO - 

2018-05-02 10:38:16,265 - memory_profile6_log - INFO - Recalculating fitted models...
2018-05-02 10:38:16,266 - memory_profile6_log - INFO - 

2018-05-02 10:38:16,453 - memory_profile6_log - INFO - fitted_models after cobine:

2018-05-02 10:38:16,453 - memory_profile6_log - INFO - 

2018-05-02 10:38:16,463 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 10:38:16,464 - memory_profile6_log - INFO - 

2018-05-02 10:38:16,467 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 324874
2018-05-02 10:38:16,467 - memory_profile6_log - INFO - 

2018-05-02 10:38:57,555 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-05-02 10:38:57,640 - memory_profile6_log - INFO -                                                   user_id           topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt  p0_posterior  is_general  rank
168612  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1149084290        3395.843537            3405.843537   0.025773        27      2.372418       False   1.0
168622  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790314285         102.376743             112.376743   0.051546        27      0.156557       False   2.0
168613  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1157057159          66.108992              76.108992   0.046392        27      0.095428       False   3.0
168610  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1038512181          71.312714              81.312714   0.015464        27      0.033984       False   4.0
2018-05-02 10:38:57,641 - memory_profile6_log - INFO - 

2018-05-02 10:38:57,641 - memory_profile6_log - INFO - Len of model_transform: 59559
2018-05-02 10:38:57,642 - memory_profile6_log - INFO - Len of df_dt: 100
2018-05-02 10:38:57,644 - memory_profile6_log - INFO - Total train time: 43.064s
2018-05-02 10:38:57,644 - memory_profile6_log - INFO - memory left before cleaning: 82.900 percent memory...
2018-05-02 10:38:57,647 - memory_profile6_log - INFO - cleaning up some objects...
2018-05-02 10:38:57,651 - memory_profile6_log - INFO - deleting df_dut...
2018-05-02 10:38:57,653 - memory_profile6_log - INFO - deleting df_dt...
2018-05-02 10:38:57,654 - memory_profile6_log - INFO - deleting df_input...
2018-05-02 10:38:57,654 - memory_profile6_log - INFO - deleting df_input_X...
2018-05-02 10:38:57,655 - memory_profile6_log - INFO - deleting df_current...
2018-05-02 10:38:57,657 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-05-02 10:38:57,671 - memory_profile6_log - INFO - deleting model_fit...
2018-05-02 10:38:57,674 - memory_profile6_log - INFO - deleting result...
2018-05-02 10:38:57,681 - memory_profile6_log - INFO - deleting model_transform...
2018-05-02 10:38:57,683 - memory_profile6_log - INFO - memory left after cleaning: 82.700 percent memory...
2018-05-02 10:38:57,684 - memory_profile6_log - INFO - Begin saving trained data...
2018-05-02 10:38:57,690 - memory_profile6_log - INFO - Using ElasticSearch as storage...
2018-05-02 10:38:57,729 - memory_profile6_log - INFO -                                                user_id  topic_id  is_general  rank
20                107e02cd-54af-40fd-a602-e6105250ae8c  22661216        True   1.0
88   1610c90a82d14-0b6eb95493c02d-4323461-100200-16...  22617325        True   1.0
147  1610c92e11ec4a-0dcda7bb5f421e-4323461-100200-1...  22551890        True   1.0
226  1610c945d23697-026cb4a68def18-4323461-100200-1...  22553321        True   1.0
268  1610c9581a66e6-09411fca5fe1ec-4323461-100200-1...  22553321        True   1.0
2018-05-02 10:38:57,730 - memory_profile6_log - INFO - 

2018-05-02 10:38:57,730 - memory_profile6_log - INFO - Saving fitted_models as history to Elasticsearch...
2018-05-02 10:38:57,821 - memory_profile6_log - INFO -                                 user_id    topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  0d0b2c1c-224c-4804-949a-949c452bdd6f  1037574949          20.851671              30.851671        NaN        15
1  0d0b2c1c-224c-4804-949a-949c452bdd6f  1044283041        2365.824645            2375.824645        NaN        15
2  0d0b2c1c-224c-4804-949a-949c452bdd6f    22553543          16.796588              26.796588        NaN        15
3  0d0b2c1c-224c-4804-949a-949c452bdd6f    22596701          26.895959              36.895959        NaN        15
4  0d0b2c1c-224c-4804-949a-949c452bdd6f    22662160         232.939337             242.939337        NaN        15
2018-05-02 10:38:57,822 - memory_profile6_log - INFO - 

2018-05-02 10:38:57,828 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 10:38:57,828 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 10:38:57,828 - memory_profile6_log - INFO - ================================================

2018-05-02 10:38:57,829 - memory_profile6_log - INFO -    113    380.0 MiB    380.0 MiB   @profile

2018-05-02 10:38:57,832 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-05-02 10:38:57,832 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-05-02 10:38:57,836 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-05-02 10:38:57,838 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-05-02 10:38:57,838 - memory_profile6_log - INFO -    118                                      saveto="datastore", fitted_models_hist=None):

2018-05-02 10:38:57,838 - memory_profile6_log - INFO -    119                                 """

2018-05-02 10:38:57,839 - memory_profile6_log - INFO -    120                                     Main Process

2018-05-02 10:38:57,839 - memory_profile6_log - INFO -    121                                 """

2018-05-02 10:38:57,839 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-05-02 10:38:57,844 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-05-02 10:38:57,844 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-05-02 10:38:57,845 - memory_profile6_log - INFO -    125    380.0 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-05-02 10:38:57,846 - memory_profile6_log - INFO -    126    389.9 MiB      9.9 MiB       df_dut = df_input.copy(deep=True)

2018-05-02 10:38:57,846 - memory_profile6_log - INFO -    127    389.9 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 10:38:57,848 - memory_profile6_log - INFO -    128                             

2018-05-02 10:38:57,848 - memory_profile6_log - INFO -    129                                 # D(t)

2018-05-02 10:38:57,848 - memory_profile6_log - INFO -    130    389.9 MiB      0.0 MiB       df_dt = df_current.copy(deep=True)

2018-05-02 10:38:57,848 - memory_profile6_log - INFO -    131    389.9 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 10:38:57,848 - memory_profile6_log - INFO -    132                             

2018-05-02 10:38:57,849 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-05-02 10:38:57,849 - memory_profile6_log - INFO -    134    389.9 MiB      0.0 MiB       t0 = time.time()

2018-05-02 10:38:57,849 - memory_profile6_log - INFO -    135    389.9 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-05-02 10:38:57,851 - memory_profile6_log - INFO -    136    389.9 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-05-02 10:38:57,851 - memory_profile6_log - INFO -    137    389.9 MiB      0.0 MiB       logger.info("apply on: %d total history...)", len(df_hist))

2018-05-02 10:38:57,851 - memory_profile6_log - INFO -    138                             

2018-05-02 10:38:57,857 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-05-02 10:38:57,858 - memory_profile6_log - INFO -    140    389.9 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-05-02 10:38:57,858 - memory_profile6_log - INFO -    141                             

2018-05-02 10:38:57,858 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-05-02 10:38:57,859 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-05-02 10:38:57,862 - memory_profile6_log - INFO -    144    392.0 MiB      2.1 MiB       NB = BR.processX(df_dut)

2018-05-02 10:38:57,862 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-05-02 10:38:57,864 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-05-02 10:38:57,864 - memory_profile6_log - INFO -    147    404.6 MiB     12.6 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-05-02 10:38:57,865 - memory_profile6_log - INFO -    148                                 """

2018-05-02 10:38:57,865 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-05-02 10:38:57,868 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-05-02 10:38:57,868 - memory_profile6_log - INFO -    151                                 """

2018-05-02 10:38:57,871 - memory_profile6_log - INFO -    152    404.6 MiB      0.0 MiB       fitby_sigmant = False

2018-05-02 10:38:57,871 - memory_profile6_log - INFO -    153    404.6 MiB      0.0 MiB       uniques_fit_hist = None

2018-05-02 10:38:57,871 - memory_profile6_log - INFO -    154    404.6 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-05-02 10:38:57,872 - memory_profile6_log - INFO -    155    404.6 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-05-02 10:38:57,874 - memory_profile6_log - INFO -    156    417.0 MiB     12.4 MiB                            'is_general']]

2018-05-02 10:38:57,874 - memory_profile6_log - INFO -    157                             

2018-05-02 10:38:57,875 - memory_profile6_log - INFO -    158                                 # get sigma_Nt from fitted_models_hist

2018-05-02 10:38:57,875 - memory_profile6_log - INFO -    159    417.0 MiB      0.0 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-05-02 10:38:57,875 - memory_profile6_log - INFO -    160    417.0 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-05-02 10:38:57,875 - memory_profile6_log - INFO -    161    417.0 MiB      0.0 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-05-02 10:38:57,875 - memory_profile6_log - INFO -    162    417.0 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-05-02 10:38:57,878 - memory_profile6_log - INFO -    163                             

2018-05-02 10:38:57,881 - memory_profile6_log - INFO -    164                                 # begin fit

2018-05-02 10:38:57,881 - memory_profile6_log - INFO -    165    417.0 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-05-02 10:38:57,881 - memory_profile6_log - INFO -    166    417.0 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-05-02 10:38:57,881 - memory_profile6_log - INFO -    167    449.1 MiB     32.1 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-05-02 10:38:57,882 - memory_profile6_log - INFO -    168    449.1 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-05-02 10:38:57,884 - memory_profile6_log - INFO -    169    449.1 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-05-02 10:38:57,884 - memory_profile6_log - INFO -    170                                 # print model_fit[['num_x', 'num_x', 'sigma_Nt', 'date_all_click']].loc[model_fit['user_id']=="1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157"].head(5)

2018-05-02 10:38:57,884 - memory_profile6_log - INFO -    171                             

2018-05-02 10:38:57,887 - memory_profile6_log - INFO -    172                                 # ~~ and Transform ~~

2018-05-02 10:38:57,890 - memory_profile6_log - INFO -    173                                 #   handling current news interest == current date

2018-05-02 10:38:57,891 - memory_profile6_log - INFO -    174    449.1 MiB      0.0 MiB       if df_dt.empty:

2018-05-02 10:38:57,891 - memory_profile6_log - INFO -    175                                     print "Cek your df_dt, cannot be emtpy!!"

2018-05-02 10:38:57,892 - memory_profile6_log - INFO -    176                                     return None

2018-05-02 10:38:57,892 - memory_profile6_log - INFO -    177    449.1 MiB      0.0 MiB       NB = BR.processX(df_dt)

2018-05-02 10:38:57,895 - memory_profile6_log - INFO -    178    449.1 MiB      0.0 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-05-02 10:38:57,895 - memory_profile6_log - INFO -    179                             

2018-05-02 10:38:57,897 - memory_profile6_log - INFO -    180    449.1 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-05-02 10:38:57,897 - memory_profile6_log - INFO -    181    436.7 MiB    -12.4 MiB                            'num_x', 'num_y', 'is_general']]

2018-05-02 10:38:57,898 - memory_profile6_log - INFO -    182                             

2018-05-02 10:38:57,901 - memory_profile6_log - INFO -    183                                 # print "model_fit dtypes:\n", model_fit.dtypes

2018-05-02 10:38:57,903 - memory_profile6_log - INFO -    184                                 # print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-05-02 10:38:57,904 - memory_profile6_log - INFO -    185    436.7 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-05-02 10:38:57,904 - memory_profile6_log - INFO -    186    436.7 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-05-02 10:38:57,905 - memory_profile6_log - INFO -    187    436.7 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-05-02 10:38:57,905 - memory_profile6_log - INFO -    188    436.7 MiB      0.0 MiB                                                                                           "topic_id", "user_id"]],

2018-05-02 10:38:57,907 - memory_profile6_log - INFO -    189    459.6 MiB     22.9 MiB                                                     verbose=False)

2018-05-02 10:38:57,907 - memory_profile6_log - INFO -    190                             

2018-05-02 10:38:57,907 - memory_profile6_log - INFO -    191                                 # ~~~ filter is general and specific topic ~~~

2018-05-02 10:38:57,907 - memory_profile6_log - INFO -    192                                 # the idea is just we need to rerank every topic according

2018-05-02 10:38:57,907 - memory_profile6_log - INFO -    193                                 #    to user_id and and is_general by p0_posterior

2018-05-02 10:38:57,908 - memory_profile6_log - INFO -    194    459.6 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-05-02 10:38:57,908 - memory_profile6_log - INFO -    195    462.1 MiB      2.5 MiB                                     'is_general']].groupby(['topic_id',

2018-05-02 10:38:57,910 - memory_profile6_log - INFO -    196    459.6 MiB     -2.5 MiB                                                             'is_general']

2018-05-02 10:38:57,914 - memory_profile6_log - INFO -    197                                                                                      ).size().to_frame().reset_index()

2018-05-02 10:38:57,915 - memory_profile6_log - INFO -    198                             

2018-05-02 10:38:57,917 - memory_profile6_log - INFO -    199                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-05-02 10:38:57,917 - memory_profile6_log - INFO -    200                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-05-02 10:38:57,917 - memory_profile6_log - INFO -    201    459.6 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-05-02 10:38:57,918 - memory_profile6_log - INFO -    202                             

2018-05-02 10:38:57,921 - memory_profile6_log - INFO -    203                                 # ~ start by provide rank for each topic type ~

2018-05-02 10:38:57,921 - memory_profile6_log - INFO -    204    478.0 MiB     18.4 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-05-02 10:38:57,921 - memory_profile6_log - INFO -    205    482.3 MiB      4.3 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-05-02 10:38:57,921 - memory_profile6_log - INFO -    206                             

2018-05-02 10:38:57,923 - memory_profile6_log - INFO -    207                                 # ~ set threshold to filter output

2018-05-02 10:38:57,924 - memory_profile6_log - INFO -    208    482.3 MiB      0.0 MiB       if threshold > 0:

2018-05-02 10:38:57,930 - memory_profile6_log - INFO -    209    482.3 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-05-02 10:38:57,930 - memory_profile6_log - INFO -    210    482.3 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-05-02 10:38:57,930 - memory_profile6_log - INFO -    211    462.7 MiB    -19.6 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-05-02 10:38:57,931 - memory_profile6_log - INFO -    212                             

2018-05-02 10:38:57,931 - memory_profile6_log - INFO -    213    462.7 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 10:38:57,933 - memory_profile6_log - INFO -    214    462.7 MiB      0.0 MiB       print model_transform[model_transform['user_id']=='1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157']

2018-05-02 10:38:57,933 - memory_profile6_log - INFO -    215    462.7 MiB      0.0 MiB       logger.info("Len of model_transform: %d", len(model_transform))

2018-05-02 10:38:57,934 - memory_profile6_log - INFO -    216    462.7 MiB      0.0 MiB       logger.info("Len of df_dt: %d", len(df_dt))

2018-05-02 10:38:57,937 - memory_profile6_log - INFO -    217    462.7 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-05-02 10:38:57,937 - memory_profile6_log - INFO -    218                             

2018-05-02 10:38:57,938 - memory_profile6_log - INFO -    219    462.7 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-05-02 10:38:57,938 - memory_profile6_log - INFO -    220                             

2018-05-02 10:38:57,940 - memory_profile6_log - INFO -    221    462.7 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-05-02 10:38:57,940 - memory_profile6_log - INFO -    222    462.7 MiB      0.0 MiB       del df_dut

2018-05-02 10:38:57,940 - memory_profile6_log - INFO -    223    462.7 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-05-02 10:38:57,940 - memory_profile6_log - INFO -    224    462.7 MiB      0.0 MiB       del df_dt

2018-05-02 10:38:57,940 - memory_profile6_log - INFO -    225    462.7 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-05-02 10:38:57,940 - memory_profile6_log - INFO -    226    462.7 MiB      0.0 MiB       del df_input

2018-05-02 10:38:57,941 - memory_profile6_log - INFO -    227    462.7 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-05-02 10:38:57,941 - memory_profile6_log - INFO -    228    462.7 MiB      0.0 MiB       del df_input_X

2018-05-02 10:38:57,943 - memory_profile6_log - INFO -    229    462.7 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-05-02 10:38:57,943 - memory_profile6_log - INFO -    230    462.7 MiB      0.0 MiB       del df_current

2018-05-02 10:38:57,947 - memory_profile6_log - INFO -    231    462.7 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-05-02 10:38:57,950 - memory_profile6_log - INFO -    232    462.7 MiB      0.0 MiB       del map_topic_isgeneral

2018-05-02 10:38:57,950 - memory_profile6_log - INFO -    233    462.7 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-05-02 10:38:57,950 - memory_profile6_log - INFO -    234    435.5 MiB    -27.3 MiB       del model_fit

2018-05-02 10:38:57,951 - memory_profile6_log - INFO -    235    435.5 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-05-02 10:38:57,953 - memory_profile6_log - INFO -    236    435.5 MiB      0.0 MiB       del result

2018-05-02 10:38:57,954 - memory_profile6_log - INFO -    237    435.5 MiB      0.0 MiB       logger.info("deleting result...")

2018-05-02 10:38:57,954 - memory_profile6_log - INFO -    238                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 10:38:57,956 - memory_profile6_log - INFO -    239                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 10:38:57,956 - memory_profile6_log - INFO -    240                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 10:38:57,960 - memory_profile6_log - INFO -    241    435.5 MiB      0.0 MiB       if savetrain:

2018-05-02 10:38:57,960 - memory_profile6_log - INFO -    242    435.9 MiB      0.4 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-05-02 10:38:57,963 - memory_profile6_log - INFO -    243    435.9 MiB      0.0 MiB           del model_transform

2018-05-02 10:38:57,963 - memory_profile6_log - INFO -    244    435.9 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-05-02 10:38:57,963 - memory_profile6_log - INFO -    245    435.9 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-05-02 10:38:57,963 - memory_profile6_log - INFO -    246                             

2018-05-02 10:38:57,964 - memory_profile6_log - INFO -    247    435.9 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-05-02 10:38:57,964 - memory_profile6_log - INFO -    248                                     # ~ Place your code to save the training model here ~

2018-05-02 10:38:57,966 - memory_profile6_log - INFO -    249    435.9 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-05-02 10:38:57,966 - memory_profile6_log - INFO -    250                                         logger.info("Using google datastore as storage...")

2018-05-02 10:38:57,966 - memory_profile6_log - INFO -    251                                         if multproc:

2018-05-02 10:38:57,967 - memory_profile6_log - INFO -    252                                             # ~ save transform models ~

2018-05-02 10:38:57,967 - memory_profile6_log - INFO -    253                                             logger.info("Saving main Transform model to Google DataStore...")

2018-05-02 10:38:57,967 - memory_profile6_log - INFO -    254                                             logger.info("Saving total data: %d", len(model_transformsv))

2018-05-02 10:38:57,967 - memory_profile6_log - INFO -    255                                             mh.saveDataStorePutMulti(model_transformsv)

2018-05-02 10:38:57,973 - memory_profile6_log - INFO -    256                             

2018-05-02 10:38:57,974 - memory_profile6_log - INFO -    257                                             # ~ save fitted models ~

2018-05-02 10:38:57,976 - memory_profile6_log - INFO -    258                                             logger.info("Saving fitted_models as history to Google DataStore...")

2018-05-02 10:38:57,976 - memory_profile6_log - INFO -    259                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 10:38:57,976 - memory_profile6_log - INFO -    260                                             fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-05-02 10:38:57,979 - memory_profile6_log - INFO -    261                                             logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-05-02 10:38:57,979 - memory_profile6_log - INFO -    262                                             X_split = np.array_split(fitted_models_sigmant, 10)

2018-05-02 10:38:57,980 - memory_profile6_log - INFO -    263                                             logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-05-02 10:38:57,980 - memory_profile6_log - INFO -    264                                             for ix in range(len(X_split)):

2018-05-02 10:38:57,980 - memory_profile6_log - INFO -    265                                                 logger.info("processing batch-%d", ix)

2018-05-02 10:38:57,984 - memory_profile6_log - INFO -    266                                                 mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-05-02 10:38:57,986 - memory_profile6_log - INFO -    267                             

2018-05-02 10:38:57,987 - memory_profile6_log - INFO -    268                                             del X_split

2018-05-02 10:38:57,987 - memory_profile6_log - INFO -    269                                             logger.info("deleting X_split...")

2018-05-02 10:38:57,989 - memory_profile6_log - INFO -    270                                             del save_sigma_nt

2018-05-02 10:38:57,989 - memory_profile6_log - INFO -    271                                             logger.info("deleting save_sigma_nt...")

2018-05-02 10:38:57,990 - memory_profile6_log - INFO -    272                             

2018-05-02 10:38:57,990 - memory_profile6_log - INFO -    273                                             del BR

2018-05-02 10:38:57,990 - memory_profile6_log - INFO -    274                                             logger.info("deleting BR...")

2018-05-02 10:38:57,990 - memory_profile6_log - INFO -    275                             

2018-05-02 10:38:57,990 - memory_profile6_log - INFO -    276    435.9 MiB      0.0 MiB           elif str(saveto).lower() == "elastic":

2018-05-02 10:38:57,992 - memory_profile6_log - INFO -    277    435.9 MiB      0.0 MiB               logger.info("Using ElasticSearch as storage...")

2018-05-02 10:38:57,992 - memory_profile6_log - INFO -    278    435.9 MiB      0.0 MiB               logging.info("Saving main Transform model to Elasticsearch...")

2018-05-02 10:38:57,992 - memory_profile6_log - INFO -    279    435.9 MiB      0.0 MiB               print model_transformsv.head(5)

2018-05-02 10:38:57,993 - memory_profile6_log - INFO -    280                                         """

2018-05-02 10:38:57,997 - memory_profile6_log - INFO -    281                                         X_split = np.array_split(model_transformsv, 5)

2018-05-02 10:38:58,000 - memory_profile6_log - INFO -    282                                         logger.info("Saving total data: %d", len(model_transformsv))

2018-05-02 10:38:58,000 - memory_profile6_log - INFO -    283                                         logger.info("Len of X_split for batch save model_transformsv: %d", len(X_split))

2018-05-02 10:38:58,000 - memory_profile6_log - INFO -    284                                         for ix in range(len(X_split)):

2018-05-02 10:38:58,002 - memory_profile6_log - INFO -    285                                             logger.info("processing batch-%d", ix)

2018-05-02 10:38:58,005 - memory_profile6_log - INFO -    286                                             mh.saveElasticS(X_split[ix])

2018-05-02 10:38:58,006 - memory_profile6_log - INFO -    287                                         del X_split

2018-05-02 10:38:58,006 - memory_profile6_log - INFO -    288                                         """

2018-05-02 10:38:58,006 - memory_profile6_log - INFO -    289                             

2018-05-02 10:38:58,010 - memory_profile6_log - INFO -    290    435.9 MiB      0.0 MiB               logger.info("Saving fitted_models as history to Elasticsearch...")

2018-05-02 10:38:58,013 - memory_profile6_log - INFO -    291    436.1 MiB      0.2 MiB               save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 10:38:58,013 - memory_profile6_log - INFO -    292    454.9 MiB     18.8 MiB               fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-05-02 10:38:58,013 - memory_profile6_log - INFO -    293    454.9 MiB      0.0 MiB               print fitted_models_sigmant.head(5)

2018-05-02 10:38:58,015 - memory_profile6_log - INFO -    294                                         """

2018-05-02 10:38:58,015 - memory_profile6_log - INFO -    295                                         X_split = np.array_split(fitted_models_sigmant, 5)

2018-05-02 10:38:58,016 - memory_profile6_log - INFO -    296                                         logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-05-02 10:38:58,016 - memory_profile6_log - INFO -    297                                         logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-05-02 10:38:58,016 - memory_profile6_log - INFO -    298                                         for ix in range(len(X_split)):

2018-05-02 10:38:58,016 - memory_profile6_log - INFO -    299                                             logger.info("processing batch-%d", ix)

2018-05-02 10:38:58,016 - memory_profile6_log - INFO -    300                                             mh.saveElasticS(X_split[ix], esindex_name="transform_hist_index",  estype_name='transform_hist_type')

2018-05-02 10:38:58,017 - memory_profile6_log - INFO -    301                                         del X_split

2018-05-02 10:38:58,023 - memory_profile6_log - INFO -    302                                         """

2018-05-02 10:38:58,023 - memory_profile6_log - INFO -    303    454.9 MiB      0.0 MiB               return 

2018-05-02 10:38:58,025 - memory_profile6_log - INFO -    304                                         del BR

2018-05-02 10:38:58,026 - memory_profile6_log - INFO -    305                                         logger.info("deleting BR...")

2018-05-02 10:38:58,026 - memory_profile6_log - INFO -    306                                         del save_sigma_nt

2018-05-02 10:38:58,026 - memory_profile6_log - INFO -    307                                         logger.info("deleting save_sigma_nt...")

2018-05-02 10:38:58,029 - memory_profile6_log - INFO -    308                                         del fitted_models_sigmant

2018-05-02 10:38:58,029 - memory_profile6_log - INFO -    309                                         logger.info("deleting fitted_models_sigmant...")

2018-05-02 10:38:58,029 - memory_profile6_log - INFO -    310                             

2018-05-02 10:38:58,029 - memory_profile6_log - INFO -    311                                     # need save sigma_nt for daily train

2018-05-02 10:38:58,030 - memory_profile6_log - INFO -    312                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-05-02 10:38:58,030 - memory_profile6_log - INFO -    313                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-05-02 10:38:58,030 - memory_profile6_log - INFO -    314                                     if start_date and end_date:

2018-05-02 10:38:58,035 - memory_profile6_log - INFO -    315                                         if not fitby_sigmant:

2018-05-02 10:38:58,036 - memory_profile6_log - INFO -    316                                             logging.info("Saving sigma Nt...")

2018-05-02 10:38:58,038 - memory_profile6_log - INFO -    317                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 10:38:58,038 - memory_profile6_log - INFO -    318                                             save_sigma_nt['start_date'] = start_date

2018-05-02 10:38:58,039 - memory_profile6_log - INFO -    319                                             save_sigma_nt['end_date'] = end_date

2018-05-02 10:38:58,039 - memory_profile6_log - INFO -    320                                             print save_sigma_nt.head(5)

2018-05-02 10:38:58,039 - memory_profile6_log - INFO -    321                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-05-02 10:38:58,039 - memory_profile6_log - INFO -    322                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-05-02 10:38:58,040 - memory_profile6_log - INFO -    323                                 return

2018-05-02 10:38:58,040 - memory_profile6_log - INFO - 


2018-05-02 10:38:58,040 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-05-02 10:55:20,085 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 10:55:20,088 - memory_profile6_log - INFO - date_generated: 
2018-05-02 10:55:20,088 - memory_profile6_log - INFO -  
2018-05-02 10:55:20,088 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 10, 55, 20, 86000)]
2018-05-02 10:55:20,088 - memory_profile6_log - INFO - 

2018-05-02 10:55:20,089 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 10:55:20,089 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 10:55:20,091 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 10:55:20,216 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 10:55:20,220 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 10:57:49,516 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 10:57:49,519 - memory_profile6_log - INFO - date_generated: 
2018-05-02 10:57:49,519 - memory_profile6_log - INFO -  
2018-05-02 10:57:49,519 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 10, 57, 49, 517000)]
2018-05-02 10:57:49,520 - memory_profile6_log - INFO - 

2018-05-02 10:57:49,520 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 10:57:49,520 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 10:57:49,522 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 10:57:49,661 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 10:57:49,664 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 10:59:12,762 - memory_profile6_log - INFO - size of df: 82.28 MB
2018-05-02 10:59:12,763 - memory_profile6_log - INFO - getting total: 324874 training data(genuine interest) for date: 2018-05-01
2018-05-02 10:59:12,811 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-05-02 10:59:12,812 - memory_profile6_log - INFO - Appending history data...
2018-05-02 10:59:12,812 - memory_profile6_log - INFO - processing batch-0
2018-05-02 10:59:12,812 - memory_profile6_log - INFO - creating list history data...
2018-05-02 10:59:12,907 - memory_profile6_log - INFO - call history data...
2018-05-02 11:00:26,558 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 11:00:27,663 - memory_profile6_log - INFO - processing batch-1
2018-05-02 11:00:27,664 - memory_profile6_log - INFO - creating list history data...
2018-05-02 11:00:27,720 - memory_profile6_log - INFO - call history data...
2018-05-02 11:01:42,207 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 11:01:43,107 - memory_profile6_log - INFO - processing batch-2
2018-05-02 11:01:43,108 - memory_profile6_log - INFO - creating list history data...
2018-05-02 11:01:43,167 - memory_profile6_log - INFO - call history data...
2018-05-02 11:02:57,904 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 11:02:58,926 - memory_profile6_log - INFO - processing batch-3
2018-05-02 11:02:58,927 - memory_profile6_log - INFO - creating list history data...
2018-05-02 11:02:58,992 - memory_profile6_log - INFO - call history data...
2018-05-02 11:04:14,296 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 11:04:14,992 - memory_profile6_log - INFO - processing batch-4
2018-05-02 11:04:14,993 - memory_profile6_log - INFO - creating list history data...
2018-05-02 11:04:15,053 - memory_profile6_log - INFO - call history data...
2018-05-02 11:05:29,855 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 11:05:30,279 - memory_profile6_log - INFO - Appending training data...
2018-05-02 11:05:30,282 - memory_profile6_log - INFO - len datalist: 1
2018-05-02 11:05:30,282 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-05-02 11:05:30,298 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 11:05:30,299 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 11:05:30,301 - memory_profile6_log - INFO - ================================================

2018-05-02 11:05:30,302 - memory_profile6_log - INFO -    353     86.6 MiB     86.6 MiB   @profile

2018-05-02 11:05:30,302 - memory_profile6_log - INFO -    354                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-05-02 11:05:30,303 - memory_profile6_log - INFO -    355     86.6 MiB      0.0 MiB       bq_client = client

2018-05-02 11:05:30,305 - memory_profile6_log - INFO -    356     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 11:05:30,306 - memory_profile6_log - INFO -    357                             

2018-05-02 11:05:30,308 - memory_profile6_log - INFO -    358     86.6 MiB      0.0 MiB       datalist = []

2018-05-02 11:05:30,311 - memory_profile6_log - INFO -    359     86.6 MiB      0.0 MiB       datalist_hist = []

2018-05-02 11:05:30,312 - memory_profile6_log - INFO -    360                             

2018-05-02 11:05:30,312 - memory_profile6_log - INFO -    361     86.6 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-05-02 11:05:30,313 - memory_profile6_log - INFO -    362    399.8 MiB      0.0 MiB       for ndate in date_generated:

2018-05-02 11:05:30,315 - memory_profile6_log - INFO -    363    366.3 MiB    279.7 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-05-02 11:05:30,315 - memory_profile6_log - INFO -    364    366.3 MiB      0.0 MiB           if tframe is not None:

2018-05-02 11:05:30,316 - memory_profile6_log - INFO -    365    366.3 MiB      0.0 MiB               if not tframe.empty:

2018-05-02 11:05:30,318 - memory_profile6_log - INFO -    366    376.2 MiB      9.8 MiB                   X_split = np.array_split(tframe, 5)

2018-05-02 11:05:30,318 - memory_profile6_log - INFO -    367    376.2 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 11:05:30,322 - memory_profile6_log - INFO -    368    376.2 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-05-02 11:05:30,322 - memory_profile6_log - INFO -    369    400.6 MiB     -6.2 MiB                   for ix in range(len(X_split)):

2018-05-02 11:05:30,323 - memory_profile6_log - INFO -    370                                                 # ~ loading history

2018-05-02 11:05:30,325 - memory_profile6_log - INFO -    371                                                 """

2018-05-02 11:05:30,325 - memory_profile6_log - INFO -    372                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-05-02 11:05:30,326 - memory_profile6_log - INFO -    373                                                 """

2018-05-02 11:05:30,326 - memory_profile6_log - INFO -    374    400.6 MiB     -5.4 MiB                       logger.info("processing batch-%d", ix)

2018-05-02 11:05:30,328 - memory_profile6_log - INFO -    375                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-05-02 11:05:30,328 - memory_profile6_log - INFO -    376    400.6 MiB     -5.4 MiB                       logger.info("creating list history data...")

2018-05-02 11:05:30,329 - memory_profile6_log - INFO -    377                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-05-02 11:05:30,332 - memory_profile6_log - INFO -    378    400.6 MiB      0.6 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 11:05:30,334 - memory_profile6_log - INFO -    379                             

2018-05-02 11:05:30,335 - memory_profile6_log - INFO -    380    400.6 MiB     -6.1 MiB                       logger.info("call history data...")

2018-05-02 11:05:30,335 - memory_profile6_log - INFO -    381    421.5 MiB     75.1 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-05-02 11:05:30,336 - memory_profile6_log - INFO -    382                             

2018-05-02 11:05:30,338 - memory_profile6_log - INFO -    383                                                 # me = os.getpid()

2018-05-02 11:05:30,338 - memory_profile6_log - INFO -    384                                                 # kill_proc_tree(me)

2018-05-02 11:05:30,339 - memory_profile6_log - INFO -    385                             

2018-05-02 11:05:30,341 - memory_profile6_log - INFO -    386    421.5 MiB    -72.9 MiB                       logger.info("done collecting history data, appending now...")

2018-05-02 11:05:30,344 - memory_profile6_log - INFO -    387    422.6 MiB -11579.7 MiB                       for m in h_frame:

2018-05-02 11:05:30,345 - memory_profile6_log - INFO -    388    422.6 MiB -11507.7 MiB                           if m is not None:

2018-05-02 11:05:30,346 - memory_profile6_log - INFO -    389    422.6 MiB -11505.4 MiB                               if len(m) > 0:

2018-05-02 11:05:30,348 - memory_profile6_log - INFO -    390    422.6 MiB  -9277.2 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-05-02 11:05:30,348 - memory_profile6_log - INFO -    391    400.6 MiB   -140.5 MiB                       del h_frame

2018-05-02 11:05:30,349 - memory_profile6_log - INFO -    392    400.6 MiB     -8.4 MiB                       del lhistory

2018-05-02 11:05:30,351 - memory_profile6_log - INFO -    393                             

2018-05-02 11:05:30,351 - memory_profile6_log - INFO -    394    399.8 MiB     -0.9 MiB                   logger.info("Appending training data...")

2018-05-02 11:05:30,355 - memory_profile6_log - INFO -    395    399.8 MiB      0.0 MiB                   datalist.append(tframe)

2018-05-02 11:05:30,357 - memory_profile6_log - INFO -    396                                     else: 

2018-05-02 11:05:30,358 - memory_profile6_log - INFO -    397                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-05-02 11:05:30,358 - memory_profile6_log - INFO -    398    399.8 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-05-02 11:05:30,358 - memory_profile6_log - INFO -    399    399.8 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-05-02 11:05:30,359 - memory_profile6_log - INFO -    400                             

2018-05-02 11:05:30,361 - memory_profile6_log - INFO -    401    399.8 MiB      0.0 MiB       return datalist, datalist_hist

2018-05-02 11:05:30,361 - memory_profile6_log - INFO - 


2018-05-02 11:05:31,454 - memory_profile6_log - INFO - size of big_frame_hist: 27.00 MB
2018-05-02 11:05:31,546 - memory_profile6_log - INFO - size of big_frame: 82.28 MB
2018-05-02 11:05:31,562 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-05-02 11:05:53,427 - memory_profile6_log - INFO - size of df: 24.50 MB
2018-05-02 11:05:53,428 - memory_profile6_log - INFO - getting total: 100000 training data(current date interest)
2018-05-02 11:05:53,469 - memory_profile6_log - INFO - size of current_frame: 25.27 MB
2018-05-02 11:05:53,470 - memory_profile6_log - INFO - loading time of: 424874 total genuine-current interest data ~ take 483.829s
2018-05-02 11:05:53,480 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 11:05:53,482 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 11:05:53,483 - memory_profile6_log - INFO - ================================================

2018-05-02 11:05:53,483 - memory_profile6_log - INFO -    403     86.5 MiB     86.5 MiB   @profile

2018-05-02 11:05:53,483 - memory_profile6_log - INFO -    404                             def preprocess(cpu, cd, query_fit, date_generated):

2018-05-02 11:05:53,484 - memory_profile6_log - INFO -    405     86.6 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-05-02 11:05:53,486 - memory_profile6_log - INFO -    406     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 11:05:53,486 - memory_profile6_log - INFO -    407                             

2018-05-02 11:05:53,486 - memory_profile6_log - INFO -    408                                 # ~~~ Begin collecting data ~~~

2018-05-02 11:05:53,486 - memory_profile6_log - INFO -    409     86.6 MiB      0.0 MiB       t0 = time.time()

2018-05-02 11:05:53,487 - memory_profile6_log - INFO -    410    375.6 MiB    288.9 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-05-02 11:05:53,490 - memory_profile6_log - INFO -    411    375.6 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-05-02 11:05:53,492 - memory_profile6_log - INFO -    412                                     logger.info("Training cannot be empty..")

2018-05-02 11:05:53,493 - memory_profile6_log - INFO -    413                                     return False

2018-05-02 11:05:53,493 - memory_profile6_log - INFO -    414    383.7 MiB      8.1 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-05-02 11:05:53,493 - memory_profile6_log - INFO -    415                                 # print "big_frame_hist:\n", big_frame_hist.head(20)

2018-05-02 11:05:53,494 - memory_profile6_log - INFO -    416    383.7 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-05-02 11:05:53,494 - memory_profile6_log - INFO -    417                             

2018-05-02 11:05:53,496 - memory_profile6_log - INFO -    418    391.4 MiB      7.8 MiB       big_frame = pd.concat(datalist)

2018-05-02 11:05:53,496 - memory_profile6_log - INFO -    419    391.4 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-05-02 11:05:53,496 - memory_profile6_log - INFO -    420    384.0 MiB     -7.4 MiB       del datalist

2018-05-02 11:05:53,496 - memory_profile6_log - INFO -    421                             

2018-05-02 11:05:53,496 - memory_profile6_log - INFO -    422    384.0 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 11:05:53,497 - memory_profile6_log - INFO -    423                             

2018-05-02 11:05:53,497 - memory_profile6_log - INFO -    424                                 # ~ get current news interest ~

2018-05-02 11:05:53,497 - memory_profile6_log - INFO -    425    384.0 MiB      0.0 MiB       if not cd:

2018-05-02 11:05:53,499 - memory_profile6_log - INFO -    426    384.0 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-05-02 11:05:53,503 - memory_profile6_log - INFO -    427    423.9 MiB     39.9 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-05-02 11:05:53,505 - memory_profile6_log - INFO -    428    423.9 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-05-02 11:05:53,506 - memory_profile6_log - INFO -    429                                 else:

2018-05-02 11:05:53,506 - memory_profile6_log - INFO -    430                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-05-02 11:05:53,506 - memory_profile6_log - INFO -    431                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date) LIMIT 100000"

2018-05-02 11:05:53,506 - memory_profile6_log - INFO -    432                             

2018-05-02 11:05:53,507 - memory_profile6_log - INFO -    433                                     # safe handling of query parameter

2018-05-02 11:05:53,509 - memory_profile6_log - INFO -    434                                     query_params = [

2018-05-02 11:05:53,509 - memory_profile6_log - INFO -    435                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-05-02 11:05:53,510 - memory_profile6_log - INFO -    436                                     ]

2018-05-02 11:05:53,510 - memory_profile6_log - INFO -    437                             

2018-05-02 11:05:53,512 - memory_profile6_log - INFO -    438                                     job_config.query_parameters = query_params

2018-05-02 11:05:53,512 - memory_profile6_log - INFO -    439                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-05-02 11:05:53,516 - memory_profile6_log - INFO -    440                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-05-02 11:05:53,516 - memory_profile6_log - INFO -    441                             

2018-05-02 11:05:53,517 - memory_profile6_log - INFO -    442    423.9 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-05-02 11:05:53,519 - memory_profile6_log - INFO -    443    423.9 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-05-02 11:05:53,519 - memory_profile6_log - INFO -    444    423.9 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-05-02 11:05:53,519 - memory_profile6_log - INFO -    445    423.9 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-05-02 11:05:53,519 - memory_profile6_log - INFO -    446    423.9 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 11:05:53,520 - memory_profile6_log - INFO -    447    423.9 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-05-02 11:05:53,520 - memory_profile6_log - INFO -    448                             

2018-05-02 11:05:53,522 - memory_profile6_log - INFO -    449    423.9 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-05-02 11:05:53,522 - memory_profile6_log - INFO - 


2018-05-02 11:05:53,529 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-05-02 11:05:53,559 - memory_profile6_log - INFO - train on: 324874 total genuine interest data(D(u, t))
2018-05-02 11:05:53,561 - memory_profile6_log - INFO - transform on: 100000 total current data(D(t))
2018-05-02 11:05:53,562 - memory_profile6_log - INFO - apply on: 99031 total history...)
2018-05-02 11:05:53,885 - memory_profile6_log - INFO - len of uniques_fit_hist:99031
2018-05-02 11:05:53,954 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:28983
2018-05-02 11:05:54,559 - memory_profile6_log - INFO - self.sum_all_nt before concate:

2018-05-02 11:05:54,599 - memory_profile6_log - INFO -                                                  user_id  sigma_Nt
15052  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        27
2018-05-02 11:05:54,601 - memory_profile6_log - INFO - 

2018-05-02 11:05:54,671 - memory_profile6_log - INFO - self.sum_all_nt after concate:

2018-05-02 11:05:54,697 - memory_profile6_log - INFO -                                                  user_id  sigma_Nt
15052  1616f009d96b1-0285d8288a5bce-70217860-38400-16...      1288
2018-05-02 11:05:54,697 - memory_profile6_log - INFO - 

2018-05-02 11:05:54,973 - memory_profile6_log - INFO - Len of model_fit: 324874
2018-05-02 11:05:54,973 - memory_profile6_log - INFO - Len of df_dut: 324874
2018-05-02 11:05:55,647 - memory_profile6_log - INFO - Len of fitted_models on main class: 324874
2018-05-02 11:05:55,648 - memory_profile6_log - INFO - 

2018-05-02 11:05:55,650 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-05-02 11:05:55,651 - memory_profile6_log - INFO - 

2018-05-02 11:05:55,651 - memory_profile6_log - INFO - fitted_model_hist:

2018-05-02 11:05:55,651 - memory_profile6_log - INFO - 

2018-05-02 11:05:55,713 - memory_profile6_log - INFO -      pt_posterior_x_Nt   topic_id                                            user_id
64           15.991521   22553543  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
68           83.719466   22601470  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
49          516.753870   27313197  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
114          84.916311   27431099  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
134          28.579819   33020303  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
242          94.876510   39301645  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
44           67.425244   41639090  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
64           15.283089  107151123  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
2018-05-02 11:05:55,713 - memory_profile6_log - INFO - 

2018-05-02 11:05:55,766 - memory_profile6_log - INFO - len of fitted models after concat: 423905
2018-05-02 11:05:55,767 - memory_profile6_log - INFO - 

2018-05-02 11:05:55,769 - memory_profile6_log - INFO - Recalculating fitted models...
2018-05-02 11:05:55,769 - memory_profile6_log - INFO - 

2018-05-02 11:05:56,062 - memory_profile6_log - INFO - fitted_models after cobine:

2018-05-02 11:05:56,062 - memory_profile6_log - INFO - 

2018-05-02 11:05:56,134 - memory_profile6_log - INFO -      pt_posterior_x_Nt   topic_id                                            user_id
64           15.991521   22553543  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
68           83.719466   22601470  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
49          516.753870   27313197  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
114          84.916311   27431099  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
134          28.579819   33020303  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
242          94.876510   39301645  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
44           67.425244   41639090  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
64           15.283089  107151123  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
2018-05-02 11:05:56,134 - memory_profile6_log - INFO - 

2018-05-02 11:05:56,135 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 324874
2018-05-02 11:05:56,137 - memory_profile6_log - INFO - 

2018-05-02 11:06:41,056 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-05-02 11:06:41,316 - memory_profile6_log - INFO -                                                   user_id           topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt  p0_posterior  is_general  rank
168616  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22601470         180.583539             190.583539   0.044653      1288      0.006556        True   1.0
168615  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22553543          38.386972              48.386972   0.120457      1288      0.004490        True   2.0
168618  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27431099         188.011932             198.011932   0.022179      1288      0.003383        True   3.0
168623  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           33020303          93.333873             103.333873   0.029263      1288      0.002330        True   4.0
168617  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27313197        1059.941030            1069.941030   0.000854      1288      0.000704        True   5.0
168612  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1149084290        3395.843537            3405.843537   0.001374      1288      0.003606       False   1.0
168611  1616f009d96b1-0285d8288a5bce-70217860-38400-16...          107151123         106.692904             116.692904   0.025277      1288      0.002272       False   2.0
168624  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           39301645         351.528952             361.528952   0.007730      1288      0.002153       False   3.0
168622  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790314285         102.376743             112.376743   0.014437      1288      0.001250       False   4.0
168610  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1038512181          71.312714              81.312714   0.009756      1288      0.000611       False   5.0
168614  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1161375937          56.687372              66.687372   0.010643      1288      0.000547       False   6.0
168609  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1037574949          20.851671              30.851671   0.016895      1288      0.000402       False   7.0
168625  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           39825972          33.653947              43.653947   0.011754      1288      0.000395       False   8.0
168613  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1157057159          66.108992              76.108992   0.006072      1288      0.000356       False   9.0
168608  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1023751122         771.544049             781.544049   0.000378      1288      0.000227       False  10.0
168627  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           62323193         776.343701             786.343701   0.000367      1288      0.000222       False  11.0
168620  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790313523        2655.260638            2665.260638   0.000099      1288      0.000202       False  12.0
168619  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790312980        5254.621053            5264.621053   0.000049      1288      0.000200       False  13.0
168626  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           41639090        1067.804002            1077.804002   0.000208      1288      0.000173       False  14.0
168621  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790314267         597.116029             607.116029   0.000088      1288      0.000041       False  15.0
2018-05-02 11:06:41,319 - memory_profile6_log - INFO - 

2018-05-02 11:06:41,319 - memory_profile6_log - INFO - Len of model_transform: 304208
2018-05-02 11:06:41,321 - memory_profile6_log - INFO - Len of df_dt: 100000
2018-05-02 11:06:41,322 - memory_profile6_log - INFO - Total train time: 47.534s
2018-05-02 11:06:41,323 - memory_profile6_log - INFO - memory left before cleaning: 87.300 percent memory...
2018-05-02 11:06:41,325 - memory_profile6_log - INFO - cleaning up some objects...
2018-05-02 11:06:41,325 - memory_profile6_log - INFO - deleting df_dut...
2018-05-02 11:06:41,326 - memory_profile6_log - INFO - deleting df_dt...
2018-05-02 11:06:41,328 - memory_profile6_log - INFO - deleting df_input...
2018-05-02 11:06:41,335 - memory_profile6_log - INFO - deleting df_input_X...
2018-05-02 11:06:41,338 - memory_profile6_log - INFO - deleting df_current...
2018-05-02 11:06:41,338 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-05-02 11:06:41,354 - memory_profile6_log - INFO - deleting model_fit...
2018-05-02 11:06:41,355 - memory_profile6_log - INFO - deleting result...
2018-05-02 11:14:22,888 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 11:14:22,891 - memory_profile6_log - INFO - date_generated: 
2018-05-02 11:14:22,892 - memory_profile6_log - INFO -  
2018-05-02 11:14:22,892 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 11, 14, 22, 889000)]
2018-05-02 11:14:22,894 - memory_profile6_log - INFO - 

2018-05-02 11:14:22,894 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 11:14:22,894 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 11:14:22,895 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 11:14:23,025 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 11:14:23,028 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 11:14:52,851 - memory_profile6_log - INFO - size of df: 25.34 MB
2018-05-02 11:14:52,852 - memory_profile6_log - INFO - getting total: 100000 training data(genuine interest) for date: 2018-05-01
2018-05-02 11:14:52,880 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-05-02 11:14:52,881 - memory_profile6_log - INFO - Appending history data...
2018-05-02 11:14:52,882 - memory_profile6_log - INFO - processing batch-0
2018-05-02 11:14:52,884 - memory_profile6_log - INFO - creating list history data...
2018-05-02 11:14:52,940 - memory_profile6_log - INFO - call history data...
2018-05-02 11:15:40,864 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 11:15:41,709 - memory_profile6_log - INFO - processing batch-1
2018-05-02 11:15:41,710 - memory_profile6_log - INFO - creating list history data...
2018-05-02 11:15:41,729 - memory_profile6_log - INFO - call history data...
2018-05-02 11:16:27,246 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 11:16:28,125 - memory_profile6_log - INFO - processing batch-2
2018-05-02 11:16:28,127 - memory_profile6_log - INFO - creating list history data...
2018-05-02 11:16:28,154 - memory_profile6_log - INFO - call history data...
2018-05-02 11:17:11,698 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 11:17:12,484 - memory_profile6_log - INFO - processing batch-3
2018-05-02 11:17:12,486 - memory_profile6_log - INFO - creating list history data...
2018-05-02 11:17:12,510 - memory_profile6_log - INFO - call history data...
2018-05-02 11:17:54,750 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 11:17:55,323 - memory_profile6_log - INFO - processing batch-4
2018-05-02 11:17:55,325 - memory_profile6_log - INFO - creating list history data...
2018-05-02 11:17:55,345 - memory_profile6_log - INFO - call history data...
2018-05-02 11:18:42,095 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 11:18:42,433 - memory_profile6_log - INFO - Appending training data...
2018-05-02 11:18:42,434 - memory_profile6_log - INFO - len datalist: 1
2018-05-02 11:18:42,436 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-05-02 11:18:42,440 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 11:18:42,440 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 11:18:42,441 - memory_profile6_log - INFO - ================================================

2018-05-02 11:18:42,443 - memory_profile6_log - INFO -    354     86.6 MiB     86.6 MiB   @profile

2018-05-02 11:18:42,444 - memory_profile6_log - INFO -    355                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-05-02 11:18:42,446 - memory_profile6_log - INFO -    356     86.6 MiB      0.0 MiB       bq_client = client

2018-05-02 11:18:42,447 - memory_profile6_log - INFO -    357     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 11:18:42,447 - memory_profile6_log - INFO -    358                             

2018-05-02 11:18:42,448 - memory_profile6_log - INFO -    359     86.6 MiB      0.0 MiB       datalist = []

2018-05-02 11:18:42,450 - memory_profile6_log - INFO -    360     86.6 MiB      0.0 MiB       datalist_hist = []

2018-05-02 11:18:42,450 - memory_profile6_log - INFO -    361                             

2018-05-02 11:18:42,450 - memory_profile6_log - INFO -    362     86.6 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-05-02 11:18:42,451 - memory_profile6_log - INFO -    363    328.1 MiB      0.0 MiB       for ndate in date_generated:

2018-05-02 11:18:42,451 - memory_profile6_log - INFO -    364    309.7 MiB    223.1 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-05-02 11:18:42,454 - memory_profile6_log - INFO -    365    309.7 MiB      0.0 MiB           if tframe is not None:

2018-05-02 11:18:42,456 - memory_profile6_log - INFO -    366    309.7 MiB      0.0 MiB               if not tframe.empty:

2018-05-02 11:18:42,457 - memory_profile6_log - INFO -    367    314.5 MiB      4.8 MiB                   X_split = np.array_split(tframe, 5)

2018-05-02 11:18:42,457 - memory_profile6_log - INFO -    368    314.5 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 11:18:42,457 - memory_profile6_log - INFO -    369    314.5 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-05-02 11:18:42,459 - memory_profile6_log - INFO -    370    328.4 MiB     -2.0 MiB                   for ix in range(len(X_split)):

2018-05-02 11:18:42,459 - memory_profile6_log - INFO -    371                                                 # ~ loading history

2018-05-02 11:18:42,460 - memory_profile6_log - INFO -    372                                                 """

2018-05-02 11:18:42,460 - memory_profile6_log - INFO -    373                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-05-02 11:18:42,461 - memory_profile6_log - INFO -    374                                                 """

2018-05-02 11:18:42,461 - memory_profile6_log - INFO -    375    328.4 MiB     -1.6 MiB                       logger.info("processing batch-%d", ix)

2018-05-02 11:18:42,463 - memory_profile6_log - INFO -    376                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-05-02 11:18:42,467 - memory_profile6_log - INFO -    377    328.4 MiB     -1.6 MiB                       logger.info("creating list history data...")

2018-05-02 11:18:42,467 - memory_profile6_log - INFO -    378                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-05-02 11:18:42,469 - memory_profile6_log - INFO -    379    328.6 MiB     -0.1 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 11:18:42,470 - memory_profile6_log - INFO -    380                             

2018-05-02 11:18:42,470 - memory_profile6_log - INFO -    381    328.6 MiB     -2.1 MiB                       logger.info("call history data...")

2018-05-02 11:18:42,470 - memory_profile6_log - INFO -    382    328.3 MiB      9.8 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-05-02 11:18:42,471 - memory_profile6_log - INFO -    383                             

2018-05-02 11:18:42,473 - memory_profile6_log - INFO -    384                                                 # me = os.getpid()

2018-05-02 11:18:42,473 - memory_profile6_log - INFO -    385                                                 # kill_proc_tree(me)

2018-05-02 11:18:42,473 - memory_profile6_log - INFO -    386                             

2018-05-02 11:18:42,474 - memory_profile6_log - INFO -    387    328.3 MiB     -1.4 MiB                       logger.info("done collecting history data, appending now...")

2018-05-02 11:18:42,476 - memory_profile6_log - INFO -    388    328.4 MiB   -297.3 MiB                       for m in h_frame:

2018-05-02 11:18:42,479 - memory_profile6_log - INFO -    389    328.4 MiB   -295.9 MiB                           if m is not None:

2018-05-02 11:18:42,480 - memory_profile6_log - INFO -    390    328.4 MiB   -295.9 MiB                               if len(m) > 0:

2018-05-02 11:18:42,480 - memory_profile6_log - INFO -    391    328.4 MiB   -243.4 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-05-02 11:18:42,480 - memory_profile6_log - INFO -    392    328.4 MiB     -2.0 MiB                       del h_frame

2018-05-02 11:18:42,482 - memory_profile6_log - INFO -    393    328.4 MiB     -2.0 MiB                       del lhistory

2018-05-02 11:18:42,483 - memory_profile6_log - INFO -    394                             

2018-05-02 11:18:42,483 - memory_profile6_log - INFO -    395    328.1 MiB     -0.4 MiB                   logger.info("Appending training data...")

2018-05-02 11:18:42,483 - memory_profile6_log - INFO -    396    328.1 MiB      0.0 MiB                   datalist.append(tframe)

2018-05-02 11:18:42,484 - memory_profile6_log - INFO -    397                                     else: 

2018-05-02 11:18:42,486 - memory_profile6_log - INFO -    398                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-05-02 11:18:42,486 - memory_profile6_log - INFO -    399    328.1 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-05-02 11:18:42,486 - memory_profile6_log - INFO -    400    328.1 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-05-02 11:18:42,490 - memory_profile6_log - INFO -    401                             

2018-05-02 11:18:42,492 - memory_profile6_log - INFO -    402    328.1 MiB      0.0 MiB       return datalist, datalist_hist

2018-05-02 11:18:42,493 - memory_profile6_log - INFO - 


2018-05-02 11:18:43,372 - memory_profile6_log - INFO - size of big_frame_hist: 8.37 MB
2018-05-02 11:18:43,404 - memory_profile6_log - INFO - size of big_frame: 25.34 MB
2018-05-02 11:18:43,410 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-05-02 11:19:01,839 - memory_profile6_log - INFO - size of df: 24.50 MB
2018-05-02 11:19:01,841 - memory_profile6_log - INFO - getting total: 100000 training data(current date interest)
2018-05-02 11:19:01,872 - memory_profile6_log - INFO - size of current_frame: 25.27 MB
2018-05-02 11:19:01,874 - memory_profile6_log - INFO - loading time of: 200000 total genuine-current interest data ~ take 278.870s
2018-05-02 11:19:01,881 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 11:19:01,881 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 11:19:01,881 - memory_profile6_log - INFO - ================================================

2018-05-02 11:19:01,884 - memory_profile6_log - INFO -    404     86.4 MiB     86.4 MiB   @profile

2018-05-02 11:19:01,884 - memory_profile6_log - INFO -    405                             def preprocess(cpu, cd, query_fit, date_generated):

2018-05-02 11:19:01,887 - memory_profile6_log - INFO -    406     86.6 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-05-02 11:19:01,888 - memory_profile6_log - INFO -    407     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 11:19:01,890 - memory_profile6_log - INFO -    408                             

2018-05-02 11:19:01,891 - memory_profile6_log - INFO -    409                                 # ~~~ Begin collecting data ~~~

2018-05-02 11:19:01,891 - memory_profile6_log - INFO -    410     86.6 MiB      0.0 MiB       t0 = time.time()

2018-05-02 11:19:01,891 - memory_profile6_log - INFO -    411    326.5 MiB    239.9 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-05-02 11:19:01,891 - memory_profile6_log - INFO -    412    326.5 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-05-02 11:19:01,891 - memory_profile6_log - INFO -    413                                     logger.info("Training cannot be empty..")

2018-05-02 11:19:01,895 - memory_profile6_log - INFO -    414                                     return False

2018-05-02 11:19:01,897 - memory_profile6_log - INFO -    415    327.2 MiB      0.7 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-05-02 11:19:01,898 - memory_profile6_log - INFO -    416                                 # print "big_frame_hist:\n", big_frame_hist.head(20)

2018-05-02 11:19:01,898 - memory_profile6_log - INFO -    417    327.2 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-05-02 11:19:01,898 - memory_profile6_log - INFO -    418                             

2018-05-02 11:19:01,900 - memory_profile6_log - INFO -    419    328.7 MiB      1.5 MiB       big_frame = pd.concat(datalist)

2018-05-02 11:19:01,900 - memory_profile6_log - INFO -    420    328.7 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-05-02 11:19:01,900 - memory_profile6_log - INFO -    421    327.2 MiB     -1.5 MiB       del datalist

2018-05-02 11:19:01,901 - memory_profile6_log - INFO -    422                             

2018-05-02 11:19:01,901 - memory_profile6_log - INFO -    423    327.2 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 11:19:01,901 - memory_profile6_log - INFO -    424                             

2018-05-02 11:19:01,901 - memory_profile6_log - INFO -    425                                 # ~ get current news interest ~

2018-05-02 11:19:01,901 - memory_profile6_log - INFO -    426    327.2 MiB      0.0 MiB       if not cd:

2018-05-02 11:19:01,901 - memory_profile6_log - INFO -    427    327.2 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-05-02 11:19:01,903 - memory_profile6_log - INFO -    428    336.6 MiB      9.3 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-05-02 11:19:01,903 - memory_profile6_log - INFO -    429    336.6 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-05-02 11:19:01,904 - memory_profile6_log - INFO -    430                                 else:

2018-05-02 11:19:01,910 - memory_profile6_log - INFO -    431                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-05-02 11:19:01,911 - memory_profile6_log - INFO -    432                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date) LIMIT 100000"

2018-05-02 11:19:01,911 - memory_profile6_log - INFO -    433                             

2018-05-02 11:19:01,913 - memory_profile6_log - INFO -    434                                     # safe handling of query parameter

2018-05-02 11:19:01,914 - memory_profile6_log - INFO -    435                                     query_params = [

2018-05-02 11:19:01,914 - memory_profile6_log - INFO -    436                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-05-02 11:19:01,915 - memory_profile6_log - INFO -    437                                     ]

2018-05-02 11:19:01,915 - memory_profile6_log - INFO -    438                             

2018-05-02 11:19:01,917 - memory_profile6_log - INFO -    439                                     job_config.query_parameters = query_params

2018-05-02 11:19:01,917 - memory_profile6_log - INFO -    440                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-05-02 11:19:01,921 - memory_profile6_log - INFO -    441                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-05-02 11:19:01,921 - memory_profile6_log - INFO -    442                             

2018-05-02 11:19:01,923 - memory_profile6_log - INFO -    443    336.6 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-05-02 11:19:01,924 - memory_profile6_log - INFO -    444    336.6 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-05-02 11:19:01,924 - memory_profile6_log - INFO -    445    336.7 MiB      0.1 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-05-02 11:19:01,924 - memory_profile6_log - INFO -    446    336.7 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-05-02 11:19:01,924 - memory_profile6_log - INFO -    447    336.7 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 11:19:01,926 - memory_profile6_log - INFO -    448    336.7 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-05-02 11:19:01,926 - memory_profile6_log - INFO -    449                             

2018-05-02 11:19:01,926 - memory_profile6_log - INFO -    450    336.7 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-05-02 11:19:01,927 - memory_profile6_log - INFO - 


2018-05-02 11:19:01,931 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-05-02 11:19:01,946 - memory_profile6_log - INFO - train on: 100000 total genuine interest data(D(u, t))
2018-05-02 11:19:01,947 - memory_profile6_log - INFO - transform on: 100000 total current data(D(t))
2018-05-02 11:19:01,947 - memory_profile6_log - INFO - apply on: 30700 total history...)
2018-05-02 11:19:02,046 - memory_profile6_log - INFO - len of uniques_fit_hist:30700
2018-05-02 11:19:02,062 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:11725
2018-05-02 11:19:02,223 - memory_profile6_log - INFO - self.sum_all_nt before concate:

2018-05-02 11:19:02,246 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 11:19:02,247 - memory_profile6_log - INFO - 

2018-05-02 11:19:02,282 - memory_profile6_log - INFO - self.sum_all_nt after concate:

2018-05-02 11:19:02,296 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 11:19:02,296 - memory_profile6_log - INFO - 

2018-05-02 11:19:02,382 - memory_profile6_log - INFO - Len of model_fit: 100000
2018-05-02 11:19:02,384 - memory_profile6_log - INFO - Len of df_dut: 100000
2018-05-02 11:19:02,721 - memory_profile6_log - INFO - Len of fitted_models on main class: 100000
2018-05-02 11:19:02,723 - memory_profile6_log - INFO - 

2018-05-02 11:19:02,724 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-05-02 11:19:02,726 - memory_profile6_log - INFO - 

2018-05-02 11:19:02,726 - memory_profile6_log - INFO - fitted_model_hist:

2018-05-02 11:19:02,727 - memory_profile6_log - INFO - 

2018-05-02 11:19:02,746 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 11:19:02,747 - memory_profile6_log - INFO - 

2018-05-02 11:19:02,767 - memory_profile6_log - INFO - len of fitted models after concat: 130700
2018-05-02 11:19:02,769 - memory_profile6_log - INFO - 

2018-05-02 11:19:02,769 - memory_profile6_log - INFO - Recalculating fitted models...
2018-05-02 11:19:02,770 - memory_profile6_log - INFO - 

2018-05-02 11:19:02,858 - memory_profile6_log - INFO - fitted_models after cobine:

2018-05-02 11:19:02,861 - memory_profile6_log - INFO - 

2018-05-02 11:19:02,878 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 11:19:02,880 - memory_profile6_log - INFO - 

2018-05-02 11:19:02,881 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 100000
2018-05-02 11:19:02,881 - memory_profile6_log - INFO - 

2018-05-02 11:19:19,566 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-05-02 11:19:19,625 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, topic_id, pt_posterior_x_Nt, smoothed_pt_posterior, p0_cat_ci, sigma_Nt, p0_posterior, is_general, rank]
Index: []
2018-05-02 11:19:19,627 - memory_profile6_log - INFO - 

2018-05-02 11:19:19,628 - memory_profile6_log - INFO - Len of model_transform: 94820
2018-05-02 11:19:19,628 - memory_profile6_log - INFO - Len of df_dt: 100000
2018-05-02 11:19:19,630 - memory_profile6_log - INFO - Total train time: 17.638s
2018-05-02 11:19:19,631 - memory_profile6_log - INFO - memory left before cleaning: 86.900 percent memory...
2018-05-02 11:19:19,631 - memory_profile6_log - INFO - cleaning up some objects...
2018-05-02 11:19:19,632 - memory_profile6_log - INFO - deleting df_dut...
2018-05-02 11:19:19,634 - memory_profile6_log - INFO - deleting df_dt...
2018-05-02 11:19:19,637 - memory_profile6_log - INFO - deleting df_input...
2018-05-02 11:19:19,641 - memory_profile6_log - INFO - deleting df_input_X...
2018-05-02 11:19:19,641 - memory_profile6_log - INFO - deleting df_current...
2018-05-02 11:19:19,642 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-05-02 11:19:19,648 - memory_profile6_log - INFO - deleting model_fit...
2018-05-02 11:19:19,650 - memory_profile6_log - INFO - deleting result...
2018-05-02 11:19:19,671 - memory_profile6_log - INFO - deleting model_transform...
2018-05-02 11:19:19,671 - memory_profile6_log - INFO - memory left after cleaning: 86.800 percent memory...
2018-05-02 11:19:19,673 - memory_profile6_log - INFO - Begin saving trained data...
2018-05-02 11:19:19,674 - memory_profile6_log - INFO - Using ElasticSearch as storage...
2018-05-02 11:21:05,319 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 11:21:05,322 - memory_profile6_log - INFO - date_generated: 
2018-05-02 11:21:05,322 - memory_profile6_log - INFO -  
2018-05-02 11:21:05,323 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 11, 21, 5, 319000)]
2018-05-02 11:21:05,323 - memory_profile6_log - INFO - 

2018-05-02 11:21:05,325 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 11:21:05,325 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 11:21:05,325 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 11:21:05,490 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 11:21:05,493 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 11:28:42,834 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 11:28:42,838 - memory_profile6_log - INFO - date_generated: 
2018-05-02 11:28:42,838 - memory_profile6_log - INFO -  
2018-05-02 11:28:42,838 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 11, 28, 42, 835000)]
2018-05-02 11:28:42,839 - memory_profile6_log - INFO - 

2018-05-02 11:28:42,839 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 11:28:42,841 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 11:28:42,842 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 11:28:43,000 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 11:28:43,005 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 11:31:00,536 - memory_profile6_log - INFO - size of df: 82.28 MB
2018-05-02 11:31:00,536 - memory_profile6_log - INFO - getting total: 324874 training data(genuine interest) for date: 2018-05-01
2018-05-02 11:31:00,589 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-05-02 11:31:00,591 - memory_profile6_log - INFO - Appending history data...
2018-05-02 11:31:00,592 - memory_profile6_log - INFO - processing batch-0
2018-05-02 11:31:00,592 - memory_profile6_log - INFO - creating list history data...
2018-05-02 11:31:00,691 - memory_profile6_log - INFO - call history data...
2018-05-02 11:32:49,220 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 11:32:50,453 - memory_profile6_log - INFO - processing batch-1
2018-05-02 11:32:50,456 - memory_profile6_log - INFO - creating list history data...
2018-05-02 11:32:50,523 - memory_profile6_log - INFO - call history data...
2018-05-02 11:34:27,313 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 11:34:28,671 - memory_profile6_log - INFO - processing batch-2
2018-05-02 11:34:28,674 - memory_profile6_log - INFO - creating list history data...
2018-05-02 11:34:28,752 - memory_profile6_log - INFO - call history data...
2018-05-02 11:36:26,720 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 11:36:27,638 - memory_profile6_log - INFO - processing batch-3
2018-05-02 11:36:27,638 - memory_profile6_log - INFO - creating list history data...
2018-05-02 11:36:27,714 - memory_profile6_log - INFO - call history data...
2018-05-02 11:38:00,101 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 11:38:00,960 - memory_profile6_log - INFO - processing batch-4
2018-05-02 11:38:00,960 - memory_profile6_log - INFO - creating list history data...
2018-05-02 11:38:01,016 - memory_profile6_log - INFO - call history data...
2018-05-02 11:39:36,049 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 11:39:36,538 - memory_profile6_log - INFO - Appending training data...
2018-05-02 11:39:36,539 - memory_profile6_log - INFO - len datalist: 1
2018-05-02 11:39:36,540 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-05-02 11:39:36,553 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 11:39:36,555 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 11:39:36,555 - memory_profile6_log - INFO - ================================================

2018-05-02 11:39:36,558 - memory_profile6_log - INFO -    355     86.5 MiB     86.5 MiB   @profile

2018-05-02 11:39:36,559 - memory_profile6_log - INFO -    356                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-05-02 11:39:36,559 - memory_profile6_log - INFO -    357     86.5 MiB      0.0 MiB       bq_client = client

2018-05-02 11:39:36,562 - memory_profile6_log - INFO -    358     86.5 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 11:39:36,562 - memory_profile6_log - INFO -    359                             

2018-05-02 11:39:36,563 - memory_profile6_log - INFO -    360     86.5 MiB      0.0 MiB       datalist = []

2018-05-02 11:39:36,565 - memory_profile6_log - INFO -    361     86.5 MiB      0.0 MiB       datalist_hist = []

2018-05-02 11:39:36,565 - memory_profile6_log - INFO -    362                             

2018-05-02 11:39:36,566 - memory_profile6_log - INFO -    363     86.5 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-05-02 11:39:36,569 - memory_profile6_log - INFO -    364    402.4 MiB      0.0 MiB       for ndate in date_generated:

2018-05-02 11:39:36,571 - memory_profile6_log - INFO -    365    366.2 MiB    279.7 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-05-02 11:39:36,572 - memory_profile6_log - INFO -    366    366.2 MiB      0.0 MiB           if tframe is not None:

2018-05-02 11:39:36,572 - memory_profile6_log - INFO -    367    366.2 MiB      0.0 MiB               if not tframe.empty:

2018-05-02 11:39:36,573 - memory_profile6_log - INFO -    368    376.4 MiB     10.1 MiB                   X_split = np.array_split(tframe, 5)

2018-05-02 11:39:36,573 - memory_profile6_log - INFO -    369    376.4 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 11:39:36,582 - memory_profile6_log - INFO -    370    376.4 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-05-02 11:39:36,582 - memory_profile6_log - INFO -    371    402.5 MiB     -8.2 MiB                   for ix in range(len(X_split)):

2018-05-02 11:39:36,584 - memory_profile6_log - INFO -    372                                                 # ~ loading history

2018-05-02 11:39:36,585 - memory_profile6_log - INFO -    373                                                 """

2018-05-02 11:39:36,586 - memory_profile6_log - INFO -    374                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-05-02 11:39:36,588 - memory_profile6_log - INFO -    375                                                 """

2018-05-02 11:39:36,588 - memory_profile6_log - INFO -    376    402.5 MiB     -8.1 MiB                       logger.info("processing batch-%d", ix)

2018-05-02 11:39:36,589 - memory_profile6_log - INFO -    377                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-05-02 11:39:36,592 - memory_profile6_log - INFO -    378    402.5 MiB     -8.1 MiB                       logger.info("creating list history data...")

2018-05-02 11:39:36,594 - memory_profile6_log - INFO -    379                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-05-02 11:39:36,595 - memory_profile6_log - INFO -    380    401.2 MiB     -3.7 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 11:39:36,595 - memory_profile6_log - INFO -    381                             

2018-05-02 11:39:36,596 - memory_profile6_log - INFO -    382    401.2 MiB     -5.8 MiB                       logger.info("call history data...")

2018-05-02 11:39:36,598 - memory_profile6_log - INFO -    383    421.0 MiB     75.2 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-05-02 11:39:36,598 - memory_profile6_log - INFO -    384                             

2018-05-02 11:39:36,599 - memory_profile6_log - INFO -    385                                                 # me = os.getpid()

2018-05-02 11:39:36,604 - memory_profile6_log - INFO -    386                                                 # kill_proc_tree(me)

2018-05-02 11:39:36,605 - memory_profile6_log - INFO -    387                             

2018-05-02 11:39:36,605 - memory_profile6_log - INFO -    388    421.0 MiB    -69.3 MiB                       logger.info("done collecting history data, appending now...")

2018-05-02 11:39:36,607 - memory_profile6_log - INFO -    389    421.9 MiB -10923.2 MiB                       for m in h_frame:

2018-05-02 11:39:36,608 - memory_profile6_log - INFO -    390    421.8 MiB -10859.6 MiB                           if m is not None:

2018-05-02 11:39:36,608 - memory_profile6_log - INFO -    391    421.8 MiB -10829.2 MiB                               if len(m) > 0:

2018-05-02 11:39:36,609 - memory_profile6_log - INFO -    392    421.9 MiB  -8801.0 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-05-02 11:39:36,611 - memory_profile6_log - INFO -    393    402.5 MiB   -132.2 MiB                       del h_frame

2018-05-02 11:39:36,614 - memory_profile6_log - INFO -    394    402.5 MiB     -8.2 MiB                       del lhistory

2018-05-02 11:39:36,615 - memory_profile6_log - INFO -    395                             

2018-05-02 11:39:36,615 - memory_profile6_log - INFO -    396    402.4 MiB     -0.1 MiB                   logger.info("Appending training data...")

2018-05-02 11:39:36,617 - memory_profile6_log - INFO -    397    402.4 MiB      0.0 MiB                   datalist.append(tframe)

2018-05-02 11:39:36,618 - memory_profile6_log - INFO -    398                                     else: 

2018-05-02 11:39:36,618 - memory_profile6_log - INFO -    399                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-05-02 11:39:36,621 - memory_profile6_log - INFO -    400    402.4 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-05-02 11:39:36,621 - memory_profile6_log - INFO -    401    402.4 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-05-02 11:39:36,624 - memory_profile6_log - INFO -    402                             

2018-05-02 11:39:36,625 - memory_profile6_log - INFO -    403    402.4 MiB      0.0 MiB       return datalist, datalist_hist

2018-05-02 11:39:36,627 - memory_profile6_log - INFO - 


2018-05-02 11:39:37,818 - memory_profile6_log - INFO - size of big_frame_hist: 27.00 MB
2018-05-02 11:39:37,940 - memory_profile6_log - INFO - size of big_frame: 82.28 MB
2018-05-02 11:39:37,993 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-05-02 11:40:16,779 - memory_profile6_log - INFO - size of df: 24.51 MB
2018-05-02 11:40:16,780 - memory_profile6_log - INFO - getting total: 100000 training data(current date interest)
2018-05-02 11:40:16,826 - memory_profile6_log - INFO - size of current_frame: 25.27 MB
2018-05-02 11:40:16,828 - memory_profile6_log - INFO - loading time of: 424874 total genuine-current interest data ~ take 693.852s
2018-05-02 11:40:16,841 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 11:40:16,841 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 11:40:16,842 - memory_profile6_log - INFO - ================================================

2018-05-02 11:40:16,842 - memory_profile6_log - INFO -    405     86.4 MiB     86.4 MiB   @profile

2018-05-02 11:40:16,844 - memory_profile6_log - INFO -    406                             def preprocess(cpu, cd, query_fit, date_generated):

2018-05-02 11:40:16,845 - memory_profile6_log - INFO -    407     86.5 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-05-02 11:40:16,845 - memory_profile6_log - INFO -    408     86.5 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 11:40:16,849 - memory_profile6_log - INFO -    409                             

2018-05-02 11:40:16,851 - memory_profile6_log - INFO -    410                                 # ~~~ Begin collecting data ~~~

2018-05-02 11:40:16,851 - memory_profile6_log - INFO -    411     86.5 MiB      0.0 MiB       t0 = time.time()

2018-05-02 11:40:16,851 - memory_profile6_log - INFO -    412    376.5 MiB    290.0 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-05-02 11:40:16,852 - memory_profile6_log - INFO -    413    376.5 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-05-02 11:40:16,854 - memory_profile6_log - INFO -    414                                     logger.info("Training cannot be empty..")

2018-05-02 11:40:16,854 - memory_profile6_log - INFO -    415                                     return False

2018-05-02 11:40:16,855 - memory_profile6_log - INFO -    416    385.0 MiB      8.4 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-05-02 11:40:16,855 - memory_profile6_log - INFO -    417                                 # print "big_frame_hist:\n", big_frame_hist.head(20)

2018-05-02 11:40:16,857 - memory_profile6_log - INFO -    418    385.0 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-05-02 11:40:16,861 - memory_profile6_log - INFO -    419                             

2018-05-02 11:40:16,861 - memory_profile6_log - INFO -    420    392.7 MiB      7.8 MiB       big_frame = pd.concat(datalist)

2018-05-02 11:40:16,862 - memory_profile6_log - INFO -    421    392.7 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-05-02 11:40:16,864 - memory_profile6_log - INFO -    422    385.3 MiB     -7.4 MiB       del datalist

2018-05-02 11:40:16,864 - memory_profile6_log - INFO -    423                             

2018-05-02 11:40:16,865 - memory_profile6_log - INFO -    424    385.3 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 11:40:16,867 - memory_profile6_log - INFO -    425                             

2018-05-02 11:40:16,867 - memory_profile6_log - INFO -    426                                 # ~ get current news interest ~

2018-05-02 11:40:16,868 - memory_profile6_log - INFO -    427    385.3 MiB      0.0 MiB       if not cd:

2018-05-02 11:40:16,868 - memory_profile6_log - INFO -    428    385.3 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-05-02 11:40:16,872 - memory_profile6_log - INFO -    429    426.1 MiB     40.8 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-05-02 11:40:16,874 - memory_profile6_log - INFO -    430    426.1 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-05-02 11:40:16,875 - memory_profile6_log - INFO -    431                                 else:

2018-05-02 11:40:16,875 - memory_profile6_log - INFO -    432                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-05-02 11:40:16,878 - memory_profile6_log - INFO -    433                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-05-02 11:40:16,878 - memory_profile6_log - INFO -    434                             

2018-05-02 11:40:16,881 - memory_profile6_log - INFO -    435                                     # safe handling of query parameter

2018-05-02 11:40:16,884 - memory_profile6_log - INFO -    436                                     query_params = [

2018-05-02 11:40:16,885 - memory_profile6_log - INFO -    437                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-05-02 11:40:16,885 - memory_profile6_log - INFO -    438                                     ]

2018-05-02 11:40:16,887 - memory_profile6_log - INFO -    439                             

2018-05-02 11:40:16,888 - memory_profile6_log - INFO -    440                                     job_config.query_parameters = query_params

2018-05-02 11:40:16,888 - memory_profile6_log - INFO -    441                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-05-02 11:40:16,888 - memory_profile6_log - INFO -    442                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-05-02 11:40:16,888 - memory_profile6_log - INFO -    443                             

2018-05-02 11:40:16,890 - memory_profile6_log - INFO -    444    426.1 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-05-02 11:40:16,891 - memory_profile6_log - INFO -    445    426.1 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-05-02 11:40:16,891 - memory_profile6_log - INFO -    446    426.2 MiB      0.1 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-05-02 11:40:16,891 - memory_profile6_log - INFO -    447    426.2 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-05-02 11:40:16,895 - memory_profile6_log - INFO -    448    426.2 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 11:40:16,897 - memory_profile6_log - INFO -    449    426.2 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-05-02 11:40:16,900 - memory_profile6_log - INFO -    450                             

2018-05-02 11:40:16,901 - memory_profile6_log - INFO -    451    426.2 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-05-02 11:40:16,903 - memory_profile6_log - INFO - 


2018-05-02 11:40:16,911 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-05-02 11:40:16,944 - memory_profile6_log - INFO - train on: 324874 total genuine interest data(D(u, t))
2018-05-02 11:40:16,944 - memory_profile6_log - INFO - transform on: 100000 total current data(D(t))
2018-05-02 11:40:16,946 - memory_profile6_log - INFO - apply on: 99031 total history...)
2018-05-02 11:40:17,289 - memory_profile6_log - INFO - len of uniques_fit_hist:99031
2018-05-02 11:40:17,361 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:28983
2018-05-02 11:40:18,148 - memory_profile6_log - INFO - self.sum_all_nt before concate:

2018-05-02 11:40:18,217 - memory_profile6_log - INFO -                                                  user_id  sigma_Nt
15052  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        27
2018-05-02 11:40:18,217 - memory_profile6_log - INFO - 

2018-05-02 11:40:18,311 - memory_profile6_log - INFO - self.sum_all_nt after concate:

2018-05-02 11:40:18,339 - memory_profile6_log - INFO -                                                  user_id  sigma_Nt
15052  1616f009d96b1-0285d8288a5bce-70217860-38400-16...      1288
2018-05-02 11:40:18,341 - memory_profile6_log - INFO - 

2018-05-02 11:40:18,723 - memory_profile6_log - INFO - Len of model_fit: 324874
2018-05-02 11:40:18,723 - memory_profile6_log - INFO - Len of df_dut: 324874
2018-05-02 11:40:19,598 - memory_profile6_log - INFO - Len of fitted_models on main class: 324874
2018-05-02 11:40:19,598 - memory_profile6_log - INFO - 

2018-05-02 11:40:19,599 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-05-02 11:40:19,601 - memory_profile6_log - INFO - 

2018-05-02 11:40:19,601 - memory_profile6_log - INFO - fitted_model_hist:

2018-05-02 11:40:19,604 - memory_profile6_log - INFO - 

2018-05-02 11:40:19,678 - memory_profile6_log - INFO -      pt_posterior_x_Nt   topic_id                                            user_id
38           15.991521   22553543  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
216          83.719466   22601470  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
21          516.753870   27313197  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
160          84.916311   27431099  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
138          28.579819   33020303  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
205          94.876510   39301645  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
44           67.425244   41639090  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
53           15.283089  107151123  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
2018-05-02 11:40:19,680 - memory_profile6_log - INFO - 

2018-05-02 11:40:19,750 - memory_profile6_log - INFO - len of fitted models after concat: 423905
2018-05-02 11:40:19,750 - memory_profile6_log - INFO - 

2018-05-02 11:40:19,752 - memory_profile6_log - INFO - Recalculating fitted models...
2018-05-02 11:40:19,753 - memory_profile6_log - INFO - 

2018-05-02 11:40:20,138 - memory_profile6_log - INFO - fitted_models after cobine:

2018-05-02 11:40:20,141 - memory_profile6_log - INFO - 

2018-05-02 11:40:20,226 - memory_profile6_log - INFO -      pt_posterior_x_Nt   topic_id                                            user_id
38           15.991521   22553543  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
216          83.719466   22601470  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
21          516.753870   27313197  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
160          84.916311   27431099  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
138          28.579819   33020303  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
205          94.876510   39301645  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
44           67.425244   41639090  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
53           15.283089  107151123  1616f009d96b1-0285d8288a5bce-70217860-38400-16...
2018-05-02 11:40:20,227 - memory_profile6_log - INFO - 

2018-05-02 11:40:20,230 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 324874
2018-05-02 11:40:20,232 - memory_profile6_log - INFO - 

2018-05-02 11:41:08,687 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-05-02 11:41:08,944 - memory_profile6_log - INFO -                                                   user_id           topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt  p0_posterior  is_general  rank
168616  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22601470         180.583539             190.583539   0.047214      1288      0.006932        True   1.0
168615  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22553543          38.386972              48.386972   0.131697      1288      0.004909        True   2.0
168618  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27431099         188.011932             198.011932   0.024931      1288      0.003803        True   3.0
168623  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           33020303          93.333873             103.333873   0.030118      1288      0.002398        True   4.0
168617  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27313197        1059.941030            1069.941030   0.000769      1288      0.000634        True   5.0
168612  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1149084290        3395.843537            3405.843537   0.001226      1288      0.003217       False   1.0
168624  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           39301645         351.528952             361.528952   0.009377      1288      0.002612       False   2.0
168611  1616f009d96b1-0285d8288a5bce-70217860-38400-16...          107151123         106.692904             116.692904   0.026585      1288      0.002390       False   3.0
168622  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790314285         102.376743             112.376743   0.013473      1288      0.001166       False   4.0
168614  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1161375937          56.687372              66.687372   0.011116      1288      0.000571       False   5.0
168625  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           39825972          33.653947              43.653947   0.013328      1288      0.000448       False   6.0
168610  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1038512181          71.312714              81.312714   0.006583      1288      0.000412       False   7.0
168609  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1037574949          20.851671              30.851671   0.017247      1288      0.000410       False   8.0
168613  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1157057159          66.108992              76.108992   0.005126      1288      0.000301       False   9.0
168620  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790313523        2655.260638            2665.260638   0.000111      1288      0.000227       False  10.0
168608  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1023751122         771.544049             781.544049   0.000357      1288      0.000215       False  11.0
168627  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           62323193         776.343701             786.343701   0.000332      1288      0.000201       False  12.0
168626  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           41639090        1067.804002            1077.804002   0.000211      1288      0.000175       False  13.0
168619  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790312980        5254.621053            5264.621053   0.000040      1288      0.000163       False  14.0
168621  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790314267         597.116029             607.116029   0.000095      1288      0.000045       False  15.0
2018-05-02 11:41:08,947 - memory_profile6_log - INFO - 

2018-05-02 11:41:08,950 - memory_profile6_log - INFO - Len of model_transform: 304489
2018-05-02 11:41:08,953 - memory_profile6_log - INFO - Len of df_dt: 100000
2018-05-02 11:41:08,953 - memory_profile6_log - INFO - Total train time: 51.780s
2018-05-02 11:41:08,954 - memory_profile6_log - INFO - memory left before cleaning: 87.500 percent memory...
2018-05-02 11:41:08,959 - memory_profile6_log - INFO - cleaning up some objects...
2018-05-02 11:41:08,960 - memory_profile6_log - INFO - deleting df_dut...
2018-05-02 11:41:08,960 - memory_profile6_log - INFO - deleting df_dt...
2018-05-02 11:41:08,961 - memory_profile6_log - INFO - deleting df_input...
2018-05-02 11:41:08,966 - memory_profile6_log - INFO - deleting df_input_X...
2018-05-02 11:41:08,967 - memory_profile6_log - INFO - deleting df_current...
2018-05-02 11:41:08,970 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-05-02 11:41:08,984 - memory_profile6_log - INFO - deleting model_fit...
2018-05-02 11:41:08,986 - memory_profile6_log - INFO - deleting result...
2018-05-02 11:41:09,030 - memory_profile6_log - INFO - list(model_transformsv) before rename: 
2018-05-02 11:41:09,032 - memory_profile6_log - INFO -  
2018-05-02 11:41:09,032 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'is_general', 'p0_posterior', 'rank', 'date']
2018-05-02 11:41:09,033 - memory_profile6_log - INFO - 

2018-05-02 11:41:09,053 - memory_profile6_log - INFO - list(model_transformsv) after rename: 
2018-05-02 11:41:09,055 - memory_profile6_log - INFO - 

2018-05-02 11:43:38,555 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 11:43:38,562 - memory_profile6_log - INFO - date_generated: 
2018-05-02 11:43:38,562 - memory_profile6_log - INFO -  
2018-05-02 11:43:38,562 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 11, 43, 38, 557000)]
2018-05-02 11:43:38,563 - memory_profile6_log - INFO - 

2018-05-02 11:43:38,563 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 11:43:38,565 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 11:43:38,565 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 11:43:38,822 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 11:43:38,828 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 11:45:24,095 - memory_profile6_log - INFO - size of df: 82.28 MB
2018-05-02 11:45:24,096 - memory_profile6_log - INFO - getting total: 324874 training data(genuine interest) for date: 2018-05-01
2018-05-02 11:45:24,157 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-05-02 11:45:24,160 - memory_profile6_log - INFO - Appending history data...
2018-05-02 11:45:24,161 - memory_profile6_log - INFO - processing batch-0
2018-05-02 11:45:24,161 - memory_profile6_log - INFO - creating list history data...
2018-05-02 11:45:24,229 - memory_profile6_log - INFO - call history data...
2018-05-02 11:46:04,430 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 11:46:05,145 - memory_profile6_log - INFO - processing batch-1
2018-05-02 11:46:05,148 - memory_profile6_log - INFO - creating list history data...
2018-05-02 11:46:05,157 - memory_profile6_log - INFO - call history data...
2018-05-02 11:46:44,401 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 11:46:45,102 - memory_profile6_log - INFO - processing batch-2
2018-05-02 11:46:45,104 - memory_profile6_log - INFO - creating list history data...
2018-05-02 11:46:45,115 - memory_profile6_log - INFO - call history data...
2018-05-02 11:47:25,766 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 11:47:25,917 - memory_profile6_log - INFO - processing batch-3
2018-05-02 11:47:25,917 - memory_profile6_log - INFO - creating list history data...
2018-05-02 11:47:25,926 - memory_profile6_log - INFO - call history data...
2018-05-02 11:48:03,407 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 11:48:03,887 - memory_profile6_log - INFO - processing batch-4
2018-05-02 11:48:03,888 - memory_profile6_log - INFO - creating list history data...
2018-05-02 11:48:03,898 - memory_profile6_log - INFO - call history data...
2018-05-02 11:48:39,019 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 11:48:39,270 - memory_profile6_log - INFO - Appending training data...
2018-05-02 11:48:39,272 - memory_profile6_log - INFO - len datalist: 1
2018-05-02 11:48:39,273 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-05-02 11:48:39,276 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 11:48:39,278 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 11:48:39,279 - memory_profile6_log - INFO - ================================================

2018-05-02 11:48:39,279 - memory_profile6_log - INFO -    356     86.9 MiB     86.9 MiB   @profile

2018-05-02 11:48:39,282 - memory_profile6_log - INFO -    357                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-05-02 11:48:39,283 - memory_profile6_log - INFO -    358     86.9 MiB      0.0 MiB       bq_client = client

2018-05-02 11:48:39,285 - memory_profile6_log - INFO -    359     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 11:48:39,286 - memory_profile6_log - INFO -    360                             

2018-05-02 11:48:39,286 - memory_profile6_log - INFO -    361     86.9 MiB      0.0 MiB       datalist = []

2018-05-02 11:48:39,288 - memory_profile6_log - INFO -    362     86.9 MiB      0.0 MiB       datalist_hist = []

2018-05-02 11:48:39,289 - memory_profile6_log - INFO -    363                             

2018-05-02 11:48:39,292 - memory_profile6_log - INFO -    364     86.9 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-05-02 11:48:39,293 - memory_profile6_log - INFO -    365    377.5 MiB      0.0 MiB       for ndate in date_generated:

2018-05-02 11:48:39,295 - memory_profile6_log - INFO -    366    365.7 MiB    278.8 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-05-02 11:48:39,296 - memory_profile6_log - INFO -    367    365.7 MiB      0.0 MiB           if tframe is not None:

2018-05-02 11:48:39,296 - memory_profile6_log - INFO -    368    365.7 MiB      0.0 MiB               if not tframe.empty:

2018-05-02 11:48:39,298 - memory_profile6_log - INFO -    369    375.5 MiB      9.8 MiB                   X_split = np.array_split(tframe, 5)

2018-05-02 11:48:39,299 - memory_profile6_log - INFO -    370    375.5 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 11:48:39,299 - memory_profile6_log - INFO -    371    375.5 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-05-02 11:48:39,302 - memory_profile6_log - INFO -    372    377.5 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-05-02 11:48:39,303 - memory_profile6_log - INFO -    373                                                 # ~ loading history

2018-05-02 11:48:39,305 - memory_profile6_log - INFO -    374                                                 """

2018-05-02 11:48:39,305 - memory_profile6_log - INFO -    375                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-05-02 11:48:39,306 - memory_profile6_log - INFO -    376                                                 """

2018-05-02 11:48:39,311 - memory_profile6_log - INFO -    377    377.4 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-05-02 11:48:39,312 - memory_profile6_log - INFO -    378                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-05-02 11:48:39,312 - memory_profile6_log - INFO -    379    377.4 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-05-02 11:48:39,315 - memory_profile6_log - INFO -    380    377.4 MiB      0.5 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-05-02 11:48:39,315 - memory_profile6_log - INFO -    381                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 11:48:39,316 - memory_profile6_log - INFO -    382                             

2018-05-02 11:48:39,318 - memory_profile6_log - INFO -    383    377.4 MiB      0.0 MiB                       logger.info("call history data...")

2018-05-02 11:48:39,319 - memory_profile6_log - INFO -    384    377.5 MiB      1.2 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-05-02 11:48:39,319 - memory_profile6_log - INFO -    385                             

2018-05-02 11:48:39,321 - memory_profile6_log - INFO -    386                                                 # me = os.getpid()

2018-05-02 11:48:39,323 - memory_profile6_log - INFO -    387                                                 # kill_proc_tree(me)

2018-05-02 11:48:39,325 - memory_profile6_log - INFO -    388                             

2018-05-02 11:48:39,325 - memory_profile6_log - INFO -    389    377.5 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-05-02 11:48:39,326 - memory_profile6_log - INFO -    390    377.5 MiB      0.0 MiB                       for m in h_frame:

2018-05-02 11:48:39,328 - memory_profile6_log - INFO -    391    377.5 MiB      0.0 MiB                           if m is not None:

2018-05-02 11:48:39,328 - memory_profile6_log - INFO -    392    377.5 MiB      0.0 MiB                               if len(m) > 0:

2018-05-02 11:48:39,328 - memory_profile6_log - INFO -    393    377.5 MiB      0.2 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-05-02 11:48:39,329 - memory_profile6_log - INFO -    394    377.5 MiB      0.0 MiB                       del h_frame

2018-05-02 11:48:39,329 - memory_profile6_log - INFO -    395    377.5 MiB      0.0 MiB                       del lhistory

2018-05-02 11:48:39,331 - memory_profile6_log - INFO -    396                             

2018-05-02 11:48:39,332 - memory_profile6_log - INFO -    397    377.5 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-05-02 11:48:39,335 - memory_profile6_log - INFO -    398    377.5 MiB      0.0 MiB                   datalist.append(tframe)

2018-05-02 11:48:39,336 - memory_profile6_log - INFO -    399                                     else: 

2018-05-02 11:48:39,338 - memory_profile6_log - INFO -    400                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-05-02 11:48:39,338 - memory_profile6_log - INFO -    401    377.5 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-05-02 11:48:39,339 - memory_profile6_log - INFO -    402    377.5 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-05-02 11:48:39,341 - memory_profile6_log - INFO -    403                             

2018-05-02 11:48:39,342 - memory_profile6_log - INFO -    404    377.5 MiB      0.0 MiB       return datalist, datalist_hist

2018-05-02 11:48:39,342 - memory_profile6_log - INFO - 


2018-05-02 11:48:40,082 - memory_profile6_log - INFO - size of big_frame_hist: 317.73 KB
2018-05-02 11:48:40,191 - memory_profile6_log - INFO - size of big_frame: 82.28 MB
2018-05-02 11:48:40,209 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-05-02 11:49:02,091 - memory_profile6_log - INFO - size of df: 24.51 MB
2018-05-02 11:49:02,092 - memory_profile6_log - INFO - getting total: 100000 training data(current date interest)
2018-05-02 11:49:02,131 - memory_profile6_log - INFO - size of current_frame: 25.27 MB
2018-05-02 11:49:02,131 - memory_profile6_log - INFO - loading time of: 424874 total genuine-current interest data ~ take 323.344s
2018-05-02 11:49:02,137 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 11:49:02,137 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 11:49:02,138 - memory_profile6_log - INFO - ================================================

2018-05-02 11:49:02,138 - memory_profile6_log - INFO -    406     86.8 MiB     86.8 MiB   @profile

2018-05-02 11:49:02,140 - memory_profile6_log - INFO -    407                             def preprocess(cpu, cd, query_fit, date_generated):

2018-05-02 11:49:02,141 - memory_profile6_log - INFO -    408     86.9 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-05-02 11:49:02,142 - memory_profile6_log - INFO -    409     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 11:49:02,144 - memory_profile6_log - INFO -    410                             

2018-05-02 11:49:02,144 - memory_profile6_log - INFO -    411                                 # ~~~ Begin collecting data ~~~

2018-05-02 11:49:02,145 - memory_profile6_log - INFO -    412     86.9 MiB      0.0 MiB       t0 = time.time()

2018-05-02 11:49:02,147 - memory_profile6_log - INFO -    413    376.5 MiB    289.6 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-05-02 11:49:02,147 - memory_profile6_log - INFO -    414    376.5 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-05-02 11:49:02,148 - memory_profile6_log - INFO -    415                                     logger.info("Training cannot be empty..")

2018-05-02 11:49:02,150 - memory_profile6_log - INFO -    416                                     return False

2018-05-02 11:49:02,153 - memory_profile6_log - INFO -    417    377.3 MiB      0.8 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-05-02 11:49:02,154 - memory_profile6_log - INFO -    418                                 # print "big_frame_hist:\n", big_frame_hist.head(20)

2018-05-02 11:49:02,154 - memory_profile6_log - INFO -    419    377.3 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-05-02 11:49:02,155 - memory_profile6_log - INFO -    420                             

2018-05-02 11:49:02,155 - memory_profile6_log - INFO -    421    384.8 MiB      7.4 MiB       big_frame = pd.concat(datalist)

2018-05-02 11:49:02,157 - memory_profile6_log - INFO -    422    384.8 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-05-02 11:49:02,157 - memory_profile6_log - INFO -    423    377.3 MiB     -7.4 MiB       del datalist

2018-05-02 11:49:02,157 - memory_profile6_log - INFO -    424                             

2018-05-02 11:49:02,157 - memory_profile6_log - INFO -    425    377.3 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 11:49:02,158 - memory_profile6_log - INFO -    426                             

2018-05-02 11:49:02,158 - memory_profile6_log - INFO -    427                                 # ~ get current news interest ~

2018-05-02 11:49:02,160 - memory_profile6_log - INFO -    428    377.3 MiB      0.0 MiB       if not cd:

2018-05-02 11:49:02,161 - memory_profile6_log - INFO -    429    377.3 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-05-02 11:49:02,164 - memory_profile6_log - INFO -    430    388.4 MiB     11.0 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-05-02 11:49:02,164 - memory_profile6_log - INFO -    431    388.4 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-05-02 11:49:02,167 - memory_profile6_log - INFO -    432                                 else:

2018-05-02 11:49:02,167 - memory_profile6_log - INFO -    433                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-05-02 11:49:02,167 - memory_profile6_log - INFO -    434                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-05-02 11:49:02,168 - memory_profile6_log - INFO -    435                             

2018-05-02 11:49:02,168 - memory_profile6_log - INFO -    436                                     # safe handling of query parameter

2018-05-02 11:49:02,170 - memory_profile6_log - INFO -    437                                     query_params = [

2018-05-02 11:49:02,170 - memory_profile6_log - INFO -    438                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-05-02 11:49:02,171 - memory_profile6_log - INFO -    439                                     ]

2018-05-02 11:49:02,171 - memory_profile6_log - INFO -    440                             

2018-05-02 11:49:02,171 - memory_profile6_log - INFO -    441                                     job_config.query_parameters = query_params

2018-05-02 11:49:02,171 - memory_profile6_log - INFO -    442                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-05-02 11:49:02,174 - memory_profile6_log - INFO -    443                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-05-02 11:49:02,177 - memory_profile6_log - INFO -    444                             

2018-05-02 11:49:02,177 - memory_profile6_log - INFO -    445    388.4 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-05-02 11:49:02,177 - memory_profile6_log - INFO -    446    388.4 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-05-02 11:49:02,178 - memory_profile6_log - INFO -    447    389.1 MiB      0.8 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-05-02 11:49:02,180 - memory_profile6_log - INFO -    448    389.1 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-05-02 11:49:02,180 - memory_profile6_log - INFO -    449    389.1 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 11:49:02,181 - memory_profile6_log - INFO -    450    389.1 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-05-02 11:49:02,183 - memory_profile6_log - INFO -    451                             

2018-05-02 11:49:02,186 - memory_profile6_log - INFO -    452    389.1 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-05-02 11:49:02,187 - memory_profile6_log - INFO - 


2018-05-02 11:49:02,193 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-05-02 11:49:02,217 - memory_profile6_log - INFO - train on: 324874 total genuine interest data(D(u, t))
2018-05-02 11:49:02,219 - memory_profile6_log - INFO - transform on: 100000 total current data(D(t))
2018-05-02 11:49:02,220 - memory_profile6_log - INFO - apply on: 1137 total history...)
2018-05-02 11:49:02,566 - memory_profile6_log - INFO - len of uniques_fit_hist:1137
2018-05-02 11:49:02,572 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:1086
2018-05-02 11:49:03,288 - memory_profile6_log - INFO - self.sum_all_nt before concate:

2018-05-02 11:49:03,325 - memory_profile6_log - INFO -                                                  user_id  sigma_Nt
15052  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        27
2018-05-02 11:49:03,326 - memory_profile6_log - INFO - 

2018-05-02 11:49:03,388 - memory_profile6_log - INFO - self.sum_all_nt after concate:

2018-05-02 11:49:03,417 - memory_profile6_log - INFO -                                                  user_id  sigma_Nt
15052  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        27
2018-05-02 11:49:03,417 - memory_profile6_log - INFO - 

2018-05-02 11:49:03,767 - memory_profile6_log - INFO - Len of model_fit: 324874
2018-05-02 11:49:03,769 - memory_profile6_log - INFO - Len of df_dut: 324874
2018-05-02 11:49:04,519 - memory_profile6_log - INFO - Len of fitted_models on main class: 324874
2018-05-02 11:49:04,520 - memory_profile6_log - INFO - 

2018-05-02 11:49:04,522 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-05-02 11:49:04,523 - memory_profile6_log - INFO - 

2018-05-02 11:49:04,523 - memory_profile6_log - INFO - fitted_model_hist:

2018-05-02 11:49:04,526 - memory_profile6_log - INFO - 

2018-05-02 11:49:04,539 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 11:49:04,539 - memory_profile6_log - INFO - 

2018-05-02 11:49:04,595 - memory_profile6_log - INFO - len of fitted models after concat: 326011
2018-05-02 11:49:04,596 - memory_profile6_log - INFO - 

2018-05-02 11:49:04,598 - memory_profile6_log - INFO - Recalculating fitted models...
2018-05-02 11:49:04,598 - memory_profile6_log - INFO - 

2018-05-02 11:49:04,867 - memory_profile6_log - INFO - fitted_models after cobine:

2018-05-02 11:49:04,868 - memory_profile6_log - INFO - 

2018-05-02 11:49:04,882 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 11:49:04,884 - memory_profile6_log - INFO - 

2018-05-02 11:49:04,885 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 324874
2018-05-02 11:49:04,888 - memory_profile6_log - INFO - 

2018-05-02 11:49:50,736 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-05-02 11:49:51,019 - memory_profile6_log - INFO -                                                   user_id           topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt  p0_posterior  is_general  rank
168616  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22601470          96.864073             106.864073   0.047214        27      0.136365        True   1.0
168615  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22553543          22.395451              32.395451   0.131697        27      0.115308        True   2.0
168618  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27431099         103.095622             113.095622   0.024931        27      0.076206        True   3.0
168623  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           33020303          64.754054              74.754054   0.030118        27      0.060849        True   4.0
168617  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27313197         543.187160             553.187160   0.000769        27      0.011496        True   5.0
168612  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1149084290        3395.843537            3405.843537   0.001226        27      0.112872       False   1.0
168611  1616f009d96b1-0285d8288a5bce-70217860-38400-16...          107151123          91.409815             101.409815   0.026585        27      0.072863       False   2.0
168624  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           39301645         256.652442             266.652442   0.009377        27      0.067582       False   3.0
168622  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790314285         102.376743             112.376743   0.013473        27      0.040921       False   4.0
168614  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1161375937          56.687372              66.687372   0.011116        27      0.020036       False   5.0
168625  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           39825972          33.653947              43.653947   0.013328        27      0.015724       False   6.0
168610  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1038512181          71.312714              81.312714   0.006583        27      0.014468       False   7.0
168609  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1037574949          20.851671              30.851671   0.017247        27      0.014381       False   8.0
168613  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1157057159          66.108992              76.108992   0.005126        27      0.010544       False   9.0
168620  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790313523        2655.260638            2665.260638   0.000111        27      0.007964       False  10.0
168608  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1023751122         771.544049             781.544049   0.000357        27      0.007537       False  11.0
168627  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           62323193         776.343701             786.343701   0.000332        27      0.007049       False  12.0
168626  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           41639090        1000.378758            1010.378758   0.000211        27      0.005764       False  13.0
168619  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790312980        5254.621053            5264.621053   0.000040        27      0.005720       False  14.0
168621  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790314267         597.116029             607.116029   0.000095        27      0.001567       False  15.0
2018-05-02 11:49:51,022 - memory_profile6_log - INFO - 

2018-05-02 11:49:51,023 - memory_profile6_log - INFO - Len of model_transform: 304488
2018-05-02 11:49:51,025 - memory_profile6_log - INFO - Len of df_dt: 100000
2018-05-02 11:49:51,026 - memory_profile6_log - INFO - Total train time: 48.570s
2018-05-02 11:49:51,026 - memory_profile6_log - INFO - memory left before cleaning: 84.500 percent memory...
2018-05-02 11:49:51,028 - memory_profile6_log - INFO - cleaning up some objects...
2018-05-02 11:49:51,029 - memory_profile6_log - INFO - deleting df_dut...
2018-05-02 11:49:51,029 - memory_profile6_log - INFO - deleting df_dt...
2018-05-02 11:49:51,035 - memory_profile6_log - INFO - deleting df_input...
2018-05-02 11:49:51,039 - memory_profile6_log - INFO - deleting df_input_X...
2018-05-02 11:49:51,040 - memory_profile6_log - INFO - deleting df_current...
2018-05-02 11:49:51,042 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-05-02 11:49:51,059 - memory_profile6_log - INFO - deleting model_fit...
2018-05-02 11:49:51,059 - memory_profile6_log - INFO - deleting result...
2018-05-02 11:49:51,104 - memory_profile6_log - INFO - list(model_transformsv) before rename: 
2018-05-02 11:49:51,105 - memory_profile6_log - INFO -  
2018-05-02 11:49:51,111 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'is_general', 'p0_posterior', 'rank', 'date']
2018-05-02 11:49:51,111 - memory_profile6_log - INFO - 

2018-05-02 11:49:51,112 - memory_profile6_log - INFO - 304488
2018-05-02 11:49:51,115 - memory_profile6_log - INFO - 

2018-05-02 11:49:51,147 - memory_profile6_log - INFO - list(model_transformsv) after rename: 
2018-05-02 11:49:51,148 - memory_profile6_log - INFO -  
2018-05-02 11:49:51,150 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'topic_is_general', 'interest_score', 'interest_rank', 'interest_score_created_at']
2018-05-02 11:49:51,151 - memory_profile6_log - INFO - 

2018-05-02 11:49:51,151 - memory_profile6_log - INFO - 304488
2018-05-02 11:49:51,153 - memory_profile6_log - INFO - 

2018-05-02 11:49:51,154 - memory_profile6_log - INFO - deleting model_transform...
2018-05-02 11:49:51,154 - memory_profile6_log - INFO - memory left after cleaning: 84.400 percent memory...
2018-05-02 11:49:51,157 - memory_profile6_log - INFO - Begin saving trained data...
2018-05-02 11:49:51,160 - memory_profile6_log - INFO - Using ElasticSearch as storage...
2018-05-02 11:49:51,191 - memory_profile6_log - INFO - Saving total data: 304488
2018-05-02 11:49:51,194 - memory_profile6_log - INFO - Len of X_split for batch save model_transformsv: 5
2018-05-02 11:49:51,194 - memory_profile6_log - INFO - processing batch-0
2018-05-02 11:59:55,049 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 11:59:55,052 - memory_profile6_log - INFO - date_generated: 
2018-05-02 11:59:55,053 - memory_profile6_log - INFO -  
2018-05-02 11:59:55,053 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 11, 59, 55, 51000)]
2018-05-02 11:59:55,055 - memory_profile6_log - INFO - 

2018-05-02 11:59:55,055 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 11:59:55,055 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 11:59:55,056 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 11:59:55,230 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 11:59:55,233 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 12:01:38,898 - memory_profile6_log - INFO - size of df: 82.28 MB
2018-05-02 12:01:38,900 - memory_profile6_log - INFO - getting total: 324874 training data(genuine interest) for date: 2018-05-01
2018-05-02 12:01:38,960 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-05-02 12:01:38,963 - memory_profile6_log - INFO - Appending history data...
2018-05-02 12:01:38,963 - memory_profile6_log - INFO - processing batch-0
2018-05-02 12:01:38,966 - memory_profile6_log - INFO - creating list history data...
2018-05-02 12:01:39,020 - memory_profile6_log - INFO - call history data...
2018-05-02 12:02:17,515 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 12:02:18,256 - memory_profile6_log - INFO - processing batch-1
2018-05-02 12:02:18,256 - memory_profile6_log - INFO - creating list history data...
2018-05-02 12:02:18,266 - memory_profile6_log - INFO - call history data...
2018-05-02 12:02:56,006 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 12:02:56,825 - memory_profile6_log - INFO - processing batch-2
2018-05-02 12:02:56,826 - memory_profile6_log - INFO - creating list history data...
2018-05-02 12:02:56,836 - memory_profile6_log - INFO - call history data...
2018-05-02 12:03:34,200 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 12:03:34,371 - memory_profile6_log - INFO - processing batch-3
2018-05-02 12:03:34,371 - memory_profile6_log - INFO - creating list history data...
2018-05-02 12:03:34,381 - memory_profile6_log - INFO - call history data...
2018-05-02 12:04:22,453 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 12:04:22,887 - memory_profile6_log - INFO - processing batch-4
2018-05-02 12:04:22,888 - memory_profile6_log - INFO - creating list history data...
2018-05-02 12:04:22,895 - memory_profile6_log - INFO - call history data...
2018-05-02 12:05:07,154 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 12:05:07,453 - memory_profile6_log - INFO - Appending training data...
2018-05-02 12:05:07,454 - memory_profile6_log - INFO - len datalist: 1
2018-05-02 12:05:07,456 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-05-02 12:05:07,457 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 12:05:07,459 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 12:05:07,460 - memory_profile6_log - INFO - ================================================

2018-05-02 12:05:07,460 - memory_profile6_log - INFO -    356     86.9 MiB     86.9 MiB   @profile

2018-05-02 12:05:07,461 - memory_profile6_log - INFO -    357                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-05-02 12:05:07,463 - memory_profile6_log - INFO -    358     86.9 MiB      0.0 MiB       bq_client = client

2018-05-02 12:05:07,464 - memory_profile6_log - INFO -    359     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 12:05:07,467 - memory_profile6_log - INFO -    360                             

2018-05-02 12:05:07,467 - memory_profile6_log - INFO -    361     86.9 MiB      0.0 MiB       datalist = []

2018-05-02 12:05:07,467 - memory_profile6_log - INFO -    362     86.9 MiB      0.0 MiB       datalist_hist = []

2018-05-02 12:05:07,469 - memory_profile6_log - INFO -    363                             

2018-05-02 12:05:07,470 - memory_profile6_log - INFO -    364     86.9 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-05-02 12:05:07,470 - memory_profile6_log - INFO -    365    377.9 MiB      0.0 MiB       for ndate in date_generated:

2018-05-02 12:05:07,470 - memory_profile6_log - INFO -    366    366.6 MiB    279.7 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-05-02 12:05:07,471 - memory_profile6_log - INFO -    367    366.6 MiB      0.0 MiB           if tframe is not None:

2018-05-02 12:05:07,473 - memory_profile6_log - INFO -    368    366.6 MiB      0.0 MiB               if not tframe.empty:

2018-05-02 12:05:07,476 - memory_profile6_log - INFO -    369    376.2 MiB      9.6 MiB                   X_split = np.array_split(tframe, 5)

2018-05-02 12:05:07,477 - memory_profile6_log - INFO -    370    376.2 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 12:05:07,479 - memory_profile6_log - INFO -    371    376.2 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-05-02 12:05:07,480 - memory_profile6_log - INFO -    372    377.9 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-05-02 12:05:07,480 - memory_profile6_log - INFO -    373                                                 # ~ loading history

2018-05-02 12:05:07,482 - memory_profile6_log - INFO -    374                                                 """

2018-05-02 12:05:07,483 - memory_profile6_log - INFO -    375                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-05-02 12:05:07,483 - memory_profile6_log - INFO -    376                                                 """

2018-05-02 12:05:07,486 - memory_profile6_log - INFO -    377    377.8 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-05-02 12:05:07,487 - memory_profile6_log - INFO -    378                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-05-02 12:05:07,489 - memory_profile6_log - INFO -    379    377.8 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-05-02 12:05:07,490 - memory_profile6_log - INFO -    380    377.8 MiB      0.5 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-05-02 12:05:07,490 - memory_profile6_log - INFO -    381                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 12:05:07,492 - memory_profile6_log - INFO -    382                             

2018-05-02 12:05:07,493 - memory_profile6_log - INFO -    383    377.8 MiB      0.0 MiB                       logger.info("call history data...")

2018-05-02 12:05:07,493 - memory_profile6_log - INFO -    384    377.9 MiB      1.0 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-05-02 12:05:07,493 - memory_profile6_log - INFO -    385                             

2018-05-02 12:05:07,496 - memory_profile6_log - INFO -    386                                                 # me = os.getpid()

2018-05-02 12:05:07,499 - memory_profile6_log - INFO -    387                                                 # kill_proc_tree(me)

2018-05-02 12:05:07,500 - memory_profile6_log - INFO -    388                             

2018-05-02 12:05:07,500 - memory_profile6_log - INFO -    389    377.9 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-05-02 12:05:07,502 - memory_profile6_log - INFO -    390    377.9 MiB      0.0 MiB                       for m in h_frame:

2018-05-02 12:05:07,503 - memory_profile6_log - INFO -    391    377.9 MiB      0.0 MiB                           if m is not None:

2018-05-02 12:05:07,503 - memory_profile6_log - INFO -    392    377.9 MiB      0.0 MiB                               if len(m) > 0:

2018-05-02 12:05:07,503 - memory_profile6_log - INFO -    393    377.9 MiB      0.2 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-05-02 12:05:07,507 - memory_profile6_log - INFO -    394    377.9 MiB      0.0 MiB                       del h_frame

2018-05-02 12:05:07,509 - memory_profile6_log - INFO -    395    377.9 MiB      0.0 MiB                       del lhistory

2018-05-02 12:05:07,509 - memory_profile6_log - INFO -    396                             

2018-05-02 12:05:07,510 - memory_profile6_log - INFO -    397    377.9 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-05-02 12:05:07,512 - memory_profile6_log - INFO -    398    377.9 MiB      0.0 MiB                   datalist.append(tframe)

2018-05-02 12:05:07,513 - memory_profile6_log - INFO -    399                                     else: 

2018-05-02 12:05:07,513 - memory_profile6_log - INFO -    400                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-05-02 12:05:07,515 - memory_profile6_log - INFO -    401    377.9 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-05-02 12:05:07,515 - memory_profile6_log - INFO -    402    377.9 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-05-02 12:05:07,516 - memory_profile6_log - INFO -    403                             

2018-05-02 12:05:07,519 - memory_profile6_log - INFO -    404    377.9 MiB      0.0 MiB       return datalist, datalist_hist

2018-05-02 12:05:07,520 - memory_profile6_log - INFO - 


2018-05-02 12:05:08,237 - memory_profile6_log - INFO - size of big_frame_hist: 317.73 KB
2018-05-02 12:05:08,345 - memory_profile6_log - INFO - size of big_frame: 82.28 MB
2018-05-02 12:05:08,361 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-05-02 12:05:35,512 - memory_profile6_log - INFO - size of df: 24.51 MB
2018-05-02 12:05:35,513 - memory_profile6_log - INFO - getting total: 100000 training data(current date interest)
2018-05-02 12:05:35,553 - memory_profile6_log - INFO - size of current_frame: 25.27 MB
2018-05-02 12:05:35,555 - memory_profile6_log - INFO - loading time of: 424874 total genuine-current interest data ~ take 340.355s
2018-05-02 12:05:35,559 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 12:05:35,561 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 12:05:35,562 - memory_profile6_log - INFO - ================================================

2018-05-02 12:05:35,562 - memory_profile6_log - INFO -    406     86.8 MiB     86.8 MiB   @profile

2018-05-02 12:05:35,565 - memory_profile6_log - INFO -    407                             def preprocess(cpu, cd, query_fit, date_generated):

2018-05-02 12:05:35,565 - memory_profile6_log - INFO -    408     86.9 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-05-02 12:05:35,568 - memory_profile6_log - INFO -    409     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 12:05:35,572 - memory_profile6_log - INFO -    410                             

2018-05-02 12:05:35,573 - memory_profile6_log - INFO -    411                                 # ~~~ Begin collecting data ~~~

2018-05-02 12:05:35,576 - memory_profile6_log - INFO -    412     86.9 MiB      0.0 MiB       t0 = time.time()

2018-05-02 12:05:35,578 - memory_profile6_log - INFO -    413    377.9 MiB    291.0 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-05-02 12:05:35,578 - memory_profile6_log - INFO -    414    377.9 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-05-02 12:05:35,579 - memory_profile6_log - INFO -    415                                     logger.info("Training cannot be empty..")

2018-05-02 12:05:35,579 - memory_profile6_log - INFO -    416                                     return False

2018-05-02 12:05:35,581 - memory_profile6_log - INFO -    417    378.7 MiB      0.8 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-05-02 12:05:35,582 - memory_profile6_log - INFO -    418                                 # print "big_frame_hist:\n", big_frame_hist.head(20)

2018-05-02 12:05:35,582 - memory_profile6_log - INFO -    419    378.7 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-05-02 12:05:35,582 - memory_profile6_log - INFO -    420                             

2018-05-02 12:05:35,584 - memory_profile6_log - INFO -    421    386.1 MiB      7.4 MiB       big_frame = pd.concat(datalist)

2018-05-02 12:05:35,584 - memory_profile6_log - INFO -    422    386.2 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-05-02 12:05:35,588 - memory_profile6_log - INFO -    423    378.7 MiB     -7.4 MiB       del datalist

2018-05-02 12:05:35,588 - memory_profile6_log - INFO -    424                             

2018-05-02 12:05:35,589 - memory_profile6_log - INFO -    425    378.7 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 12:05:35,591 - memory_profile6_log - INFO -    426                             

2018-05-02 12:05:35,592 - memory_profile6_log - INFO -    427                                 # ~ get current news interest ~

2018-05-02 12:05:35,592 - memory_profile6_log - INFO -    428    378.7 MiB      0.0 MiB       if not cd:

2018-05-02 12:05:35,592 - memory_profile6_log - INFO -    429    378.7 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-05-02 12:05:35,592 - memory_profile6_log - INFO -    430    386.9 MiB      8.2 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-05-02 12:05:35,594 - memory_profile6_log - INFO -    431    386.9 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-05-02 12:05:35,594 - memory_profile6_log - INFO -    432                                 else:

2018-05-02 12:05:35,595 - memory_profile6_log - INFO -    433                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-05-02 12:05:35,595 - memory_profile6_log - INFO -    434                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-05-02 12:05:35,595 - memory_profile6_log - INFO -    435                             

2018-05-02 12:05:35,599 - memory_profile6_log - INFO -    436                                     # safe handling of query parameter

2018-05-02 12:05:35,601 - memory_profile6_log - INFO -    437                                     query_params = [

2018-05-02 12:05:35,601 - memory_profile6_log - INFO -    438                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-05-02 12:05:35,602 - memory_profile6_log - INFO -    439                                     ]

2018-05-02 12:05:35,604 - memory_profile6_log - INFO -    440                             

2018-05-02 12:05:35,604 - memory_profile6_log - INFO -    441                                     job_config.query_parameters = query_params

2018-05-02 12:05:35,605 - memory_profile6_log - INFO -    442                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-05-02 12:05:35,605 - memory_profile6_log - INFO -    443                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-05-02 12:05:35,605 - memory_profile6_log - INFO -    444                             

2018-05-02 12:05:35,607 - memory_profile6_log - INFO -    445    386.9 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-05-02 12:05:35,607 - memory_profile6_log - INFO -    446    386.9 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-05-02 12:05:35,608 - memory_profile6_log - INFO -    447    386.9 MiB     -0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-05-02 12:05:35,608 - memory_profile6_log - INFO -    448    386.9 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-05-02 12:05:35,608 - memory_profile6_log - INFO -    449    386.9 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 12:05:35,612 - memory_profile6_log - INFO -    450    386.9 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-05-02 12:05:35,615 - memory_profile6_log - INFO -    451                             

2018-05-02 12:05:35,615 - memory_profile6_log - INFO -    452    386.9 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-05-02 12:05:35,618 - memory_profile6_log - INFO - 


2018-05-02 12:05:35,627 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-05-02 12:05:35,660 - memory_profile6_log - INFO - train on: 324874 total genuine interest data(D(u, t))
2018-05-02 12:05:35,661 - memory_profile6_log - INFO - transform on: 100000 total current data(D(t))
2018-05-02 12:05:35,667 - memory_profile6_log - INFO - apply on: 1137 total history...)
2018-05-02 12:05:36,049 - memory_profile6_log - INFO - len of uniques_fit_hist:1137
2018-05-02 12:05:36,056 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:1086
2018-05-02 12:05:36,803 - memory_profile6_log - INFO - self.sum_all_nt before concate:

2018-05-02 12:05:36,842 - memory_profile6_log - INFO -                                                  user_id  sigma_Nt
15052  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        27
2018-05-02 12:05:36,842 - memory_profile6_log - INFO - 

2018-05-02 12:05:36,907 - memory_profile6_log - INFO - self.sum_all_nt after concate:

2018-05-02 12:05:36,934 - memory_profile6_log - INFO -                                                  user_id  sigma_Nt
15052  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        27
2018-05-02 12:05:36,936 - memory_profile6_log - INFO - 

2018-05-02 12:05:37,270 - memory_profile6_log - INFO - Len of model_fit: 324874
2018-05-02 12:05:37,273 - memory_profile6_log - INFO - Len of df_dut: 324874
2018-05-02 12:05:38,122 - memory_profile6_log - INFO - Len of fitted_models on main class: 324874
2018-05-02 12:05:38,124 - memory_profile6_log - INFO - 

2018-05-02 12:05:38,125 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-05-02 12:05:38,125 - memory_profile6_log - INFO - 

2018-05-02 12:05:38,127 - memory_profile6_log - INFO - fitted_model_hist:

2018-05-02 12:05:38,128 - memory_profile6_log - INFO - 

2018-05-02 12:05:38,140 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 12:05:38,141 - memory_profile6_log - INFO - 

2018-05-02 12:05:38,186 - memory_profile6_log - INFO - len of fitted models after concat: 326011
2018-05-02 12:05:38,187 - memory_profile6_log - INFO - 

2018-05-02 12:05:38,188 - memory_profile6_log - INFO - Recalculating fitted models...
2018-05-02 12:05:38,191 - memory_profile6_log - INFO - 

2018-05-02 12:05:38,480 - memory_profile6_log - INFO - fitted_models after cobine:

2018-05-02 12:05:38,480 - memory_profile6_log - INFO - 

2018-05-02 12:05:38,490 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 12:05:38,492 - memory_profile6_log - INFO - 

2018-05-02 12:05:38,493 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 324874
2018-05-02 12:05:38,494 - memory_profile6_log - INFO - 

2018-05-02 12:06:26,946 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-05-02 12:06:27,217 - memory_profile6_log - INFO -                                                   user_id           topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt  p0_posterior  is_general  rank
168616  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22601470          96.864073             106.864073   0.047214        27      0.136365        True   1.0
168615  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22553543          22.395451              32.395451   0.131697        27      0.115308        True   2.0
168618  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27431099         103.095622             113.095622   0.024931        27      0.076206        True   3.0
168623  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           33020303          64.754054              74.754054   0.030118        27      0.060849        True   4.0
168617  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27313197         543.187160             553.187160   0.000769        27      0.011496        True   5.0
168612  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1149084290        3395.843537            3405.843537   0.001226        27      0.112872       False   1.0
168611  1616f009d96b1-0285d8288a5bce-70217860-38400-16...          107151123          91.409815             101.409815   0.026585        27      0.072863       False   2.0
168624  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           39301645         256.652442             266.652442   0.009377        27      0.067582       False   3.0
168622  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790314285         102.376743             112.376743   0.013473        27      0.040921       False   4.0
168614  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1161375937          56.687372              66.687372   0.011116        27      0.020036       False   5.0
168625  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           39825972          33.653947              43.653947   0.013328        27      0.015724       False   6.0
168610  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1038512181          71.312714              81.312714   0.006583        27      0.014468       False   7.0
168609  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1037574949          20.851671              30.851671   0.017247        27      0.014381       False   8.0
168613  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1157057159          66.108992              76.108992   0.005126        27      0.010544       False   9.0
168620  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790313523        2655.260638            2665.260638   0.000111        27      0.007964       False  10.0
168608  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1023751122         771.544049             781.544049   0.000357        27      0.007537       False  11.0
168627  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           62323193         776.343701             786.343701   0.000332        27      0.007049       False  12.0
168626  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           41639090        1000.378758            1010.378758   0.000211        27      0.005764       False  13.0
168619  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790312980        5254.621053            5264.621053   0.000040        27      0.005720       False  14.0
168621  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790314267         597.116029             607.116029   0.000095        27      0.001567       False  15.0
2018-05-02 12:06:27,227 - memory_profile6_log - INFO - 

2018-05-02 12:06:27,233 - memory_profile6_log - INFO - Len of model_transform: 304488
2018-05-02 12:06:27,234 - memory_profile6_log - INFO - Len of df_dt: 100000
2018-05-02 12:06:27,236 - memory_profile6_log - INFO - Total train time: 51.331s
2018-05-02 12:06:27,237 - memory_profile6_log - INFO - memory left before cleaning: 87.700 percent memory...
2018-05-02 12:06:27,240 - memory_profile6_log - INFO - cleaning up some objects...
2018-05-02 12:06:27,240 - memory_profile6_log - INFO - deleting df_dut...
2018-05-02 12:06:27,242 - memory_profile6_log - INFO - deleting df_dt...
2018-05-02 12:06:27,246 - memory_profile6_log - INFO - deleting df_input...
2018-05-02 12:06:27,252 - memory_profile6_log - INFO - deleting df_input_X...
2018-05-02 12:06:27,253 - memory_profile6_log - INFO - deleting df_current...
2018-05-02 12:06:27,256 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-05-02 12:06:27,276 - memory_profile6_log - INFO - deleting model_fit...
2018-05-02 12:06:27,278 - memory_profile6_log - INFO - deleting result...
2018-05-02 12:06:27,336 - memory_profile6_log - INFO - list(model_transformsv) before rename: 
2018-05-02 12:06:27,338 - memory_profile6_log - INFO -  
2018-05-02 12:06:27,341 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'is_general', 'p0_posterior', 'rank', 'date']
2018-05-02 12:06:27,342 - memory_profile6_log - INFO - 

2018-05-02 12:06:27,342 - memory_profile6_log - INFO - 304488
2018-05-02 12:06:27,344 - memory_profile6_log - INFO - 

2018-05-02 12:06:27,381 - memory_profile6_log - INFO - list(model_transformsv) after rename: 
2018-05-02 12:06:27,382 - memory_profile6_log - INFO -  
2018-05-02 12:06:27,384 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'topic_is_general', 'interest_score', 'interest_rank', 'interest_score_created_at']
2018-05-02 12:06:27,384 - memory_profile6_log - INFO - 

2018-05-02 12:06:27,385 - memory_profile6_log - INFO - 304488
2018-05-02 12:06:27,388 - memory_profile6_log - INFO - 

2018-05-02 12:06:27,388 - memory_profile6_log - INFO - deleting model_transform...
2018-05-02 12:06:27,391 - memory_profile6_log - INFO - memory left after cleaning: 87.600 percent memory...
2018-05-02 12:06:27,392 - memory_profile6_log - INFO - Begin saving trained data...
2018-05-02 12:06:27,394 - memory_profile6_log - INFO - Using ElasticSearch as storage...
2018-05-02 12:06:27,428 - memory_profile6_log - INFO - Saving total data: 304488
2018-05-02 12:06:27,430 - memory_profile6_log - INFO - Len of X_split for batch save model_transformsv: 5
2018-05-02 12:06:27,430 - memory_profile6_log - INFO - processing batch-0
2018-05-02 12:11:51,391 - memory_profile6_log - INFO - processing batch-1
2018-05-02 12:17:16,068 - memory_profile6_log - INFO - processing batch-2
2018-05-02 12:24:15,509 - memory_profile6_log - INFO - processing batch-3
2018-05-02 12:30:49,430 - memory_profile6_log - INFO - processing batch-4
2018-05-02 12:38:09,888 - memory_profile6_log - INFO - Saving fitted_models as history to Elasticsearch...
2018-05-02 12:38:09,982 - memory_profile6_log - INFO - Saving total data: 324874
2018-05-02 12:38:09,983 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 5
2018-05-02 12:38:09,984 - memory_profile6_log - INFO - processing batch-0
2018-05-02 12:45:40,278 - memory_profile6_log - INFO - processing batch-1
2018-05-02 12:51:07,164 - memory_profile6_log - INFO - processing batch-2
2018-05-02 12:57:48,927 - memory_profile6_log - INFO - processing batch-3
2018-05-02 13:03:03,615 - memory_profile6_log - INFO - processing batch-4
2018-05-02 13:08:47,013 - memory_profile6_log - INFO - deleting BR...
2018-05-02 13:08:47,016 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-05-02 13:08:47,022 - memory_profile6_log - INFO - deleting fitted_models_sigmant...
2018-05-02 13:08:47,039 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 13:08:47,039 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 13:08:47,042 - memory_profile6_log - INFO - ================================================

2018-05-02 13:08:47,043 - memory_profile6_log - INFO -    113    386.9 MiB    386.9 MiB   @profile

2018-05-02 13:08:47,043 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-05-02 13:08:47,043 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-05-02 13:08:47,046 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-05-02 13:08:47,046 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-05-02 13:08:47,046 - memory_profile6_log - INFO -    118                                      saveto="datastore", fitted_models_hist=None):

2018-05-02 13:08:47,046 - memory_profile6_log - INFO -    119                                 """

2018-05-02 13:08:47,048 - memory_profile6_log - INFO -    120                                     Main Process

2018-05-02 13:08:47,048 - memory_profile6_log - INFO -    121                                 """

2018-05-02 13:08:47,048 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-05-02 13:08:47,051 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-05-02 13:08:47,051 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-05-02 13:08:47,052 - memory_profile6_log - INFO -    125    386.9 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-05-02 13:08:47,052 - memory_profile6_log - INFO -    126    396.9 MiB     10.0 MiB       df_dut = df_input.copy(deep=True)

2018-05-02 13:08:47,052 - memory_profile6_log - INFO -    127    396.9 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 13:08:47,053 - memory_profile6_log - INFO -    128                             

2018-05-02 13:08:47,053 - memory_profile6_log - INFO -    129                                 # D(t)

2018-05-02 13:08:47,055 - memory_profile6_log - INFO -    130    398.5 MiB      1.6 MiB       df_dt = df_current.copy(deep=True)

2018-05-02 13:08:47,055 - memory_profile6_log - INFO -    131    400.8 MiB      2.3 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 13:08:47,055 - memory_profile6_log - INFO -    132                             

2018-05-02 13:08:47,055 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-05-02 13:08:47,056 - memory_profile6_log - INFO -    134    400.8 MiB      0.0 MiB       t0 = time.time()

2018-05-02 13:08:47,056 - memory_profile6_log - INFO -    135    400.8 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-05-02 13:08:47,058 - memory_profile6_log - INFO -    136    400.8 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-05-02 13:08:47,058 - memory_profile6_log - INFO -    137    400.8 MiB      0.0 MiB       logger.info("apply on: %d total history...)", len(df_hist))

2018-05-02 13:08:47,058 - memory_profile6_log - INFO -    138                             

2018-05-02 13:08:47,059 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-05-02 13:08:47,059 - memory_profile6_log - INFO -    140    400.8 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-05-02 13:08:47,059 - memory_profile6_log - INFO -    141                             

2018-05-02 13:08:47,059 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-05-02 13:08:47,063 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-05-02 13:08:47,063 - memory_profile6_log - INFO -    144    401.2 MiB      0.4 MiB       NB = BR.processX(df_dut)

2018-05-02 13:08:47,063 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-05-02 13:08:47,065 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-05-02 13:08:47,065 - memory_profile6_log - INFO -    147    414.1 MiB     12.9 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-05-02 13:08:47,065 - memory_profile6_log - INFO -    148                                 """

2018-05-02 13:08:47,065 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-05-02 13:08:47,065 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-05-02 13:08:47,066 - memory_profile6_log - INFO -    151                                 """

2018-05-02 13:08:47,066 - memory_profile6_log - INFO -    152    414.1 MiB      0.0 MiB       fitby_sigmant = False

2018-05-02 13:08:47,066 - memory_profile6_log - INFO -    153    414.1 MiB      0.0 MiB       uniques_fit_hist = None

2018-05-02 13:08:47,068 - memory_profile6_log - INFO -    154    414.1 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-05-02 13:08:47,068 - memory_profile6_log - INFO -    155    414.1 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-05-02 13:08:47,068 - memory_profile6_log - INFO -    156    426.5 MiB     12.4 MiB                            'is_general']]

2018-05-02 13:08:47,069 - memory_profile6_log - INFO -    157                             

2018-05-02 13:08:47,069 - memory_profile6_log - INFO -    158                                 # get sigma_Nt from fitted_models_hist

2018-05-02 13:08:47,069 - memory_profile6_log - INFO -    159    426.5 MiB      0.1 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-05-02 13:08:47,071 - memory_profile6_log - INFO -    160    426.5 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-05-02 13:08:47,071 - memory_profile6_log - INFO -    161    426.6 MiB      0.0 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-05-02 13:08:47,071 - memory_profile6_log - INFO -    162    426.6 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-05-02 13:08:47,075 - memory_profile6_log - INFO -    163                             

2018-05-02 13:08:47,075 - memory_profile6_log - INFO -    164                                 # begin fit

2018-05-02 13:08:47,075 - memory_profile6_log - INFO -    165    426.6 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-05-02 13:08:47,076 - memory_profile6_log - INFO -    166    426.6 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-05-02 13:08:47,076 - memory_profile6_log - INFO -    167    455.3 MiB     28.7 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-05-02 13:08:47,076 - memory_profile6_log - INFO -    168    455.3 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-05-02 13:08:47,078 - memory_profile6_log - INFO -    169    455.3 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-05-02 13:08:47,078 - memory_profile6_log - INFO -    170                                 # print model_fit[['num_x', 'num_x', 'sigma_Nt', 'date_all_click']].loc[model_fit['user_id']=="1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157"].head(5)

2018-05-02 13:08:47,086 - memory_profile6_log - INFO -    171                             

2018-05-02 13:08:47,091 - memory_profile6_log - INFO -    172                                 # ~~ and Transform ~~

2018-05-02 13:08:47,091 - memory_profile6_log - INFO -    173                                 #   handling current news interest == current date

2018-05-02 13:08:47,092 - memory_profile6_log - INFO -    174    455.3 MiB      0.0 MiB       if df_dt.empty:

2018-05-02 13:08:47,092 - memory_profile6_log - INFO -    175                                     print "Cek your df_dt, cannot be emtpy!!"

2018-05-02 13:08:47,094 - memory_profile6_log - INFO -    176                                     return None

2018-05-02 13:08:47,095 - memory_profile6_log - INFO -    177    458.2 MiB      2.9 MiB       NB = BR.processX(df_dt)

2018-05-02 13:08:47,095 - memory_profile6_log - INFO -    178    459.7 MiB      1.5 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-05-02 13:08:47,095 - memory_profile6_log - INFO -    179                             

2018-05-02 13:08:47,096 - memory_profile6_log - INFO -    180    459.7 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-05-02 13:08:47,096 - memory_profile6_log - INFO -    181    451.9 MiB     -7.8 MiB                            'num_x', 'num_y', 'is_general']]

2018-05-02 13:08:47,096 - memory_profile6_log - INFO -    182                             

2018-05-02 13:08:47,098 - memory_profile6_log - INFO -    183                                 # print "model_fit dtypes:\n", model_fit.dtypes

2018-05-02 13:08:47,098 - memory_profile6_log - INFO -    184                                 # print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-05-02 13:08:47,098 - memory_profile6_log - INFO -    185    451.9 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-05-02 13:08:47,101 - memory_profile6_log - INFO -    186    451.9 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-05-02 13:08:47,101 - memory_profile6_log - INFO -    187    451.9 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-05-02 13:08:47,102 - memory_profile6_log - INFO -    188    451.9 MiB      0.0 MiB                                                                                           "topic_id", "user_id"]],

2018-05-02 13:08:47,105 - memory_profile6_log - INFO -    189    475.2 MiB     23.3 MiB                                                     verbose=False)

2018-05-02 13:08:47,387 - memory_profile6_log - INFO -    190                             

2018-05-02 13:08:47,390 - memory_profile6_log - INFO -    191                                 # ~~~ filter is general and specific topic ~~~

2018-05-02 13:08:47,391 - memory_profile6_log - INFO -    192                                 # the idea is just we need to rerank every topic according

2018-05-02 13:08:47,391 - memory_profile6_log - INFO -    193                                 #    to user_id and and is_general by p0_posterior

2018-05-02 13:08:47,391 - memory_profile6_log - INFO -    194    475.2 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-05-02 13:08:47,392 - memory_profile6_log - INFO -    195    477.7 MiB      2.5 MiB                                     'is_general']].groupby(['topic_id',

2018-05-02 13:08:47,392 - memory_profile6_log - INFO -    196    475.3 MiB     -2.5 MiB                                                             'is_general']

2018-05-02 13:08:47,392 - memory_profile6_log - INFO -    197                                                                                      ).size().to_frame().reset_index()

2018-05-02 13:08:47,394 - memory_profile6_log - INFO -    198                             

2018-05-02 13:08:47,394 - memory_profile6_log - INFO -    199                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-05-02 13:08:47,394 - memory_profile6_log - INFO -    200                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-05-02 13:08:47,394 - memory_profile6_log - INFO -    201    475.3 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-05-02 13:08:47,401 - memory_profile6_log - INFO -    202                             

2018-05-02 13:08:47,401 - memory_profile6_log - INFO -    203                                 # ~ start by provide rank for each topic type ~

2018-05-02 13:08:47,403 - memory_profile6_log - INFO -    204    493.7 MiB     18.4 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-05-02 13:08:47,404 - memory_profile6_log - INFO -    205    497.5 MiB      3.8 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-05-02 13:08:47,404 - memory_profile6_log - INFO -    206                             

2018-05-02 13:08:47,404 - memory_profile6_log - INFO -    207                                 # ~ set threshold to filter output

2018-05-02 13:08:47,404 - memory_profile6_log - INFO -    208    497.5 MiB      0.0 MiB       if threshold > 0:

2018-05-02 13:08:47,404 - memory_profile6_log - INFO -    209    497.5 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-05-02 13:08:47,405 - memory_profile6_log - INFO -    210    497.5 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-05-02 13:08:47,405 - memory_profile6_log - INFO -    211    496.4 MiB     -1.1 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-05-02 13:08:47,407 - memory_profile6_log - INFO -    212                             

2018-05-02 13:08:47,407 - memory_profile6_log - INFO -    213    496.4 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 13:08:47,407 - memory_profile6_log - INFO -    214    496.5 MiB      0.0 MiB       print model_transform[model_transform['user_id']=='1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157']

2018-05-02 13:08:47,407 - memory_profile6_log - INFO -    215    496.5 MiB      0.0 MiB       logger.info("Len of model_transform: %d", len(model_transform))

2018-05-02 13:08:47,407 - memory_profile6_log - INFO -    216    496.5 MiB      0.0 MiB       logger.info("Len of df_dt: %d", len(df_dt))

2018-05-02 13:08:47,411 - memory_profile6_log - INFO -    217    496.5 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-05-02 13:08:47,411 - memory_profile6_log - INFO -    218                             

2018-05-02 13:08:47,413 - memory_profile6_log - INFO -    219    496.5 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-05-02 13:08:47,414 - memory_profile6_log - INFO -    220                             

2018-05-02 13:08:47,414 - memory_profile6_log - INFO -    221    496.5 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-05-02 13:08:47,417 - memory_profile6_log - INFO -    222    496.5 MiB      0.0 MiB       del df_dut

2018-05-02 13:08:47,417 - memory_profile6_log - INFO -    223    496.5 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-05-02 13:08:47,418 - memory_profile6_log - INFO -    224    496.5 MiB      0.0 MiB       del df_dt

2018-05-02 13:08:47,418 - memory_profile6_log - INFO -    225    496.5 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-05-02 13:08:47,420 - memory_profile6_log - INFO -    226    496.5 MiB      0.0 MiB       del df_input

2018-05-02 13:08:47,420 - memory_profile6_log - INFO -    227    496.5 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-05-02 13:08:47,424 - memory_profile6_log - INFO -    228    492.4 MiB     -4.1 MiB       del df_input_X

2018-05-02 13:08:47,424 - memory_profile6_log - INFO -    229    492.4 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-05-02 13:08:47,424 - memory_profile6_log - INFO -    230    492.4 MiB      0.0 MiB       del df_current

2018-05-02 13:08:47,424 - memory_profile6_log - INFO -    231    492.4 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-05-02 13:08:47,426 - memory_profile6_log - INFO -    232    492.4 MiB      0.0 MiB       del map_topic_isgeneral

2018-05-02 13:08:47,426 - memory_profile6_log - INFO -    233    492.4 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-05-02 13:08:47,427 - memory_profile6_log - INFO -    234    465.1 MiB    -27.3 MiB       del model_fit

2018-05-02 13:08:47,427 - memory_profile6_log - INFO -    235    465.1 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-05-02 13:08:47,427 - memory_profile6_log - INFO -    236    465.1 MiB      0.0 MiB       del result

2018-05-02 13:08:47,428 - memory_profile6_log - INFO -    237    465.1 MiB      0.0 MiB       logger.info("deleting result...")

2018-05-02 13:08:47,428 - memory_profile6_log - INFO -    238                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 13:08:47,430 - memory_profile6_log - INFO -    239                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 13:08:47,430 - memory_profile6_log - INFO -    240                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 13:08:47,430 - memory_profile6_log - INFO -    241    465.1 MiB      0.0 MiB       if savetrain:

2018-05-02 13:08:47,434 - memory_profile6_log - INFO -    242    474.4 MiB      9.3 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'p0_posterior', 'rank']].copy(deep=True)

2018-05-02 13:08:47,436 - memory_profile6_log - INFO -    243    476.7 MiB      2.3 MiB           model_transformsv['date'] = current_date  # we need manually adding date, because table not support

2018-05-02 13:08:47,437 - memory_profile6_log - INFO -    244    476.7 MiB      0.0 MiB           model_transformsv['date'] = pd.to_datetime(model_transformsv['date'],

2018-05-02 13:08:47,437 - memory_profile6_log - INFO -    245    476.7 MiB      0.0 MiB                                                      format='%Y-%m-%d', errors='coerce')

2018-05-02 13:08:47,437 - memory_profile6_log - INFO -    246    476.7 MiB      0.0 MiB           print "list(model_transformsv) before rename: ", list(model_transformsv)       

2018-05-02 13:08:47,437 - memory_profile6_log - INFO -    247    476.7 MiB      0.0 MiB           print len(model_transformsv)                

2018-05-02 13:08:47,437 - memory_profile6_log - INFO -    248    476.7 MiB      0.0 MiB           model_transformsv = model_transformsv.rename(columns={'is_general': 'topic_is_general', 'p0_posterior': 'interest_score',

2018-05-02 13:08:47,438 - memory_profile6_log - INFO -    249    476.7 MiB      0.0 MiB                                                                 'rank':'interest_rank' , 'date':'interest_score_created_at'})

2018-05-02 13:08:47,438 - memory_profile6_log - INFO -    250    476.7 MiB      0.0 MiB           print "list(model_transformsv) after rename: ", list(model_transformsv)    

2018-05-02 13:08:47,440 - memory_profile6_log - INFO -    251    476.7 MiB      0.0 MiB           print len(model_transformsv) 

2018-05-02 13:08:47,440 - memory_profile6_log - INFO -    252    476.7 MiB      0.0 MiB           del model_transform

2018-05-02 13:08:47,441 - memory_profile6_log - INFO -    253    476.7 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-05-02 13:08:47,441 - memory_profile6_log - INFO -    254    476.7 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-05-02 13:08:47,443 - memory_profile6_log - INFO -    255                             

2018-05-02 13:08:47,443 - memory_profile6_log - INFO -    256    476.7 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-05-02 13:08:47,447 - memory_profile6_log - INFO -    257                                     # ~ Place your code to save the training model here ~

2018-05-02 13:08:47,447 - memory_profile6_log - INFO -    258    476.7 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-05-02 13:08:47,450 - memory_profile6_log - INFO -    259                                         logger.info("Using google datastore as storage...")

2018-05-02 13:08:47,450 - memory_profile6_log - INFO -    260                                         if multproc:

2018-05-02 13:08:47,450 - memory_profile6_log - INFO -    261                                             # ~ save transform models ~

2018-05-02 13:08:47,450 - memory_profile6_log - INFO -    262                                             logger.info("Saving main Transform model to Google DataStore...")

2018-05-02 13:08:47,451 - memory_profile6_log - INFO -    263                                             logger.info("Saving total data: %d", len(model_transformsv))

2018-05-02 13:08:47,451 - memory_profile6_log - INFO -    264                                             mh.saveDataStorePutMulti(model_transformsv)

2018-05-02 13:08:47,453 - memory_profile6_log - INFO -    265                             

2018-05-02 13:08:47,453 - memory_profile6_log - INFO -    266                                             # ~ save fitted models ~

2018-05-02 13:08:47,453 - memory_profile6_log - INFO -    267                                             logger.info("Saving fitted_models as history to Google DataStore...")

2018-05-02 13:08:47,453 - memory_profile6_log - INFO -    268                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 13:08:47,454 - memory_profile6_log - INFO -    269                                             fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-05-02 13:08:47,459 - memory_profile6_log - INFO -    270                                             logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-05-02 13:08:47,461 - memory_profile6_log - INFO -    271                                             X_split = np.array_split(fitted_models_sigmant, 10)

2018-05-02 13:08:47,463 - memory_profile6_log - INFO -    272                                             logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-05-02 13:08:47,463 - memory_profile6_log - INFO -    273                                             for ix in range(len(X_split)):

2018-05-02 13:08:47,463 - memory_profile6_log - INFO -    274                                                 logger.info("processing batch-%d", ix)

2018-05-02 13:08:47,464 - memory_profile6_log - INFO -    275                                                 mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-05-02 13:08:47,464 - memory_profile6_log - INFO -    276                             

2018-05-02 13:08:47,466 - memory_profile6_log - INFO -    277                                             del X_split

2018-05-02 13:08:47,467 - memory_profile6_log - INFO -    278                                             logger.info("deleting X_split...")

2018-05-02 13:08:47,467 - memory_profile6_log - INFO -    279                                             del save_sigma_nt

2018-05-02 13:08:47,470 - memory_profile6_log - INFO -    280                                             logger.info("deleting save_sigma_nt...")

2018-05-02 13:08:47,470 - memory_profile6_log - INFO -    281                             

2018-05-02 13:08:47,471 - memory_profile6_log - INFO -    282                                             del BR

2018-05-02 13:08:47,471 - memory_profile6_log - INFO -    283                                             logger.info("deleting BR...")

2018-05-02 13:08:47,473 - memory_profile6_log - INFO -    284                             

2018-05-02 13:08:47,473 - memory_profile6_log - INFO -    285    476.7 MiB      0.0 MiB           elif str(saveto).lower() == "elastic":

2018-05-02 13:08:47,473 - memory_profile6_log - INFO -    286    476.7 MiB      0.0 MiB               logger.info("Using ElasticSearch as storage...")

2018-05-02 13:08:47,474 - memory_profile6_log - INFO -    287    476.7 MiB      0.0 MiB               logging.info("Saving main Transform model to Elasticsearch...")

2018-05-02 13:08:47,476 - memory_profile6_log - INFO -    288                                         # print model_transformsv.head(5)

2018-05-02 13:08:47,476 - memory_profile6_log - INFO -    289                                         

2018-05-02 13:08:47,477 - memory_profile6_log - INFO -    290    487.3 MiB     10.6 MiB               X_split = np.array_split(model_transformsv, 5)

2018-05-02 13:08:47,482 - memory_profile6_log - INFO -    291    487.3 MiB      0.0 MiB               logger.info("Saving total data: %d", len(model_transformsv))

2018-05-02 13:08:47,483 - memory_profile6_log - INFO -    292    487.3 MiB      0.0 MiB               logger.info("Len of X_split for batch save model_transformsv: %d", len(X_split))

2018-05-02 13:08:47,483 - memory_profile6_log - INFO -    293    487.3 MiB    -22.6 MiB               for ix in range(len(X_split)):

2018-05-02 13:08:47,483 - memory_profile6_log - INFO -    294    487.3 MiB   -212.7 MiB                   logger.info("processing batch-%d", ix)

2018-05-02 13:08:47,484 - memory_profile6_log - INFO -    295    438.6 MiB   -276.9 MiB                   mh.saveElasticS(X_split[ix])

2018-05-02 13:08:47,486 - memory_profile6_log - INFO -    296    409.6 MiB    -77.7 MiB               del X_split

2018-05-02 13:08:47,486 - memory_profile6_log - INFO -    297                             

2018-05-02 13:08:47,486 - memory_profile6_log - INFO -    298    409.6 MiB      0.0 MiB               logger.info("Saving fitted_models as history to Elasticsearch...")

2018-05-02 13:08:47,486 - memory_profile6_log - INFO -    299    410.0 MiB      0.4 MiB               save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 13:08:47,487 - memory_profile6_log - INFO -    300    428.9 MiB     18.9 MiB               fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-05-02 13:08:47,487 - memory_profile6_log - INFO -    301                                         # print fitted_models_sigmant.head(5)

2018-05-02 13:08:47,489 - memory_profile6_log - INFO -    302                                         

2018-05-02 13:08:47,490 - memory_profile6_log - INFO -    303    442.8 MiB     13.9 MiB               X_split = np.array_split(fitted_models_sigmant, 5)

2018-05-02 13:08:47,494 - memory_profile6_log - INFO -    304    442.8 MiB      0.0 MiB               logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-05-02 13:08:47,496 - memory_profile6_log - INFO -    305    442.8 MiB      0.0 MiB               logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-05-02 13:08:47,496 - memory_profile6_log - INFO -    306    442.8 MiB     -2.4 MiB               for ix in range(len(X_split)):

2018-05-02 13:08:47,497 - memory_profile6_log - INFO -    307    442.8 MiB   -373.7 MiB                   logger.info("processing batch-%d", ix)

2018-05-02 13:08:47,499 - memory_profile6_log - INFO -    308    354.7 MiB   -461.8 MiB                   mh.saveElasticS(X_split[ix], esindex_name="fitted_hist_index",  estype_name='fitted_hist_type')

2018-05-02 13:08:47,500 - memory_profile6_log - INFO -    309    339.8 MiB   -103.0 MiB               del X_split

2018-05-02 13:08:47,500 - memory_profile6_log - INFO -    310                                         

2018-05-02 13:08:47,502 - memory_profile6_log - INFO -    311    339.9 MiB      0.0 MiB               del BR

2018-05-02 13:08:47,502 - memory_profile6_log - INFO -    312    339.9 MiB      0.0 MiB               logger.info("deleting BR...")

2018-05-02 13:08:47,503 - memory_profile6_log - INFO -    313    339.9 MiB      0.0 MiB               del save_sigma_nt

2018-05-02 13:08:47,506 - memory_profile6_log - INFO -    314    339.9 MiB      0.0 MiB               logger.info("deleting save_sigma_nt...")

2018-05-02 13:08:47,506 - memory_profile6_log - INFO -    315    325.0 MiB    -14.9 MiB               del fitted_models_sigmant

2018-05-02 13:08:47,507 - memory_profile6_log - INFO -    316    325.0 MiB      0.0 MiB               logger.info("deleting fitted_models_sigmant...")

2018-05-02 13:08:47,509 - memory_profile6_log - INFO -    317                             

2018-05-02 13:08:47,509 - memory_profile6_log - INFO -    318                                     # need save sigma_nt for daily train

2018-05-02 13:08:47,509 - memory_profile6_log - INFO -    319                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-05-02 13:08:47,509 - memory_profile6_log - INFO -    320                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-05-02 13:08:47,510 - memory_profile6_log - INFO -    321    325.0 MiB      0.0 MiB           if start_date and end_date:

2018-05-02 13:08:47,512 - memory_profile6_log - INFO -    322                                         if not fitby_sigmant:

2018-05-02 13:08:47,512 - memory_profile6_log - INFO -    323                                             logging.info("Saving sigma Nt...")

2018-05-02 13:08:47,513 - memory_profile6_log - INFO -    324                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 13:08:47,513 - memory_profile6_log - INFO -    325                                             save_sigma_nt['start_date'] = start_date

2018-05-02 13:08:47,513 - memory_profile6_log - INFO -    326                                             save_sigma_nt['end_date'] = end_date

2018-05-02 13:08:47,519 - memory_profile6_log - INFO -    327                                             print save_sigma_nt.head(5)

2018-05-02 13:08:47,520 - memory_profile6_log - INFO -    328                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-05-02 13:08:47,522 - memory_profile6_log - INFO -    329                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-05-02 13:08:47,522 - memory_profile6_log - INFO -    330    325.0 MiB      0.0 MiB       return

2018-05-02 13:08:47,523 - memory_profile6_log - INFO - 


2018-05-02 13:08:47,523 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-05-02 14:25:12,342 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 14:25:12,384 - memory_profile6_log - INFO - date_generated: 
2018-05-02 14:25:12,384 - memory_profile6_log - INFO -  
2018-05-02 14:25:12,385 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 14, 25, 12, 381000)]
2018-05-02 14:25:12,385 - memory_profile6_log - INFO - 

2018-05-02 14:25:12,387 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 14:25:12,387 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 14:25:12,388 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 14:25:12,536 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 14:25:12,542 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 14:26:12,710 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 14:26:12,713 - memory_profile6_log - INFO - date_generated: 
2018-05-02 14:26:12,713 - memory_profile6_log - INFO -  
2018-05-02 14:26:12,713 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 14, 26, 12, 711000)]
2018-05-02 14:26:12,713 - memory_profile6_log - INFO - 

2018-05-02 14:26:12,713 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 14:26:12,714 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 14:26:12,714 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 14:26:12,858 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 14:26:12,861 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 14:26:16,154 - memory_profile6_log - INFO - size of df: 2.69 KB
2018-05-02 14:26:16,154 - memory_profile6_log - INFO - getting total: 10 training data(genuine interest) for date: 2018-05-01
2018-05-02 14:26:16,171 - memory_profile6_log - INFO - loading history data from elastic...
2018-05-02 14:26:16,174 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-05-02 14:26:16,174 - memory_profile6_log - INFO - Appending history data...
2018-05-02 14:26:51,838 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 14:26:51,842 - memory_profile6_log - INFO - date_generated: 
2018-05-02 14:26:51,842 - memory_profile6_log - INFO -  
2018-05-02 14:26:51,842 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 14, 26, 51, 840000)]
2018-05-02 14:26:51,844 - memory_profile6_log - INFO - 

2018-05-02 14:26:51,844 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 14:26:51,844 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 14:26:51,845 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 14:26:52,000 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 14:26:52,003 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 14:26:54,742 - memory_profile6_log - INFO - size of df: 2.69 KB
2018-05-02 14:26:54,743 - memory_profile6_log - INFO - getting total: 10 training data(genuine interest) for date: 2018-05-01
2018-05-02 14:26:54,763 - memory_profile6_log - INFO - loading history data from elastic...
2018-05-02 14:26:54,765 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-05-02 14:26:54,766 - memory_profile6_log - INFO - Appending history data...
2018-05-02 14:27:21,000 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 14:27:21,003 - memory_profile6_log - INFO - date_generated: 
2018-05-02 14:27:21,003 - memory_profile6_log - INFO -  
2018-05-02 14:27:21,003 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 14, 27, 21)]
2018-05-02 14:27:21,003 - memory_profile6_log - INFO - 

2018-05-02 14:27:21,003 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 14:27:21,005 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 14:27:21,005 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 14:27:21,177 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 14:27:21,181 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 14:27:23,848 - memory_profile6_log - INFO - size of df: 2.69 KB
2018-05-02 14:27:23,849 - memory_profile6_log - INFO - getting total: 10 training data(genuine interest) for date: 2018-05-01
2018-05-02 14:27:23,867 - memory_profile6_log - INFO - loading history data from elastic...
2018-05-02 14:27:23,868 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-05-02 14:27:23,868 - memory_profile6_log - INFO - Appending history data...
2018-05-02 14:27:57,065 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 14:27:57,068 - memory_profile6_log - INFO - date_generated: 
2018-05-02 14:27:57,068 - memory_profile6_log - INFO -  
2018-05-02 14:27:57,069 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 14, 27, 57, 66000)]
2018-05-02 14:27:57,069 - memory_profile6_log - INFO - 

2018-05-02 14:27:57,069 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 14:27:57,069 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 14:27:57,069 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 14:27:57,246 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 14:27:57,252 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 14:27:59,839 - memory_profile6_log - INFO - size of df: 2.69 KB
2018-05-02 14:27:59,841 - memory_profile6_log - INFO - getting total: 10 training data(genuine interest) for date: 2018-05-01
2018-05-02 14:27:59,858 - memory_profile6_log - INFO - loading history data from elastic...
2018-05-02 14:27:59,859 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-05-02 14:27:59,861 - memory_profile6_log - INFO - Appending history data...
2018-05-02 14:27:59,872 - memory_profile6_log - INFO - Pandas(Index=0, date=Timestamp('2018-05-01 00:00:00+0000', tz='UTC'), user_id=u'1622a85a9ff160-06f9eb8dd5fb24-586e6731-55188-1622a85aa001a0', topic_id=u'10960288', is_general=True, num=1L)
2018-05-02 14:27:59,875 - memory_profile6_log - INFO - 

2018-05-02 14:28:28,766 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 14:28:28,770 - memory_profile6_log - INFO - date_generated: 
2018-05-02 14:28:28,770 - memory_profile6_log - INFO -  
2018-05-02 14:28:28,772 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 14, 28, 28, 769000)]
2018-05-02 14:28:28,772 - memory_profile6_log - INFO - 

2018-05-02 14:28:28,773 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 14:28:28,773 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 14:28:28,773 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 14:28:28,917 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 14:28:28,921 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 14:28:32,634 - memory_profile6_log - INFO - size of df: 2.69 KB
2018-05-02 14:28:32,637 - memory_profile6_log - INFO - getting total: 10 training data(genuine interest) for date: 2018-05-01
2018-05-02 14:28:32,653 - memory_profile6_log - INFO - loading history data from elastic...
2018-05-02 14:28:32,654 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-05-02 14:28:32,655 - memory_profile6_log - INFO - Appending history data...
2018-05-02 14:28:32,663 - memory_profile6_log - INFO - Pandas(Index=0, date=Timestamp('2018-05-01 00:00:00+0000', tz='UTC'), user_id=u'1622a85a9ff160-06f9eb8dd5fb24-586e6731-55188-1622a85aa001a0', topic_id=u'10960288', is_general=True, num=1L)
2018-05-02 14:28:32,664 - memory_profile6_log - INFO - 

2018-05-02 14:30:16,591 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 14:30:16,594 - memory_profile6_log - INFO - date_generated: 
2018-05-02 14:30:16,594 - memory_profile6_log - INFO -  
2018-05-02 14:30:16,595 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 14, 30, 16, 592000)]
2018-05-02 14:30:16,595 - memory_profile6_log - INFO - 

2018-05-02 14:30:16,595 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 14:30:16,595 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 14:30:16,595 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 14:30:16,753 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 14:30:16,757 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 14:30:20,667 - memory_profile6_log - INFO - size of df: 2.69 KB
2018-05-02 14:30:20,668 - memory_profile6_log - INFO - getting total: 10 training data(genuine interest) for date: 2018-05-01
2018-05-02 14:30:20,688 - memory_profile6_log - INFO - loading history data from elastic...
2018-05-02 14:30:20,690 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-05-02 14:30:20,691 - memory_profile6_log - INFO - Appending history data...
2018-05-02 14:30:53,153 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 14:30:53,155 - memory_profile6_log - INFO - date_generated: 
2018-05-02 14:30:53,155 - memory_profile6_log - INFO -  
2018-05-02 14:30:53,155 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 14, 30, 53, 154000)]
2018-05-02 14:30:53,155 - memory_profile6_log - INFO - 

2018-05-02 14:30:53,157 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 14:30:53,157 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 14:30:53,157 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 14:30:53,301 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 14:30:53,305 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 14:30:57,282 - memory_profile6_log - INFO - size of df: 2.69 KB
2018-05-02 14:30:57,285 - memory_profile6_log - INFO - getting total: 10 training data(genuine interest) for date: 2018-05-01
2018-05-02 14:30:57,302 - memory_profile6_log - INFO - loading history data from elastic...
2018-05-02 14:30:57,305 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-05-02 14:30:57,305 - memory_profile6_log - INFO - Appending history data...
2018-05-02 14:30:58,763 - memory_profile6_log - INFO - <class 'pandas.core.frame.DataFrame'>
2018-05-02 14:30:58,765 - memory_profile6_log - INFO - 

2018-05-02 14:30:58,796 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, topic_id, pt_posterior_x_Nt, smoothed_pt_posterior, p0_cat_ci, sigma_Nt]
Index: []
2018-05-02 14:30:58,798 - memory_profile6_log - INFO - 

2018-05-02 14:32:24,427 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 14:32:24,430 - memory_profile6_log - INFO - date_generated: 
2018-05-02 14:32:24,430 - memory_profile6_log - INFO -  
2018-05-02 14:32:24,430 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 14, 32, 24, 428000)]
2018-05-02 14:32:24,431 - memory_profile6_log - INFO - 

2018-05-02 14:32:24,431 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 14:32:24,433 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 14:32:24,434 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 14:32:24,615 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 14:32:24,618 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 14:32:27,815 - memory_profile6_log - INFO - size of df: 2.69 KB
2018-05-02 14:32:27,816 - memory_profile6_log - INFO - getting total: 10 training data(genuine interest) for date: 2018-05-01
2018-05-02 14:32:27,851 - memory_profile6_log - INFO - loading history data from elastic...
2018-05-02 14:32:27,854 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-05-02 14:32:27,855 - memory_profile6_log - INFO - Appending history data...
2018-05-02 14:32:29,687 - memory_profile6_log - INFO - <class 'pandas.core.frame.DataFrame'>
2018-05-02 14:32:29,687 - memory_profile6_log - INFO - 

2018-05-02 14:32:29,717 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, topic_id, pt_posterior_x_Nt, smoothed_pt_posterior, p0_cat_ci, sigma_Nt]
Index: []
2018-05-02 14:32:29,719 - memory_profile6_log - INFO - 

2018-05-02 14:32:29,720 - memory_profile6_log - INFO - uid: 1622a85a9ff160-06f9eb8dd5fb24-586e6731-55188-1622a85aa001a0 with topic_id: 10960288 is empty!
2018-05-02 14:32:29,721 - memory_profile6_log - INFO - 

2018-05-02 14:32:31,266 - memory_profile6_log - INFO - <class 'pandas.core.frame.DataFrame'>
2018-05-02 14:32:31,266 - memory_profile6_log - INFO - 

2018-05-02 14:32:31,279 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, topic_id, pt_posterior_x_Nt, smoothed_pt_posterior, p0_cat_ci, sigma_Nt]
Index: []
2018-05-02 14:32:31,280 - memory_profile6_log - INFO - 

2018-05-02 14:32:31,282 - memory_profile6_log - INFO - uid: 16132066cfd38-0939023b230c51-54235b07-38400-16132066cff159 with topic_id: 10960288 is empty!
2018-05-02 14:32:31,283 - memory_profile6_log - INFO - 

2018-05-02 14:32:32,687 - memory_profile6_log - INFO - <class 'pandas.core.frame.DataFrame'>
2018-05-02 14:32:32,690 - memory_profile6_log - INFO - 

2018-05-02 14:32:32,700 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, topic_id, pt_posterior_x_Nt, smoothed_pt_posterior, p0_cat_ci, sigma_Nt]
Index: []
2018-05-02 14:32:32,703 - memory_profile6_log - INFO - 

2018-05-02 14:32:32,703 - memory_profile6_log - INFO - uid: 1615a671bec23-0e0b2ccdad1393-466b6c38-38400-1615a671bef20 with topic_id: 22551890 is empty!
2018-05-02 14:32:32,706 - memory_profile6_log - INFO - 

2018-05-02 14:32:34,440 - memory_profile6_log - INFO - <class 'pandas.core.frame.DataFrame'>
2018-05-02 14:32:34,441 - memory_profile6_log - INFO - 

2018-05-02 14:32:34,459 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, topic_id, pt_posterior_x_Nt, smoothed_pt_posterior, p0_cat_ci, sigma_Nt]
Index: []
2018-05-02 14:32:34,460 - memory_profile6_log - INFO - 

2018-05-02 14:32:34,463 - memory_profile6_log - INFO - uid: 1622a85a9ff160-06f9eb8dd5fb24-586e6731-55188-1622a85aa001a0 with topic_id: 22552186 is empty!
2018-05-02 14:32:34,464 - memory_profile6_log - INFO - 

2018-05-02 14:32:36,187 - memory_profile6_log - INFO - <class 'pandas.core.frame.DataFrame'>
2018-05-02 14:32:36,190 - memory_profile6_log - INFO - 

2018-05-02 14:32:36,207 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, topic_id, pt_posterior_x_Nt, smoothed_pt_posterior, p0_cat_ci, sigma_Nt]
Index: []
2018-05-02 14:32:36,210 - memory_profile6_log - INFO - 

2018-05-02 14:32:36,213 - memory_profile6_log - INFO - uid: 16130d526bd0-0d4fa924401735-83c521d-38400-16130d526bf1bf with topic_id: 22552186 is empty!
2018-05-02 14:32:36,214 - memory_profile6_log - INFO - 

2018-05-02 14:37:27,516 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 14:37:27,519 - memory_profile6_log - INFO - date_generated: 
2018-05-02 14:37:27,520 - memory_profile6_log - INFO -  
2018-05-02 14:37:27,522 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 14, 37, 27, 518000)]
2018-05-02 14:37:27,522 - memory_profile6_log - INFO - 

2018-05-02 14:37:27,523 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 14:37:27,523 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 14:37:27,523 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 14:37:27,687 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 14:37:27,690 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 14:39:06,108 - memory_profile6_log - INFO - size of df: 82.28 MB
2018-05-02 14:39:06,109 - memory_profile6_log - INFO - getting total: 324874 training data(genuine interest) for date: 2018-05-01
2018-05-02 14:39:06,170 - memory_profile6_log - INFO - loading history data from elastic...
2018-05-02 14:39:06,171 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-05-02 14:39:06,173 - memory_profile6_log - INFO - Appending history data...
2018-05-02 14:39:08,987 - memory_profile6_log - INFO - uid: 1622a85a9ff160-06f9eb8dd5fb24-586e6731-55188-1622a85aa001a0 with topic_id: 10960288 is empty!
2018-05-02 14:39:10,209 - memory_profile6_log - INFO - uid: 16132066cfd38-0939023b230c51-54235b07-38400-16132066cff159 with topic_id: 10960288 is empty!
2018-05-02 14:39:11,446 - memory_profile6_log - INFO - uid: 1614b58b81c108-0ce296bb0a45e1-55454929-38400-1614b58b81e13f with topic_id: 10960288 is empty!
2018-05-02 14:39:13,282 - memory_profile6_log - INFO - uid: 16141c4bcefc9-0958eed353b901-646e0a3e-38400-16141c4bcf16a with topic_id: 10960288 is empty!
2018-05-02 14:39:15,430 - memory_profile6_log - INFO - uid: 1612dbdd0d983-0f55011ccfbb4f-362a0770-38400-1612dbdd0dbfb with topic_id: 10960288 is empty!
2018-05-02 14:39:17,592 - memory_profile6_log - INFO - uid: 16140595aef60-0f425b909e43ea-5768397b-100200-16140595af0c5 with topic_id: 10960288 is empty!
2018-05-02 14:39:19,125 - memory_profile6_log - INFO - uid: 161859e4ba915-075b9b0927a6f2-21223579-38400-161859e4baa132 with topic_id: 10960288 is empty!
2018-05-02 14:39:20,655 - memory_profile6_log - INFO - uid: 161f923a82713-06347c63a91799-2819301e-38400-161f923a82827 with topic_id: 10960288 is empty!
2018-05-02 14:39:21,776 - memory_profile6_log - INFO - uid: 16156103d339a-00477eb8f5b5bb-e4f5b5f-38400-16156103d3510d with topic_id: 10960288 is empty!
2018-05-02 14:39:23,121 - memory_profile6_log - INFO - uid: 162891bb15b6e-0ee3fb2a0c4b9a-5e446d3e-38400-162891bb15ce with topic_id: 10960288 is empty!
2018-05-02 14:39:24,760 - memory_profile6_log - INFO - uid: 162c8aff3d07-02be497cc5c8c5-420c4473-49a10-162c8aff3d319 with topic_id: 10960288 is empty!
2018-05-02 14:39:26,490 - memory_profile6_log - INFO - uid: 161372bf53849-0801d0baa94a39-20051367-38400-161372bf53ae4 with topic_id: 10960288 is empty!
2018-05-02 14:39:27,730 - memory_profile6_log - INFO - uid: 162ec1a4a3338-0a1184d8346aca-335a3821-38400-162ec1a4a36131 with topic_id: 10960288 is empty!
2018-05-02 14:39:29,267 - memory_profile6_log - INFO - uid: 161372cdd0419-0b0bcddfea6f52-7c65030b-38400-161372cdd0631 with topic_id: 10960288 is empty!
2018-05-02 14:39:30,822 - memory_profile6_log - INFO - uid: 1613c68ffc58-034e95b5534567-542f227e-38400-1613c68ffc66 with topic_id: 10960288 is empty!
2018-05-02 14:39:32,335 - memory_profile6_log - INFO - uid: 161e98f7ad7ce-0b5498330f319b-1f4a6b4a-38400-161e98f7ad8d0 with topic_id: 10960288 is empty!
2018-05-02 14:39:34,181 - memory_profile6_log - INFO - uid: 1612d965734ae-0c57e117dd5071-17387a5f-38400-1612d96573734 with topic_id: 10960288 is empty!
2018-05-02 14:39:35,361 - memory_profile6_log - INFO - uid: 16217d9eec01-003f629186be7c-164b4536-38400-16217d9eec5be with topic_id: 10960288 is empty!
2018-05-02 14:39:36,944 - memory_profile6_log - INFO - uid: 1613af76a39c-0417a7aba-40765531-38400-1613af76a3b1 with topic_id: 10960288 is empty!
2018-05-02 14:39:38,579 - memory_profile6_log - INFO - uid: 1614b38f639533-0868e316b31c26-5068307b-100200-1614b38f63a16c with topic_id: 10960288 is empty!
2018-05-02 14:39:40,117 - memory_profile6_log - INFO - uid: 1631aaca42140-0e05e27c-5d660819-38400-1631aaca423b6 with topic_id: 10960288 is empty!
2018-05-02 14:39:41,615 - memory_profile6_log - INFO - uid: 16130b4129d6-062803f82578b9-2502631f-38400-16130b4129fd3 with topic_id: 10960288 is empty!
2018-05-02 14:39:43,086 - memory_profile6_log - INFO - uid: 161443aa3a12-0f8a76d8c1bd5-5975453a-38400-161443aa3a3bc with topic_id: 10960288 is empty!
2018-05-02 14:39:44,322 - memory_profile6_log - INFO - uid: 1631a75da4bf1-05ae92c53-525c4311-38400-1631a75da4eef with topic_id: 10960288 is empty!
2018-05-02 14:39:46,160 - memory_profile6_log - INFO - uid: 16319692dfc149-005b182d6-29524211-38400-16319692dfe97 with topic_id: 10960288 is empty!
2018-05-02 14:39:47,788 - memory_profile6_log - INFO - uid: 16137374fc21a-0bb38dbb493c17-76313118-4a640-16137374fc39c5 with topic_id: 10960288 is empty!
2018-05-02 14:39:49,540 - memory_profile6_log - INFO - uid: 1631c52dc5472-06f06bc6-117b752c-38400-1631c52dc5693 with topic_id: 10960288 is empty!
2018-05-02 14:39:51,101 - memory_profile6_log - INFO - uid: 1631b65e2ed2f-0dd4ef9cd-732c4466-38400-1631b65e2efcb with topic_id: 10960288 is empty!
2018-05-02 14:39:52,915 - memory_profile6_log - INFO - uid: 161319969e360-0e16696eed72dd-6621737c-38400-161319969e672 with topic_id: 10960288 is empty!
2018-05-02 14:39:54,137 - memory_profile6_log - INFO - uid: 1631a8d3d5161-0ef31e3f3-276a283e-34080-1631a8d3d53f with topic_id: 10960288 is empty!
2018-05-02 14:39:55,480 - memory_profile6_log - INFO - uid: 1631aef97fa4c-0c9caf8ec-255c4214-38400-1631aef97fb49 with topic_id: 10960288 is empty!
2018-05-02 14:39:57,216 - memory_profile6_log - INFO - uid: 162bf562bfaec-01c75c2ba3e63a-45795176-55188-162bf562bfb67 with topic_id: 10960288 is empty!
2018-05-02 14:39:59,058 - memory_profile6_log - INFO - uid: 161c5df2d911-0da271a03-e700544-38400-161c5df2d938 with topic_id: 10960288 is empty!
2018-05-02 14:40:01,210 - memory_profile6_log - INFO - uid: 1614085b2ea12b-0c47a308c32467-7c267d5b-38400-1614085b2ecd2 with topic_id: 10960288 is empty!
2018-05-02 14:40:02,750 - memory_profile6_log - INFO - uid: 16131ea703e34b-001e058df915388-4d757756-3d10d-16131ea703f895 with topic_id: 10960288 is empty!
2018-05-02 14:40:04,275 - memory_profile6_log - INFO - uid: 1610da3389f59a-055abbc071e83-79601f35-100200-1610da338a047f with topic_id: 10960288 is empty!
2018-05-02 14:40:06,127 - memory_profile6_log - INFO - uid: 1618e1cce9938-0333b825b0924e-3a626075-3f480-1618e1cce9a129 with topic_id: 10960288 is empty!
2018-05-02 14:40:07,539 - memory_profile6_log - INFO - uid: 16141fa4fd749-08e8bc62701e3c-74217c59-38400-16141fa4fdbb9 with topic_id: 10960288 is empty!
2018-05-02 14:40:09,539 - memory_profile6_log - INFO - uid: 161dbcb89a519-00bb8d1e2614a8-5049663e-38400-161dbcb89a71f with topic_id: 10960288 is empty!
2018-05-02 14:40:11,138 - memory_profile6_log - INFO - uid: 1610d7ae25128-03b9b51b86799d-1c411b27-100200-1610d7ae25221c with topic_id: 10960288 is empty!
2018-05-02 14:40:12,690 - memory_profile6_log - INFO - uid: 1610cffb117c1-0d4678545a8eb88-173a7640-ff000-1610cffb1183ae with topic_id: 10960288 is empty!
2018-05-02 14:40:14,415 - memory_profile6_log - INFO - uid: 16240fa1ff421c-0d4f2f298bf50d-54237a32-43113-16240fa1ff6354 with topic_id: 10960288 is empty!
2018-05-02 14:40:15,892 - memory_profile6_log - INFO - uid: 162e501aa48f1-0bf95e2b21a6a5-406d4131-38400-162e501aa4a137 with topic_id: 10960288 is empty!
2018-05-02 14:40:17,447 - memory_profile6_log - INFO - uid: 1611c1535b9e6-032688edb6f1ff-393d5d0e-13c680-1611c1535bb190 with topic_id: 10960288 is empty!
2018-05-02 14:40:19,032 - memory_profile6_log - INFO - uid: 1612d24173b25-06b2f404ce4602-5f6c6e3d-38400-1612d24173e84 with topic_id: 10960288 is empty!
2018-05-02 14:40:20,740 - memory_profile6_log - INFO - uid: 163125055ef4c-0d22e3943da071-5640673d-49a10-163125055f0d with topic_id: 10960288 is empty!
2018-05-02 14:40:21,894 - memory_profile6_log - INFO - uid: 16214e8be6b12f-082bcca6dfe9e6-60277379-29b80-16214e8be6ee6 with topic_id: 10960288 is empty!
2018-05-02 14:40:23,631 - memory_profile6_log - INFO - uid: 162f5c9c1911cc-04435b6a7a8e89-5149653f-38400-162f5c9c193155 with topic_id: 10960288 is empty!
2018-05-02 14:40:24,964 - memory_profile6_log - INFO - uid: 16178ca5d5b58-04b00654312536-6b713c72-2c600-16178ca5d5da1 with topic_id: 10960288 is empty!
2018-05-02 14:40:26,709 - memory_profile6_log - INFO - uid: 1613045cefd3-07a82235a06f33-4c6b4504-38400-1613045cf00ad with topic_id: 10960288 is empty!
2018-05-02 14:40:28,240 - memory_profile6_log - INFO - uid: 16307d74ea126-035f4b3bf16dc7-27091b58-38400-16307d74ea4c8 with topic_id: 10960288 is empty!
2018-05-02 14:40:29,782 - memory_profile6_log - INFO - uid: 161453df40712-0d9fc101d5909b-5f7c683a-38400-161453df40830 with topic_id: 10960288 is empty!
2018-05-02 14:40:31,321 - memory_profile6_log - INFO - uid: 1612d9842bb17-0f7c6f75e26811-74217c59-38400-1612d9842be5 with topic_id: 10960288 is empty!
2018-05-02 14:40:32,641 - memory_profile6_log - INFO - uid: 161361ead26c-05e8d69f9-292a5039-38400-161361ead282a with topic_id: 10960288 is empty!
2018-05-02 14:40:33,875 - memory_profile6_log - INFO - uid: 1617e783d6674-0af89136e-503e066e-38400-1617e783d68f9 with topic_id: 10960288 is empty!
2018-05-02 14:40:35,342 - memory_profile6_log - INFO - uid: 161d4676cd527-062e8f6f6240ce-7e6a1669-38400-161d4676cd710e with topic_id: 10960288 is empty!
2018-05-02 14:40:37,145 - memory_profile6_log - INFO - uid: 1631a6e6daf14-0f92deaab-23504311-38400-1631a6e6db1131 with topic_id: 10960288 is empty!
2018-05-02 14:40:38,681 - memory_profile6_log - INFO - uid: 1623d6e890467-0e3806bdb6eda4-75712b39-38400-1623d6e89069a with topic_id: 10960288 is empty!
2018-05-02 14:40:40,483 - memory_profile6_log - INFO - uid: 162d95f588054f-07a6c6a08530c3-3c3c5a0a-100200-162d95f5881aac with topic_id: 10960288 is empty!
2018-05-02 14:40:42,066 - memory_profile6_log - INFO - uid: 1612d22d2c34a-0810e56d1d98ac-7c09110d-38400-1612d22d2c584 with topic_id: 10960288 is empty!
2018-05-02 14:40:43,697 - memory_profile6_log - INFO - uid: 1631b55a263ef-099e17414-776f292a-38400-1631b55a26537 with topic_id: 10960288 is empty!
2018-05-02 14:40:45,144 - memory_profile6_log - INFO - uid: 1624dee584f103-0c971af4040714-5e406d38-38400-1624dee5851255 with topic_id: 10960288 is empty!
2018-05-02 14:40:46,367 - memory_profile6_log - INFO - uid: 1631b0aacd230-08f422957-622d3d3a-38400-1631b0aacd493 with topic_id: 10960288 is empty!
2018-05-02 14:40:47,605 - memory_profile6_log - INFO - uid: 16132cb56f1d8-0b2ae6c160ae34-73417426-38400-16132cb56f3e8 with topic_id: 10960288 is empty!
2018-05-02 14:40:49,237 - memory_profile6_log - INFO - uid: 1618346e93844-0b94173a7-3e440014-38400-1618346e93ce with topic_id: 10960288 is empty!
2018-05-02 14:40:51,289 - memory_profile6_log - INFO - uid: 16315d82e0745e-070ec1aa65ecce-b34356b-1fa400-16315d82e087b1 with topic_id: 10960288 is empty!
2018-05-02 14:40:52,829 - memory_profile6_log - INFO - uid: 1624263b7e39e-0a0948f46d8bec-6525794e-29b80-1624263b7e634 with topic_id: 10960288 is empty!
2018-05-02 14:40:55,069 - memory_profile6_log - INFO - uid: 1628041d20013-018fde53cdeb-5c62612c-38400-1628041d2022 with topic_id: 10960288 is empty!
2018-05-02 14:40:56,696 - memory_profile6_log - INFO - uid: 16117f73d6135-0d2da8c0070ba3-454c0b2b-100200-16117f73d6289 with topic_id: 10960288 is empty!
2018-05-02 14:40:58,019 - memory_profile6_log - INFO - uid: 1622c3b40594-007cdd5d233534-5c6e6a3f-49a10-1622c3b405a0 with topic_id: 10960288 is empty!
2018-05-02 14:40:59,275 - memory_profile6_log - INFO - uid: 16319b72c9816-0257757f4-68384a08-4ddac.bed873794-16319b72c9a36 with topic_id: 10960288 is empty!
2018-05-02 14:41:00,802 - memory_profile6_log - INFO - uid: 1618fa44d843e-0672126f40e687-2143623f-38400-1618fa44d8580 with topic_id: 10960288 is empty!
2018-05-02 14:41:02,345 - memory_profile6_log - INFO - uid: 1631c0869f0bf-021c26bc5-25534011-38400-1631c0869f3aa with topic_id: 10960288 is empty!
2018-05-02 14:41:03,887 - memory_profile6_log - INFO - uid: 163190cd4110-0a67b31f5-5e475d1a-38400-163190cd41395 with topic_id: 10960288 is empty!
2018-05-02 14:41:05,921 - memory_profile6_log - INFO - uid: 1613d4b4fb2e8-02b1c89df4381d-3060661d-38400-1613d4b4fb4a5 with topic_id: 10960288 is empty!
2018-05-02 14:41:07,569 - memory_profile6_log - INFO - uid: 16133b040a25b4-09359ecad04b658-7636321b-3d10d-16133b040a3b0e with topic_id: 10960288 is empty!
2018-05-02 14:41:08,788 - memory_profile6_log - INFO - uid: 1631a28ae2df-08b008fc7-55061743-38400-1631a28ae2f73 with topic_id: 10960288 is empty!
2018-05-02 14:41:10,028 - memory_profile6_log - INFO - uid: 161b4dbf1bce2-0c8bed85cd097e-496e5238-38400-161b4dbf1be67 with topic_id: 10960288 is empty!
2018-05-02 14:41:11,562 - memory_profile6_log - INFO - uid: 162ed3b1b7354-06740ff4c30265-110f0c0e-2c880-162ed3b1b7570 with topic_id: 10960288 is empty!
2018-05-02 14:41:13,407 - memory_profile6_log - INFO - uid: 1631a98b99b32e-08fc77b83-1f577941-38400-1631a98b99c1ef with topic_id: 10960288 is empty!
2018-05-02 14:41:14,931 - memory_profile6_log - INFO - uid: 1630675840846-0f0b8ee437f0ad-4a4e280b-38400-1630675840aec with topic_id: 10960288 is empty!
2018-05-02 14:41:16,275 - memory_profile6_log - INFO - uid: 1616bec51a8134-011c6d130c52b3-e605869-38400-1616bec51a977 with topic_id: 10960288 is empty!
2018-05-02 14:41:18,232 - memory_profile6_log - INFO - uid: 16193ff0ba3b5-0bfb48ad82bcf4-70237260-38400-16193ff0ba578 with topic_id: 10960288 is empty!
2018-05-02 14:41:19,960 - memory_profile6_log - INFO - uid: 1626a5f5d3bc9-024fbee948a0d2-75725242-38400-1626a5f5d3d51 with topic_id: 10960288 is empty!
2018-05-02 14:41:21,438 - memory_profile6_log - INFO - uid: 1618f85cb7d51-0c179f644c9fd6-5b7f6a32-55188-1618f85cb7efc with topic_id: 10960288 is empty!
2018-05-02 14:41:22,927 - memory_profile6_log - INFO - uid: 162b97fc357e79-03d06fd6a58f02-b34356b-1fa400-162b97fc358795 with topic_id: 10960288 is empty!
2018-05-02 14:41:24,467 - memory_profile6_log - INFO - uid: 1631b3c3557121-04b385be9-525c4311-38400-1631b3c355883 with topic_id: 10960288 is empty!
2018-05-02 14:41:26,107 - memory_profile6_log - INFO - uid: 1612dc2968e130-0924add77cb4c2-5e1d5f51-38400-1612dc2969010d with topic_id: 10960288 is empty!
2018-05-02 14:41:27,688 - memory_profile6_log - INFO - uid: 162eb6aa093146-0b99d0e2ab0dc2-7e261d12-38400-162eb6aa0952a with topic_id: 10960288 is empty!
2018-05-02 14:41:29,371 - memory_profile6_log - INFO - uid: 162f0e2327550f-0aeaf3cdb19385-b34356b-144000-162f0e232766e5 with topic_id: 10960288 is empty!
2018-05-02 14:41:30,601 - memory_profile6_log - INFO - uid: 1631af17145137-0881eea59774e8-65306457-38400-1631af17146102 with topic_id: 10960288 is empty!
2018-05-02 14:41:32,030 - memory_profile6_log - INFO - uid: 1612d17c069a5-00087b6c4b57a8-20e0644-38400-1612d17c06a47 with topic_id: 10960288 is empty!
2018-05-02 14:41:33,368 - memory_profile6_log - INFO - uid: 162be8a351711a-07ed2883a48e4-113f7d3a-43113-162be8a351a0 with topic_id: 10960288 is empty!
2018-05-02 14:41:35,315 - memory_profile6_log - INFO - uid: 16191662c9258-09addd678f327d-5143673f-38400-16191662c93f3 with topic_id: 10960288 is empty!
2018-05-02 14:41:37,369 - memory_profile6_log - INFO - uid: 1616bc9798b204-014a6b9f800f29-556b3f73-100200-1616bc9798c31 with topic_id: 10960288 is empty!
2018-05-02 14:41:39,507 - memory_profile6_log - INFO - uid: 1613b633b25c0-0bdcce86256146-423e257c-38400-1613b633b2688 with topic_id: 10960288 is empty!
2018-05-02 14:41:41,361 - memory_profile6_log - INFO - uid: 1612d4cc048107-0cb52c343-16157579-38400-1612d4cc0495 with topic_id: 10960288 is empty!
2018-05-02 14:41:43,309 - memory_profile6_log - INFO - uid: 1619ae25ac21c4-0a201616c05ea5-594b6b3b-38400-1619ae25ac31a5 with topic_id: 10960288 is empty!
2018-05-02 14:41:44,898 - memory_profile6_log - INFO - uid: 162e6d24b39b9-02a7af14635fc1-56e061c-3f480-162e6d24b3a1e0 with topic_id: 10960288 is empty!
2018-05-02 14:41:46,881 - memory_profile6_log - INFO - uid: 16137f14bef96-0ab695aa2c7712-46696b2c-38400-16137f14bf23a with topic_id: 10960288 is empty!
2018-05-02 14:41:48,973 - memory_profile6_log - INFO - uid: 1610dd1b6ed542-0735720fbf1862-4323461-100200-1610dd1b6ee4c9 with topic_id: 10960288 is empty!
2018-05-02 14:41:50,677 - memory_profile6_log - INFO - uid: 162e111616a156-04abcfb59ab6d3-b34356b-13c680-162e111616bef with topic_id: 10960288 is empty!
2018-05-02 14:41:52,111 - memory_profile6_log - INFO - uid: 161830ef3e091-0b06c6374395e2-3c40760c-38400-161830ef3e2d9 with topic_id: 10960288 is empty!
2018-05-02 14:41:53,953 - memory_profile6_log - INFO - uid: 1628551295a138-005bb342-4732b16-2c600-1628551295b29 with topic_id: 10960288 is empty!
2018-05-02 14:41:55,801 - memory_profile6_log - INFO - uid: 1615af027051bd-0719ec16be2fbe-6e3a3311-38400-1615af027071d0 with topic_id: 10960288 is empty!
2018-05-02 14:41:58,256 - memory_profile6_log - INFO - uid: 161bb71fb60de-0ca5f2f569fc27-a35346f-100200-161bb71fb6114 with topic_id: 10960288 is empty!
2018-05-02 14:41:59,792 - memory_profile6_log - INFO - uid: 1619c7df013112-07d824e63cc2db-204a2f45-38400-1619c7df015251 with topic_id: 10960288 is empty!
2018-05-02 14:42:01,016 - memory_profile6_log - INFO - uid: 1631aed82fbae-0d8443d97-255c4214-38400-1631aed82feb6 with topic_id: 10960288 is empty!
2018-05-02 14:42:02,380 - memory_profile6_log - INFO - uid: 162d7f8c7484-028e1cf3da1c14-4b3b1c71-38400-162d7f8c74ac1 with topic_id: 10960288 is empty!
2018-05-02 14:42:03,786 - memory_profile6_log - INFO - uid: 16319376f76b3-0d0fd6bb2-66282845-38400-16319376f7ab0 with topic_id: 10960288 is empty!
2018-05-02 14:42:05,312 - memory_profile6_log - INFO - uid: 1627aab2e1a1d9-02410b174539b-294b6e3e-49a10-1627aab2e1b170 with topic_id: 10960288 is empty!
2018-05-02 14:42:06,553 - memory_profile6_log - INFO - uid: 16139cdec7b96-0829510180b703-5977613d-38400-16139cdec7c5 with topic_id: 10960288 is empty!
2018-05-02 14:42:07,786 - memory_profile6_log - INFO - uid: 161bceccad054-0a95ddefae6e24-535b6b3e-38400-161bceccad1c with topic_id: 10960288 is empty!
2018-05-02 14:42:09,749 - memory_profile6_log - INFO - uid: 163198658222e-0263c61b8-622d3d3a-38400-1631986582467 with topic_id: 10960288 is empty!
2018-05-02 14:42:11,250 - memory_profile6_log - INFO - uid: 16137526646e2-00ab1a96362039-2c756d7c-38400-161375266479b with topic_id: 10960288 is empty!
2018-05-02 14:42:12,753 - memory_profile6_log - INFO - uid: 16285808115123-059a94a43804eb-42243104-38400-162858081171e1 with topic_id: 10960288 is empty!
2018-05-02 14:42:14,233 - memory_profile6_log - INFO - uid: 1612dd3932a132-01e79497c090bd-55d3141-4a640-1612dd3932b83b with topic_id: 10960288 is empty!
2018-05-02 14:42:16,089 - memory_profile6_log - INFO - uid: 161188e4965248-08014c6f5407a5-454c0a2b-100200-161188e49661b7 with topic_id: 10960288 is empty!
2018-05-02 14:42:17,717 - memory_profile6_log - INFO - uid: 161658e73db12a-0da25c80fadba2-4d707656-2c600-161658e73dc706 with topic_id: 10960288 is empty!
2018-05-02 14:42:18,772 - memory_profile6_log - INFO - uid: 16239c256f367-0b060e7b8e3836-8343565-144000-16239c256f579f with topic_id: 10960288 is empty!
2018-05-02 14:42:20,375 - memory_profile6_log - INFO - uid: 16131014f9e81-06988a678226bd-5610725c-38400-16131014fa065 with topic_id: 10960288 is empty!
2018-05-02 14:42:21,687 - memory_profile6_log - INFO - uid: 1631bbf83fff-0ee5ab16d-652b3140-38400-1631bbf840212 with topic_id: 10960288 is empty!
2018-05-02 14:42:23,346 - memory_profile6_log - INFO - uid: 1631a32cdde58-07d72e97e-23504311-38400-1631a32cde0dd with topic_id: 10960288 is empty!
2018-05-02 14:42:25,187 - memory_profile6_log - INFO - uid: 1631a48724a4e-0e79957dc-66745744-42f13.0a9419638-1631a48724cd5 with topic_id: 10960288 is empty!
2018-05-02 14:42:26,720 - memory_profile6_log - INFO - uid: 16141c34808190-0c30ec1c177146-5450747b-49a10-16141c34809b8 with topic_id: 10960288 is empty!
2018-05-02 14:42:27,881 - memory_profile6_log - INFO - uid: 1630a5605a8bc-03e85196f9e53d-31460c1e-38400-1630a5605ab107 with topic_id: 10960288 is empty!
2018-05-02 14:42:29,183 - memory_profile6_log - INFO - uid: 162ebd87aa7246-0e118dc03d08198-2e694c35-3d10d-162ebd87aa87e with topic_id: 10960288 is empty!
2018-05-02 14:42:30,752 - memory_profile6_log - INFO - uid: 1631a1edbb844-00d908b16-34116429-38400-1631a1edbbb14 with topic_id: 10960288 is empty!
2018-05-02 14:42:32,559 - memory_profile6_log - INFO - uid: 1613b7343a12b-08015b16a66854-b275059-38400-1613b7343a381 with topic_id: 10960288 is empty!
2018-05-02 14:42:34,404 - memory_profile6_log - INFO - uid: 1630d9f8d94c3-03d8ec8af18c488-17347840-100200-1630d9f8d963f7 with topic_id: 10960288 is empty!
2018-05-02 14:42:36,348 - memory_profile6_log - INFO - uid: 163178c591736e-0917b1504d777b-33677f06-fa000-163178c59183b6 with topic_id: 10960288 is empty!
2018-05-02 14:42:37,782 - memory_profile6_log - INFO - uid: 1631b3f8e58bb-02ef4ad95-21020b08-13d65b.5f4f3025-1631b3f8e5ae with topic_id: 10960288 is empty!
2018-05-02 14:42:39,618 - memory_profile6_log - INFO - uid: 1631a368d64a5-0305f3d77-12791c06-38400-1631a368d6612f with topic_id: 10960288 is empty!
2018-05-02 14:42:41,724 - memory_profile6_log - INFO - uid: 1630587b2892e6-0003f6b3fd5f31-46534e68-ff000-1630587b28a1a9 with topic_id: 10960288 is empty!
2018-05-02 14:42:43,447 - memory_profile6_log - INFO - uid: 161f5d77c8111-07b9699b88a8bc-49566e-13c680-161f5d77c8220d with topic_id: 10960288 is empty!
2018-05-02 14:42:44,947 - memory_profile6_log - INFO - uid: 16306c3b7ac290-03c54f6789018f-b34356b-13c680-16306c3b7ad35 with topic_id: 10960288 is empty!
2018-05-02 14:42:46,730 - memory_profile6_log - INFO - uid: 1631a16b4fc3b-06c6be30a-12791c06-38400-1631a16b4fec1 with topic_id: 10960288 is empty!
2018-05-02 14:42:48,220 - memory_profile6_log - INFO - uid: 162aa0e3361128-0e67b34ab300b-37120502-38400-162aa0e3362152 with topic_id: 10960288 is empty!
2018-05-02 14:42:49,704 - memory_profile6_log - INFO - uid: 9c21818e-b621-49cb-8358-5e2e0b85e997 with topic_id: 10960288 is empty!
2018-05-02 14:42:51,292 - memory_profile6_log - INFO - uid: 162cd1eb1b4a51-07127212595c2e-b34356b-100200-162cd1eb1b57f7 with topic_id: 10960288 is empty!
2018-05-02 14:42:53,142 - memory_profile6_log - INFO - uid: 16319b2768fa6-07db82404-712e4164-38400-16319b2769164 with topic_id: 10960288 is empty!
2018-05-02 14:42:54,984 - memory_profile6_log - INFO - uid: 1611caa95a4f9-06996f2a7abeb1-32637402-100200-1611caa95a529c with topic_id: 10960288 is empty!
2018-05-02 14:42:56,644 - memory_profile6_log - INFO - uid: 162d3c1201291-080d6b899df155-425f4a54-38400-162d3c1201429 with topic_id: 11911567 is empty!
2018-05-02 14:42:58,678 - memory_profile6_log - INFO - uid: 16160ee575b2d-00018d63fd2739-6c227946-29b80-16160ee575e25 with topic_id: 11911567 is empty!
2018-05-02 14:43:00,308 - memory_profile6_log - INFO - uid: 4b8bbb96-3b59-4c58-a796-d470388861b4 with topic_id: 11911567 is empty!
2018-05-02 14:43:02,055 - memory_profile6_log - INFO - uid: 162dcbf9037449-0453bf1d62a7b68-43574130-c0000-162dcbf9038352 with topic_id: 11911567 is empty!
2018-05-02 14:43:03,585 - memory_profile6_log - INFO - uid: 16319db651d376-022f0df179d6eb-7047503f-d3a64-16319db651e1c7 with topic_id: 11911567 is empty!
2018-05-02 14:43:04,822 - memory_profile6_log - INFO - uid: 16210a8e4406d-001df6ebf743e7-78247d6a-38400-16210a8e44245 with topic_id: 11911567 is empty!
2018-05-02 14:43:06,357 - memory_profile6_log - INFO - uid: 161c2835d0530-0dd7b36b942b01-5a736035-38400-161c2835d0710c with topic_id: 11911567 is empty!
2018-05-02 14:43:08,200 - memory_profile6_log - INFO - uid: 1631b3362d9f6-022580045-67511f4c-34080-1631b3362dcb4 with topic_id: 11911567 is empty!
2018-05-02 14:43:09,529 - memory_profile6_log - INFO - uid: 1611cc37aea8f-0524a2afaa8ae7-4323461-100200-1611cc37aeb706 with topic_id: 22284859 is empty!
2018-05-02 14:43:10,934 - memory_profile6_log - INFO - uid: 16140a1bb3317b-0227c6baa13e94-4d0e7b73-49a10-16140a1bb3430 with topic_id: 22291119 is empty!
2018-05-02 14:43:12,497 - memory_profile6_log - INFO - uid: 161b1c43258b4-05ae8030da08d3-70261414-38400-161b1c4325954 with topic_id: 22291119 is empty!
2018-05-02 14:43:14,641 - memory_profile6_log - INFO - uid: 161b5a157895d-06fc27e0227f9d-39626775-2c740-161b5a1578abf with topic_id: 22291119 is empty!
2018-05-02 14:43:16,184 - memory_profile6_log - INFO - uid: 1616f1ef90840b-0ea3a0a1aa3b85-2b29265f-3d10d-1616f1ef90a3ce with topic_id: 22291119 is empty!
2018-05-02 14:43:18,019 - memory_profile6_log - INFO - uid: 162d3c1201291-080d6b899df155-425f4a54-38400-162d3c1201429 with topic_id: 22291119 is empty!
2018-05-02 14:43:20,170 - memory_profile6_log - INFO - uid: 16147c15e16b7-047d7cc4889e7f-13c511b-38400-16147c15e17368 with topic_id: 22291119 is empty!
2018-05-02 14:43:21,377 - memory_profile6_log - INFO - uid: 161304f2f4a13-010059df-5c71754f-38400-161304f2f4f1 with topic_id: 22291119 is empty!
2018-05-02 14:43:22,944 - memory_profile6_log - INFO - uid: 1613ad2a094d4-005f69cced376c-78261712-38400-1613ad2a09619 with topic_id: 22291119 is empty!
2018-05-02 14:43:24,788 - memory_profile6_log - INFO - uid: 1621b12fc9a3e-0334ef427b0207-21d1435-38400-1621b12fc9d28 with topic_id: 22291119 is empty!
2018-05-02 14:43:26,621 - memory_profile6_log - INFO - uid: 1612d881de72b-08e6aef2c373c-5f74623d-38400-1612d881deb78 with topic_id: 22291119 is empty!
2018-05-02 14:43:27,808 - memory_profile6_log - INFO - uid: 162630c48b275f-00575561320aae-3a614f0b-100200-162630c48b4ed8 with topic_id: 22291119 is empty!
2018-05-02 14:43:29,384 - memory_profile6_log - INFO - uid: 1616efdcc1041-09320b68337155-5a102731-38400-1616efdcc1250 with topic_id: 22291119 is empty!
2018-05-02 14:43:30,898 - memory_profile6_log - INFO - uid: 1624e36035a2df-0f3fc7800c0e1-3d304855-38400-1624e36035b8 with topic_id: 22291119 is empty!
2018-05-02 14:43:32,493 - memory_profile6_log - INFO - uid: 1631a07910363-06d98a67fae217-1443334b-3f480-1631a07910660 with topic_id: 22291119 is empty!
2018-05-02 14:43:34,299 - memory_profile6_log - INFO - uid: 16317e96f2d60-00f8747c1-62415878-38400-16317e96f3189 with topic_id: 22291119 is empty!
2018-05-02 14:43:35,720 - memory_profile6_log - INFO - uid: 1612ffbe3be4c-034a0b30b-5c717549-38400-1612ffbe3c041 with topic_id: 22291119 is empty!
2018-05-02 14:43:37,375 - memory_profile6_log - INFO - uid: 1612db74988ea-07340e5d576e1c-39626277-38400-1612db7498a1e6 with topic_id: 22291119 is empty!
2018-05-02 14:43:39,523 - memory_profile6_log - INFO - uid: 162c189d4cc277-0f70f842a4bf04-3d5515-38400-162c189d4ce30f with topic_id: 22291119 is empty!
2018-05-02 14:43:41,378 - memory_profile6_log - INFO - uid: 16132e89c8ec2-0f23ddfe741ceb-70261016-38400-16132e89c909 with topic_id: 22291119 is empty!
2018-05-02 14:43:43,217 - memory_profile6_log - INFO - uid: 1623e1506d1a4-099cf1986-5672072d-38400-1623e1506d435 with topic_id: 22291119 is empty!
2018-05-02 14:43:44,858 - memory_profile6_log - INFO - uid: 16141b47539ec-0348bc2b1472cf-282b543f-38400-16141b4753b73 with topic_id: 22291119 is empty!
2018-05-02 14:43:46,586 - memory_profile6_log - INFO - uid: 16130b4129d6-062803f82578b9-2502631f-38400-16130b4129fd3 with topic_id: 22291119 is empty!
2018-05-02 14:43:47,842 - memory_profile6_log - INFO - uid: 163189019048b-02befde74-25534011-38400-16318901906cf with topic_id: 22291119 is empty!
2018-05-02 14:43:49,658 - memory_profile6_log - INFO - uid: 1631a16b4fc3b-06c6be30a-12791c06-38400-1631a16b4fec1 with topic_id: 22291119 is empty!
2018-05-02 14:43:51,612 - memory_profile6_log - INFO - uid: 1631967f2deb9-03469ad1e-283a4a06-59c02.d62921548-1631967f2e0ce with topic_id: 22291119 is empty!
2018-05-02 14:43:52,687 - memory_profile6_log - INFO - uid: 162ad9f69a6102-03715ebbf-444a072a-e1000-162ad9f69a735e with topic_id: 22291119 is empty!
2018-05-02 14:43:55,203 - memory_profile6_log - INFO - uid: 162df199bbb73-0837f23797f376-7d261e12-38400-162df199bc6b3 with topic_id: 22291119 is empty!
2018-05-02 14:43:57,104 - memory_profile6_log - INFO - uid: 16237f16974ae-09008aa96923c1-354f6004-38400-16237f169772e with topic_id: 22291119 is empty!
2018-05-02 14:43:58,272 - memory_profile6_log - INFO - uid: 161a71aba6f2c6-0b32d079333cdd-43795646-3f480-161a71aba70331 with topic_id: 22291119 is empty!
2018-05-02 14:44:00,532 - memory_profile6_log - INFO - uid: 1615fb733ae23-052a9bc1ea3451-1a566541-2c880-1615fb733b25d with topic_id: 22291119 is empty!
2018-05-02 14:44:01,947 - memory_profile6_log - INFO - uid: 1619229325143-0b2216d771b0bc-26041a52-38400-1619229325356 with topic_id: 22291119 is empty!
2018-05-02 14:44:03,177 - memory_profile6_log - INFO - uid: 1612ea294487-0933975ff7d0eb-604c6527-38400-1612ea2944d9e with topic_id: 22291119 is empty!
2018-05-02 14:44:04,361 - memory_profile6_log - INFO - uid: 1612e362f96476-01808e70b349d5-2b2b553d-38400-1612e362f972ce with topic_id: 22291119 is empty!
2018-05-02 14:44:05,898 - memory_profile6_log - INFO - uid: 161e22048b48e-0a98a66ac0b23d-7785204-38400-161e22048b51c with topic_id: 22291119 is empty!
2018-05-02 14:44:07,173 - memory_profile6_log - INFO - uid: 162f9c95a01132-077313bb8412d7-6665069-38400-162f9c95a024 with topic_id: 22291119 is empty!
2018-05-02 14:44:08,720 - memory_profile6_log - INFO - uid: 1612d73610650-015985f9a5a7b9-335f7f55-2c880-1612d73610a60 with topic_id: 22291119 is empty!
2018-05-02 14:44:10,240 - memory_profile6_log - INFO - uid: 79208647-713f-4498-9d40-1b247b383f8f with topic_id: 22291119 is empty!
2018-05-02 14:44:11,490 - memory_profile6_log - INFO - uid: 1612d33b1249c-099084af9d3f1e-282b543f-38400-1612d33b1253e with topic_id: 22291119 is empty!
2018-05-02 14:44:12,812 - memory_profile6_log - INFO - uid: 1612f6ee55f203-07b8a767918a63-74217e40-55188-1612f6ee56017c with topic_id: 22291119 is empty!
2018-05-02 14:44:14,234 - memory_profile6_log - INFO - uid: 161f78f539f14-0596c320f83202-59686c32-38400-161f78f53a116 with topic_id: 22291119 is empty!
2018-05-02 14:44:16,390 - memory_profile6_log - INFO - uid: 1612d27460b85-01fc92dbe-6410f06-7e900-1612d27460d60 with topic_id: 22291119 is empty!
2018-05-02 14:44:18,447 - memory_profile6_log - INFO - uid: 1617b2ac6ec4c0-03b6b0bf117ba2-2f5f376a-3d10d-1617b2ac6edabd with topic_id: 22291119 is empty!
2018-05-02 14:44:20,082 - memory_profile6_log - INFO - uid: 16263873a4039-0e60668e09d15b-5969683c-38400-16263873a4255 with topic_id: 22291119 is empty!
2018-05-02 14:44:21,631 - memory_profile6_log - INFO - uid: 16318e23ed5a3-032d1df52-5d547f1f-29aaa.aaaaaaaac-16318e23ed840 with topic_id: 22291119 is empty!
2018-05-02 14:44:23,155 - memory_profile6_log - INFO - uid: 1631a8d3d5161-0ef31e3f3-276a283e-34080-1631a8d3d53f with topic_id: 22291119 is empty!
2018-05-02 14:44:24,664 - memory_profile6_log - INFO - uid: 161f9ee8a3b293-023f0b1972b6ca-3e3d5f01-1aeaa0-161f9ee8a3cc56 with topic_id: 22291119 is empty!
2018-05-02 14:44:26,328 - memory_profile6_log - INFO - uid: 161319969e360-0e16696eed72dd-6621737c-38400-161319969e672 with topic_id: 22291119 is empty!
2018-05-02 14:44:27,651 - memory_profile6_log - INFO - uid: 1617e451b0a2d-003fa4aaf96a07-d110937-38400-1617e451b0b6 with topic_id: 22291119 is empty!
2018-05-02 14:44:29,299 - memory_profile6_log - INFO - uid: 16147a2c1f45e-0dd5990fa-33524007-38400-16147a2c1f83e with topic_id: 22291119 is empty!
2018-05-02 14:44:31,141 - memory_profile6_log - INFO - uid: 1612e1557370-093db6521-5d650603-38400-1612e15573bf with topic_id: 22291119 is empty!
2018-05-02 14:44:32,667 - memory_profile6_log - INFO - uid: 16281c693785d-023abce60e5c6f-4c6c661b-38400-16281c6937944 with topic_id: 22291119 is empty!
2018-05-02 14:44:34,203 - memory_profile6_log - INFO - uid: 1612d750d264d0-0dc45741d72248-10e0644-55188-1612d750d27486 with topic_id: 22291119 is empty!
2018-05-02 14:44:35,755 - memory_profile6_log - INFO - uid: 1613bbf293b35-02335df456c858-13c511b-38400-1613bbf293d28 with topic_id: 22291119 is empty!
2018-05-02 14:44:37,289 - memory_profile6_log - INFO - uid: 1614a60d7831a-054ededc8f4282-441b6b09-2c880-1614a60d78560 with topic_id: 22291119 is empty!
2018-05-02 14:44:39,437 - memory_profile6_log - INFO - uid: 161468993d92c8-07007226563ad4-5c7f6735-410a0-161468993da1c5 with topic_id: 22291119 is empty!
2018-05-02 14:44:41,382 - memory_profile6_log - INFO - uid: 161379cd04571-0abce53559cc8-f2a0f79-38400-161379cd04710 with topic_id: 22291119 is empty!
2018-05-02 14:44:43,424 - memory_profile6_log - INFO - uid: 16130185ba7a3-0d5664a8d7669b-227a4115-8d272-16130185baa9e with topic_id: 22291119 is empty!
2018-05-02 14:44:45,596 - memory_profile6_log - INFO - uid: 161d0f084e849-02c80335e75ce7-7d433d61-38400-161d0f084eaa3 with topic_id: 22291119 is empty!
2018-05-02 14:44:47,122 - memory_profile6_log - INFO - uid: 161334980fbb-0e6523eaab40ae-23c5019-38400-161334980ff70 with topic_id: 22291119 is empty!
2018-05-02 14:44:48,642 - memory_profile6_log - INFO - uid: 16183d115d83-09c176fa603caa-350a1241-38400-16183d115d92f with topic_id: 22291119 is empty!
2018-05-02 14:44:50,486 - memory_profile6_log - INFO - uid: 162a478d9e75e-0114949c94673c-4d084974-38400-162a478d9e917c with topic_id: 22291119 is empty!
2018-05-02 14:44:52,329 - memory_profile6_log - INFO - uid: 1612dc3d7746ec-08ad9c7529d4658-6c323735-4a640-1612dc3d7757e5 with topic_id: 22291119 is empty!
2018-05-02 14:44:54,799 - memory_profile6_log - INFO - uid: 1612df744f51a7-04225a81f26981-57271f75-38400-1612df744f7d9 with topic_id: 22291119 is empty!
2018-05-02 14:44:56,585 - memory_profile6_log - INFO - uid: 16240fa1ff421c-0d4f2f298bf50d-54237a32-43113-16240fa1ff6354 with topic_id: 22291119 is empty!
2018-05-02 14:44:57,875 - memory_profile6_log - INFO - uid: 1612fcec9615c-02aa79eca7e809-2c590766-2c600-1612fcec96392 with topic_id: 22291119 is empty!
2018-05-02 14:44:59,101 - memory_profile6_log - INFO - uid: 1612eae85227a-0d2513e8188d1b-73261116-38400-1612eae8523223 with topic_id: 22291119 is empty!
2018-05-02 14:45:00,536 - memory_profile6_log - INFO - uid: 16193e8bca7169-0a82b8c7643cd-43795646-38400-16193e8bca81a9 with topic_id: 22291119 is empty!
2018-05-02 14:45:02,163 - memory_profile6_log - INFO - uid: 162cec2b209d0-096b8f5ca2e96c-5a426a39-38400-162cec2b20a4b with topic_id: 22291119 is empty!
2018-05-02 14:45:03,769 - memory_profile6_log - INFO - uid: 162191930adc0-0e3f43804a657b8-17357b40-100200-162191930af133 with topic_id: 22291119 is empty!
2018-05-02 14:45:05,030 - memory_profile6_log - INFO - uid: 1612d25034b29-0700925cd16c27-7c277761-38400-1612d25034d55 with topic_id: 22291119 is empty!
2018-05-02 14:45:06,423 - memory_profile6_log - INFO - uid: 1612dab29f0ba-02beec302a7857-38636771-38400-1612dab29f181 with topic_id: 22291119 is empty!
2018-05-02 14:45:08,420 - memory_profile6_log - INFO - uid: 162a01dc956135-01c373e99fbf71-2a2a5133-38400-162a01dc95748 with topic_id: 22291119 is empty!
2018-05-02 14:45:10,155 - memory_profile6_log - INFO - uid: 1612d507207221-0112015ce33a2e-39530956-c0000-1612d507209295 with topic_id: 22291119 is empty!
2018-05-02 14:45:11,611 - memory_profile6_log - INFO - uid: 1612d19e916e1-0271ae3d234675-a29723c-38400-1612d19e918a0 with topic_id: 22291119 is empty!
2018-05-02 14:45:13,229 - memory_profile6_log - INFO - uid: 162e9b38c1c6f-0b0108db-6849147b-2c880-162e9b38c471 with topic_id: 22291119 is empty!
2018-05-02 14:45:15,065 - memory_profile6_log - INFO - uid: 161381c9a1b6b-06ae10f1f34d78-72271610-38400-161381c9a1c57 with topic_id: 22291119 is empty!
2018-05-02 14:45:16,602 - memory_profile6_log - INFO - uid: 1617414a7d81-08731b07fdc70a-3a626075-32000-1617414a7dcb8 with topic_id: 22291119 is empty!
2018-05-02 14:45:18,450 - memory_profile6_log - INFO - uid: 1612e6c17c85e-0ad97991821f9e-54b545d-29b80-1612e6c17cb87 with topic_id: 22291119 is empty!
2018-05-02 14:45:19,976 - memory_profile6_log - INFO - uid: 1612d16b05772-082d52fb228ffe-e0744-38400-1612d16b0896a with topic_id: 22291119 is empty!
2018-05-02 14:45:21,187 - memory_profile6_log - INFO - uid: 1612d33c46bc6-06092574a6355d-56412c73-c0000-1612d33c46c72 with topic_id: 22291119 is empty!
2018-05-02 14:45:22,448 - memory_profile6_log - INFO - uid: 162a021d12655-093a7565c61c11-6a0c2c50-8d272-162a021d128f5 with topic_id: 22291119 is empty!
2018-05-02 15:51:43,773 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 15:51:43,822 - memory_profile6_log - INFO - date_generated: 
2018-05-02 15:51:43,822 - memory_profile6_log - INFO -  
2018-05-02 15:51:43,822 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 15, 51, 43, 815000)]
2018-05-02 15:51:43,823 - memory_profile6_log - INFO - 

2018-05-02 15:51:43,825 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 15:51:43,825 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 15:51:43,826 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 15:51:44,237 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 15:51:44,247 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 15:52:15,812 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 15:52:15,821 - memory_profile6_log - INFO - date_generated: 
2018-05-02 15:52:15,822 - memory_profile6_log - INFO -  
2018-05-02 15:52:15,822 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 15, 52, 15, 814000)]
2018-05-02 15:52:15,823 - memory_profile6_log - INFO - 

2018-05-02 15:52:15,825 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 15:52:15,825 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 15:52:15,826 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 15:52:16,226 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 15:52:16,236 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 15:55:35,395 - memory_profile6_log - INFO - size of df: 82.28 MB
2018-05-02 15:55:35,398 - memory_profile6_log - INFO - getting total: 324874 training data(genuine interest) for date: 2018-05-01
2018-05-02 15:55:35,447 - memory_profile6_log - INFO - loading history data from datastore...
2018-05-02 15:55:35,448 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-05-02 15:55:35,450 - memory_profile6_log - INFO - Appending history data...
2018-05-02 15:55:35,450 - memory_profile6_log - INFO - processing batch-0
2018-05-02 15:55:35,450 - memory_profile6_log - INFO - creating list history data...
2018-05-02 15:55:35,549 - memory_profile6_log - INFO - call history data...
2018-05-02 15:57:31,305 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 15:57:32,566 - memory_profile6_log - INFO - processing batch-1
2018-05-02 15:57:32,568 - memory_profile6_log - INFO - creating list history data...
2018-05-02 15:57:32,630 - memory_profile6_log - INFO - call history data...
2018-05-02 16:00:07,211 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 16:00:08,256 - memory_profile6_log - INFO - processing batch-2
2018-05-02 16:00:08,257 - memory_profile6_log - INFO - creating list history data...
2018-05-02 16:00:08,318 - memory_profile6_log - INFO - call history data...
2018-05-02 16:02:04,634 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 16:02:05,532 - memory_profile6_log - INFO - processing batch-3
2018-05-02 16:02:05,532 - memory_profile6_log - INFO - creating list history data...
2018-05-02 16:02:05,598 - memory_profile6_log - INFO - call history data...
2018-05-02 16:03:41,497 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 16:03:42,234 - memory_profile6_log - INFO - processing batch-4
2018-05-02 16:03:42,236 - memory_profile6_log - INFO - creating list history data...
2018-05-02 16:03:42,295 - memory_profile6_log - INFO - call history data...
2018-05-02 16:05:23,335 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 16:05:23,796 - memory_profile6_log - INFO - Appending training data...
2018-05-02 16:05:23,798 - memory_profile6_log - INFO - len datalist: 1
2018-05-02 16:05:23,799 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-05-02 16:05:23,812 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 16:05:23,813 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 16:05:23,815 - memory_profile6_log - INFO - ================================================

2018-05-02 16:05:23,815 - memory_profile6_log - INFO -    356     86.7 MiB     86.7 MiB   @profile

2018-05-02 16:05:23,818 - memory_profile6_log - INFO -    357                             def BQPreprocess(cpu, date_generated, client, query_fit, loadfrom="datastore"):

2018-05-02 16:05:23,818 - memory_profile6_log - INFO -    358     86.7 MiB      0.0 MiB       bq_client = client

2018-05-02 16:05:23,819 - memory_profile6_log - INFO -    359     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 16:05:23,819 - memory_profile6_log - INFO -    360                             

2018-05-02 16:05:23,821 - memory_profile6_log - INFO -    361     86.7 MiB      0.0 MiB       datalist = []

2018-05-02 16:05:23,822 - memory_profile6_log - INFO -    362     86.7 MiB      0.0 MiB       datalist_hist = []

2018-05-02 16:05:23,822 - memory_profile6_log - INFO -    363                             

2018-05-02 16:05:23,822 - memory_profile6_log - INFO -    364     86.7 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-05-02 16:05:23,823 - memory_profile6_log - INFO -    365    402.4 MiB      0.0 MiB       for ndate in date_generated:

2018-05-02 16:05:23,823 - memory_profile6_log - INFO -    366    365.4 MiB    278.7 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-05-02 16:05:23,825 - memory_profile6_log - INFO -    367    365.4 MiB      0.0 MiB           if tframe is not None:

2018-05-02 16:05:23,825 - memory_profile6_log - INFO -    368    365.4 MiB      0.0 MiB               if not tframe.empty:

2018-05-02 16:05:23,828 - memory_profile6_log - INFO -    369    365.4 MiB      0.0 MiB                   if loadfrom.strip().lower() == 'datastore':

2018-05-02 16:05:23,831 - memory_profile6_log - INFO -    370    376.0 MiB     10.6 MiB                       X_split = np.array_split(tframe, 5)

2018-05-02 16:05:23,832 - memory_profile6_log - INFO -    371    376.0 MiB      0.0 MiB                       logger.info("loading history data from datastore...")

2018-05-02 16:05:23,832 - memory_profile6_log - INFO -    372    376.0 MiB      0.0 MiB                       logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 16:05:23,832 - memory_profile6_log - INFO -    373    376.0 MiB      0.0 MiB                       logger.info("Appending history data...")

2018-05-02 16:05:23,834 - memory_profile6_log - INFO -    374    402.4 MiB     -4.9 MiB                       for ix in range(len(X_split)):

2018-05-02 16:05:23,835 - memory_profile6_log - INFO -    375                                                     # ~ loading history

2018-05-02 16:05:23,836 - memory_profile6_log - INFO -    376                                                     """

2018-05-02 16:05:23,838 - memory_profile6_log - INFO -    377                                                         disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-05-02 16:05:23,841 - memory_profile6_log - INFO -    378                                                     """

2018-05-02 16:05:23,842 - memory_profile6_log - INFO -    379    402.3 MiB     -4.9 MiB                           logger.info("processing batch-%d", ix)

2018-05-02 16:05:23,842 - memory_profile6_log - INFO -    380                                                     # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-05-02 16:05:23,844 - memory_profile6_log - INFO -    381    402.3 MiB     -4.9 MiB                           logger.info("creating list history data...")

2018-05-02 16:05:23,845 - memory_profile6_log - INFO -    382                                                     #lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-05-02 16:05:23,845 - memory_profile6_log - INFO -    383    402.3 MiB      1.7 MiB                           lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 16:05:23,846 - memory_profile6_log - INFO -    384                             

2018-05-02 16:05:23,848 - memory_profile6_log - INFO -    385    402.3 MiB     -3.8 MiB                           logger.info("call history data...")

2018-05-02 16:05:23,848 - memory_profile6_log - INFO -    386    420.5 MiB     76.1 MiB                           h_frame = mh.loadDSHistory(lhistory)

2018-05-02 16:05:23,848 - memory_profile6_log - INFO -    387                             

2018-05-02 16:05:23,852 - memory_profile6_log - INFO -    388                                                     # me = os.getpid()

2018-05-02 16:05:23,854 - memory_profile6_log - INFO -    389                                                     # kill_proc_tree(me)

2018-05-02 16:05:23,855 - memory_profile6_log - INFO -    390                             

2018-05-02 16:05:23,855 - memory_profile6_log - INFO -    391    420.5 MiB    -66.7 MiB                           logger.info("done collecting history data, appending now...")

2018-05-02 16:05:23,857 - memory_profile6_log - INFO -    392    421.0 MiB -10356.9 MiB                           for m in h_frame:

2018-05-02 16:05:23,858 - memory_profile6_log - INFO -    393    421.0 MiB -10291.0 MiB                               if m is not None:

2018-05-02 16:05:23,858 - memory_profile6_log - INFO -    394    421.0 MiB -10288.7 MiB                                   if len(m) > 0:

2018-05-02 16:05:23,858 - memory_profile6_log - INFO -    395    421.0 MiB  -8374.5 MiB                                       datalist_hist.append(pd.DataFrame(m))

2018-05-02 16:05:23,859 - memory_profile6_log - INFO -    396    402.4 MiB   -128.3 MiB                           del h_frame

2018-05-02 16:05:23,861 - memory_profile6_log - INFO -    397    402.4 MiB     -6.5 MiB                           del lhistory

2018-05-02 16:05:23,861 - memory_profile6_log - INFO -    398                             

2018-05-02 16:05:23,865 - memory_profile6_log - INFO -    399    402.4 MiB      0.0 MiB                       logger.info("Appending training data...")

2018-05-02 16:05:23,867 - memory_profile6_log - INFO -    400    402.4 MiB      0.0 MiB                       datalist.append(tframe)

2018-05-02 16:05:23,868 - memory_profile6_log - INFO -    401                                             elif loadfrom.strip().lower() == 'elastic':

2018-05-02 16:05:23,868 - memory_profile6_log - INFO -    402                                                 X_split = np.array_split(tframe, 5)

2018-05-02 16:05:23,868 - memory_profile6_log - INFO -    403                                                 logger.info("loading history data from elastic...")

2018-05-02 16:05:23,869 - memory_profile6_log - INFO -    404                                                 logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 16:05:23,871 - memory_profile6_log - INFO -    405                                                 logger.info("Appending history data...")

2018-05-02 16:05:23,871 - memory_profile6_log - INFO -    406                                                 for ix in range(len(X_split)):

2018-05-02 16:05:23,872 - memory_profile6_log - INFO -    407                                                     for framedata in X_split[ix].itertuples():

2018-05-02 16:05:23,877 - memory_profile6_log - INFO -    408                                                         inside_data = mh.loadESHistory(framedata.user_id, framedata.topic_id ,

2018-05-02 16:05:23,878 - memory_profile6_log - INFO -    409                                                                                        esindex_name='transform_hist_index',

2018-05-02 16:05:23,878 - memory_profile6_log - INFO -    410                                                                                        estype_name='transform_hist_type')

2018-05-02 16:05:23,878 - memory_profile6_log - INFO -    411                             

2018-05-02 16:05:23,880 - memory_profile6_log - INFO -    412                                                         if not inside_data.empty:

2018-05-02 16:05:23,881 - memory_profile6_log - INFO -    413                                                             datalist_hist.append(inside_data)

2018-05-02 16:05:23,881 - memory_profile6_log - INFO -    414                                                             del inside_data

2018-05-02 16:05:23,881 - memory_profile6_log - INFO -    415                                                         else:

2018-05-02 16:05:23,882 - memory_profile6_log - INFO -    416                                                             logger.info("uid: %s with topic_id: %s is empty!" % (framedata.user_id, framedata.topic_id))

2018-05-02 16:05:23,884 - memory_profile6_log - INFO -    417                                             else:

2018-05-02 16:05:23,887 - memory_profile6_log - INFO -    418                                                 logger.info("Unknows source is selected !")

2018-05-02 16:05:23,888 - memory_profile6_log - INFO -    419                                                 break

2018-05-02 16:05:23,888 - memory_profile6_log - INFO -    420                                     else: 

2018-05-02 16:05:23,890 - memory_profile6_log - INFO -    421                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-05-02 16:05:23,891 - memory_profile6_log - INFO -    422    402.4 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-05-02 16:05:23,891 - memory_profile6_log - INFO -    423    402.4 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-05-02 16:05:23,892 - memory_profile6_log - INFO -    424                             

2018-05-02 16:05:23,892 - memory_profile6_log - INFO -    425    402.4 MiB      0.0 MiB       return datalist, datalist_hist

2018-05-02 16:05:23,894 - memory_profile6_log - INFO - 


2018-05-02 16:05:24,884 - memory_profile6_log - INFO - size of big_frame_hist: 27.00 MB
2018-05-02 16:05:24,979 - memory_profile6_log - INFO - size of big_frame: 82.28 MB
2018-05-02 16:05:25,002 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-05-02 16:32:48,507 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 16:32:48,510 - memory_profile6_log - INFO - date_generated: 
2018-05-02 16:32:48,510 - memory_profile6_log - INFO -  
2018-05-02 16:32:48,512 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 16, 32, 48, 509000)]
2018-05-02 16:32:48,512 - memory_profile6_log - INFO - 

2018-05-02 16:32:48,513 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 16:32:48,513 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 16:32:48,513 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 16:32:48,654 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 16:32:48,657 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 16:33:23,565 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 16:33:23,568 - memory_profile6_log - INFO - date_generated: 
2018-05-02 16:33:23,569 - memory_profile6_log - INFO -  
2018-05-02 16:33:23,569 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 16, 33, 23, 566000)]
2018-05-02 16:33:23,569 - memory_profile6_log - INFO - 

2018-05-02 16:33:23,569 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 16:33:23,569 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 16:33:23,571 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 16:33:23,733 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 16:33:23,737 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 16:34:12,016 - memory_profile6_log - INFO - size of df: 38.00 MB
2018-05-02 16:34:12,019 - memory_profile6_log - INFO - getting total: 150000 training data(genuine interest) for date: 2018-05-01
2018-05-02 16:34:12,049 - memory_profile6_log - INFO - loading history data from datastore...
2018-05-02 16:34:12,051 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-05-02 16:34:12,052 - memory_profile6_log - INFO - Appending history data...
2018-05-02 16:34:12,052 - memory_profile6_log - INFO - processing batch-0
2018-05-02 16:34:12,055 - memory_profile6_log - INFO - creating list history data...
2018-05-02 16:34:12,094 - memory_profile6_log - INFO - call history data...
2018-05-02 16:34:47,130 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 16:34:47,881 - memory_profile6_log - INFO - processing batch-1
2018-05-02 16:34:47,881 - memory_profile6_log - INFO - creating list history data...
2018-05-02 16:34:47,890 - memory_profile6_log - INFO - call history data...
2018-05-02 16:35:21,786 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 16:35:22,453 - memory_profile6_log - INFO - processing batch-2
2018-05-02 16:35:22,453 - memory_profile6_log - INFO - creating list history data...
2018-05-02 16:35:22,460 - memory_profile6_log - INFO - call history data...
2018-05-02 16:35:57,760 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 16:35:58,151 - memory_profile6_log - INFO - processing batch-3
2018-05-02 16:35:58,154 - memory_profile6_log - INFO - creating list history data...
2018-05-02 16:35:58,161 - memory_profile6_log - INFO - call history data...
2018-05-02 16:36:32,167 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 16:36:32,582 - memory_profile6_log - INFO - processing batch-4
2018-05-02 16:36:32,584 - memory_profile6_log - INFO - creating list history data...
2018-05-02 16:36:32,598 - memory_profile6_log - INFO - call history data...
2018-05-02 16:37:07,676 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 16:37:07,897 - memory_profile6_log - INFO - Appending training data...
2018-05-02 16:37:07,898 - memory_profile6_log - INFO - len datalist: 1
2018-05-02 16:37:07,901 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-05-02 16:37:07,901 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 16:37:07,903 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 16:37:07,903 - memory_profile6_log - INFO - ================================================

2018-05-02 16:37:07,904 - memory_profile6_log - INFO -    358     86.6 MiB     86.6 MiB   @profile

2018-05-02 16:37:07,904 - memory_profile6_log - INFO -    359                             def BQPreprocess(cpu, date_generated, client, query_fit, loadfrom="datastore"):

2018-05-02 16:37:07,905 - memory_profile6_log - INFO -    360     86.6 MiB      0.0 MiB       bq_client = client

2018-05-02 16:37:07,905 - memory_profile6_log - INFO -    361     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 16:37:07,908 - memory_profile6_log - INFO -    362                             

2018-05-02 16:37:07,911 - memory_profile6_log - INFO -    363     86.6 MiB      0.0 MiB       datalist = []

2018-05-02 16:37:07,911 - memory_profile6_log - INFO -    364     86.6 MiB      0.0 MiB       datalist_hist = []

2018-05-02 16:37:07,913 - memory_profile6_log - INFO -    365                             

2018-05-02 16:37:07,914 - memory_profile6_log - INFO -    366     86.6 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-05-02 16:37:07,914 - memory_profile6_log - INFO -    367    318.0 MiB      0.0 MiB       for ndate in date_generated:

2018-05-02 16:37:07,915 - memory_profile6_log - INFO -    368    311.0 MiB    224.3 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-05-02 16:37:07,917 - memory_profile6_log - INFO -    369    311.0 MiB      0.0 MiB           if tframe is not None:

2018-05-02 16:37:07,917 - memory_profile6_log - INFO -    370    311.0 MiB      0.0 MiB               if not tframe.empty:

2018-05-02 16:37:07,921 - memory_profile6_log - INFO -    371    311.0 MiB      0.0 MiB                   if loadfrom.strip().lower() == 'datastore':

2018-05-02 16:37:07,921 - memory_profile6_log - INFO -    372    315.8 MiB      4.8 MiB                       X_split = np.array_split(tframe, 5)

2018-05-02 16:37:07,923 - memory_profile6_log - INFO -    373    315.8 MiB      0.0 MiB                       logger.info("loading history data from datastore...")

2018-05-02 16:37:07,924 - memory_profile6_log - INFO -    374    315.8 MiB      0.0 MiB                       logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 16:37:07,926 - memory_profile6_log - INFO -    375    315.8 MiB      0.0 MiB                       logger.info("Appending history data...")

2018-05-02 16:37:07,927 - memory_profile6_log - INFO -    376    318.0 MiB      0.0 MiB                       for ix in range(len(X_split)):

2018-05-02 16:37:07,927 - memory_profile6_log - INFO -    377                                                     # ~ loading history

2018-05-02 16:37:07,928 - memory_profile6_log - INFO -    378                                                     """

2018-05-02 16:37:07,931 - memory_profile6_log - INFO -    379                                                         disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-05-02 16:37:07,933 - memory_profile6_log - INFO -    380                                                     """

2018-05-02 16:37:07,934 - memory_profile6_log - INFO -    381    318.0 MiB      0.0 MiB                           logger.info("processing batch-%d", ix)

2018-05-02 16:37:07,934 - memory_profile6_log - INFO -    382                                                     # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-05-02 16:37:07,936 - memory_profile6_log - INFO -    383    318.0 MiB      0.0 MiB                           logger.info("creating list history data...")

2018-05-02 16:37:07,936 - memory_profile6_log - INFO -    384    318.0 MiB      0.6 MiB                           lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-05-02 16:37:07,937 - memory_profile6_log - INFO -    385                                                     #lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 16:37:07,937 - memory_profile6_log - INFO -    386                             

2018-05-02 16:37:07,937 - memory_profile6_log - INFO -    387    318.0 MiB      0.0 MiB                           logger.info("call history data...")

2018-05-02 16:37:07,938 - memory_profile6_log - INFO -    388    318.0 MiB      1.4 MiB                           h_frame = mh.loadDSHistory(lhistory)

2018-05-02 16:37:07,940 - memory_profile6_log - INFO -    389                             

2018-05-02 16:37:07,940 - memory_profile6_log - INFO -    390                                                     # me = os.getpid()

2018-05-02 16:37:07,944 - memory_profile6_log - INFO -    391                                                     # kill_proc_tree(me)

2018-05-02 16:37:07,944 - memory_profile6_log - INFO -    392                             

2018-05-02 16:37:07,946 - memory_profile6_log - INFO -    393    318.0 MiB      0.0 MiB                           logger.info("done collecting history data, appending now...")

2018-05-02 16:37:07,946 - memory_profile6_log - INFO -    394    318.0 MiB      0.0 MiB                           for m in h_frame:

2018-05-02 16:37:07,947 - memory_profile6_log - INFO -    395    318.0 MiB      0.0 MiB                               if m is not None:

2018-05-02 16:37:07,947 - memory_profile6_log - INFO -    396    318.0 MiB      0.0 MiB                                   if len(m) > 0:

2018-05-02 16:37:07,947 - memory_profile6_log - INFO -    397    318.0 MiB      0.2 MiB                                       datalist_hist.append(pd.DataFrame(m))

2018-05-02 16:37:07,948 - memory_profile6_log - INFO -    398    318.0 MiB      0.0 MiB                           del h_frame

2018-05-02 16:37:07,950 - memory_profile6_log - INFO -    399    318.0 MiB      0.0 MiB                           del lhistory

2018-05-02 16:37:07,950 - memory_profile6_log - INFO -    400                             

2018-05-02 16:37:07,951 - memory_profile6_log - INFO -    401    318.0 MiB      0.0 MiB                       logger.info("Appending training data...")

2018-05-02 16:37:07,954 - memory_profile6_log - INFO -    402    318.0 MiB      0.0 MiB                       datalist.append(tframe)

2018-05-02 16:37:07,954 - memory_profile6_log - INFO -    403                                             elif loadfrom.strip().lower() == 'elastic':

2018-05-02 16:37:07,956 - memory_profile6_log - INFO -    404                                                 X_split = np.array_split(tframe, 5)

2018-05-02 16:37:07,957 - memory_profile6_log - INFO -    405                                                 logger.info("loading history data from elastic...")

2018-05-02 16:37:07,957 - memory_profile6_log - INFO -    406                                                 logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 16:37:07,957 - memory_profile6_log - INFO -    407                                                 logger.info("Appending history data...")

2018-05-02 16:37:07,959 - memory_profile6_log - INFO -    408                                                 for ix in range(len(X_split)):

2018-05-02 16:37:07,960 - memory_profile6_log - INFO -    409                                                     for framedata in X_split[ix].itertuples():

2018-05-02 16:37:07,960 - memory_profile6_log - INFO -    410                                                         inside_data = mh.loadESHistory(framedata.user_id, framedata.topic_id ,

2018-05-02 16:37:07,961 - memory_profile6_log - INFO -    411                                                                                        esindex_name='transform_hist_index',

2018-05-02 16:37:07,964 - memory_profile6_log - INFO -    412                                                                                        estype_name='transform_hist_type')

2018-05-02 16:37:07,966 - memory_profile6_log - INFO -    413                             

2018-05-02 16:37:07,967 - memory_profile6_log - INFO -    414                                                         if not inside_data.empty:

2018-05-02 16:37:07,967 - memory_profile6_log - INFO -    415                                                             datalist_hist.append(inside_data)

2018-05-02 16:37:07,969 - memory_profile6_log - INFO -    416                                                             del inside_data

2018-05-02 16:37:07,969 - memory_profile6_log - INFO -    417                                                         else:

2018-05-02 16:37:07,970 - memory_profile6_log - INFO -    418                                                             logger.info("uid: %s with topic_id: %s is empty!" % (framedata.user_id, framedata.topic_id))

2018-05-02 16:37:07,970 - memory_profile6_log - INFO -    419                                             else:

2018-05-02 16:37:07,971 - memory_profile6_log - INFO -    420                                                 logger.info("Unknows source is selected !")

2018-05-02 16:37:07,973 - memory_profile6_log - INFO -    421                                                 break

2018-05-02 16:37:07,973 - memory_profile6_log - INFO -    422                                     else: 

2018-05-02 16:37:07,976 - memory_profile6_log - INFO -    423                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-05-02 16:37:07,977 - memory_profile6_log - INFO -    424    318.0 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-05-02 16:37:07,977 - memory_profile6_log - INFO -    425    318.0 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-05-02 16:37:07,979 - memory_profile6_log - INFO -    426                             

2018-05-02 16:37:07,980 - memory_profile6_log - INFO -    427    318.0 MiB      0.0 MiB       return datalist, datalist_hist

2018-05-02 16:37:07,980 - memory_profile6_log - INFO - 


2018-05-02 16:37:08,714 - memory_profile6_log - INFO - size of big_frame_hist: 373.20 KB
2018-05-02 16:37:08,759 - memory_profile6_log - INFO - size of big_frame: 38.00 MB
2018-05-02 16:37:08,766 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-05-02 16:37:33,538 - memory_profile6_log - INFO - size of df: 24.55 MB
2018-05-02 16:37:33,539 - memory_profile6_log - INFO - getting total: 100000 training data(current date interest)
2018-05-02 16:37:33,573 - memory_profile6_log - INFO - size of current_frame: 25.31 MB
2018-05-02 16:37:33,575 - memory_profile6_log - INFO - loading time of: 250000 total genuine-current interest data ~ take 249.877s
2018-05-02 16:37:33,578 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 16:37:33,579 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 16:37:33,581 - memory_profile6_log - INFO - ================================================

2018-05-02 16:37:33,581 - memory_profile6_log - INFO -    429     86.5 MiB     86.5 MiB   @profile

2018-05-02 16:37:33,582 - memory_profile6_log - INFO -    430                             def preprocess(cpu, cd, query_fit, date_generated):

2018-05-02 16:37:33,582 - memory_profile6_log - INFO -    431     86.6 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-05-02 16:37:33,585 - memory_profile6_log - INFO -    432     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 16:37:33,588 - memory_profile6_log - INFO -    433                             

2018-05-02 16:37:33,589 - memory_profile6_log - INFO -    434                                 # ~~~ Begin collecting data ~~~

2018-05-02 16:37:33,591 - memory_profile6_log - INFO -    435     86.6 MiB      0.0 MiB       t0 = time.time()

2018-05-02 16:37:33,592 - memory_profile6_log - INFO -    436    318.0 MiB    231.4 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-05-02 16:37:33,592 - memory_profile6_log - INFO -    437    318.0 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-05-02 16:37:33,595 - memory_profile6_log - INFO -    438                                     logger.info("Training cannot be empty..")

2018-05-02 16:37:33,596 - memory_profile6_log - INFO -    439                                     return False

2018-05-02 16:37:33,598 - memory_profile6_log - INFO -    440    318.8 MiB      0.8 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-05-02 16:37:33,598 - memory_profile6_log - INFO -    441                                 # print "big_frame_hist:\n", big_frame_hist.head(20)

2018-05-02 16:37:33,598 - memory_profile6_log - INFO -    442    318.8 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-05-02 16:37:33,598 - memory_profile6_log - INFO -    443                             

2018-05-02 16:37:33,599 - memory_profile6_log - INFO -    444    322.4 MiB      3.6 MiB       big_frame = pd.concat(datalist)

2018-05-02 16:37:33,599 - memory_profile6_log - INFO -    445    322.4 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-05-02 16:37:33,599 - memory_profile6_log - INFO -    446    318.9 MiB     -3.4 MiB       del datalist

2018-05-02 16:37:33,601 - memory_profile6_log - INFO -    447                             

2018-05-02 16:37:33,601 - memory_profile6_log - INFO -    448    318.9 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 16:37:33,601 - memory_profile6_log - INFO -    449                             

2018-05-02 16:37:33,602 - memory_profile6_log - INFO -    450                                 # ~ get current news interest ~

2018-05-02 16:37:33,602 - memory_profile6_log - INFO -    451    318.9 MiB      0.0 MiB       if not cd:

2018-05-02 16:37:33,602 - memory_profile6_log - INFO -    452    318.9 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-05-02 16:37:33,604 - memory_profile6_log - INFO -    453    333.9 MiB     15.0 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-05-02 16:37:33,604 - memory_profile6_log - INFO -    454    333.9 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-05-02 16:37:33,604 - memory_profile6_log - INFO -    455                                 else:

2018-05-02 16:37:33,608 - memory_profile6_log - INFO -    456                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-05-02 16:37:33,609 - memory_profile6_log - INFO -    457                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date) limit 150000"

2018-05-02 16:37:33,611 - memory_profile6_log - INFO -    458                             

2018-05-02 16:37:33,611 - memory_profile6_log - INFO -    459                                     # safe handling of query parameter

2018-05-02 16:37:33,611 - memory_profile6_log - INFO -    460                                     query_params = [

2018-05-02 16:37:33,611 - memory_profile6_log - INFO -    461                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-05-02 16:37:33,612 - memory_profile6_log - INFO -    462                                     ]

2018-05-02 16:37:33,612 - memory_profile6_log - INFO -    463                             

2018-05-02 16:37:33,612 - memory_profile6_log - INFO -    464                                     job_config.query_parameters = query_params

2018-05-02 16:37:33,614 - memory_profile6_log - INFO -    465                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-05-02 16:37:33,614 - memory_profile6_log - INFO -    466                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-05-02 16:37:33,614 - memory_profile6_log - INFO -    467                             

2018-05-02 16:37:33,615 - memory_profile6_log - INFO -    468    334.0 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-05-02 16:37:33,619 - memory_profile6_log - INFO -    469    334.0 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-05-02 16:37:33,621 - memory_profile6_log - INFO -    470    333.2 MiB     -0.8 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-05-02 16:37:33,622 - memory_profile6_log - INFO -    471    333.2 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-05-02 16:37:33,624 - memory_profile6_log - INFO -    472    333.2 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 16:37:33,624 - memory_profile6_log - INFO -    473    333.2 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-05-02 16:37:33,625 - memory_profile6_log - INFO -    474                             

2018-05-02 16:37:33,625 - memory_profile6_log - INFO -    475    333.2 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-05-02 16:37:33,627 - memory_profile6_log - INFO - 


2018-05-02 16:37:33,631 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-05-02 16:37:33,651 - memory_profile6_log - INFO - train on: 150000 total genuine interest data(D(u, t))
2018-05-02 16:37:33,651 - memory_profile6_log - INFO - transform on: 100000 total current data(D(t))
2018-05-02 16:37:33,654 - memory_profile6_log - INFO - apply on: 1335 total history...)
2018-05-02 16:37:33,835 - memory_profile6_log - INFO - len of uniques_fit_hist:1335
2018-05-02 16:37:33,842 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:1240
2018-05-02 16:37:34,155 - memory_profile6_log - INFO - self.sum_all_nt before concate:

2018-05-02 16:37:34,230 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 16:37:34,232 - memory_profile6_log - INFO - 

2018-05-02 16:37:34,266 - memory_profile6_log - INFO - self.sum_all_nt after concate:

2018-05-02 16:37:34,289 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 16:37:34,290 - memory_profile6_log - INFO - 

2018-05-02 16:37:34,453 - memory_profile6_log - INFO - Len of model_fit: 150000
2018-05-02 16:37:34,454 - memory_profile6_log - INFO - Len of df_dut: 150000
2018-05-02 16:37:34,915 - memory_profile6_log - INFO - Len of fitted_models on main class: 150000
2018-05-02 16:37:34,917 - memory_profile6_log - INFO - 

2018-05-02 16:37:34,917 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-05-02 16:37:34,920 - memory_profile6_log - INFO - 

2018-05-02 16:37:34,921 - memory_profile6_log - INFO - fitted_model_hist:

2018-05-02 16:37:34,921 - memory_profile6_log - INFO - 

2018-05-02 16:37:34,931 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 16:37:34,933 - memory_profile6_log - INFO - 

2018-05-02 16:37:34,963 - memory_profile6_log - INFO - len of fitted models after concat: 151335
2018-05-02 16:37:34,963 - memory_profile6_log - INFO - 

2018-05-02 16:37:34,964 - memory_profile6_log - INFO - Recalculating fitted models...
2018-05-02 16:37:34,967 - memory_profile6_log - INFO - 

2018-05-02 16:37:35,072 - memory_profile6_log - INFO - fitted_models after cobine:

2018-05-02 16:37:35,072 - memory_profile6_log - INFO - 

2018-05-02 16:37:35,085 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 16:37:35,085 - memory_profile6_log - INFO - 

2018-05-02 16:37:35,088 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 150000
2018-05-02 16:37:35,089 - memory_profile6_log - INFO - 

2018-05-02 16:38:00,911 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-05-02 16:38:00,993 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, topic_id, pt_posterior_x_Nt, smoothed_pt_posterior, p0_cat_ci, sigma_Nt, p0_posterior, is_general, rank]
Index: []
2018-05-02 16:38:00,994 - memory_profile6_log - INFO - 

2018-05-02 16:38:00,996 - memory_profile6_log - INFO - Len of model_transform: 118957
2018-05-02 16:38:00,996 - memory_profile6_log - INFO - Len of df_dt: 100000
2018-05-02 16:38:00,996 - memory_profile6_log - INFO - Total train time: 27.282s
2018-05-02 16:38:00,997 - memory_profile6_log - INFO - memory left before cleaning: 90.700 percent memory...
2018-05-02 16:38:00,999 - memory_profile6_log - INFO - cleaning up some objects...
2018-05-02 16:38:01,000 - memory_profile6_log - INFO - deleting df_dut...
2018-05-02 16:38:01,000 - memory_profile6_log - INFO - deleting df_dt...
2018-05-02 16:38:01,002 - memory_profile6_log - INFO - deleting df_input...
2018-05-02 16:38:01,006 - memory_profile6_log - INFO - deleting df_input_X...
2018-05-02 16:38:01,006 - memory_profile6_log - INFO - deleting df_current...
2018-05-02 16:38:01,007 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-05-02 16:38:01,016 - memory_profile6_log - INFO - deleting model_fit...
2018-05-02 16:38:01,017 - memory_profile6_log - INFO - deleting result...
2018-05-02 16:38:01,038 - memory_profile6_log - INFO - list(model_transformsv) before rename: 
2018-05-02 16:38:01,039 - memory_profile6_log - INFO -  
2018-05-02 16:38:01,039 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'is_general', 'p0_posterior', 'rank', 'date']
2018-05-02 16:38:01,040 - memory_profile6_log - INFO - 

2018-05-02 16:38:01,042 - memory_profile6_log - INFO - 118957
2018-05-02 16:38:01,042 - memory_profile6_log - INFO - 

2018-05-02 16:38:01,059 - memory_profile6_log - INFO - list(model_transformsv) after rename: 
2018-05-02 16:38:01,059 - memory_profile6_log - INFO -  
2018-05-02 16:38:01,059 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'topic_is_general', 'interest_score', 'interest_rank', 'interest_score_created_at']
2018-05-02 16:38:01,061 - memory_profile6_log - INFO - 

2018-05-02 16:38:01,062 - memory_profile6_log - INFO - 118957
2018-05-02 16:38:01,062 - memory_profile6_log - INFO - 

2018-05-02 16:38:01,063 - memory_profile6_log - INFO - deleting model_transform...
2018-05-02 16:38:01,063 - memory_profile6_log - INFO - memory left after cleaning: 90.600 percent memory...
2018-05-02 16:38:01,065 - memory_profile6_log - INFO - Begin saving trained data...
2018-05-02 16:38:01,065 - memory_profile6_log - INFO - Using ElasticSearch as storage...
2018-05-02 16:38:01,085 - memory_profile6_log - INFO - Saving total data: 118957
2018-05-02 16:38:01,086 - memory_profile6_log - INFO - Len of X_split for batch save model_transformsv: 5
2018-05-02 16:38:01,088 - memory_profile6_log - INFO - processing batch-0
2018-05-02 16:41:26,160 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 16:41:26,161 - memory_profile6_log - INFO - date_generated: 
2018-05-02 16:41:26,161 - memory_profile6_log - INFO -  
2018-05-02 16:41:26,161 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 16, 41, 26, 160000)]
2018-05-02 16:41:26,161 - memory_profile6_log - INFO - 

2018-05-02 16:41:26,161 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 16:41:26,161 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 16:41:26,163 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 16:41:26,299 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 16:41:26,302 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 16:41:34,681 - memory_profile6_log - INFO - size of df: 2.53 MB
2018-05-02 16:41:34,683 - memory_profile6_log - INFO - getting total: 10000 training data(genuine interest) for date: 2018-05-01
2018-05-02 16:41:34,700 - memory_profile6_log - INFO - loading history data from datastore...
2018-05-02 16:41:34,701 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-05-02 16:41:34,703 - memory_profile6_log - INFO - Appending history data...
2018-05-02 16:41:34,704 - memory_profile6_log - INFO - processing batch-0
2018-05-02 16:41:34,706 - memory_profile6_log - INFO - creating list history data...
2018-05-02 16:41:34,773 - memory_profile6_log - INFO - call history data...
2018-05-02 16:42:10,061 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 16:42:10,719 - memory_profile6_log - INFO - processing batch-1
2018-05-02 16:42:10,720 - memory_profile6_log - INFO - creating list history data...
2018-05-02 16:42:10,727 - memory_profile6_log - INFO - call history data...
2018-05-02 16:42:47,174 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 16:42:47,796 - memory_profile6_log - INFO - processing batch-2
2018-05-02 16:42:47,796 - memory_profile6_log - INFO - creating list history data...
2018-05-02 16:42:47,805 - memory_profile6_log - INFO - call history data...
2018-05-02 16:43:22,167 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 16:43:22,551 - memory_profile6_log - INFO - processing batch-3
2018-05-02 16:43:22,552 - memory_profile6_log - INFO - creating list history data...
2018-05-02 16:43:22,559 - memory_profile6_log - INFO - call history data...
2018-05-02 16:43:57,105 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 16:43:57,526 - memory_profile6_log - INFO - processing batch-4
2018-05-02 16:43:57,528 - memory_profile6_log - INFO - creating list history data...
2018-05-02 16:43:57,536 - memory_profile6_log - INFO - call history data...
2018-05-02 16:44:32,441 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 16:44:32,713 - memory_profile6_log - INFO - Appending training data...
2018-05-02 16:44:32,714 - memory_profile6_log - INFO - len datalist: 1
2018-05-02 16:44:32,716 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-05-02 16:44:32,717 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 16:44:32,717 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 16:44:32,719 - memory_profile6_log - INFO - ================================================

2018-05-02 16:44:32,720 - memory_profile6_log - INFO -    359     87.1 MiB     87.1 MiB   @profile

2018-05-02 16:44:32,720 - memory_profile6_log - INFO -    360                             def BQPreprocess(cpu, date_generated, client, query_fit, loadfrom="datastore"):

2018-05-02 16:44:32,720 - memory_profile6_log - INFO -    361     87.1 MiB      0.0 MiB       bq_client = client

2018-05-02 16:44:32,721 - memory_profile6_log - INFO -    362     87.1 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 16:44:32,721 - memory_profile6_log - INFO -    363                             

2018-05-02 16:44:32,723 - memory_profile6_log - INFO -    364     87.1 MiB      0.0 MiB       datalist = []

2018-05-02 16:44:32,726 - memory_profile6_log - INFO -    365     87.1 MiB      0.0 MiB       datalist_hist = []

2018-05-02 16:44:32,726 - memory_profile6_log - INFO -    366                             

2018-05-02 16:44:32,727 - memory_profile6_log - INFO -    367     87.1 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-05-02 16:44:32,729 - memory_profile6_log - INFO -    368    117.0 MiB      0.0 MiB       for ndate in date_generated:

2018-05-02 16:44:32,729 - memory_profile6_log - INFO -    369    116.1 MiB     29.1 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-05-02 16:44:32,730 - memory_profile6_log - INFO -    370    116.1 MiB      0.0 MiB           if tframe is not None:

2018-05-02 16:44:32,730 - memory_profile6_log - INFO -    371    116.1 MiB      0.0 MiB               if not tframe.empty:

2018-05-02 16:44:32,730 - memory_profile6_log - INFO -    372    116.1 MiB      0.0 MiB                   if loadfrom.strip().lower() == 'datastore':

2018-05-02 16:44:32,732 - memory_profile6_log - INFO -    373    116.2 MiB      0.1 MiB                       X_split = np.array_split(tframe, 5)

2018-05-02 16:44:32,733 - memory_profile6_log - INFO -    374    116.2 MiB      0.0 MiB                       logger.info("loading history data from datastore...")

2018-05-02 16:44:32,733 - memory_profile6_log - INFO -    375    116.2 MiB      0.0 MiB                       logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 16:44:32,733 - memory_profile6_log - INFO -    376    116.2 MiB      0.0 MiB                       logger.info("Appending history data...")

2018-05-02 16:44:32,737 - memory_profile6_log - INFO -    377    117.0 MiB     -0.1 MiB                       for ix in range(len(X_split)):

2018-05-02 16:44:32,737 - memory_profile6_log - INFO -    378                                                     # ~ loading history

2018-05-02 16:44:32,739 - memory_profile6_log - INFO -    379                                                     """

2018-05-02 16:44:32,740 - memory_profile6_log - INFO -    380                                                         disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-05-02 16:44:32,740 - memory_profile6_log - INFO -    381                                                     """

2018-05-02 16:44:32,742 - memory_profile6_log - INFO -    382    117.0 MiB     -0.1 MiB                           logger.info("processing batch-%d", ix)

2018-05-02 16:44:32,743 - memory_profile6_log - INFO -    383                                                     # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-05-02 16:44:32,744 - memory_profile6_log - INFO -    384    117.0 MiB     -0.1 MiB                           logger.info("creating list history data...")

2018-05-02 16:44:32,744 - memory_profile6_log - INFO -    385    117.0 MiB      0.3 MiB                           lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-05-02 16:44:32,747 - memory_profile6_log - INFO -    386                                                     #lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 16:44:32,749 - memory_profile6_log - INFO -    387                             

2018-05-02 16:44:32,750 - memory_profile6_log - INFO -    388    117.0 MiB     -0.1 MiB                           logger.info("call history data...")

2018-05-02 16:44:32,750 - memory_profile6_log - INFO -    389    117.0 MiB      0.2 MiB                           h_frame = mh.loadDSHistory(lhistory)

2018-05-02 16:44:32,752 - memory_profile6_log - INFO -    390                             

2018-05-02 16:44:32,753 - memory_profile6_log - INFO -    391                                                     # me = os.getpid()

2018-05-02 16:44:32,753 - memory_profile6_log - INFO -    392                                                     # kill_proc_tree(me)

2018-05-02 16:44:32,753 - memory_profile6_log - INFO -    393                             

2018-05-02 16:44:32,755 - memory_profile6_log - INFO -    394    117.0 MiB      0.0 MiB                           logger.info("done collecting history data, appending now...")

2018-05-02 16:44:32,756 - memory_profile6_log - INFO -    395    117.0 MiB    -12.0 MiB                           for m in h_frame:

2018-05-02 16:44:32,756 - memory_profile6_log - INFO -    396    117.0 MiB    -12.1 MiB                               if m is not None:

2018-05-02 16:44:32,759 - memory_profile6_log - INFO -    397    117.0 MiB    -12.1 MiB                                   if len(m) > 0:

2018-05-02 16:44:32,760 - memory_profile6_log - INFO -    398    117.0 MiB     -9.4 MiB                                       datalist_hist.append(pd.DataFrame(m))

2018-05-02 16:44:32,762 - memory_profile6_log - INFO -    399    117.0 MiB     -0.1 MiB                           del h_frame

2018-05-02 16:44:32,762 - memory_profile6_log - INFO -    400    117.0 MiB     -0.1 MiB                           del lhistory

2018-05-02 16:44:32,763 - memory_profile6_log - INFO -    401                             

2018-05-02 16:44:32,763 - memory_profile6_log - INFO -    402    117.0 MiB      0.0 MiB                       logger.info("Appending training data...")

2018-05-02 16:44:32,765 - memory_profile6_log - INFO -    403    117.0 MiB      0.0 MiB                       datalist.append(tframe)

2018-05-02 16:44:32,765 - memory_profile6_log - INFO -    404                                             elif loadfrom.strip().lower() == 'elastic':

2018-05-02 16:44:32,766 - memory_profile6_log - INFO -    405                                                 X_split = np.array_split(tframe, 5)

2018-05-02 16:44:32,766 - memory_profile6_log - INFO -    406                                                 logger.info("loading history data from elastic...")

2018-05-02 16:44:32,769 - memory_profile6_log - INFO -    407                                                 logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 16:44:32,770 - memory_profile6_log - INFO -    408                                                 logger.info("Appending history data...")

2018-05-02 16:44:32,772 - memory_profile6_log - INFO -    409                                                 for ix in range(len(X_split)):

2018-05-02 16:44:32,773 - memory_profile6_log - INFO -    410                                                     for framedata in X_split[ix].itertuples():

2018-05-02 16:44:32,773 - memory_profile6_log - INFO -    411                                                         inside_data = mh.loadESHistory(framedata.user_id, framedata.topic_id ,

2018-05-02 16:44:32,773 - memory_profile6_log - INFO -    412                                                                                        esindex_name='fitted_hist_index',

2018-05-02 16:44:32,775 - memory_profile6_log - INFO -    413                                                                                        estype_name='fitted_hist_type')

2018-05-02 16:44:32,776 - memory_profile6_log - INFO -    414                             

2018-05-02 16:44:32,776 - memory_profile6_log - INFO -    415                                                         if not inside_data.empty:

2018-05-02 16:44:32,776 - memory_profile6_log - INFO -    416                                                             datalist_hist.append(inside_data)

2018-05-02 16:44:32,778 - memory_profile6_log - INFO -    417                                                             del inside_data

2018-05-02 16:44:32,778 - memory_profile6_log - INFO -    418                                                         else:

2018-05-02 16:44:32,782 - memory_profile6_log - INFO -    419                                                             logger.info("uid: %s with topic_id: %s is empty!" % (framedata.user_id, framedata.topic_id))

2018-05-02 16:44:32,782 - memory_profile6_log - INFO -    420                                             else:

2018-05-02 16:44:32,783 - memory_profile6_log - INFO -    421                                                 logger.info("Unknows source is selected !")

2018-05-02 16:44:32,785 - memory_profile6_log - INFO -    422                                                 break

2018-05-02 16:44:32,785 - memory_profile6_log - INFO -    423                                     else: 

2018-05-02 16:44:32,786 - memory_profile6_log - INFO -    424                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-05-02 16:44:32,786 - memory_profile6_log - INFO -    425    117.0 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-05-02 16:44:32,789 - memory_profile6_log - INFO -    426    117.0 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-05-02 16:44:32,789 - memory_profile6_log - INFO -    427                             

2018-05-02 16:44:32,792 - memory_profile6_log - INFO -    428    117.0 MiB      0.0 MiB       return datalist, datalist_hist

2018-05-02 16:44:32,792 - memory_profile6_log - INFO - 


2018-05-02 16:44:33,519 - memory_profile6_log - INFO - size of big_frame_hist: 479.88 KB
2018-05-02 16:44:33,530 - memory_profile6_log - INFO - size of big_frame: 2.53 MB
2018-05-02 16:44:33,533 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-05-02 16:44:56,408 - memory_profile6_log - INFO - size of df: 4.93 MB
2018-05-02 16:44:56,410 - memory_profile6_log - INFO - getting total: 20000 training data(current date interest)
2018-05-02 16:44:56,424 - memory_profile6_log - INFO - size of current_frame: 5.09 MB
2018-05-02 16:44:56,426 - memory_profile6_log - INFO - loading time of: 30000 total genuine-current interest data ~ take 210.160s
2018-05-02 16:44:56,430 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 16:44:56,430 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 16:44:56,430 - memory_profile6_log - INFO - ================================================

2018-05-02 16:44:56,434 - memory_profile6_log - INFO -    430     86.9 MiB     86.9 MiB   @profile

2018-05-02 16:44:56,436 - memory_profile6_log - INFO -    431                             def preprocess(cpu, cd, query_fit, date_generated):

2018-05-02 16:44:56,438 - memory_profile6_log - INFO -    432     87.1 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-05-02 16:44:56,438 - memory_profile6_log - INFO -    433     87.1 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 16:44:56,443 - memory_profile6_log - INFO -    434                             

2018-05-02 16:44:56,444 - memory_profile6_log - INFO -    435                                 # ~~~ Begin collecting data ~~~

2018-05-02 16:44:56,444 - memory_profile6_log - INFO -    436     87.1 MiB      0.0 MiB       t0 = time.time()

2018-05-02 16:44:56,444 - memory_profile6_log - INFO -    437    117.0 MiB     29.9 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-05-02 16:44:56,446 - memory_profile6_log - INFO -    438    117.0 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-05-02 16:44:56,448 - memory_profile6_log - INFO -    439                                     logger.info("Training cannot be empty..")

2018-05-02 16:44:56,450 - memory_profile6_log - INFO -    440                                     return False

2018-05-02 16:44:56,451 - memory_profile6_log - INFO -    441    117.0 MiB      0.1 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-05-02 16:44:56,451 - memory_profile6_log - INFO -    442                                 # print "big_frame_hist:\n", big_frame_hist.head(20)

2018-05-02 16:44:56,453 - memory_profile6_log - INFO -    443    117.0 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-05-02 16:44:56,453 - memory_profile6_log - INFO -    444                             

2018-05-02 16:44:56,453 - memory_profile6_log - INFO -    445    117.0 MiB      0.0 MiB       big_frame = pd.concat(datalist)

2018-05-02 16:44:56,453 - memory_profile6_log - INFO -    446    117.0 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-05-02 16:44:56,453 - memory_profile6_log - INFO -    447    117.0 MiB      0.0 MiB       del datalist

2018-05-02 16:44:56,454 - memory_profile6_log - INFO -    448                             

2018-05-02 16:44:56,454 - memory_profile6_log - INFO -    449    117.0 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 16:44:56,454 - memory_profile6_log - INFO -    450                             

2018-05-02 16:44:56,456 - memory_profile6_log - INFO -    451                                 # ~ get current news interest ~

2018-05-02 16:44:56,456 - memory_profile6_log - INFO -    452    117.0 MiB      0.0 MiB       if not cd:

2018-05-02 16:44:56,457 - memory_profile6_log - INFO -    453    117.0 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-05-02 16:44:56,457 - memory_profile6_log - INFO -    454    141.4 MiB     24.3 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-05-02 16:44:56,463 - memory_profile6_log - INFO -    455    141.4 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-05-02 16:44:56,463 - memory_profile6_log - INFO -    456                                 else:

2018-05-02 16:44:56,464 - memory_profile6_log - INFO -    457                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-05-02 16:44:56,466 - memory_profile6_log - INFO -    458                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date) limit 10000"

2018-05-02 16:44:56,467 - memory_profile6_log - INFO -    459                             

2018-05-02 16:44:56,467 - memory_profile6_log - INFO -    460                                     # safe handling of query parameter

2018-05-02 16:44:56,469 - memory_profile6_log - INFO -    461                                     query_params = [

2018-05-02 16:44:56,469 - memory_profile6_log - INFO -    462                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-05-02 16:44:56,469 - memory_profile6_log - INFO -    463                                     ]

2018-05-02 16:44:56,470 - memory_profile6_log - INFO -    464                             

2018-05-02 16:44:56,470 - memory_profile6_log - INFO -    465                                     job_config.query_parameters = query_params

2018-05-02 16:44:56,470 - memory_profile6_log - INFO -    466                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-05-02 16:44:56,470 - memory_profile6_log - INFO -    467                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-05-02 16:44:56,470 - memory_profile6_log - INFO -    468                             

2018-05-02 16:44:56,470 - memory_profile6_log - INFO -    469    141.4 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-05-02 16:44:56,474 - memory_profile6_log - INFO -    470    141.4 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-05-02 16:44:56,476 - memory_profile6_log - INFO -    471    141.4 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-05-02 16:44:56,477 - memory_profile6_log - INFO -    472    141.4 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-05-02 16:44:56,477 - memory_profile6_log - INFO -    473    141.4 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 16:44:56,479 - memory_profile6_log - INFO -    474    141.4 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-05-02 16:44:56,480 - memory_profile6_log - INFO -    475                             

2018-05-02 16:44:56,480 - memory_profile6_log - INFO -    476    141.4 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-05-02 16:44:56,480 - memory_profile6_log - INFO - 


2018-05-02 16:44:56,486 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-05-02 16:44:56,494 - memory_profile6_log - INFO - train on: 10000 total genuine interest data(D(u, t))
2018-05-02 16:44:56,496 - memory_profile6_log - INFO - transform on: 20000 total current data(D(t))
2018-05-02 16:44:56,497 - memory_profile6_log - INFO - apply on: 1717 total history...)
2018-05-02 16:44:56,546 - memory_profile6_log - INFO - len of uniques_fit_hist:1717
2018-05-02 16:44:56,552 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:858
2018-05-02 16:44:56,614 - memory_profile6_log - INFO - self.sum_all_nt before concate:

2018-05-02 16:44:56,634 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 16:44:56,634 - memory_profile6_log - INFO - 

2018-05-02 16:44:56,648 - memory_profile6_log - INFO - self.sum_all_nt after concate:

2018-05-02 16:44:56,661 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 16:44:56,661 - memory_profile6_log - INFO - 

2018-05-02 16:44:56,711 - memory_profile6_log - INFO - Len of model_fit: 10000
2018-05-02 16:44:56,713 - memory_profile6_log - INFO - Len of df_dut: 10000
2018-05-02 16:44:56,859 - memory_profile6_log - INFO - Len of fitted_models on main class: 10000
2018-05-02 16:44:56,861 - memory_profile6_log - INFO - 

2018-05-02 16:44:56,861 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-05-02 16:44:56,862 - memory_profile6_log - INFO - 

2018-05-02 16:44:56,865 - memory_profile6_log - INFO - fitted_model_hist:

2018-05-02 16:44:56,865 - memory_profile6_log - INFO - 

2018-05-02 16:44:56,875 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 16:44:56,875 - memory_profile6_log - INFO - 

2018-05-02 16:44:56,884 - memory_profile6_log - INFO - len of fitted models after concat: 11717
2018-05-02 16:44:56,885 - memory_profile6_log - INFO - 

2018-05-02 16:44:56,887 - memory_profile6_log - INFO - Recalculating fitted models...
2018-05-02 16:44:56,888 - memory_profile6_log - INFO - 

2018-05-02 16:44:56,914 - memory_profile6_log - INFO - fitted_models after cobine:

2018-05-02 16:44:56,914 - memory_profile6_log - INFO - 

2018-05-02 16:44:56,924 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 16:44:56,926 - memory_profile6_log - INFO - 

2018-05-02 16:44:56,927 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 10000
2018-05-02 16:44:56,927 - memory_profile6_log - INFO - 

2018-05-02 16:44:58,476 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-05-02 16:44:58,499 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, topic_id, pt_posterior_x_Nt, smoothed_pt_posterior, p0_cat_ci, sigma_Nt, p0_posterior, is_general, rank]
Index: []
2018-05-02 16:44:58,500 - memory_profile6_log - INFO - 

2018-05-02 16:44:58,500 - memory_profile6_log - INFO - Len of model_transform: 5057
2018-05-02 16:44:58,503 - memory_profile6_log - INFO - Len of df_dt: 20000
2018-05-02 16:44:58,503 - memory_profile6_log - INFO - Total train time: 1.991s
2018-05-02 16:44:58,505 - memory_profile6_log - INFO - memory left before cleaning: 84.900 percent memory...
2018-05-02 16:44:58,506 - memory_profile6_log - INFO - cleaning up some objects...
2018-05-02 16:44:58,506 - memory_profile6_log - INFO - deleting df_dut...
2018-05-02 16:44:58,507 - memory_profile6_log - INFO - deleting df_dt...
2018-05-02 16:44:58,510 - memory_profile6_log - INFO - deleting df_input...
2018-05-02 16:44:58,513 - memory_profile6_log - INFO - deleting df_input_X...
2018-05-02 16:44:58,513 - memory_profile6_log - INFO - deleting df_current...
2018-05-02 16:44:58,515 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-05-02 16:44:58,516 - memory_profile6_log - INFO - deleting model_fit...
2018-05-02 16:44:58,517 - memory_profile6_log - INFO - deleting result...
2018-05-02 16:44:58,526 - memory_profile6_log - INFO - list(model_transformsv) before rename: 
2018-05-02 16:44:58,528 - memory_profile6_log - INFO -  
2018-05-02 16:44:58,529 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'is_general', 'p0_posterior', 'rank', 'date']
2018-05-02 16:44:58,533 - memory_profile6_log - INFO - 

2018-05-02 16:44:58,535 - memory_profile6_log - INFO - 5057
2018-05-02 16:44:58,536 - memory_profile6_log - INFO - 

2018-05-02 16:44:58,539 - memory_profile6_log - INFO - list(model_transformsv) after rename: 
2018-05-02 16:44:58,542 - memory_profile6_log - INFO -  
2018-05-02 16:44:58,542 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'topic_is_general', 'interest_score', 'interest_rank', 'interest_score_created_at']
2018-05-02 16:44:58,543 - memory_profile6_log - INFO - 

2018-05-02 16:44:58,546 - memory_profile6_log - INFO - 5057
2018-05-02 16:44:58,546 - memory_profile6_log - INFO - 

2018-05-02 16:44:58,548 - memory_profile6_log - INFO - deleting model_transform...
2018-05-02 16:44:58,551 - memory_profile6_log - INFO - memory left after cleaning: 84.900 percent memory...
2018-05-02 16:44:58,552 - memory_profile6_log - INFO - Begin saving trained data...
2018-05-02 16:44:58,552 - memory_profile6_log - INFO - Using ElasticSearch as storage...
2018-05-02 16:44:58,552 - memory_profile6_log - INFO - Saving fitted_models as history to Elasticsearch...
2018-05-02 16:44:58,575 - memory_profile6_log - INFO - fitted_models_sigmant: 
2018-05-02 16:44:58,575 - memory_profile6_log - INFO -  
2018-05-02 16:44:58,788 - memory_profile6_log - INFO -                                                 user_id           topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt                                          uid_topid
0     1610d721660810-0ce20c8b312a34-14117f59-100200-...           27430823          22.153600              32.153600        NaN         1  1610d721660810-0ce20c8b312a34-14117f59-100200-...
1     161116e16f2ac7-0b1f1a1b0f9047-4323461-100200-1...           59048170       13846.000000           13856.000000   0.000378         3  161116e16f2ac7-0b1f1a1b0f9047-4323461-100200-1...
2     16112f87fa7b6f-0b57fa2e866932-4323461-1fa400-1...           27430823          55.231974              65.231974        NaN       603  16112f87fa7b6f-0b57fa2e866932-4323461-1fa400-1...
3     161285256c03f5-065f4919abd206-7c2d6751-1fa400-...         1037574949          32.502347              42.502347        NaN        64  161285256c03f5-065f4919abd206-7c2d6751-1fa400-...
4     161285256c03f5-065f4919abd206-7c2d6751-1fa400-...         1038512181          63.223744              73.223744   0.016302        64  161285256c03f5-065f4919abd206-7c2d6751-1fa400-...
5     161285256c03f5-065f4919abd206-7c2d6751-1fa400-...         1065124711         903.541559             913.541559   0.005724        64  161285256c03f5-065f4919abd206-7c2d6751-1fa400-...
6     161285256c03f5-065f4919abd206-7c2d6751-1fa400-...           22596701          17.075951              27.075951        NaN        64  161285256c03f5-065f4919abd206-7c2d6751-1fa400-...
7     161285256c03f5-065f4919abd206-7c2d6751-1fa400-...          273342551         162.894118             172.894118   0.004795        64  161285256c03f5-065f4919abd206-7c2d6751-1fa400-...
8     161285256c03f5-065f4919abd206-7c2d6751-1fa400-...  27431110790312332        1258.727273            1268.727273   0.027140        64  161285256c03f5-065f4919abd206-7c2d6751-1fa400-...
9     161285256c03f5-065f4919abd206-7c2d6751-1fa400-...  27431110790314226        3461.500000            3471.500000   0.000029        64  161285256c03f5-065f4919abd206-7c2d6751-1fa400-...
10    161285256c03f5-065f4919abd206-7c2d6751-1fa400-...  27431110790314240        2769.200000            2779.200000   0.000668        64  161285256c03f5-065f4919abd206-7c2d6751-1fa400-...
11    161285256c03f5-065f4919abd206-7c2d6751-1fa400-...  27431110790314246         814.470588             824.470588   0.005056        64  161285256c03f5-065f4919abd206-7c2d6751-1fa400-...
12    161285256c03f5-065f4919abd206-7c2d6751-1fa400-...           27435908          34.701754              44.701754   0.013454        64  161285256c03f5-065f4919abd206-7c2d6751-1fa400-...
13    161285256c03f5-065f4919abd206-7c2d6751-1fa400-...          339797100        2307.666667            2317.666667   0.000203        64  161285256c03f5-065f4919abd206-7c2d6751-1fa400-...
14    161285256c03f5-065f4919abd206-7c2d6751-1fa400-...           39825972          31.044843              41.044843        NaN        64  161285256c03f5-065f4919abd206-7c2d6751-1fa400-...
15    161285256c03f5-065f4919abd206-7c2d6751-1fa400-...           47732569        3461.500000            3471.500000   0.000145        64  161285256c03f5-065f4919abd206-7c2d6751-1fa400-...
16    1612cf2dca04e-04537d4ab97b66-3c660c68-38400-16...         1039929544        2769.200000            2779.200000   0.000814       482  1612cf2dca04e-04537d4ab97b66-3c660c68-38400-16...
17    1612cf2dca04e-04537d4ab97b66-3c660c68-38400-16...          107151123          74.042781              84.042781        NaN       482  1612cf2dca04e-04537d4ab97b66-3c660c68-38400-16...
18    1612cf2dca04e-04537d4ab97b66-3c660c68-38400-16...         1157057159          58.919149              68.919149   0.008398       482  1612cf2dca04e-04537d4ab97b66-3c660c68-38400-16...
19    1612cf2dca04e-04537d4ab97b66-3c660c68-38400-16...         1161375937          52.847328              62.847328   0.019004       482  1612cf2dca04e-04537d4ab97b66-3c660c68-38400-16...
20    1612cf2dca04e-04537d4ab97b66-3c660c68-38400-16...          136993925        1153.833333            1163.833333   0.000378       482  1612cf2dca04e-04537d4ab97b66-3c660c68-38400-16...
21    1612cf2dca04e-04537d4ab97b66-3c660c68-38400-16...          188400148         230.685258             240.685258        NaN       482  1612cf2dca04e-04537d4ab97b66-3c660c68-38400-16...
22    1612cf2dca04e-04537d4ab97b66-3c660c68-38400-16...          189391506        3043.783333            3053.783333   0.007700       482  1612cf2dca04e-04537d4ab97b66-3c660c68-38400-16...
23    1612cf2dca04e-04537d4ab97b66-3c660c68-38400-16...           22552186         408.382443             418.382443        NaN       482  1612cf2dca04e-04537d4ab97b66-3c660c68-38400-16...
24    1612cf2dca04e-04537d4ab97b66-3c660c68-38400-16...           22552349         229.874945             239.874945        NaN       482  1612cf2dca04e-04537d4ab97b66-3c660c68-38400-16...
25    1612cf2dca04e-04537d4ab97b66-3c660c68-38400-16...           22553321         419.289487             429.289487        NaN       482  1612cf2dca04e-04537d4ab97b66-3c660c68-38400-16...
26    1612cf2dca04e-04537d4ab97b66-3c660c68-38400-16...           27428266          90.250203             100.250203        NaN       482  1612cf2dca04e-04537d4ab97b66-3c660c68-38400-16...
27    1612cf2dca04e-04537d4ab97b66-3c660c68-38400-16...           27428824        1476.098642            1486.098642   0.008282       482  1612cf2dca04e-04537d4ab97b66-3c660c68-38400-16...
28    1612cf2dca04e-04537d4ab97b66-3c660c68-38400-16...  27431110790314222          72.492147              82.492147   0.015517       482  1612cf2dca04e-04537d4ab97b66-3c660c68-38400-16...
29    1612cf2dca04e-04537d4ab97b66-3c660c68-38400-16...  27431110790314280          73.648936              83.648936        NaN       482  1612cf2dca04e-04537d4ab97b66-3c660c68-38400-16...
...                                                 ...                ...                ...                    ...        ...       ...                                                ...
9970  1631bee758d74-020c607f8-1f415862-5020a.72f0539...          188400148          67.213592              77.213592        NaN        52  1631bee758d74-020c607f8-1f415862-5020a.72f0539...
9971  1631bee758d74-020c607f8-1f415862-5020a.72f0539...           22552349          33.045346              43.045346        NaN        52  1631bee758d74-020c607f8-1f415862-5020a.72f0539...
9972  1631bee758d74-020c607f8-1f415862-5020a.72f0539...           22596701          15.132240              25.132240        NaN        52  1631bee758d74-020c607f8-1f415862-5020a.72f0539...
9973  1631bee758d74-020c607f8-1f415862-5020a.72f0539...           22601470          27.472222              37.472222        NaN        52  1631bee758d74-020c607f8-1f415862-5020a.72f0539...
9974  1631bee758d74-020c607f8-1f415862-5020a.72f0539...           22661796          43.678233              53.678233   0.013221        52  1631bee758d74-020c607f8-1f415862-5020a.72f0539...
9975  1631bee758d74-020c607f8-1f415862-5020a.72f0539...           27428266          81.928994              91.928994        NaN        52  1631bee758d74-020c607f8-1f415862-5020a.72f0539...
9976  1631bee758d74-020c607f8-1f415862-5020a.72f0539...           27430823          44.307200              54.307200        NaN        52  1631bee758d74-020c607f8-1f415862-5020a.72f0539...
9977  1631bee758d74-020c607f8-1f415862-5020a.72f0539...           27431099          39.673352              49.673352        NaN        52  1631bee758d74-020c607f8-1f415862-5020a.72f0539...
9978  1631bee758d74-020c607f8-1f415862-5020a.72f0539...  27431110790314202         282.571429             292.571429   0.000872        52  1631bee758d74-020c607f8-1f415862-5020a.72f0539...
9979  1631bee758d74-020c607f8-1f415862-5020a.72f0539...  27431110790314285         188.380952             198.380952   0.032574        52  1631bee758d74-020c607f8-1f415862-5020a.72f0539...
9980  1631bee758d74-020c607f8-1f415862-5020a.72f0539...  27431110790314292         865.375000             875.375000   0.000174        52  1631bee758d74-020c607f8-1f415862-5020a.72f0539...
9981  1631bee758d74-020c607f8-1f415862-5020a.72f0539...  27431110790314298         865.375000             875.375000   0.002063        52  1631bee758d74-020c607f8-1f415862-5020a.72f0539...
9982  1631bee758d74-020c607f8-1f415862-5020a.72f0539...           27432949         195.014085             205.014085        NaN        52  1631bee758d74-020c607f8-1f415862-5020a.72f0539...
9983  1631bee758d74-020c607f8-1f415862-5020a.72f0539...           33020303         251.745455             261.745455        NaN        52  1631bee758d74-020c607f8-1f415862-5020a.72f0539...
9984  1631bee758d74-020c607f8-1f415862-5020a.72f0539...           33023784         629.363636             639.363636   0.001162        52  1631bee758d74-020c607f8-1f415862-5020a.72f0539...
9985  1631bee758d74-020c607f8-1f415862-5020a.72f0539...           36060446         144.229167             154.229167        NaN        52  1631bee758d74-020c607f8-1f415862-5020a.72f0539...
9986  1631bee758d74-020c607f8-1f415862-5020a.72f0539...          374466485        6923.000000            6933.000000   0.000203        52  1631bee758d74-020c607f8-1f415862-5020a.72f0539...
9987  1631bee758d74-020c607f8-1f415862-5020a.72f0539...           39320363        2769.200000            2779.200000   0.002673        52  1631bee758d74-020c607f8-1f415862-5020a.72f0539...
9988  1631bee758d74-020c607f8-1f415862-5020a.72f0539...           39519316         204.369004             214.369004        NaN        52  1631bee758d74-020c607f8-1f415862-5020a.72f0539...
9989  1631bee758d74-020c607f8-1f415862-5020a.72f0539...           39825972          62.089686              72.089686        NaN        52  1631bee758d74-020c607f8-1f415862-5020a.72f0539...
9990  1631bee758d74-020c607f8-1f415862-5020a.72f0539...           40711095         395.600000             405.600000   0.000930        52  1631bee758d74-020c607f8-1f415862-5020a.72f0539...
9991  1631bee758d74-020c607f8-1f415862-5020a.72f0539...           40723504         197.800000             207.800000        NaN        52  1631bee758d74-020c607f8-1f415862-5020a.72f0539...
9992  1631bee758d74-020c607f8-1f415862-5020a.72f0539...           41226371        2769.200000            2779.200000   0.000378        52  1631bee758d74-020c607f8-1f415862-5020a.72f0539...
9993  1631bee758d74-020c607f8-1f415862-5020a.72f0539...           68346322        6923.000000            6933.000000   0.002499        52  1631bee758d74-020c607f8-1f415862-5020a.72f0539...
9994  1631bee758d74-020c607f8-1f415862-5020a.72f0539...          913951142        1258.727273            1268.727273   0.012408        52  1631bee758d74-020c607f8-1f415862-5020a.72f0539...
9995  1631c569624b3-0f1b3197d-77087700-35160-1631c56...           22553543         142.323054             152.323054        NaN         7  1631c569624b3-0f1b3197d-77087700-35160-1631c56...
9996  1631c5d02609f-0e5259b4c-187f190a-38400-1631c5d...          107151123         222.128342             232.128342        NaN        12  1631c5d02609f-0e5259b4c-187f190a-38400-1631c5d...
9997  1631c5d02609f-0e5259b4c-187f190a-38400-1631c5d...           22553543         121.991189             131.991189        NaN        12  1631c5d02609f-0e5259b4c-187f190a-38400-1631c5d...
9998  1631c5d02609f-0e5259b4c-187f190a-38400-1631c5d...           39301645         374.216216             384.216216        NaN        12  1631c5d02609f-0e5259b4c-187f190a-38400-1631c5d...
9999  1631c5d3b5ff-0b3380e83-68384a08-4ddac.bed87379...           22553543         142.323054             152.323054        NaN         7  1631c5d3b5ff-0b3380e83-68384a08-4ddac.bed87379...

[10000 rows x 7 columns]
2018-05-02 16:44:58,805 - memory_profile6_log - INFO - 

2018-05-02 16:44:58,816 - memory_profile6_log - INFO - Saving total data: 10000
2018-05-02 16:44:58,818 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 5
2018-05-02 16:44:58,819 - memory_profile6_log - INFO - processing batch-0
2018-05-02 16:45:00,085 - memory_profile6_log - INFO - index does not exist, creating index
2018-05-02 16:45:00,086 - memory_profile6_log - INFO - 

2018-05-02 16:45:16,569 - memory_profile6_log - INFO - processing batch-1
2018-05-02 16:45:31,798 - memory_profile6_log - INFO - processing batch-2
2018-05-02 16:45:51,078 - memory_profile6_log - INFO - processing batch-3
2018-05-02 16:46:14,286 - memory_profile6_log - INFO - processing batch-4
2018-05-02 16:46:30,703 - memory_profile6_log - INFO - deleting BR...
2018-05-02 16:46:30,703 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-05-02 16:46:30,744 - memory_profile6_log - INFO - deleting fitted_models_sigmant...
2018-05-02 16:46:30,746 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 16:46:30,746 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 16:46:30,749 - memory_profile6_log - INFO - ================================================

2018-05-02 16:46:30,749 - memory_profile6_log - INFO -    113    141.4 MiB    141.4 MiB   @profile

2018-05-02 16:46:30,750 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-05-02 16:46:30,750 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-05-02 16:46:30,750 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-05-02 16:46:30,750 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-05-02 16:46:30,752 - memory_profile6_log - INFO -    118                                      saveto="datastore", fitted_models_hist=None):

2018-05-02 16:46:30,752 - memory_profile6_log - INFO -    119                                 """

2018-05-02 16:46:30,755 - memory_profile6_log - INFO -    120                                     Main Process

2018-05-02 16:46:30,755 - memory_profile6_log - INFO -    121                                 """

2018-05-02 16:46:30,756 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-05-02 16:46:30,756 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-05-02 16:46:30,756 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-05-02 16:46:30,756 - memory_profile6_log - INFO -    125    141.4 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-05-02 16:46:30,756 - memory_profile6_log - INFO -    126    141.4 MiB      0.0 MiB       df_dut = df_input.copy(deep=True)

2018-05-02 16:46:30,757 - memory_profile6_log - INFO -    127    141.4 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 16:46:30,757 - memory_profile6_log - INFO -    128                             

2018-05-02 16:46:30,759 - memory_profile6_log - INFO -    129                                 # D(t)

2018-05-02 16:46:30,759 - memory_profile6_log - INFO -    130    141.4 MiB      0.0 MiB       df_dt = df_current.copy(deep=True)

2018-05-02 16:46:30,759 - memory_profile6_log - INFO -    131    141.4 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 16:46:30,759 - memory_profile6_log - INFO -    132                             

2018-05-02 16:46:30,760 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-05-02 16:46:30,760 - memory_profile6_log - INFO -    134    141.4 MiB      0.0 MiB       t0 = time.time()

2018-05-02 16:46:30,760 - memory_profile6_log - INFO -    135    141.4 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-05-02 16:46:30,762 - memory_profile6_log - INFO -    136    141.4 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-05-02 16:46:30,763 - memory_profile6_log - INFO -    137    141.4 MiB      0.0 MiB       logger.info("apply on: %d total history...)", len(df_hist))

2018-05-02 16:46:30,763 - memory_profile6_log - INFO -    138                             

2018-05-02 16:46:30,763 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-05-02 16:46:30,767 - memory_profile6_log - INFO -    140    141.4 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-05-02 16:46:30,767 - memory_profile6_log - INFO -    141                             

2018-05-02 16:46:30,769 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-05-02 16:46:30,769 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-05-02 16:46:30,769 - memory_profile6_log - INFO -    144    141.7 MiB      0.3 MiB       NB = BR.processX(df_dut)

2018-05-02 16:46:30,770 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-05-02 16:46:30,770 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-05-02 16:46:30,772 - memory_profile6_log - INFO -    147    141.8 MiB      0.1 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-05-02 16:46:30,772 - memory_profile6_log - INFO -    148                                 """

2018-05-02 16:46:30,772 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-05-02 16:46:30,773 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-05-02 16:46:30,773 - memory_profile6_log - INFO -    151                                 """

2018-05-02 16:46:30,773 - memory_profile6_log - INFO -    152    141.8 MiB      0.0 MiB       fitby_sigmant = False

2018-05-02 16:46:30,775 - memory_profile6_log - INFO -    153    141.8 MiB      0.0 MiB       uniques_fit_hist = None

2018-05-02 16:46:30,775 - memory_profile6_log - INFO -    154    141.8 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-05-02 16:46:30,779 - memory_profile6_log - INFO -    155    141.8 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-05-02 16:46:30,779 - memory_profile6_log - INFO -    156    141.8 MiB      0.0 MiB                            'is_general']]

2018-05-02 16:46:30,779 - memory_profile6_log - INFO -    157                             

2018-05-02 16:46:30,780 - memory_profile6_log - INFO -    158                                 # get sigma_Nt from fitted_models_hist

2018-05-02 16:46:30,780 - memory_profile6_log - INFO -    159    141.9 MiB      0.0 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-05-02 16:46:30,780 - memory_profile6_log - INFO -    160    141.9 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-05-02 16:46:30,782 - memory_profile6_log - INFO -    161    141.9 MiB      0.0 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-05-02 16:46:30,782 - memory_profile6_log - INFO -    162    141.9 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-05-02 16:46:30,782 - memory_profile6_log - INFO -    163                             

2018-05-02 16:46:30,782 - memory_profile6_log - INFO -    164                                 # begin fit

2018-05-02 16:46:30,782 - memory_profile6_log - INFO -    165    141.9 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-05-02 16:46:30,782 - memory_profile6_log - INFO -    166    141.9 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-05-02 16:46:30,783 - memory_profile6_log - INFO -    167    142.0 MiB      0.1 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-05-02 16:46:30,783 - memory_profile6_log - INFO -    168    142.0 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-05-02 16:46:30,785 - memory_profile6_log - INFO -    169    142.0 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-05-02 16:46:30,785 - memory_profile6_log - INFO -    170                                 # print model_fit[['num_x', 'num_x', 'sigma_Nt', 'date_all_click']].loc[model_fit['user_id']=="1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157"].head(5)

2018-05-02 16:46:30,785 - memory_profile6_log - INFO -    171                             

2018-05-02 16:46:30,786 - memory_profile6_log - INFO -    172                                 # ~~ and Transform ~~

2018-05-02 16:46:30,786 - memory_profile6_log - INFO -    173                                 #   handling current news interest == current date

2018-05-02 16:46:30,786 - memory_profile6_log - INFO -    174    142.0 MiB      0.0 MiB       if df_dt.empty:

2018-05-02 16:46:30,786 - memory_profile6_log - INFO -    175                                     print "Cek your df_dt, cannot be emtpy!!"

2018-05-02 16:46:30,786 - memory_profile6_log - INFO -    176                                     return None

2018-05-02 16:46:30,786 - memory_profile6_log - INFO -    177    143.2 MiB      1.2 MiB       NB = BR.processX(df_dt)

2018-05-02 16:46:30,792 - memory_profile6_log - INFO -    178    143.2 MiB      0.0 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-05-02 16:46:30,792 - memory_profile6_log - INFO -    179                             

2018-05-02 16:46:30,792 - memory_profile6_log - INFO -    180    143.2 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-05-02 16:46:30,793 - memory_profile6_log - INFO -    181    143.2 MiB      0.0 MiB                            'num_x', 'num_y', 'is_general']]

2018-05-02 16:46:30,795 - memory_profile6_log - INFO -    182                             

2018-05-02 16:46:30,795 - memory_profile6_log - INFO -    183                                 # print "model_fit dtypes:\n", model_fit.dtypes

2018-05-02 16:46:30,796 - memory_profile6_log - INFO -    184                                 # print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-05-02 16:46:30,796 - memory_profile6_log - INFO -    185    143.2 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-05-02 16:46:30,796 - memory_profile6_log - INFO -    186    143.2 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-05-02 16:46:30,796 - memory_profile6_log - INFO -    187    143.2 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-05-02 16:46:30,796 - memory_profile6_log - INFO -    188    143.2 MiB      0.0 MiB                                                                                           "topic_id", "user_id"]],

2018-05-02 16:46:30,798 - memory_profile6_log - INFO -    189    146.1 MiB      2.9 MiB                                                     verbose=False)

2018-05-02 16:46:30,798 - memory_profile6_log - INFO -    190                             

2018-05-02 16:46:30,798 - memory_profile6_log - INFO -    191                                 # ~~~ filter is general and specific topic ~~~

2018-05-02 16:46:30,799 - memory_profile6_log - INFO -    192                                 # the idea is just we need to rerank every topic according

2018-05-02 16:46:30,799 - memory_profile6_log - INFO -    193                                 #    to user_id and and is_general by p0_posterior

2018-05-02 16:46:30,799 - memory_profile6_log - INFO -    194    146.1 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-05-02 16:46:30,799 - memory_profile6_log - INFO -    195    146.1 MiB      0.0 MiB                                     'is_general']].groupby(['topic_id',

2018-05-02 16:46:30,801 - memory_profile6_log - INFO -    196    146.1 MiB      0.0 MiB                                                             'is_general']

2018-05-02 16:46:30,806 - memory_profile6_log - INFO -    197                                                                                      ).size().to_frame().reset_index()

2018-05-02 16:46:30,806 - memory_profile6_log - INFO -    198                             

2018-05-02 16:46:30,806 - memory_profile6_log - INFO -    199                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-05-02 16:46:30,808 - memory_profile6_log - INFO -    200                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-05-02 16:46:30,808 - memory_profile6_log - INFO -    201    146.1 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-05-02 16:46:30,808 - memory_profile6_log - INFO -    202                             

2018-05-02 16:46:30,809 - memory_profile6_log - INFO -    203                                 # ~ start by provide rank for each topic type ~

2018-05-02 16:46:30,809 - memory_profile6_log - INFO -    204    146.1 MiB      0.0 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-05-02 16:46:30,809 - memory_profile6_log - INFO -    205    146.5 MiB      0.4 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-05-02 16:46:30,809 - memory_profile6_log - INFO -    206                             

2018-05-02 16:46:30,809 - memory_profile6_log - INFO -    207                                 # ~ set threshold to filter output

2018-05-02 16:46:30,811 - memory_profile6_log - INFO -    208    146.5 MiB      0.0 MiB       if threshold > 0:

2018-05-02 16:46:30,811 - memory_profile6_log - INFO -    209    146.5 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-05-02 16:46:30,811 - memory_profile6_log - INFO -    210    146.5 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-05-02 16:46:30,812 - memory_profile6_log - INFO -    211    146.5 MiB      0.0 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-05-02 16:46:30,812 - memory_profile6_log - INFO -    212                             

2018-05-02 16:46:30,812 - memory_profile6_log - INFO -    213    146.5 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 16:46:30,812 - memory_profile6_log - INFO -    214    146.5 MiB      0.0 MiB       print model_transform[model_transform['user_id']=='1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157']

2018-05-02 16:46:30,815 - memory_profile6_log - INFO -    215    146.5 MiB      0.0 MiB       logger.info("Len of model_transform: %d", len(model_transform))

2018-05-02 16:46:30,816 - memory_profile6_log - INFO -    216    146.5 MiB      0.0 MiB       logger.info("Len of df_dt: %d", len(df_dt))

2018-05-02 16:46:30,818 - memory_profile6_log - INFO -    217    146.5 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-05-02 16:46:30,818 - memory_profile6_log - INFO -    218                             

2018-05-02 16:46:30,819 - memory_profile6_log - INFO -    219    146.5 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-05-02 16:46:30,819 - memory_profile6_log - INFO -    220                             

2018-05-02 16:46:30,819 - memory_profile6_log - INFO -    221    146.5 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-05-02 16:46:30,821 - memory_profile6_log - INFO -    222    146.5 MiB      0.0 MiB       del df_dut

2018-05-02 16:46:30,821 - memory_profile6_log - INFO -    223    146.5 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-05-02 16:46:30,822 - memory_profile6_log - INFO -    224    146.5 MiB      0.0 MiB       del df_dt

2018-05-02 16:46:30,822 - memory_profile6_log - INFO -    225    146.5 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-05-02 16:46:30,822 - memory_profile6_log - INFO -    226    146.5 MiB      0.0 MiB       del df_input

2018-05-02 16:46:30,823 - memory_profile6_log - INFO -    227    146.5 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-05-02 16:46:30,828 - memory_profile6_log - INFO -    228    146.5 MiB      0.0 MiB       del df_input_X

2018-05-02 16:46:30,828 - memory_profile6_log - INFO -    229    146.5 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-05-02 16:46:30,828 - memory_profile6_log - INFO -    230    146.5 MiB      0.0 MiB       del df_current

2018-05-02 16:46:30,829 - memory_profile6_log - INFO -    231    146.5 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-05-02 16:46:30,829 - memory_profile6_log - INFO -    232    146.5 MiB      0.0 MiB       del map_topic_isgeneral

2018-05-02 16:46:30,831 - memory_profile6_log - INFO -    233    146.5 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-05-02 16:46:30,831 - memory_profile6_log - INFO -    234    146.5 MiB      0.0 MiB       del model_fit

2018-05-02 16:46:30,831 - memory_profile6_log - INFO -    235    146.5 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-05-02 16:46:30,832 - memory_profile6_log - INFO -    236    146.5 MiB      0.0 MiB       del result

2018-05-02 16:46:30,832 - memory_profile6_log - INFO -    237    146.5 MiB      0.0 MiB       logger.info("deleting result...")

2018-05-02 16:46:30,832 - memory_profile6_log - INFO -    238                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 16:46:30,832 - memory_profile6_log - INFO -    239                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 16:46:30,832 - memory_profile6_log - INFO -    240                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 16:46:30,834 - memory_profile6_log - INFO -    241    146.5 MiB      0.0 MiB       if savetrain:

2018-05-02 16:46:30,834 - memory_profile6_log - INFO -    242    146.5 MiB      0.0 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'p0_posterior', 'rank']].copy(deep=True)

2018-05-02 16:46:30,835 - memory_profile6_log - INFO -    243    146.5 MiB      0.0 MiB           model_transformsv['date'] = current_date  # we need manually adding date, because table not support

2018-05-02 16:46:30,835 - memory_profile6_log - INFO -    244    146.5 MiB      0.0 MiB           model_transformsv['date'] = pd.to_datetime(model_transformsv['date'],

2018-05-02 16:46:30,835 - memory_profile6_log - INFO -    245    146.5 MiB      0.0 MiB                                                      format='%Y-%m-%d', errors='coerce')

2018-05-02 16:46:30,835 - memory_profile6_log - INFO -    246    146.5 MiB      0.0 MiB           print "list(model_transformsv) before rename: ", list(model_transformsv)       

2018-05-02 16:46:30,835 - memory_profile6_log - INFO -    247    146.5 MiB      0.0 MiB           print len(model_transformsv)                

2018-05-02 16:46:30,835 - memory_profile6_log - INFO -    248    146.5 MiB      0.0 MiB           model_transformsv = model_transformsv.rename(columns={'is_general': 'topic_is_general', 'p0_posterior': 'interest_score',

2018-05-02 16:46:30,839 - memory_profile6_log - INFO -    249    146.5 MiB      0.0 MiB                                                                 'rank':'interest_rank' , 'date':'interest_score_created_at'})

2018-05-02 16:46:30,841 - memory_profile6_log - INFO -    250    146.5 MiB      0.0 MiB           print "list(model_transformsv) after rename: ", list(model_transformsv)    

2018-05-02 16:46:30,842 - memory_profile6_log - INFO -    251    146.5 MiB      0.0 MiB           print len(model_transformsv) 

2018-05-02 16:46:30,842 - memory_profile6_log - INFO -    252    146.5 MiB      0.0 MiB           del model_transform

2018-05-02 16:46:30,842 - memory_profile6_log - INFO -    253    146.5 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-05-02 16:46:30,842 - memory_profile6_log - INFO -    254    146.5 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-05-02 16:46:30,842 - memory_profile6_log - INFO -    255                             

2018-05-02 16:46:30,844 - memory_profile6_log - INFO -    256    146.5 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-05-02 16:46:30,844 - memory_profile6_log - INFO -    257                                     # ~ Place your code to save the training model here ~

2018-05-02 16:46:30,845 - memory_profile6_log - INFO -    258    146.5 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-05-02 16:46:30,845 - memory_profile6_log - INFO -    259                                         logger.info("Using google datastore as storage...")

2018-05-02 16:46:30,846 - memory_profile6_log - INFO -    260                                         if multproc:

2018-05-02 16:46:30,848 - memory_profile6_log - INFO -    261                                             # ~ save transform models ~

2018-05-02 16:46:30,848 - memory_profile6_log - INFO -    262                                             logger.info("Saving main Transform model to Google DataStore...")

2018-05-02 16:46:30,848 - memory_profile6_log - INFO -    263                                             logger.info("Saving total data: %d", len(model_transformsv))

2018-05-02 16:46:30,852 - memory_profile6_log - INFO -    264                                             mh.saveDataStorePutMulti(model_transformsv)

2018-05-02 16:46:30,854 - memory_profile6_log - INFO -    265                             

2018-05-02 16:46:30,854 - memory_profile6_log - INFO -    266                                             # ~ save fitted models ~

2018-05-02 16:46:30,855 - memory_profile6_log - INFO -    267                                             logger.info("Saving fitted_models as history to Google DataStore...")

2018-05-02 16:46:30,855 - memory_profile6_log - INFO -    268                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 16:46:30,855 - memory_profile6_log - INFO -    269                                             fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-05-02 16:46:30,857 - memory_profile6_log - INFO -    270                                             logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-05-02 16:46:30,857 - memory_profile6_log - INFO -    271                                             X_split = np.array_split(fitted_models_sigmant, 10)

2018-05-02 16:46:30,858 - memory_profile6_log - INFO -    272                                             logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-05-02 16:46:30,858 - memory_profile6_log - INFO -    273                                             for ix in range(len(X_split)):

2018-05-02 16:46:30,858 - memory_profile6_log - INFO -    274                                                 logger.info("processing batch-%d", ix)

2018-05-02 16:46:30,859 - memory_profile6_log - INFO -    275                                                 mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-05-02 16:46:30,859 - memory_profile6_log - INFO -    276                             

2018-05-02 16:46:30,864 - memory_profile6_log - INFO -    277                                             del X_split

2018-05-02 16:46:30,865 - memory_profile6_log - INFO -    278                                             logger.info("deleting X_split...")

2018-05-02 16:46:30,865 - memory_profile6_log - INFO -    279                                             del save_sigma_nt

2018-05-02 16:46:30,867 - memory_profile6_log - INFO -    280                                             logger.info("deleting save_sigma_nt...")

2018-05-02 16:46:30,867 - memory_profile6_log - INFO -    281                             

2018-05-02 16:46:30,868 - memory_profile6_log - INFO -    282                                             del BR

2018-05-02 16:46:30,868 - memory_profile6_log - INFO -    283                                             logger.info("deleting BR...")

2018-05-02 16:46:30,868 - memory_profile6_log - INFO -    284                             

2018-05-02 16:46:30,868 - memory_profile6_log - INFO -    285    146.5 MiB      0.0 MiB           elif str(saveto).lower() == "elastic":

2018-05-02 16:46:30,869 - memory_profile6_log - INFO -    286    146.5 MiB      0.0 MiB               logger.info("Using ElasticSearch as storage...")

2018-05-02 16:46:30,869 - memory_profile6_log - INFO -    287                                         """logging.info("Saving main Transform model to Elasticsearch...")

2018-05-02 16:46:30,871 - memory_profile6_log - INFO -    288                                         # print model_transformsv.head(5)

2018-05-02 16:46:30,871 - memory_profile6_log - INFO -    289                                         

2018-05-02 16:46:30,871 - memory_profile6_log - INFO -    290                                         X_split = np.array_split(model_transformsv, 5)

2018-05-02 16:46:30,871 - memory_profile6_log - INFO -    291                                         logger.info("Saving total data: %d", len(model_transformsv))

2018-05-02 16:46:30,871 - memory_profile6_log - INFO -    292                                         logger.info("Len of X_split for batch save model_transformsv: %d", len(X_split))

2018-05-02 16:46:30,877 - memory_profile6_log - INFO -    293                                         for ix in range(len(X_split)):

2018-05-02 16:46:30,877 - memory_profile6_log - INFO -    294                                             logger.info("processing batch-%d", ix)

2018-05-02 16:46:30,877 - memory_profile6_log - INFO -    295                                             mh.saveElasticS(X_split[ix])

2018-05-02 16:46:30,878 - memory_profile6_log - INFO -    296                                         del X_split

2018-05-02 16:46:30,878 - memory_profile6_log - INFO -    297                                         """

2018-05-02 16:46:30,878 - memory_profile6_log - INFO -    298                             

2018-05-02 16:46:30,878 - memory_profile6_log - INFO -    299    146.5 MiB      0.0 MiB               logger.info("Saving fitted_models as history to Elasticsearch...")

2018-05-02 16:46:30,880 - memory_profile6_log - INFO -    300    146.5 MiB      0.0 MiB               save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 16:46:30,880 - memory_profile6_log - INFO -    301    146.5 MiB      0.0 MiB               fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-05-02 16:46:30,881 - memory_profile6_log - INFO -    302                             

2018-05-02 16:46:30,882 - memory_profile6_log - INFO -    303    146.5 MiB      0.0 MiB               fitted_models_sigmant['uid_topid'] = fitted_models_sigmant["user_id"].map(str) + "_" + fitted_models_sigmant["topic_id"].map(str)

2018-05-02 16:46:30,884 - memory_profile6_log - INFO -    304    146.6 MiB      0.0 MiB               print "fitted_models_sigmant: ", fitted_models_sigmant

2018-05-02 16:46:30,884 - memory_profile6_log - INFO -    305                                         

2018-05-02 16:46:30,884 - memory_profile6_log - INFO -    306    146.6 MiB      0.0 MiB               X_split = np.array_split(fitted_models_sigmant, 5)

2018-05-02 16:46:30,884 - memory_profile6_log - INFO -    307    146.6 MiB      0.0 MiB               logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-05-02 16:46:30,888 - memory_profile6_log - INFO -    308    146.6 MiB      0.0 MiB               logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-05-02 16:46:30,888 - memory_profile6_log - INFO -    309    149.6 MiB     -8.9 MiB               for ix in range(len(X_split)):

2018-05-02 16:46:30,890 - memory_profile6_log - INFO -    310    149.6 MiB     -8.8 MiB                   logger.info("processing batch-%d", ix)

2018-05-02 16:46:30,890 - memory_profile6_log - INFO -    311    149.6 MiB     -5.8 MiB                   mh.saveElasticS(X_split[ix], esindex_name="fitted_hist_index",  estype_name='fitted_hist_type')

2018-05-02 16:46:30,891 - memory_profile6_log - INFO -    312    149.5 MiB     -0.1 MiB               del X_split

2018-05-02 16:46:30,891 - memory_profile6_log - INFO -    313                                         

2018-05-02 16:46:30,891 - memory_profile6_log - INFO -    314    149.5 MiB      0.0 MiB               del BR

2018-05-02 16:46:30,892 - memory_profile6_log - INFO -    315    149.5 MiB      0.0 MiB               logger.info("deleting BR...")

2018-05-02 16:46:30,894 - memory_profile6_log - INFO -    316    149.5 MiB      0.0 MiB               del save_sigma_nt

2018-05-02 16:46:30,894 - memory_profile6_log - INFO -    317    149.5 MiB      0.0 MiB               logger.info("deleting save_sigma_nt...")

2018-05-02 16:46:30,894 - memory_profile6_log - INFO -    318    149.5 MiB      0.0 MiB               del fitted_models_sigmant

2018-05-02 16:46:30,894 - memory_profile6_log - INFO -    319    149.5 MiB      0.0 MiB               logger.info("deleting fitted_models_sigmant...")

2018-05-02 16:46:30,895 - memory_profile6_log - INFO -    320                             

2018-05-02 16:46:30,895 - memory_profile6_log - INFO -    321                                     # need save sigma_nt for daily train

2018-05-02 16:46:30,901 - memory_profile6_log - INFO -    322                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-05-02 16:46:30,901 - memory_profile6_log - INFO -    323                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-05-02 16:46:30,901 - memory_profile6_log - INFO -    324    149.5 MiB      0.0 MiB           if start_date and end_date:

2018-05-02 16:46:30,901 - memory_profile6_log - INFO -    325                                         if not fitby_sigmant:

2018-05-02 16:46:30,903 - memory_profile6_log - INFO -    326                                             logging.info("Saving sigma Nt...")

2018-05-02 16:46:30,903 - memory_profile6_log - INFO -    327                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 16:46:30,903 - memory_profile6_log - INFO -    328                                             save_sigma_nt['start_date'] = start_date

2018-05-02 16:46:30,904 - memory_profile6_log - INFO -    329                                             save_sigma_nt['end_date'] = end_date

2018-05-02 16:46:30,904 - memory_profile6_log - INFO -    330                                             print save_sigma_nt.head(5)

2018-05-02 16:46:30,904 - memory_profile6_log - INFO -    331                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-05-02 16:46:30,904 - memory_profile6_log - INFO -    332                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-05-02 16:46:30,904 - memory_profile6_log - INFO -    333    149.5 MiB      0.0 MiB       return

2018-05-02 16:46:30,904 - memory_profile6_log - INFO - 


2018-05-02 16:46:30,905 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-05-02 16:51:50,438 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 16:51:50,443 - memory_profile6_log - INFO - date_generated: 
2018-05-02 16:51:50,443 - memory_profile6_log - INFO -  
2018-05-02 16:51:50,444 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 16, 51, 50, 440000)]
2018-05-02 16:51:50,444 - memory_profile6_log - INFO - 

2018-05-02 16:51:50,444 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 16:51:50,444 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 16:51:50,446 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 16:51:50,615 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 16:51:50,621 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 16:51:58,288 - memory_profile6_log - INFO - size of df: 2.53 MB
2018-05-02 16:51:58,289 - memory_profile6_log - INFO - getting total: 10000 training data(genuine interest) for date: 2018-05-01
2018-05-02 16:51:58,306 - memory_profile6_log - INFO - loading history data from datastore...
2018-05-02 16:51:58,308 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-05-02 16:51:58,309 - memory_profile6_log - INFO - Appending history data...
2018-05-02 16:51:58,309 - memory_profile6_log - INFO - processing batch-0
2018-05-02 16:51:58,311 - memory_profile6_log - INFO - creating list history data...
2018-05-02 16:51:58,381 - memory_profile6_log - INFO - call history data...
2018-05-02 16:52:35,851 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 16:52:36,500 - memory_profile6_log - INFO - processing batch-1
2018-05-02 16:52:36,500 - memory_profile6_log - INFO - creating list history data...
2018-05-02 16:52:36,510 - memory_profile6_log - INFO - call history data...
2018-05-02 16:53:12,970 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 16:53:13,585 - memory_profile6_log - INFO - processing batch-2
2018-05-02 16:53:13,589 - memory_profile6_log - INFO - creating list history data...
2018-05-02 16:53:13,599 - memory_profile6_log - INFO - call history data...
2018-05-02 16:53:48,782 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 16:53:49,190 - memory_profile6_log - INFO - processing batch-3
2018-05-02 16:53:49,193 - memory_profile6_log - INFO - creating list history data...
2018-05-02 16:53:49,200 - memory_profile6_log - INFO - call history data...
2018-05-02 16:54:23,775 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 16:54:24,171 - memory_profile6_log - INFO - processing batch-4
2018-05-02 16:54:24,173 - memory_profile6_log - INFO - creating list history data...
2018-05-02 16:54:24,180 - memory_profile6_log - INFO - call history data...
2018-05-02 16:54:59,854 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 16:55:00,131 - memory_profile6_log - INFO - Appending training data...
2018-05-02 16:55:00,132 - memory_profile6_log - INFO - len datalist: 1
2018-05-02 16:55:00,134 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-05-02 16:55:00,135 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 16:55:00,137 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 16:55:00,138 - memory_profile6_log - INFO - ================================================

2018-05-02 16:55:00,140 - memory_profile6_log - INFO -    360     86.8 MiB     86.8 MiB   @profile

2018-05-02 16:55:00,140 - memory_profile6_log - INFO -    361                             def BQPreprocess(cpu, date_generated, client, query_fit, loadfrom="datastore"):

2018-05-02 16:55:00,141 - memory_profile6_log - INFO -    362     86.8 MiB      0.0 MiB       bq_client = client

2018-05-02 16:55:00,144 - memory_profile6_log - INFO -    363     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 16:55:00,144 - memory_profile6_log - INFO -    364                             

2018-05-02 16:55:00,148 - memory_profile6_log - INFO -    365     86.8 MiB      0.0 MiB       datalist = []

2018-05-02 16:55:00,148 - memory_profile6_log - INFO -    366     86.8 MiB      0.0 MiB       datalist_hist = []

2018-05-02 16:55:00,150 - memory_profile6_log - INFO -    367                             

2018-05-02 16:55:00,150 - memory_profile6_log - INFO -    368     86.8 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-05-02 16:55:00,151 - memory_profile6_log - INFO -    369    116.9 MiB      0.0 MiB       for ndate in date_generated:

2018-05-02 16:55:00,151 - memory_profile6_log - INFO -    370    116.2 MiB     29.4 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-05-02 16:55:00,154 - memory_profile6_log - INFO -    371    116.2 MiB      0.0 MiB           if tframe is not None:

2018-05-02 16:55:00,155 - memory_profile6_log - INFO -    372    116.2 MiB      0.0 MiB               if not tframe.empty:

2018-05-02 16:55:00,157 - memory_profile6_log - INFO -    373    116.2 MiB      0.0 MiB                   if loadfrom.strip().lower() == 'datastore':

2018-05-02 16:55:00,158 - memory_profile6_log - INFO -    374    116.2 MiB      0.1 MiB                       X_split = np.array_split(tframe, 5)

2018-05-02 16:55:00,160 - memory_profile6_log - INFO -    375    116.2 MiB      0.0 MiB                       logger.info("loading history data from datastore...")

2018-05-02 16:55:00,161 - memory_profile6_log - INFO -    376    116.2 MiB      0.0 MiB                       logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 16:55:00,161 - memory_profile6_log - INFO -    377    116.2 MiB      0.0 MiB                       logger.info("Appending history data...")

2018-05-02 16:55:00,163 - memory_profile6_log - INFO -    378    116.9 MiB     -0.1 MiB                       for ix in range(len(X_split)):

2018-05-02 16:55:00,167 - memory_profile6_log - INFO -    379                                                     # ~ loading history

2018-05-02 16:55:00,167 - memory_profile6_log - INFO -    380                                                     """

2018-05-02 16:55:00,168 - memory_profile6_log - INFO -    381                                                         disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-05-02 16:55:00,170 - memory_profile6_log - INFO -    382                                                     """

2018-05-02 16:55:00,171 - memory_profile6_log - INFO -    383    116.9 MiB     -0.1 MiB                           logger.info("processing batch-%d", ix)

2018-05-02 16:55:00,174 - memory_profile6_log - INFO -    384                                                     # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-05-02 16:55:00,174 - memory_profile6_log - INFO -    385    116.9 MiB     -0.1 MiB                           logger.info("creating list history data...")

2018-05-02 16:55:00,176 - memory_profile6_log - INFO -    386    116.9 MiB      0.3 MiB                           lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-05-02 16:55:00,177 - memory_profile6_log - INFO -    387                                                     #lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 16:55:00,180 - memory_profile6_log - INFO -    388                             

2018-05-02 16:55:00,181 - memory_profile6_log - INFO -    389    116.9 MiB     -0.1 MiB                           logger.info("call history data...")

2018-05-02 16:55:00,183 - memory_profile6_log - INFO -    390    116.9 MiB      0.1 MiB                           h_frame = mh.loadDSHistory(lhistory)

2018-05-02 16:55:00,183 - memory_profile6_log - INFO -    391                             

2018-05-02 16:55:00,184 - memory_profile6_log - INFO -    392                                                     # me = os.getpid()

2018-05-02 16:55:00,184 - memory_profile6_log - INFO -    393                                                     # kill_proc_tree(me)

2018-05-02 16:55:00,186 - memory_profile6_log - INFO -    394                             

2018-05-02 16:55:00,186 - memory_profile6_log - INFO -    395    116.9 MiB     -0.0 MiB                           logger.info("done collecting history data, appending now...")

2018-05-02 16:55:00,190 - memory_profile6_log - INFO -    396    116.9 MiB    -24.6 MiB                           for m in h_frame:

2018-05-02 16:55:00,191 - memory_profile6_log - INFO -    397    116.9 MiB    -24.7 MiB                               if m is not None:

2018-05-02 16:55:00,193 - memory_profile6_log - INFO -    398    116.9 MiB    -24.7 MiB                                   if len(m) > 0:

2018-05-02 16:55:00,194 - memory_profile6_log - INFO -    399    116.9 MiB    -16.7 MiB                                       datalist_hist.append(pd.DataFrame(m))

2018-05-02 16:55:00,194 - memory_profile6_log - INFO -    400    116.9 MiB     -0.1 MiB                           del h_frame

2018-05-02 16:55:00,196 - memory_profile6_log - INFO -    401    116.9 MiB     -0.1 MiB                           del lhistory

2018-05-02 16:55:00,197 - memory_profile6_log - INFO -    402                             

2018-05-02 16:55:00,200 - memory_profile6_log - INFO -    403    116.9 MiB      0.0 MiB                       logger.info("Appending training data...")

2018-05-02 16:55:00,201 - memory_profile6_log - INFO -    404    116.9 MiB      0.0 MiB                       datalist.append(tframe)

2018-05-02 16:55:00,203 - memory_profile6_log - INFO -    405                                             elif loadfrom.strip().lower() == 'elastic':

2018-05-02 16:55:00,203 - memory_profile6_log - INFO -    406                                                 X_split = np.array_split(tframe, 5)

2018-05-02 16:55:00,204 - memory_profile6_log - INFO -    407                                                 logger.info("loading history data from elastic...")

2018-05-02 16:55:00,206 - memory_profile6_log - INFO -    408                                                 logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 16:55:00,206 - memory_profile6_log - INFO -    409                                                 logger.info("Appending history data...")

2018-05-02 16:55:00,207 - memory_profile6_log - INFO -    410                                                 for ix in range(len(X_split)):

2018-05-02 16:55:00,207 - memory_profile6_log - INFO -    411                                                     for framedata in X_split[ix].itertuples():

2018-05-02 16:55:00,209 - memory_profile6_log - INFO -    412                                                         inside_data = mh.loadESHistory(framedata.user_id, framedata.topic_id ,

2018-05-02 16:55:00,209 - memory_profile6_log - INFO -    413                                                                                        esindex_name='fitted_hist_index',

2018-05-02 16:55:00,213 - memory_profile6_log - INFO -    414                                                                                        estype_name='fitted_hist_type')

2018-05-02 16:55:00,213 - memory_profile6_log - INFO -    415                             

2018-05-02 16:55:00,216 - memory_profile6_log - INFO -    416                                                         if not inside_data.empty:

2018-05-02 16:55:00,217 - memory_profile6_log - INFO -    417                                                             datalist_hist.append(inside_data)

2018-05-02 16:55:00,217 - memory_profile6_log - INFO -    418                                                             del inside_data

2018-05-02 16:55:00,219 - memory_profile6_log - INFO -    419                                                         else:

2018-05-02 16:55:00,220 - memory_profile6_log - INFO -    420                                                             logger.info("uid: %s with topic_id: %s is empty!" % (framedata.user_id, framedata.topic_id))

2018-05-02 16:55:00,223 - memory_profile6_log - INFO -    421                                             else:

2018-05-02 16:55:00,223 - memory_profile6_log - INFO -    422                                                 logger.info("Unknows source is selected !")

2018-05-02 16:55:00,226 - memory_profile6_log - INFO -    423                                                 break

2018-05-02 16:55:00,226 - memory_profile6_log - INFO -    424                                     else: 

2018-05-02 16:55:00,226 - memory_profile6_log - INFO -    425                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-05-02 16:55:00,227 - memory_profile6_log - INFO -    426    116.9 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-05-02 16:55:00,229 - memory_profile6_log - INFO -    427    116.9 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-05-02 16:55:00,229 - memory_profile6_log - INFO -    428                             

2018-05-02 16:55:00,230 - memory_profile6_log - INFO -    429    116.9 MiB      0.0 MiB       return datalist, datalist_hist

2018-05-02 16:55:00,233 - memory_profile6_log - INFO - 


2018-05-02 16:55:01,010 - memory_profile6_log - INFO - size of big_frame_hist: 479.88 KB
2018-05-02 16:55:01,022 - memory_profile6_log - INFO - size of big_frame: 2.53 MB
2018-05-02 16:55:01,025 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-05-02 16:55:09,927 - memory_profile6_log - INFO - size of df: 4.93 MB
2018-05-02 16:55:09,927 - memory_profile6_log - INFO - getting total: 20000 training data(current date interest)
2018-05-02 16:55:09,941 - memory_profile6_log - INFO - size of current_frame: 5.09 MB
2018-05-02 16:55:09,943 - memory_profile6_log - INFO - loading time of: 30000 total genuine-current interest data ~ take 199.372s
2018-05-02 16:55:09,947 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 16:55:09,948 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 16:55:09,948 - memory_profile6_log - INFO - ================================================

2018-05-02 16:55:09,950 - memory_profile6_log - INFO -    431     86.7 MiB     86.7 MiB   @profile

2018-05-02 16:55:09,951 - memory_profile6_log - INFO -    432                             def preprocess(cpu, cd, query_fit, date_generated):

2018-05-02 16:55:09,951 - memory_profile6_log - INFO -    433     86.8 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-05-02 16:55:09,953 - memory_profile6_log - INFO -    434     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 16:55:09,956 - memory_profile6_log - INFO -    435                             

2018-05-02 16:55:09,957 - memory_profile6_log - INFO -    436                                 # ~~~ Begin collecting data ~~~

2018-05-02 16:55:09,959 - memory_profile6_log - INFO -    437     86.8 MiB      0.0 MiB       t0 = time.time()

2018-05-02 16:55:09,960 - memory_profile6_log - INFO -    438    116.9 MiB     30.1 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-05-02 16:55:09,960 - memory_profile6_log - INFO -    439    116.9 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-05-02 16:55:09,963 - memory_profile6_log - INFO -    440                                     logger.info("Training cannot be empty..")

2018-05-02 16:55:09,964 - memory_profile6_log - INFO -    441                                     return False

2018-05-02 16:55:09,964 - memory_profile6_log - INFO -    442    117.0 MiB      0.1 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-05-02 16:55:09,966 - memory_profile6_log - INFO -    443                                 # print "big_frame_hist:\n", big_frame_hist.head(20)

2018-05-02 16:55:09,967 - memory_profile6_log - INFO -    444    117.0 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-05-02 16:55:09,967 - memory_profile6_log - INFO -    445                             

2018-05-02 16:55:09,970 - memory_profile6_log - INFO -    446    117.0 MiB      0.0 MiB       big_frame = pd.concat(datalist)

2018-05-02 16:55:09,970 - memory_profile6_log - INFO -    447    117.0 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-05-02 16:55:09,971 - memory_profile6_log - INFO -    448    117.0 MiB      0.0 MiB       del datalist

2018-05-02 16:55:09,973 - memory_profile6_log - INFO -    449                             

2018-05-02 16:55:09,973 - memory_profile6_log - INFO -    450    117.0 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 16:55:09,973 - memory_profile6_log - INFO -    451                             

2018-05-02 16:55:09,973 - memory_profile6_log - INFO -    452                                 # ~ get current news interest ~

2018-05-02 16:55:09,973 - memory_profile6_log - INFO -    453    117.0 MiB      0.0 MiB       if not cd:

2018-05-02 16:55:09,974 - memory_profile6_log - INFO -    454    117.0 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-05-02 16:55:09,974 - memory_profile6_log - INFO -    455    141.6 MiB     24.6 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-05-02 16:55:09,974 - memory_profile6_log - INFO -    456    141.6 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-05-02 16:55:09,976 - memory_profile6_log - INFO -    457                                 else:

2018-05-02 16:55:09,976 - memory_profile6_log - INFO -    458                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-05-02 16:55:09,976 - memory_profile6_log - INFO -    459                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date) limit 10000"

2018-05-02 16:55:09,976 - memory_profile6_log - INFO -    460                             

2018-05-02 16:55:09,976 - memory_profile6_log - INFO -    461                                     # safe handling of query parameter

2018-05-02 16:55:09,983 - memory_profile6_log - INFO -    462                                     query_params = [

2018-05-02 16:55:09,984 - memory_profile6_log - INFO -    463                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-05-02 16:55:09,986 - memory_profile6_log - INFO -    464                                     ]

2018-05-02 16:55:09,987 - memory_profile6_log - INFO -    465                             

2018-05-02 16:55:09,989 - memory_profile6_log - INFO -    466                                     job_config.query_parameters = query_params

2018-05-02 16:55:09,990 - memory_profile6_log - INFO -    467                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-05-02 16:55:09,990 - memory_profile6_log - INFO -    468                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-05-02 16:55:09,990 - memory_profile6_log - INFO -    469                             

2018-05-02 16:55:09,992 - memory_profile6_log - INFO -    470    141.6 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-05-02 16:55:09,996 - memory_profile6_log - INFO -    471    141.6 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-05-02 16:55:09,996 - memory_profile6_log - INFO -    472    141.6 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-05-02 16:55:09,999 - memory_profile6_log - INFO -    473    141.6 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-05-02 16:55:09,999 - memory_profile6_log - INFO -    474    141.6 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 16:55:10,000 - memory_profile6_log - INFO -    475    141.6 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-05-02 16:55:10,000 - memory_profile6_log - INFO -    476                             

2018-05-02 16:55:10,000 - memory_profile6_log - INFO -    477    141.6 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-05-02 16:55:10,000 - memory_profile6_log - INFO - 


2018-05-02 16:55:10,006 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-05-02 16:55:10,015 - memory_profile6_log - INFO - train on: 10000 total genuine interest data(D(u, t))
2018-05-02 16:55:10,016 - memory_profile6_log - INFO - transform on: 20000 total current data(D(t))
2018-05-02 16:55:10,017 - memory_profile6_log - INFO - apply on: 1717 total history...)
2018-05-02 16:55:10,066 - memory_profile6_log - INFO - len of uniques_fit_hist:1717
2018-05-02 16:55:10,072 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:858
2018-05-02 16:55:10,134 - memory_profile6_log - INFO - self.sum_all_nt before concate:

2018-05-02 16:55:10,151 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 16:55:10,154 - memory_profile6_log - INFO - 

2018-05-02 16:55:10,167 - memory_profile6_log - INFO - self.sum_all_nt after concate:

2018-05-02 16:55:10,174 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 16:55:10,177 - memory_profile6_log - INFO - 

2018-05-02 16:55:10,226 - memory_profile6_log - INFO - Len of model_fit: 10000
2018-05-02 16:55:10,227 - memory_profile6_log - INFO - Len of df_dut: 10000
2018-05-02 16:55:10,364 - memory_profile6_log - INFO - Len of fitted_models on main class: 10000
2018-05-02 16:55:10,365 - memory_profile6_log - INFO - 

2018-05-02 16:55:10,365 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-05-02 16:55:10,367 - memory_profile6_log - INFO - 

2018-05-02 16:55:10,368 - memory_profile6_log - INFO - fitted_model_hist:

2018-05-02 16:55:10,368 - memory_profile6_log - INFO - 

2018-05-02 16:55:10,378 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 16:55:10,378 - memory_profile6_log - INFO - 

2018-05-02 16:55:10,388 - memory_profile6_log - INFO - len of fitted models after concat: 11717
2018-05-02 16:55:10,388 - memory_profile6_log - INFO - 

2018-05-02 16:55:10,390 - memory_profile6_log - INFO - Recalculating fitted models...
2018-05-02 16:55:10,391 - memory_profile6_log - INFO - 

2018-05-02 16:55:10,415 - memory_profile6_log - INFO - fitted_models after cobine:

2018-05-02 16:55:10,417 - memory_profile6_log - INFO - 

2018-05-02 16:55:10,426 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 16:55:10,427 - memory_profile6_log - INFO - 

2018-05-02 16:55:10,430 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 10000
2018-05-02 16:55:10,431 - memory_profile6_log - INFO - 

2018-05-02 16:55:11,993 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-05-02 16:55:12,013 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, topic_id, pt_posterior_x_Nt, smoothed_pt_posterior, p0_cat_ci, sigma_Nt, p0_posterior, is_general, rank]
Index: []
2018-05-02 16:55:12,013 - memory_profile6_log - INFO - 

2018-05-02 16:55:12,015 - memory_profile6_log - INFO - Len of model_transform: 5057
2018-05-02 16:55:12,016 - memory_profile6_log - INFO - Len of df_dt: 20000
2018-05-02 16:55:12,016 - memory_profile6_log - INFO - Total train time: 1.988s
2018-05-02 16:55:12,017 - memory_profile6_log - INFO - memory left before cleaning: 85.900 percent memory...
2018-05-02 16:55:12,019 - memory_profile6_log - INFO - cleaning up some objects...
2018-05-02 16:55:12,020 - memory_profile6_log - INFO - deleting df_dut...
2018-05-02 16:55:12,022 - memory_profile6_log - INFO - deleting df_dt...
2018-05-02 16:55:12,025 - memory_profile6_log - INFO - deleting df_input...
2018-05-02 16:55:12,026 - memory_profile6_log - INFO - deleting df_input_X...
2018-05-02 16:55:12,028 - memory_profile6_log - INFO - deleting df_current...
2018-05-02 16:55:12,029 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-05-02 16:55:12,029 - memory_profile6_log - INFO - deleting model_fit...
2018-05-02 16:55:12,030 - memory_profile6_log - INFO - deleting result...
2018-05-02 16:55:12,039 - memory_profile6_log - INFO - list(model_transformsv) before rename: 
2018-05-02 16:55:12,039 - memory_profile6_log - INFO -  
2018-05-02 16:55:12,040 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'is_general', 'p0_posterior', 'rank', 'date']
2018-05-02 16:55:12,042 - memory_profile6_log - INFO - 

2018-05-02 16:55:12,043 - memory_profile6_log - INFO - 5057
2018-05-02 16:55:12,045 - memory_profile6_log - INFO - 

2018-05-02 16:55:12,049 - memory_profile6_log - INFO - list(model_transformsv) after rename: 
2018-05-02 16:55:12,049 - memory_profile6_log - INFO -  
2018-05-02 16:55:12,052 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'topic_is_general', 'interest_score', 'interest_rank', 'interest_score_created_at']
2018-05-02 16:55:12,053 - memory_profile6_log - INFO - 

2018-05-02 16:55:12,055 - memory_profile6_log - INFO - 5057
2018-05-02 16:55:12,056 - memory_profile6_log - INFO - 

2018-05-02 16:55:12,058 - memory_profile6_log - INFO - deleting model_transform...
2018-05-02 16:55:12,059 - memory_profile6_log - INFO - memory left after cleaning: 85.900 percent memory...
2018-05-02 16:55:12,059 - memory_profile6_log - INFO - Begin saving trained data...
2018-05-02 16:55:12,063 - memory_profile6_log - INFO - Using ElasticSearch as storage...
2018-05-02 16:55:12,065 - memory_profile6_log - INFO - Saving fitted_models as history to Elasticsearch...
2018-05-02 16:56:22,947 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 16:56:22,950 - memory_profile6_log - INFO - date_generated: 
2018-05-02 16:56:22,950 - memory_profile6_log - INFO -  
2018-05-02 16:56:22,951 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 16, 56, 22, 949000)]
2018-05-02 16:56:22,951 - memory_profile6_log - INFO - 

2018-05-02 16:56:22,953 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 16:56:22,953 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 16:56:22,953 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 16:56:23,095 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 16:56:23,098 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 16:56:30,680 - memory_profile6_log - INFO - size of df: 2.53 MB
2018-05-02 16:56:30,680 - memory_profile6_log - INFO - getting total: 10000 training data(genuine interest) for date: 2018-05-01
2018-05-02 16:56:30,697 - memory_profile6_log - INFO - loading history data from datastore...
2018-05-02 16:56:30,697 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-05-02 16:56:30,698 - memory_profile6_log - INFO - Appending history data...
2018-05-02 16:56:30,700 - memory_profile6_log - INFO - processing batch-0
2018-05-02 16:56:30,700 - memory_profile6_log - INFO - creating list history data...
2018-05-02 16:56:30,766 - memory_profile6_log - INFO - call history data...
2018-05-02 16:57:04,654 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 16:57:05,369 - memory_profile6_log - INFO - processing batch-1
2018-05-02 16:57:05,371 - memory_profile6_log - INFO - creating list history data...
2018-05-02 16:57:05,378 - memory_profile6_log - INFO - call history data...
2018-05-02 16:57:39,951 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 16:57:40,559 - memory_profile6_log - INFO - processing batch-2
2018-05-02 16:57:40,561 - memory_profile6_log - INFO - creating list history data...
2018-05-02 16:57:40,568 - memory_profile6_log - INFO - call history data...
2018-05-02 16:58:14,661 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 16:58:15,051 - memory_profile6_log - INFO - processing batch-3
2018-05-02 16:58:15,052 - memory_profile6_log - INFO - creating list history data...
2018-05-02 16:58:15,059 - memory_profile6_log - INFO - call history data...
2018-05-02 16:58:49,815 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 16:58:50,194 - memory_profile6_log - INFO - processing batch-4
2018-05-02 16:58:50,197 - memory_profile6_log - INFO - creating list history data...
2018-05-02 16:58:50,204 - memory_profile6_log - INFO - call history data...
2018-05-02 16:59:24,585 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 16:59:24,891 - memory_profile6_log - INFO - Appending training data...
2018-05-02 16:59:24,892 - memory_profile6_log - INFO - len datalist: 1
2018-05-02 16:59:24,894 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-05-02 16:59:24,895 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 16:59:24,897 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 16:59:24,897 - memory_profile6_log - INFO - ================================================

2018-05-02 16:59:24,898 - memory_profile6_log - INFO -    360     86.7 MiB     86.7 MiB   @profile

2018-05-02 16:59:24,898 - memory_profile6_log - INFO -    361                             def BQPreprocess(cpu, date_generated, client, query_fit, loadfrom="datastore"):

2018-05-02 16:59:24,898 - memory_profile6_log - INFO -    362     86.7 MiB      0.0 MiB       bq_client = client

2018-05-02 16:59:24,900 - memory_profile6_log - INFO -    363     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 16:59:24,901 - memory_profile6_log - INFO -    364                             

2018-05-02 16:59:24,904 - memory_profile6_log - INFO -    365     86.7 MiB      0.0 MiB       datalist = []

2018-05-02 16:59:24,905 - memory_profile6_log - INFO -    366     86.7 MiB      0.0 MiB       datalist_hist = []

2018-05-02 16:59:24,907 - memory_profile6_log - INFO -    367                             

2018-05-02 16:59:24,907 - memory_profile6_log - INFO -    368     86.7 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-05-02 16:59:24,910 - memory_profile6_log - INFO -    369    116.3 MiB      0.0 MiB       for ndate in date_generated:

2018-05-02 16:59:24,910 - memory_profile6_log - INFO -    370    115.6 MiB     28.9 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-05-02 16:59:24,910 - memory_profile6_log - INFO -    371    115.6 MiB      0.0 MiB           if tframe is not None:

2018-05-02 16:59:24,911 - memory_profile6_log - INFO -    372    115.6 MiB      0.0 MiB               if not tframe.empty:

2018-05-02 16:59:24,911 - memory_profile6_log - INFO -    373    115.6 MiB      0.0 MiB                   if loadfrom.strip().lower() == 'datastore':

2018-05-02 16:59:24,914 - memory_profile6_log - INFO -    374    115.6 MiB      0.1 MiB                       X_split = np.array_split(tframe, 5)

2018-05-02 16:59:24,917 - memory_profile6_log - INFO -    375    115.6 MiB      0.0 MiB                       logger.info("loading history data from datastore...")

2018-05-02 16:59:24,917 - memory_profile6_log - INFO -    376    115.6 MiB      0.0 MiB                       logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 16:59:24,917 - memory_profile6_log - INFO -    377    115.6 MiB      0.0 MiB                       logger.info("Appending history data...")

2018-05-02 16:59:24,918 - memory_profile6_log - INFO -    378    116.3 MiB     -0.1 MiB                       for ix in range(len(X_split)):

2018-05-02 16:59:24,918 - memory_profile6_log - INFO -    379                                                     # ~ loading history

2018-05-02 16:59:24,921 - memory_profile6_log - INFO -    380                                                     """

2018-05-02 16:59:24,921 - memory_profile6_log - INFO -    381                                                         disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-05-02 16:59:24,923 - memory_profile6_log - INFO -    382                                                     """

2018-05-02 16:59:24,923 - memory_profile6_log - INFO -    383    116.3 MiB     -0.1 MiB                           logger.info("processing batch-%d", ix)

2018-05-02 16:59:24,926 - memory_profile6_log - INFO -    384                                                     # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-05-02 16:59:24,927 - memory_profile6_log - INFO -    385    116.3 MiB     -0.1 MiB                           logger.info("creating list history data...")

2018-05-02 16:59:24,927 - memory_profile6_log - INFO -    386    116.3 MiB      0.3 MiB                           lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-05-02 16:59:24,930 - memory_profile6_log - INFO -    387                                                     #lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 16:59:24,930 - memory_profile6_log - INFO -    388                             

2018-05-02 16:59:24,931 - memory_profile6_log - INFO -    389    116.3 MiB     -0.1 MiB                           logger.info("call history data...")

2018-05-02 16:59:24,933 - memory_profile6_log - INFO -    390    116.3 MiB      0.1 MiB                           h_frame = mh.loadDSHistory(lhistory)

2018-05-02 16:59:24,933 - memory_profile6_log - INFO -    391                             

2018-05-02 16:59:24,934 - memory_profile6_log - INFO -    392                                                     # me = os.getpid()

2018-05-02 16:59:24,937 - memory_profile6_log - INFO -    393                                                     # kill_proc_tree(me)

2018-05-02 16:59:24,940 - memory_profile6_log - INFO -    394                             

2018-05-02 16:59:24,940 - memory_profile6_log - INFO -    395    116.3 MiB      0.0 MiB                           logger.info("done collecting history data, appending now...")

2018-05-02 16:59:24,941 - memory_profile6_log - INFO -    396    116.3 MiB    -16.2 MiB                           for m in h_frame:

2018-05-02 16:59:24,943 - memory_profile6_log - INFO -    397    116.3 MiB    -16.2 MiB                               if m is not None:

2018-05-02 16:59:24,944 - memory_profile6_log - INFO -    398    116.3 MiB    -16.2 MiB                                   if len(m) > 0:

2018-05-02 16:59:24,947 - memory_profile6_log - INFO -    399    116.3 MiB    -11.1 MiB                                       datalist_hist.append(pd.DataFrame(m))

2018-05-02 16:59:24,947 - memory_profile6_log - INFO -    400    116.3 MiB     -0.1 MiB                           del h_frame

2018-05-02 16:59:24,950 - memory_profile6_log - INFO -    401    116.3 MiB     -0.1 MiB                           del lhistory

2018-05-02 16:59:24,950 - memory_profile6_log - INFO -    402                             

2018-05-02 16:59:24,951 - memory_profile6_log - INFO -    403    116.3 MiB     -0.0 MiB                       logger.info("Appending training data...")

2018-05-02 16:59:24,953 - memory_profile6_log - INFO -    404    116.3 MiB      0.0 MiB                       datalist.append(tframe)

2018-05-02 16:59:24,954 - memory_profile6_log - INFO -    405                                             elif loadfrom.strip().lower() == 'elastic':

2018-05-02 16:59:24,957 - memory_profile6_log - INFO -    406                                                 X_split = np.array_split(tframe, 5)

2018-05-02 16:59:24,959 - memory_profile6_log - INFO -    407                                                 logger.info("loading history data from elastic...")

2018-05-02 16:59:24,960 - memory_profile6_log - INFO -    408                                                 logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 16:59:24,960 - memory_profile6_log - INFO -    409                                                 logger.info("Appending history data...")

2018-05-02 16:59:24,963 - memory_profile6_log - INFO -    410                                                 for ix in range(len(X_split)):

2018-05-02 16:59:24,963 - memory_profile6_log - INFO -    411                                                     for framedata in X_split[ix].itertuples():

2018-05-02 16:59:24,964 - memory_profile6_log - INFO -    412                                                         inside_data = mh.loadESHistory(framedata.user_id, framedata.topic_id ,

2018-05-02 16:59:24,966 - memory_profile6_log - INFO -    413                                                                                        esindex_name='fitted_hist_index',

2018-05-02 16:59:24,966 - memory_profile6_log - INFO -    414                                                                                        estype_name='fitted_hist_type')

2018-05-02 16:59:24,970 - memory_profile6_log - INFO -    415                             

2018-05-02 16:59:24,970 - memory_profile6_log - INFO -    416                                                         if not inside_data.empty:

2018-05-02 16:59:24,971 - memory_profile6_log - INFO -    417                                                             datalist_hist.append(inside_data)

2018-05-02 16:59:24,973 - memory_profile6_log - INFO -    418                                                             del inside_data

2018-05-02 16:59:24,973 - memory_profile6_log - INFO -    419                                                         else:

2018-05-02 16:59:24,973 - memory_profile6_log - INFO -    420                                                             logger.info("uid: %s with topic_id: %s is empty!" % (framedata.user_id, framedata.topic_id))

2018-05-02 16:59:24,974 - memory_profile6_log - INFO -    421                                             else:

2018-05-02 16:59:24,974 - memory_profile6_log - INFO -    422                                                 logger.info("Unknows source is selected !")

2018-05-02 16:59:24,976 - memory_profile6_log - INFO -    423                                                 break

2018-05-02 16:59:24,976 - memory_profile6_log - INFO -    424                                     else: 

2018-05-02 16:59:24,977 - memory_profile6_log - INFO -    425                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-05-02 16:59:24,983 - memory_profile6_log - INFO -    426    116.3 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-05-02 16:59:24,983 - memory_profile6_log - INFO -    427    116.3 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-05-02 16:59:24,984 - memory_profile6_log - INFO -    428                             

2018-05-02 16:59:24,986 - memory_profile6_log - INFO -    429    116.3 MiB      0.0 MiB       return datalist, datalist_hist

2018-05-02 16:59:24,986 - memory_profile6_log - INFO - 


2018-05-02 16:59:25,752 - memory_profile6_log - INFO - size of big_frame_hist: 479.88 KB
2018-05-02 16:59:25,763 - memory_profile6_log - INFO - size of big_frame: 2.53 MB
2018-05-02 16:59:25,766 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-05-02 16:59:33,970 - memory_profile6_log - INFO - size of df: 4.93 MB
2018-05-02 16:59:33,973 - memory_profile6_log - INFO - getting total: 20000 training data(current date interest)
2018-05-02 16:59:33,986 - memory_profile6_log - INFO - size of current_frame: 5.09 MB
2018-05-02 16:59:33,987 - memory_profile6_log - INFO - loading time of: 30000 total genuine-current interest data ~ take 190.927s
2018-05-02 16:59:33,992 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 16:59:33,993 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 16:59:33,993 - memory_profile6_log - INFO - ================================================

2018-05-02 16:59:33,994 - memory_profile6_log - INFO -    431     86.6 MiB     86.6 MiB   @profile

2018-05-02 16:59:33,996 - memory_profile6_log - INFO -    432                             def preprocess(cpu, cd, query_fit, date_generated):

2018-05-02 16:59:33,997 - memory_profile6_log - INFO -    433     86.7 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-05-02 16:59:33,997 - memory_profile6_log - INFO -    434     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 16:59:33,999 - memory_profile6_log - INFO -    435                             

2018-05-02 16:59:34,002 - memory_profile6_log - INFO -    436                                 # ~~~ Begin collecting data ~~~

2018-05-02 16:59:34,003 - memory_profile6_log - INFO -    437     86.7 MiB      0.0 MiB       t0 = time.time()

2018-05-02 16:59:34,005 - memory_profile6_log - INFO -    438    116.3 MiB     29.6 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-05-02 16:59:34,006 - memory_profile6_log - INFO -    439    116.3 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-05-02 16:59:34,007 - memory_profile6_log - INFO -    440                                     logger.info("Training cannot be empty..")

2018-05-02 16:59:34,007 - memory_profile6_log - INFO -    441                                     return False

2018-05-02 16:59:34,007 - memory_profile6_log - INFO -    442    116.4 MiB      0.1 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-05-02 16:59:34,009 - memory_profile6_log - INFO -    443                                 # print "big_frame_hist:\n", big_frame_hist.head(20)

2018-05-02 16:59:34,009 - memory_profile6_log - INFO -    444    116.4 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-05-02 16:59:34,009 - memory_profile6_log - INFO -    445                             

2018-05-02 16:59:34,009 - memory_profile6_log - INFO -    446    116.4 MiB      0.0 MiB       big_frame = pd.concat(datalist)

2018-05-02 16:59:34,009 - memory_profile6_log - INFO -    447    116.4 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-05-02 16:59:34,009 - memory_profile6_log - INFO -    448    116.4 MiB      0.0 MiB       del datalist

2018-05-02 16:59:34,010 - memory_profile6_log - INFO -    449                             

2018-05-02 16:59:34,013 - memory_profile6_log - INFO -    450    116.4 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 16:59:34,016 - memory_profile6_log - INFO -    451                             

2018-05-02 16:59:34,016 - memory_profile6_log - INFO -    452                                 # ~ get current news interest ~

2018-05-02 16:59:34,016 - memory_profile6_log - INFO -    453    116.4 MiB      0.0 MiB       if not cd:

2018-05-02 16:59:34,017 - memory_profile6_log - INFO -    454    116.4 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-05-02 16:59:34,019 - memory_profile6_log - INFO -    455    141.3 MiB     24.9 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-05-02 16:59:34,019 - memory_profile6_log - INFO -    456    141.3 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-05-02 16:59:34,019 - memory_profile6_log - INFO -    457                                 else:

2018-05-02 16:59:34,023 - memory_profile6_log - INFO -    458                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-05-02 16:59:34,023 - memory_profile6_log - INFO -    459                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date) limit 10000"

2018-05-02 16:59:34,026 - memory_profile6_log - INFO -    460                             

2018-05-02 16:59:34,026 - memory_profile6_log - INFO -    461                                     # safe handling of query parameter

2018-05-02 16:59:34,026 - memory_profile6_log - INFO -    462                                     query_params = [

2018-05-02 16:59:34,026 - memory_profile6_log - INFO -    463                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-05-02 16:59:34,028 - memory_profile6_log - INFO -    464                                     ]

2018-05-02 16:59:34,029 - memory_profile6_log - INFO -    465                             

2018-05-02 16:59:34,030 - memory_profile6_log - INFO -    466                                     job_config.query_parameters = query_params

2018-05-02 16:59:34,030 - memory_profile6_log - INFO -    467                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-05-02 16:59:34,033 - memory_profile6_log - INFO -    468                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-05-02 16:59:34,036 - memory_profile6_log - INFO -    469                             

2018-05-02 16:59:34,036 - memory_profile6_log - INFO -    470    141.3 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-05-02 16:59:34,036 - memory_profile6_log - INFO -    471    141.3 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-05-02 16:59:34,039 - memory_profile6_log - INFO -    472    141.3 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-05-02 16:59:34,039 - memory_profile6_log - INFO -    473    141.3 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-05-02 16:59:34,039 - memory_profile6_log - INFO -    474    141.3 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 16:59:34,039 - memory_profile6_log - INFO -    475    141.3 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-05-02 16:59:34,040 - memory_profile6_log - INFO -    476                             

2018-05-02 16:59:34,040 - memory_profile6_log - INFO -    477    141.3 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-05-02 16:59:34,045 - memory_profile6_log - INFO - 


2018-05-02 16:59:34,049 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-05-02 16:59:34,055 - memory_profile6_log - INFO - train on: 10000 total genuine interest data(D(u, t))
2018-05-02 16:59:34,056 - memory_profile6_log - INFO - transform on: 20000 total current data(D(t))
2018-05-02 16:59:34,058 - memory_profile6_log - INFO - apply on: 1717 total history...)
2018-05-02 16:59:34,105 - memory_profile6_log - INFO - len of uniques_fit_hist:1717
2018-05-02 16:59:34,109 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:858
2018-05-02 16:59:34,171 - memory_profile6_log - INFO - self.sum_all_nt before concate:

2018-05-02 16:59:34,188 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 16:59:34,190 - memory_profile6_log - INFO - 

2018-05-02 16:59:34,203 - memory_profile6_log - INFO - self.sum_all_nt after concate:

2018-05-02 16:59:34,210 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 16:59:34,211 - memory_profile6_log - INFO - 

2018-05-02 16:59:34,253 - memory_profile6_log - INFO - Len of model_fit: 10000
2018-05-02 16:59:34,256 - memory_profile6_log - INFO - Len of df_dut: 10000
2018-05-02 16:59:34,385 - memory_profile6_log - INFO - Len of fitted_models on main class: 10000
2018-05-02 16:59:34,387 - memory_profile6_log - INFO - 

2018-05-02 16:59:34,388 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-05-02 16:59:34,388 - memory_profile6_log - INFO - 

2018-05-02 16:59:34,391 - memory_profile6_log - INFO - fitted_model_hist:

2018-05-02 16:59:34,391 - memory_profile6_log - INFO - 

2018-05-02 16:59:34,401 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 16:59:34,401 - memory_profile6_log - INFO - 

2018-05-02 16:59:34,411 - memory_profile6_log - INFO - len of fitted models after concat: 11717
2018-05-02 16:59:34,411 - memory_profile6_log - INFO - 

2018-05-02 16:59:34,413 - memory_profile6_log - INFO - Recalculating fitted models...
2018-05-02 16:59:34,414 - memory_profile6_log - INFO - 

2018-05-02 16:59:34,440 - memory_profile6_log - INFO - fitted_models after cobine:

2018-05-02 16:59:34,440 - memory_profile6_log - INFO - 

2018-05-02 16:59:34,450 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 16:59:34,450 - memory_profile6_log - INFO - 

2018-05-02 16:59:34,453 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 10000
2018-05-02 16:59:34,456 - memory_profile6_log - INFO - 

2018-05-02 16:59:36,051 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-05-02 16:59:36,069 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, topic_id, pt_posterior_x_Nt, smoothed_pt_posterior, p0_cat_ci, sigma_Nt, p0_posterior, is_general, rank]
Index: []
2018-05-02 16:59:36,071 - memory_profile6_log - INFO - 

2018-05-02 16:59:36,072 - memory_profile6_log - INFO - Len of model_transform: 5057
2018-05-02 16:59:36,072 - memory_profile6_log - INFO - Len of df_dt: 20000
2018-05-02 16:59:36,073 - memory_profile6_log - INFO - Total train time: 2.003s
2018-05-02 16:59:36,075 - memory_profile6_log - INFO - memory left before cleaning: 84.900 percent memory...
2018-05-02 16:59:36,075 - memory_profile6_log - INFO - cleaning up some objects...
2018-05-02 16:59:36,076 - memory_profile6_log - INFO - deleting df_dut...
2018-05-02 16:59:36,078 - memory_profile6_log - INFO - deleting df_dt...
2018-05-02 16:59:36,082 - memory_profile6_log - INFO - deleting df_input...
2018-05-02 16:59:36,082 - memory_profile6_log - INFO - deleting df_input_X...
2018-05-02 16:59:36,084 - memory_profile6_log - INFO - deleting df_current...
2018-05-02 16:59:36,085 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-05-02 16:59:36,086 - memory_profile6_log - INFO - deleting model_fit...
2018-05-02 16:59:36,088 - memory_profile6_log - INFO - deleting result...
2018-05-02 16:59:36,095 - memory_profile6_log - INFO - list(model_transformsv) before rename: 
2018-05-02 16:59:36,095 - memory_profile6_log - INFO -  
2018-05-02 16:59:36,096 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'is_general', 'p0_posterior', 'rank', 'date']
2018-05-02 16:59:36,098 - memory_profile6_log - INFO - 

2018-05-02 16:59:36,098 - memory_profile6_log - INFO - 5057
2018-05-02 16:59:36,099 - memory_profile6_log - INFO - 

2018-05-02 16:59:36,105 - memory_profile6_log - INFO - list(model_transformsv) after rename: 
2018-05-02 16:59:36,105 - memory_profile6_log - INFO -  
2018-05-02 16:59:36,105 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'topic_is_general', 'interest_score', 'interest_rank', 'interest_score_created_at']
2018-05-02 16:59:36,107 - memory_profile6_log - INFO - 

2018-05-02 16:59:36,108 - memory_profile6_log - INFO - 5057
2018-05-02 16:59:36,108 - memory_profile6_log - INFO - 

2018-05-02 16:59:36,109 - memory_profile6_log - INFO - deleting model_transform...
2018-05-02 16:59:36,111 - memory_profile6_log - INFO - memory left after cleaning: 84.900 percent memory...
2018-05-02 16:59:36,112 - memory_profile6_log - INFO - Begin saving trained data...
2018-05-02 16:59:36,115 - memory_profile6_log - INFO - Using ElasticSearch as storage...
2018-05-02 16:59:36,115 - memory_profile6_log - INFO - Saving fitted_models as history to Elasticsearch...
2018-05-02 16:59:36,161 - memory_profile6_log - INFO -                                            uid_topid  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  1610d721660810-0ce20c8b312a34-14117f59-100200-...          22.153600              32.153600        NaN         1
1  161116e16f2ac7-0b1f1a1b0f9047-4323461-100200-1...       13846.000000           13856.000000   0.000378         3
2  16112f87fa7b6f-0b57fa2e866932-4323461-1fa400-1...          55.231974              65.231974        NaN       603
3  161285256c03f5-065f4919abd206-7c2d6751-1fa400-...          32.502347              42.502347        NaN        64
4  161285256c03f5-065f4919abd206-7c2d6751-1fa400-...          63.223744              73.223744   0.016302        64
2018-05-02 16:59:36,163 - memory_profile6_log - INFO - 

2018-05-02 16:59:36,165 - memory_profile6_log - INFO - 161116e16f2ac7-0b1f1a1b0f9047-4323461-100200-161116e16f33d8_59048170
2018-05-02 16:59:36,167 - memory_profile6_log - INFO - 

2018-05-02 16:59:36,174 - memory_profile6_log - INFO - Saving total data: 10000
2018-05-02 16:59:36,176 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 5
2018-05-02 16:59:36,177 - memory_profile6_log - INFO - processing batch-0
2018-05-02 16:59:37,599 - memory_profile6_log - INFO - index does not exist, creating index
2018-05-02 16:59:37,601 - memory_profile6_log - INFO - 

2018-05-02 16:59:57,826 - memory_profile6_log - INFO - processing batch-1
2018-05-02 17:00:09,825 - memory_profile6_log - INFO - processing batch-2
2018-05-02 17:00:25,032 - memory_profile6_log - INFO - processing batch-3
2018-05-02 17:00:40,256 - memory_profile6_log - INFO - processing batch-4
2018-05-02 17:00:51,871 - memory_profile6_log - INFO - deleting BR...
2018-05-02 17:00:51,871 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-05-02 17:00:51,872 - memory_profile6_log - INFO - deleting fitted_models_sigmant...
2018-05-02 17:00:51,875 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 17:00:51,875 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 17:00:51,875 - memory_profile6_log - INFO - ================================================

2018-05-02 17:00:51,875 - memory_profile6_log - INFO -    113    141.3 MiB    141.3 MiB   @profile

2018-05-02 17:00:51,875 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-05-02 17:00:51,877 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-05-02 17:00:51,877 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-05-02 17:00:51,880 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-05-02 17:00:51,881 - memory_profile6_log - INFO -    118                                      saveto="datastore", fitted_models_hist=None):

2018-05-02 17:00:51,881 - memory_profile6_log - INFO -    119                                 """

2018-05-02 17:00:51,882 - memory_profile6_log - INFO -    120                                     Main Process

2018-05-02 17:00:51,884 - memory_profile6_log - INFO -    121                                 """

2018-05-02 17:00:51,884 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-05-02 17:00:51,887 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-05-02 17:00:51,888 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-05-02 17:00:51,891 - memory_profile6_log - INFO -    125    141.3 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-05-02 17:00:51,891 - memory_profile6_log - INFO -    126    141.3 MiB      0.0 MiB       df_dut = df_input.copy(deep=True)

2018-05-02 17:00:51,894 - memory_profile6_log - INFO -    127    141.3 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 17:00:51,894 - memory_profile6_log - INFO -    128                             

2018-05-02 17:00:51,895 - memory_profile6_log - INFO -    129                                 # D(t)

2018-05-02 17:00:51,895 - memory_profile6_log - INFO -    130    141.3 MiB      0.0 MiB       df_dt = df_current.copy(deep=True)

2018-05-02 17:00:51,898 - memory_profile6_log - INFO -    131    141.3 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 17:00:51,898 - memory_profile6_log - INFO -    132                             

2018-05-02 17:00:51,901 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-05-02 17:00:51,901 - memory_profile6_log - INFO -    134    141.3 MiB      0.0 MiB       t0 = time.time()

2018-05-02 17:00:51,904 - memory_profile6_log - INFO -    135    141.3 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-05-02 17:00:51,904 - memory_profile6_log - INFO -    136    141.3 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-05-02 17:00:51,904 - memory_profile6_log - INFO -    137    141.3 MiB      0.0 MiB       logger.info("apply on: %d total history...)", len(df_hist))

2018-05-02 17:00:51,905 - memory_profile6_log - INFO -    138                             

2018-05-02 17:00:51,905 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-05-02 17:00:51,905 - memory_profile6_log - INFO -    140    141.3 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-05-02 17:00:51,907 - memory_profile6_log - INFO -    141                             

2018-05-02 17:00:51,907 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-05-02 17:00:51,907 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-05-02 17:00:51,907 - memory_profile6_log - INFO -    144    141.7 MiB      0.4 MiB       NB = BR.processX(df_dut)

2018-05-02 17:00:51,907 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-05-02 17:00:51,908 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-05-02 17:00:51,908 - memory_profile6_log - INFO -    147    141.8 MiB      0.1 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-05-02 17:00:51,908 - memory_profile6_log - INFO -    148                                 """

2018-05-02 17:00:51,913 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-05-02 17:00:51,913 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-05-02 17:00:51,914 - memory_profile6_log - INFO -    151                                 """

2018-05-02 17:00:51,915 - memory_profile6_log - INFO -    152    141.8 MiB      0.0 MiB       fitby_sigmant = False

2018-05-02 17:00:51,915 - memory_profile6_log - INFO -    153    141.8 MiB      0.0 MiB       uniques_fit_hist = None

2018-05-02 17:00:51,917 - memory_profile6_log - INFO -    154    141.8 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-05-02 17:00:51,917 - memory_profile6_log - INFO -    155    141.8 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-05-02 17:00:51,917 - memory_profile6_log - INFO -    156    141.8 MiB      0.0 MiB                            'is_general']]

2018-05-02 17:00:51,917 - memory_profile6_log - INFO -    157                             

2018-05-02 17:00:51,917 - memory_profile6_log - INFO -    158                                 # get sigma_Nt from fitted_models_hist

2018-05-02 17:00:51,917 - memory_profile6_log - INFO -    159    141.8 MiB      0.0 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-05-02 17:00:51,918 - memory_profile6_log - INFO -    160    141.8 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-05-02 17:00:51,918 - memory_profile6_log - INFO -    161    141.9 MiB      0.0 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-05-02 17:00:51,920 - memory_profile6_log - INFO -    162    141.9 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-05-02 17:00:51,920 - memory_profile6_log - INFO -    163                             

2018-05-02 17:00:51,921 - memory_profile6_log - INFO -    164                                 # begin fit

2018-05-02 17:00:51,924 - memory_profile6_log - INFO -    165    141.9 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-05-02 17:00:51,927 - memory_profile6_log - INFO -    166    141.9 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-05-02 17:00:51,927 - memory_profile6_log - INFO -    167    142.0 MiB      0.1 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-05-02 17:00:51,928 - memory_profile6_log - INFO -    168    142.0 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-05-02 17:00:51,930 - memory_profile6_log - INFO -    169    142.0 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-05-02 17:00:51,930 - memory_profile6_log - INFO -    170                                 # print model_fit[['num_x', 'num_x', 'sigma_Nt', 'date_all_click']].loc[model_fit['user_id']=="1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157"].head(5)

2018-05-02 17:00:51,930 - memory_profile6_log - INFO -    171                             

2018-05-02 17:00:51,931 - memory_profile6_log - INFO -    172                                 # ~~ and Transform ~~

2018-05-02 17:00:51,933 - memory_profile6_log - INFO -    173                                 #   handling current news interest == current date

2018-05-02 17:00:51,933 - memory_profile6_log - INFO -    174    142.0 MiB      0.0 MiB       if df_dt.empty:

2018-05-02 17:00:51,936 - memory_profile6_log - INFO -    175                                     print "Cek your df_dt, cannot be emtpy!!"

2018-05-02 17:00:51,937 - memory_profile6_log - INFO -    176                                     return None

2018-05-02 17:00:51,937 - memory_profile6_log - INFO -    177    143.0 MiB      1.0 MiB       NB = BR.processX(df_dt)

2018-05-02 17:00:51,938 - memory_profile6_log - INFO -    178    143.0 MiB      0.0 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-05-02 17:00:51,940 - memory_profile6_log - INFO -    179                             

2018-05-02 17:00:51,940 - memory_profile6_log - INFO -    180    143.0 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-05-02 17:00:51,940 - memory_profile6_log - INFO -    181    143.1 MiB      0.1 MiB                            'num_x', 'num_y', 'is_general']]

2018-05-02 17:00:51,941 - memory_profile6_log - INFO -    182                             

2018-05-02 17:00:51,943 - memory_profile6_log - INFO -    183                                 # print "model_fit dtypes:\n", model_fit.dtypes

2018-05-02 17:00:51,943 - memory_profile6_log - INFO -    184                                 # print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-05-02 17:00:51,944 - memory_profile6_log - INFO -    185    143.1 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-05-02 17:00:51,944 - memory_profile6_log - INFO -    186    143.1 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-05-02 17:00:51,947 - memory_profile6_log - INFO -    187    143.1 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-05-02 17:00:51,947 - memory_profile6_log - INFO -    188    143.1 MiB      0.0 MiB                                                                                           "topic_id", "user_id"]],

2018-05-02 17:00:51,950 - memory_profile6_log - INFO -    189    146.5 MiB      3.4 MiB                                                     verbose=False)

2018-05-02 17:00:51,950 - memory_profile6_log - INFO -    190                             

2018-05-02 17:00:51,950 - memory_profile6_log - INFO -    191                                 # ~~~ filter is general and specific topic ~~~

2018-05-02 17:00:51,951 - memory_profile6_log - INFO -    192                                 # the idea is just we need to rerank every topic according

2018-05-02 17:00:51,951 - memory_profile6_log - INFO -    193                                 #    to user_id and and is_general by p0_posterior

2018-05-02 17:00:51,953 - memory_profile6_log - INFO -    194    146.5 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-05-02 17:00:51,953 - memory_profile6_log - INFO -    195    146.5 MiB      0.0 MiB                                     'is_general']].groupby(['topic_id',

2018-05-02 17:00:51,953 - memory_profile6_log - INFO -    196    146.5 MiB      0.0 MiB                                                             'is_general']

2018-05-02 17:00:51,953 - memory_profile6_log - INFO -    197                                                                                      ).size().to_frame().reset_index()

2018-05-02 17:00:51,954 - memory_profile6_log - INFO -    198                             

2018-05-02 17:00:51,954 - memory_profile6_log - INFO -    199                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-05-02 17:00:51,959 - memory_profile6_log - INFO -    200                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-05-02 17:00:51,960 - memory_profile6_log - INFO -    201    146.5 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-05-02 17:00:51,960 - memory_profile6_log - INFO -    202                             

2018-05-02 17:00:51,960 - memory_profile6_log - INFO -    203                                 # ~ start by provide rank for each topic type ~

2018-05-02 17:00:51,960 - memory_profile6_log - INFO -    204    146.6 MiB      0.0 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-05-02 17:00:51,961 - memory_profile6_log - INFO -    205    146.6 MiB      0.0 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-05-02 17:00:51,961 - memory_profile6_log - INFO -    206                             

2018-05-02 17:00:51,963 - memory_profile6_log - INFO -    207                                 # ~ set threshold to filter output

2018-05-02 17:00:51,963 - memory_profile6_log - INFO -    208    146.6 MiB      0.0 MiB       if threshold > 0:

2018-05-02 17:00:51,963 - memory_profile6_log - INFO -    209    146.6 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-05-02 17:00:51,964 - memory_profile6_log - INFO -    210    146.6 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-05-02 17:00:51,964 - memory_profile6_log - INFO -    211    146.6 MiB      0.0 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-05-02 17:00:51,964 - memory_profile6_log - INFO -    212                             

2018-05-02 17:00:51,966 - memory_profile6_log - INFO -    213    146.6 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 17:00:51,966 - memory_profile6_log - INFO -    214    146.6 MiB      0.0 MiB       print model_transform[model_transform['user_id']=='1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157']

2018-05-02 17:00:51,966 - memory_profile6_log - INFO -    215    146.6 MiB      0.0 MiB       logger.info("Len of model_transform: %d", len(model_transform))

2018-05-02 17:00:51,970 - memory_profile6_log - INFO -    216    146.6 MiB      0.0 MiB       logger.info("Len of df_dt: %d", len(df_dt))

2018-05-02 17:00:51,970 - memory_profile6_log - INFO -    217    146.6 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-05-02 17:00:51,971 - memory_profile6_log - INFO -    218                             

2018-05-02 17:00:51,973 - memory_profile6_log - INFO -    219    146.6 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-05-02 17:00:51,973 - memory_profile6_log - INFO -    220                             

2018-05-02 17:00:51,973 - memory_profile6_log - INFO -    221    146.6 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-05-02 17:00:51,973 - memory_profile6_log - INFO -    222    146.6 MiB      0.0 MiB       del df_dut

2018-05-02 17:00:51,974 - memory_profile6_log - INFO -    223    146.6 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-05-02 17:00:51,974 - memory_profile6_log - INFO -    224    146.6 MiB      0.0 MiB       del df_dt

2018-05-02 17:00:51,976 - memory_profile6_log - INFO -    225    146.6 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-05-02 17:00:51,976 - memory_profile6_log - INFO -    226    146.6 MiB      0.0 MiB       del df_input

2018-05-02 17:00:51,976 - memory_profile6_log - INFO -    227    146.6 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-05-02 17:00:51,976 - memory_profile6_log - INFO -    228    146.6 MiB      0.0 MiB       del df_input_X

2018-05-02 17:00:51,976 - memory_profile6_log - INFO -    229    146.6 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-05-02 17:00:51,977 - memory_profile6_log - INFO -    230    146.6 MiB      0.0 MiB       del df_current

2018-05-02 17:00:51,982 - memory_profile6_log - INFO -    231    146.6 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-05-02 17:00:51,982 - memory_profile6_log - INFO -    232    146.6 MiB      0.0 MiB       del map_topic_isgeneral

2018-05-02 17:00:51,983 - memory_profile6_log - INFO -    233    146.6 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-05-02 17:00:51,984 - memory_profile6_log - INFO -    234    146.6 MiB      0.0 MiB       del model_fit

2018-05-02 17:00:51,984 - memory_profile6_log - INFO -    235    146.6 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-05-02 17:00:51,984 - memory_profile6_log - INFO -    236    146.6 MiB      0.0 MiB       del result

2018-05-02 17:00:51,986 - memory_profile6_log - INFO -    237    146.6 MiB      0.0 MiB       logger.info("deleting result...")

2018-05-02 17:00:51,986 - memory_profile6_log - INFO -    238                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 17:00:51,986 - memory_profile6_log - INFO -    239                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 17:00:51,986 - memory_profile6_log - INFO -    240                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 17:00:51,986 - memory_profile6_log - INFO -    241    146.6 MiB      0.0 MiB       if savetrain:

2018-05-02 17:00:51,987 - memory_profile6_log - INFO -    242    146.6 MiB      0.0 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'p0_posterior', 'rank']].copy(deep=True)

2018-05-02 17:00:51,989 - memory_profile6_log - INFO -    243    146.6 MiB      0.0 MiB           model_transformsv['date'] = current_date  # we need manually adding date, because table not support

2018-05-02 17:00:51,989 - memory_profile6_log - INFO -    244    146.6 MiB      0.0 MiB           model_transformsv['date'] = pd.to_datetime(model_transformsv['date'],

2018-05-02 17:00:51,990 - memory_profile6_log - INFO -    245    146.6 MiB      0.0 MiB                                                      format='%Y-%m-%d', errors='coerce')

2018-05-02 17:00:51,993 - memory_profile6_log - INFO -    246    146.6 MiB      0.0 MiB           print "list(model_transformsv) before rename: ", list(model_transformsv)       

2018-05-02 17:00:51,993 - memory_profile6_log - INFO -    247    146.6 MiB      0.0 MiB           print len(model_transformsv)                

2018-05-02 17:00:51,994 - memory_profile6_log - INFO -    248    146.6 MiB      0.0 MiB           model_transformsv = model_transformsv.rename(columns={'is_general': 'topic_is_general', 'p0_posterior': 'interest_score',

2018-05-02 17:00:51,994 - memory_profile6_log - INFO -    249    146.6 MiB      0.0 MiB                                                                 'rank':'interest_rank' , 'date':'interest_score_created_at'})

2018-05-02 17:00:51,996 - memory_profile6_log - INFO -    250    146.6 MiB      0.0 MiB           print "list(model_transformsv) after rename: ", list(model_transformsv)    

2018-05-02 17:00:51,996 - memory_profile6_log - INFO -    251    146.6 MiB      0.0 MiB           print len(model_transformsv) 

2018-05-02 17:00:51,996 - memory_profile6_log - INFO -    252    146.6 MiB      0.0 MiB           del model_transform

2018-05-02 17:00:51,997 - memory_profile6_log - INFO -    253    146.6 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-05-02 17:00:51,997 - memory_profile6_log - INFO -    254    146.6 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-05-02 17:00:51,999 - memory_profile6_log - INFO -    255                             

2018-05-02 17:00:51,999 - memory_profile6_log - INFO -    256    146.6 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-05-02 17:00:51,999 - memory_profile6_log - INFO -    257                                     # ~ Place your code to save the training model here ~

2018-05-02 17:00:52,000 - memory_profile6_log - INFO -    258    146.6 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-05-02 17:00:52,000 - memory_profile6_log - INFO -    259                                         logger.info("Using google datastore as storage...")

2018-05-02 17:00:52,000 - memory_profile6_log - INFO -    260                                         if multproc:

2018-05-02 17:00:52,000 - memory_profile6_log - INFO -    261                                             # ~ save transform models ~

2018-05-02 17:00:52,002 - memory_profile6_log - INFO -    262                                             logger.info("Saving main Transform model to Google DataStore...")

2018-05-02 17:00:52,006 - memory_profile6_log - INFO -    263                                             logger.info("Saving total data: %d", len(model_transformsv))

2018-05-02 17:00:52,006 - memory_profile6_log - INFO -    264                                             mh.saveDataStorePutMulti(model_transformsv)

2018-05-02 17:00:52,009 - memory_profile6_log - INFO -    265                             

2018-05-02 17:00:52,010 - memory_profile6_log - INFO -    266                                             # ~ save fitted models ~

2018-05-02 17:00:52,010 - memory_profile6_log - INFO -    267                                             logger.info("Saving fitted_models as history to Google DataStore...")

2018-05-02 17:00:52,012 - memory_profile6_log - INFO -    268                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 17:00:52,013 - memory_profile6_log - INFO -    269                                             fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-05-02 17:00:52,015 - memory_profile6_log - INFO -    270                                             logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-05-02 17:00:52,016 - memory_profile6_log - INFO -    271                                             X_split = np.array_split(fitted_models_sigmant, 10)

2018-05-02 17:00:52,017 - memory_profile6_log - INFO -    272                                             logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-05-02 17:00:52,019 - memory_profile6_log - INFO -    273                                             for ix in range(len(X_split)):

2018-05-02 17:00:52,019 - memory_profile6_log - INFO -    274                                                 logger.info("processing batch-%d", ix)

2018-05-02 17:00:52,019 - memory_profile6_log - INFO -    275                                                 mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-05-02 17:00:52,019 - memory_profile6_log - INFO -    276                             

2018-05-02 17:00:52,019 - memory_profile6_log - INFO -    277                                             del X_split

2018-05-02 17:00:52,022 - memory_profile6_log - INFO -    278                                             logger.info("deleting X_split...")

2018-05-02 17:00:52,022 - memory_profile6_log - INFO -    279                                             del save_sigma_nt

2018-05-02 17:00:52,023 - memory_profile6_log - INFO -    280                                             logger.info("deleting save_sigma_nt...")

2018-05-02 17:00:52,023 - memory_profile6_log - INFO -    281                             

2018-05-02 17:00:52,023 - memory_profile6_log - INFO -    282                                             del BR

2018-05-02 17:00:52,026 - memory_profile6_log - INFO -    283                                             logger.info("deleting BR...")

2018-05-02 17:00:52,028 - memory_profile6_log - INFO -    284                             

2018-05-02 17:00:52,029 - memory_profile6_log - INFO -    285    146.6 MiB      0.0 MiB           elif str(saveto).lower() == "elastic":

2018-05-02 17:00:52,029 - memory_profile6_log - INFO -    286    146.6 MiB      0.0 MiB               logger.info("Using ElasticSearch as storage...")

2018-05-02 17:00:52,029 - memory_profile6_log - INFO -    287                                         """logging.info("Saving main Transform model to Elasticsearch...")

2018-05-02 17:00:52,030 - memory_profile6_log - INFO -    288                                         # print model_transformsv.head(5)

2018-05-02 17:00:52,032 - memory_profile6_log - INFO -    289                                         

2018-05-02 17:00:52,032 - memory_profile6_log - INFO -    290                                         X_split = np.array_split(model_transformsv, 5)

2018-05-02 17:00:52,032 - memory_profile6_log - INFO -    291                                         logger.info("Saving total data: %d", len(model_transformsv))

2018-05-02 17:00:52,032 - memory_profile6_log - INFO -    292                                         logger.info("Len of X_split for batch save model_transformsv: %d", len(X_split))

2018-05-02 17:00:52,033 - memory_profile6_log - INFO -    293                                         for ix in range(len(X_split)):

2018-05-02 17:00:52,033 - memory_profile6_log - INFO -    294                                             logger.info("processing batch-%d", ix)

2018-05-02 17:00:52,033 - memory_profile6_log - INFO -    295                                             mh.saveElasticS(X_split[ix])

2018-05-02 17:00:52,038 - memory_profile6_log - INFO -    296                                         del X_split

2018-05-02 17:00:52,038 - memory_profile6_log - INFO -    297                                         """

2018-05-02 17:00:52,039 - memory_profile6_log - INFO -    298                             

2018-05-02 17:00:52,039 - memory_profile6_log - INFO -    299    146.6 MiB      0.0 MiB               logger.info("Saving fitted_models as history to Elasticsearch...")

2018-05-02 17:00:52,039 - memory_profile6_log - INFO -    300    146.6 MiB      0.0 MiB               save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 17:00:52,040 - memory_profile6_log - INFO -    301    146.6 MiB      0.0 MiB               fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-05-02 17:00:52,040 - memory_profile6_log - INFO -    302                             

2018-05-02 17:00:52,040 - memory_profile6_log - INFO -    303    146.6 MiB      0.0 MiB               fitted_models_sigmant['uid_topid'] = fitted_models_sigmant["user_id"].map(str) + "_" + fitted_models_sigmant["topic_id"].map(str)

2018-05-02 17:00:52,042 - memory_profile6_log - INFO -    304    146.6 MiB      0.0 MiB               fitted_models_sigmant = fitted_models_sigmant[["uid_topid", "pt_posterior_x_Nt", "smoothed_pt_posterior", "p0_cat_ci", "sigma_Nt"]]

2018-05-02 17:00:52,042 - memory_profile6_log - INFO -    305    146.6 MiB      0.0 MiB               print fitted_models_sigmant.head(5)

2018-05-02 17:00:52,042 - memory_profile6_log - INFO -    306    146.6 MiB      0.0 MiB               print fitted_models_sigmant['uid_topid'].tolist()[1]

2018-05-02 17:00:52,042 - memory_profile6_log - INFO -    307    146.6 MiB      0.0 MiB               X_split = np.array_split(fitted_models_sigmant, 5)

2018-05-02 17:00:52,042 - memory_profile6_log - INFO -    308    146.6 MiB      0.0 MiB               logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-05-02 17:00:52,043 - memory_profile6_log - INFO -    309    146.6 MiB      0.0 MiB               logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-05-02 17:00:52,043 - memory_profile6_log - INFO -    310    148.4 MiB     -7.4 MiB               for ix in range(len(X_split)):

2018-05-02 17:00:52,043 - memory_profile6_log - INFO -    311    148.4 MiB     -7.4 MiB                   logger.info("processing batch-%d", ix)

2018-05-02 17:00:52,045 - memory_profile6_log - INFO -    312    148.4 MiB     -5.6 MiB                   mh.saveElasticS(X_split[ix], esindex_name="fitted_hist_index",  estype_name='fitted_hist_type')

2018-05-02 17:00:52,045 - memory_profile6_log - INFO -    313    148.4 MiB     -0.0 MiB               del X_split

2018-05-02 17:00:52,049 - memory_profile6_log - INFO -    314                                         

2018-05-02 17:00:52,049 - memory_profile6_log - INFO -    315    148.4 MiB      0.0 MiB               del BR

2018-05-02 17:00:52,052 - memory_profile6_log - INFO -    316    148.4 MiB      0.0 MiB               logger.info("deleting BR...")

2018-05-02 17:00:52,052 - memory_profile6_log - INFO -    317    148.4 MiB      0.0 MiB               del save_sigma_nt

2018-05-02 17:00:52,052 - memory_profile6_log - INFO -    318    148.4 MiB      0.0 MiB               logger.info("deleting save_sigma_nt...")

2018-05-02 17:00:52,053 - memory_profile6_log - INFO -    319    148.4 MiB      0.0 MiB               del fitted_models_sigmant

2018-05-02 17:00:52,055 - memory_profile6_log - INFO -    320    148.4 MiB      0.0 MiB               logger.info("deleting fitted_models_sigmant...")

2018-05-02 17:00:52,055 - memory_profile6_log - INFO -    321                             

2018-05-02 17:00:52,056 - memory_profile6_log - INFO -    322                                     # need save sigma_nt for daily train

2018-05-02 17:00:52,056 - memory_profile6_log - INFO -    323                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-05-02 17:00:52,056 - memory_profile6_log - INFO -    324                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-05-02 17:00:52,056 - memory_profile6_log - INFO -    325    148.4 MiB      0.0 MiB           if start_date and end_date:

2018-05-02 17:00:52,059 - memory_profile6_log - INFO -    326                                         if not fitby_sigmant:

2018-05-02 17:00:52,062 - memory_profile6_log - INFO -    327                                             logging.info("Saving sigma Nt...")

2018-05-02 17:00:52,062 - memory_profile6_log - INFO -    328                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 17:00:52,062 - memory_profile6_log - INFO -    329                                             save_sigma_nt['start_date'] = start_date

2018-05-02 17:00:52,063 - memory_profile6_log - INFO -    330                                             save_sigma_nt['end_date'] = end_date

2018-05-02 17:00:52,063 - memory_profile6_log - INFO -    331                                             print save_sigma_nt.head(5)

2018-05-02 17:00:52,065 - memory_profile6_log - INFO -    332                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-05-02 17:00:52,065 - memory_profile6_log - INFO -    333                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-05-02 17:00:52,065 - memory_profile6_log - INFO -    334    148.4 MiB      0.0 MiB       return

2018-05-02 17:00:52,065 - memory_profile6_log - INFO - 


2018-05-02 17:00:52,065 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-05-02 17:07:03,433 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 17:07:03,436 - memory_profile6_log - INFO - date_generated: 
2018-05-02 17:07:03,436 - memory_profile6_log - INFO -  
2018-05-02 17:07:03,436 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 17, 7, 3, 434000)]
2018-05-02 17:07:03,436 - memory_profile6_log - INFO - 

2018-05-02 17:07:03,437 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 17:07:03,437 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 17:07:03,437 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 17:07:03,575 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 17:07:03,578 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 17:07:09,801 - memory_profile6_log - INFO - size of df: 2.53 MB
2018-05-02 17:07:09,802 - memory_profile6_log - INFO - getting total: 10000 training data(genuine interest) for date: 2018-05-01
2018-05-02 17:07:09,822 - memory_profile6_log - INFO - loading history data from datastore...
2018-05-02 17:07:09,822 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-05-02 17:07:09,825 - memory_profile6_log - INFO - Appending history data...
2018-05-02 17:07:09,825 - memory_profile6_log - INFO - processing batch-0
2018-05-02 17:07:09,826 - memory_profile6_log - INFO - creating list history data...
2018-05-02 17:07:09,908 - memory_profile6_log - INFO - call history data...
2018-05-02 17:07:45,053 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 17:07:45,723 - memory_profile6_log - INFO - processing batch-1
2018-05-02 17:07:45,723 - memory_profile6_log - INFO - creating list history data...
2018-05-02 17:07:45,733 - memory_profile6_log - INFO - call history data...
2018-05-02 17:08:22,568 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 17:08:23,206 - memory_profile6_log - INFO - processing batch-2
2018-05-02 17:08:23,207 - memory_profile6_log - INFO - creating list history data...
2018-05-02 17:08:23,216 - memory_profile6_log - INFO - call history data...
2018-05-02 17:08:58,582 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 17:08:58,984 - memory_profile6_log - INFO - processing batch-3
2018-05-02 17:08:58,986 - memory_profile6_log - INFO - creating list history data...
2018-05-02 17:08:58,993 - memory_profile6_log - INFO - call history data...
2018-05-02 17:09:34,809 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 17:09:35,200 - memory_profile6_log - INFO - processing batch-4
2018-05-02 17:09:35,201 - memory_profile6_log - INFO - creating list history data...
2018-05-02 17:09:35,210 - memory_profile6_log - INFO - call history data...
2018-05-02 17:10:09,762 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 17:10:10,046 - memory_profile6_log - INFO - Appending training data...
2018-05-02 17:10:10,046 - memory_profile6_log - INFO - len datalist: 1
2018-05-02 17:10:10,049 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-05-02 17:10:10,049 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 17:10:10,051 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 17:10:10,052 - memory_profile6_log - INFO - ================================================

2018-05-02 17:10:10,052 - memory_profile6_log - INFO -    360     86.7 MiB     86.7 MiB   @profile

2018-05-02 17:10:10,055 - memory_profile6_log - INFO -    361                             def BQPreprocess(cpu, date_generated, client, query_fit, loadfrom="datastore"):

2018-05-02 17:10:10,055 - memory_profile6_log - INFO -    362     86.7 MiB      0.0 MiB       bq_client = client

2018-05-02 17:10:10,058 - memory_profile6_log - INFO -    363     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 17:10:10,059 - memory_profile6_log - INFO -    364                             

2018-05-02 17:10:10,062 - memory_profile6_log - INFO -    365     86.7 MiB      0.0 MiB       datalist = []

2018-05-02 17:10:10,062 - memory_profile6_log - INFO -    366     86.7 MiB      0.0 MiB       datalist_hist = []

2018-05-02 17:10:10,063 - memory_profile6_log - INFO -    367                             

2018-05-02 17:10:10,065 - memory_profile6_log - INFO -    368     86.7 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-05-02 17:10:10,065 - memory_profile6_log - INFO -    369    116.9 MiB      0.0 MiB       for ndate in date_generated:

2018-05-02 17:10:10,066 - memory_profile6_log - INFO -    370    116.2 MiB     29.5 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-05-02 17:10:10,072 - memory_profile6_log - INFO -    371    116.2 MiB      0.0 MiB           if tframe is not None:

2018-05-02 17:10:10,072 - memory_profile6_log - INFO -    372    116.2 MiB      0.0 MiB               if not tframe.empty:

2018-05-02 17:10:10,073 - memory_profile6_log - INFO -    373    116.2 MiB      0.0 MiB                   if loadfrom.strip().lower() == 'datastore':

2018-05-02 17:10:10,075 - memory_profile6_log - INFO -    374    116.2 MiB      0.1 MiB                       X_split = np.array_split(tframe, 5)

2018-05-02 17:10:10,078 - memory_profile6_log - INFO -    375    116.2 MiB      0.0 MiB                       logger.info("loading history data from datastore...")

2018-05-02 17:10:10,078 - memory_profile6_log - INFO -    376    116.2 MiB      0.0 MiB                       logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 17:10:10,082 - memory_profile6_log - INFO -    377    116.2 MiB      0.0 MiB                       logger.info("Appending history data...")

2018-05-02 17:10:10,082 - memory_profile6_log - INFO -    378    116.9 MiB     -0.2 MiB                       for ix in range(len(X_split)):

2018-05-02 17:10:10,085 - memory_profile6_log - INFO -    379                                                     # ~ loading history

2018-05-02 17:10:10,085 - memory_profile6_log - INFO -    380                                                     """

2018-05-02 17:10:10,088 - memory_profile6_log - INFO -    381                                                         disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-05-02 17:10:10,089 - memory_profile6_log - INFO -    382                                                     """

2018-05-02 17:10:10,091 - memory_profile6_log - INFO -    383    116.9 MiB     -0.1 MiB                           logger.info("processing batch-%d", ix)

2018-05-02 17:10:10,092 - memory_profile6_log - INFO -    384                                                     # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-05-02 17:10:10,092 - memory_profile6_log - INFO -    385    116.9 MiB     -0.1 MiB                           logger.info("creating list history data...")

2018-05-02 17:10:10,094 - memory_profile6_log - INFO -    386    116.9 MiB      0.2 MiB                           lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-05-02 17:10:10,095 - memory_profile6_log - INFO -    387                                                     #lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 17:10:10,098 - memory_profile6_log - INFO -    388                             

2018-05-02 17:10:10,099 - memory_profile6_log - INFO -    389    116.9 MiB     -0.1 MiB                           logger.info("call history data...")

2018-05-02 17:10:10,102 - memory_profile6_log - INFO -    390    116.9 MiB      0.1 MiB                           h_frame = mh.loadDSHistory(lhistory)

2018-05-02 17:10:10,104 - memory_profile6_log - INFO -    391                             

2018-05-02 17:10:10,105 - memory_profile6_log - INFO -    392                                                     # me = os.getpid()

2018-05-02 17:10:10,107 - memory_profile6_log - INFO -    393                                                     # kill_proc_tree(me)

2018-05-02 17:10:10,109 - memory_profile6_log - INFO -    394                             

2018-05-02 17:10:10,114 - memory_profile6_log - INFO -    395    116.9 MiB     -0.0 MiB                           logger.info("done collecting history data, appending now...")

2018-05-02 17:10:10,115 - memory_profile6_log - INFO -    396    116.9 MiB    -26.4 MiB                           for m in h_frame:

2018-05-02 17:10:10,115 - memory_profile6_log - INFO -    397    116.9 MiB    -26.4 MiB                               if m is not None:

2018-05-02 17:10:10,117 - memory_profile6_log - INFO -    398    116.9 MiB    -26.4 MiB                                   if len(m) > 0:

2018-05-02 17:10:10,118 - memory_profile6_log - INFO -    399    116.9 MiB    -17.4 MiB                                       datalist_hist.append(pd.DataFrame(m))

2018-05-02 17:10:10,121 - memory_profile6_log - INFO -    400    116.9 MiB     -0.2 MiB                           del h_frame

2018-05-02 17:10:10,125 - memory_profile6_log - INFO -    401    116.9 MiB     -0.2 MiB                           del lhistory

2018-05-02 17:10:10,130 - memory_profile6_log - INFO -    402                             

2018-05-02 17:10:10,132 - memory_profile6_log - INFO -    403    116.9 MiB     -0.0 MiB                       logger.info("Appending training data...")

2018-05-02 17:10:10,140 - memory_profile6_log - INFO -    404    116.9 MiB      0.0 MiB                       datalist.append(tframe)

2018-05-02 17:10:10,141 - memory_profile6_log - INFO -    405                                             elif loadfrom.strip().lower() == 'elastic':

2018-05-02 17:10:10,148 - memory_profile6_log - INFO -    406                                                 X_split = np.array_split(tframe, 5)

2018-05-02 17:10:10,148 - memory_profile6_log - INFO -    407                                                 logger.info("loading history data from elastic...")

2018-05-02 17:10:10,148 - memory_profile6_log - INFO -    408                                                 logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 17:10:10,150 - memory_profile6_log - INFO -    409                                                 logger.info("Appending history data...")

2018-05-02 17:10:10,153 - memory_profile6_log - INFO -    410                                                 for ix in range(len(X_split)):

2018-05-02 17:10:10,154 - memory_profile6_log - INFO -    411                                                     for framedata in X_split[ix].itertuples():

2018-05-02 17:10:10,155 - memory_profile6_log - INFO -    412                                                         inside_data = mh.loadESHistory(framedata.user_id, framedata.topic_id ,

2018-05-02 17:10:10,157 - memory_profile6_log - INFO -    413                                                                                        esindex_name='fitted_hist_index',

2018-05-02 17:10:10,157 - memory_profile6_log - INFO -    414                                                                                        estype_name='fitted_hist_type')

2018-05-02 17:10:10,157 - memory_profile6_log - INFO -    415                             

2018-05-02 17:10:10,158 - memory_profile6_log - INFO -    416                                                         if not inside_data.empty:

2018-05-02 17:10:10,160 - memory_profile6_log - INFO -    417                                                             datalist_hist.append(inside_data)

2018-05-02 17:10:10,164 - memory_profile6_log - INFO -    418                                                             del inside_data

2018-05-02 17:10:10,165 - memory_profile6_log - INFO -    419                                                         else:

2018-05-02 17:10:10,171 - memory_profile6_log - INFO -    420                                                             logger.info("uid: %s with topic_id: %s is empty!" % (framedata.user_id, framedata.topic_id))

2018-05-02 17:10:10,174 - memory_profile6_log - INFO -    421                                             else:

2018-05-02 17:10:10,177 - memory_profile6_log - INFO -    422                                                 logger.info("Unknows source is selected !")

2018-05-02 17:10:10,180 - memory_profile6_log - INFO -    423                                                 break

2018-05-02 17:10:10,183 - memory_profile6_log - INFO -    424                                     else: 

2018-05-02 17:10:10,190 - memory_profile6_log - INFO -    425                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-05-02 17:10:10,190 - memory_profile6_log - INFO -    426    116.9 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-05-02 17:10:10,191 - memory_profile6_log - INFO -    427    116.9 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-05-02 17:10:10,194 - memory_profile6_log - INFO -    428                             

2018-05-02 17:10:10,196 - memory_profile6_log - INFO -    429    116.9 MiB      0.0 MiB       return datalist, datalist_hist

2018-05-02 17:10:10,198 - memory_profile6_log - INFO - 


2018-05-02 17:10:10,950 - memory_profile6_log - INFO - size of big_frame_hist: 479.88 KB
2018-05-02 17:10:10,963 - memory_profile6_log - INFO - size of big_frame: 2.53 MB
2018-05-02 17:10:10,967 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-05-02 17:10:20,983 - memory_profile6_log - INFO - size of df: 4.93 MB
2018-05-02 17:10:20,983 - memory_profile6_log - INFO - getting total: 20000 training data(current date interest)
2018-05-02 17:10:20,997 - memory_profile6_log - INFO - size of current_frame: 5.09 MB
2018-05-02 17:10:20,999 - memory_profile6_log - INFO - loading time of: 30000 total genuine-current interest data ~ take 197.458s
2018-05-02 17:10:21,003 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 17:10:21,003 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 17:10:21,005 - memory_profile6_log - INFO - ================================================

2018-05-02 17:10:21,006 - memory_profile6_log - INFO -    431     86.6 MiB     86.6 MiB   @profile

2018-05-02 17:10:21,006 - memory_profile6_log - INFO -    432                             def preprocess(cpu, cd, query_fit, date_generated):

2018-05-02 17:10:21,009 - memory_profile6_log - INFO -    433     86.7 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-05-02 17:10:21,009 - memory_profile6_log - INFO -    434     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 17:10:21,009 - memory_profile6_log - INFO -    435                             

2018-05-02 17:10:21,009 - memory_profile6_log - INFO -    436                                 # ~~~ Begin collecting data ~~~

2018-05-02 17:10:21,010 - memory_profile6_log - INFO -    437     86.7 MiB      0.0 MiB       t0 = time.time()

2018-05-02 17:10:21,010 - memory_profile6_log - INFO -    438    116.9 MiB     30.2 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-05-02 17:10:21,010 - memory_profile6_log - INFO -    439    116.9 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-05-02 17:10:21,013 - memory_profile6_log - INFO -    440                                     logger.info("Training cannot be empty..")

2018-05-02 17:10:21,015 - memory_profile6_log - INFO -    441                                     return False

2018-05-02 17:10:21,016 - memory_profile6_log - INFO -    442    117.0 MiB      0.1 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-05-02 17:10:21,016 - memory_profile6_log - INFO -    443                                 # print "big_frame_hist:\n", big_frame_hist.head(20)

2018-05-02 17:10:21,017 - memory_profile6_log - INFO -    444    117.0 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-05-02 17:10:21,017 - memory_profile6_log - INFO -    445                             

2018-05-02 17:10:21,017 - memory_profile6_log - INFO -    446    117.0 MiB      0.0 MiB       big_frame = pd.concat(datalist)

2018-05-02 17:10:21,017 - memory_profile6_log - INFO -    447    117.0 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-05-02 17:10:21,019 - memory_profile6_log - INFO -    448    117.0 MiB      0.0 MiB       del datalist

2018-05-02 17:10:21,019 - memory_profile6_log - INFO -    449                             

2018-05-02 17:10:21,019 - memory_profile6_log - INFO -    450    117.0 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 17:10:21,019 - memory_profile6_log - INFO -    451                             

2018-05-02 17:10:21,019 - memory_profile6_log - INFO -    452                                 # ~ get current news interest ~

2018-05-02 17:10:21,019 - memory_profile6_log - INFO -    453    117.0 MiB      0.0 MiB       if not cd:

2018-05-02 17:10:21,020 - memory_profile6_log - INFO -    454    117.0 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-05-02 17:10:21,020 - memory_profile6_log - INFO -    455    141.9 MiB     24.9 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-05-02 17:10:21,020 - memory_profile6_log - INFO -    456    141.9 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-05-02 17:10:21,020 - memory_profile6_log - INFO -    457                                 else:

2018-05-02 17:10:21,022 - memory_profile6_log - INFO -    458                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-05-02 17:10:21,022 - memory_profile6_log - INFO -    459                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date) limit 10000"

2018-05-02 17:10:21,022 - memory_profile6_log - INFO -    460                             

2018-05-02 17:10:21,023 - memory_profile6_log - INFO -    461                                     # safe handling of query parameter

2018-05-02 17:10:21,026 - memory_profile6_log - INFO -    462                                     query_params = [

2018-05-02 17:10:21,029 - memory_profile6_log - INFO -    463                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-05-02 17:10:21,030 - memory_profile6_log - INFO -    464                                     ]

2018-05-02 17:10:21,030 - memory_profile6_log - INFO -    465                             

2018-05-02 17:10:21,032 - memory_profile6_log - INFO -    466                                     job_config.query_parameters = query_params

2018-05-02 17:10:21,032 - memory_profile6_log - INFO -    467                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-05-02 17:10:21,032 - memory_profile6_log - INFO -    468                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-05-02 17:10:21,033 - memory_profile6_log - INFO -    469                             

2018-05-02 17:10:21,033 - memory_profile6_log - INFO -    470    141.9 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-05-02 17:10:21,036 - memory_profile6_log - INFO -    471    141.9 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-05-02 17:10:21,039 - memory_profile6_log - INFO -    472    141.9 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-05-02 17:10:21,039 - memory_profile6_log - INFO -    473    141.9 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-05-02 17:10:21,039 - memory_profile6_log - INFO -    474    141.9 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 17:10:21,039 - memory_profile6_log - INFO -    475    141.9 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-05-02 17:10:21,039 - memory_profile6_log - INFO -    476                             

2018-05-02 17:10:21,039 - memory_profile6_log - INFO -    477    141.9 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-05-02 17:10:21,042 - memory_profile6_log - INFO - 


2018-05-02 17:10:21,046 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-05-02 17:10:21,053 - memory_profile6_log - INFO - train on: 10000 total genuine interest data(D(u, t))
2018-05-02 17:10:21,055 - memory_profile6_log - INFO - transform on: 20000 total current data(D(t))
2018-05-02 17:10:21,058 - memory_profile6_log - INFO - apply on: 1717 total history...)
2018-05-02 17:10:21,102 - memory_profile6_log - INFO - len of uniques_fit_hist:1717
2018-05-02 17:10:21,108 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:858
2018-05-02 17:10:21,176 - memory_profile6_log - INFO - self.sum_all_nt before concate:

2018-05-02 17:10:21,194 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 17:10:21,194 - memory_profile6_log - INFO - 

2018-05-02 17:10:21,209 - memory_profile6_log - INFO - self.sum_all_nt after concate:

2018-05-02 17:10:21,217 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 17:10:21,217 - memory_profile6_log - INFO - 

2018-05-02 17:10:21,259 - memory_profile6_log - INFO - Len of model_fit: 10000
2018-05-02 17:10:21,260 - memory_profile6_log - INFO - Len of df_dut: 10000
2018-05-02 17:10:21,397 - memory_profile6_log - INFO - Len of fitted_models on main class: 10000
2018-05-02 17:10:21,398 - memory_profile6_log - INFO - 

2018-05-02 17:10:21,398 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-05-02 17:10:21,400 - memory_profile6_log - INFO - 

2018-05-02 17:10:21,401 - memory_profile6_log - INFO - fitted_model_hist:

2018-05-02 17:10:21,401 - memory_profile6_log - INFO - 

2018-05-02 17:10:21,410 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 17:10:21,411 - memory_profile6_log - INFO - 

2018-05-02 17:10:21,418 - memory_profile6_log - INFO - len of fitted models after concat: 11717
2018-05-02 17:10:21,420 - memory_profile6_log - INFO - 

2018-05-02 17:10:21,420 - memory_profile6_log - INFO - Recalculating fitted models...
2018-05-02 17:10:21,421 - memory_profile6_log - INFO - 

2018-05-02 17:10:21,447 - memory_profile6_log - INFO - fitted_models after cobine:

2018-05-02 17:10:21,448 - memory_profile6_log - INFO - 

2018-05-02 17:10:21,457 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 17:10:21,459 - memory_profile6_log - INFO - 

2018-05-02 17:10:21,460 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 10000
2018-05-02 17:10:21,461 - memory_profile6_log - INFO - 

2018-05-02 17:10:23,003 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-05-02 17:10:23,020 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, topic_id, pt_posterior_x_Nt, smoothed_pt_posterior, p0_cat_ci, sigma_Nt, p0_posterior, is_general, rank]
Index: []
2018-05-02 17:10:23,023 - memory_profile6_log - INFO - 

2018-05-02 17:10:23,023 - memory_profile6_log - INFO - Len of model_transform: 5057
2018-05-02 17:10:23,025 - memory_profile6_log - INFO - Len of df_dt: 20000
2018-05-02 17:10:23,026 - memory_profile6_log - INFO - Total train time: 1.958s
2018-05-02 17:10:23,026 - memory_profile6_log - INFO - memory left before cleaning: 85.000 percent memory...
2018-05-02 17:10:23,028 - memory_profile6_log - INFO - cleaning up some objects...
2018-05-02 17:10:23,029 - memory_profile6_log - INFO - deleting df_dut...
2018-05-02 17:10:23,029 - memory_profile6_log - INFO - deleting df_dt...
2018-05-02 17:10:23,030 - memory_profile6_log - INFO - deleting df_input...
2018-05-02 17:10:23,033 - memory_profile6_log - INFO - deleting df_input_X...
2018-05-02 17:10:23,035 - memory_profile6_log - INFO - deleting df_current...
2018-05-02 17:10:23,036 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-05-02 17:10:23,038 - memory_profile6_log - INFO - deleting model_fit...
2018-05-02 17:10:23,039 - memory_profile6_log - INFO - deleting result...
2018-05-02 17:10:23,046 - memory_profile6_log - INFO - list(model_transformsv) before rename: 
2018-05-02 17:10:23,048 - memory_profile6_log - INFO -  
2018-05-02 17:10:23,049 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'is_general', 'p0_posterior', 'rank', 'date']
2018-05-02 17:10:23,049 - memory_profile6_log - INFO - 

2018-05-02 17:10:23,051 - memory_profile6_log - INFO - 5057
2018-05-02 17:10:23,052 - memory_profile6_log - INFO - 

2018-05-02 17:10:23,055 - memory_profile6_log - INFO - list(model_transformsv) after rename: 
2018-05-02 17:10:23,056 - memory_profile6_log - INFO -  
2018-05-02 17:10:23,058 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'topic_is_general', 'interest_score', 'interest_rank', 'interest_score_created_at']
2018-05-02 17:10:23,059 - memory_profile6_log - INFO - 

2018-05-02 17:10:23,059 - memory_profile6_log - INFO - 5057
2018-05-02 17:10:23,061 - memory_profile6_log - INFO - 

2018-05-02 17:10:23,065 - memory_profile6_log - INFO - deleting model_transform...
2018-05-02 17:10:23,066 - memory_profile6_log - INFO - memory left after cleaning: 85.000 percent memory...
2018-05-02 17:10:23,068 - memory_profile6_log - INFO - Begin saving trained data...
2018-05-02 17:10:23,069 - memory_profile6_log - INFO - Using ElasticSearch as storage...
2018-05-02 17:10:23,071 - memory_profile6_log - INFO - Saving fitted_models as history to Elasticsearch...
2018-05-02 17:10:23,117 - memory_profile6_log - INFO -                                            uid_topid  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  1610d721660810-0ce20c8b312a34-14117f59-100200-...          22.153600              32.153600        NaN         1
1  161116e16f2ac7-0b1f1a1b0f9047-4323461-100200-1...       13846.000000           13856.000000   0.000378         3
2  16112f87fa7b6f-0b57fa2e866932-4323461-1fa400-1...          55.231974              65.231974        NaN       603
3  161285256c03f5-065f4919abd206-7c2d6751-1fa400-...          32.502347              42.502347        NaN        64
4  161285256c03f5-065f4919abd206-7c2d6751-1fa400-...          63.223744              73.223744   0.016302        64
2018-05-02 17:10:23,118 - memory_profile6_log - INFO - 

2018-05-02 17:10:23,119 - memory_profile6_log - INFO - 161116e16f2ac7-0b1f1a1b0f9047-4323461-100200-161116e16f33d8_59048170
2018-05-02 17:10:23,121 - memory_profile6_log - INFO - 

2018-05-02 17:10:23,128 - memory_profile6_log - INFO - Saving total data: 10000
2018-05-02 17:10:23,131 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 5
2018-05-02 17:10:23,134 - memory_profile6_log - INFO - processing batch-0
2018-05-02 17:10:34,454 - memory_profile6_log - INFO - processing batch-1
2018-05-02 17:10:49,365 - memory_profile6_log - INFO - processing batch-2
2018-05-02 17:11:01,838 - memory_profile6_log - INFO - processing batch-3
2018-05-02 17:11:10,197 - memory_profile6_log - INFO - processing batch-4
2018-05-02 17:11:22,572 - memory_profile6_log - INFO - deleting BR...
2018-05-02 17:11:22,572 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-05-02 17:11:22,575 - memory_profile6_log - INFO - deleting fitted_models_sigmant...
2018-05-02 17:11:22,575 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 17:11:22,575 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 17:11:22,575 - memory_profile6_log - INFO - ================================================

2018-05-02 17:11:22,576 - memory_profile6_log - INFO -    113    141.9 MiB    141.9 MiB   @profile

2018-05-02 17:11:22,576 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-05-02 17:11:22,578 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-05-02 17:11:22,578 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-05-02 17:11:22,578 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-05-02 17:11:22,581 - memory_profile6_log - INFO -    118                                      saveto="datastore", fitted_models_hist=None):

2018-05-02 17:11:22,582 - memory_profile6_log - INFO -    119                                 """

2018-05-02 17:11:22,582 - memory_profile6_log - INFO -    120                                     Main Process

2018-05-02 17:11:22,582 - memory_profile6_log - INFO -    121                                 """

2018-05-02 17:11:22,582 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-05-02 17:11:22,582 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-05-02 17:11:22,584 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-05-02 17:11:22,584 - memory_profile6_log - INFO -    125    141.9 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-05-02 17:11:22,585 - memory_profile6_log - INFO -    126    141.9 MiB      0.0 MiB       df_dut = df_input.copy(deep=True)

2018-05-02 17:11:22,585 - memory_profile6_log - INFO -    127    141.9 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 17:11:22,585 - memory_profile6_log - INFO -    128                             

2018-05-02 17:11:22,594 - memory_profile6_log - INFO -    129                                 # D(t)

2018-05-02 17:11:22,595 - memory_profile6_log - INFO -    130    141.9 MiB      0.0 MiB       df_dt = df_current.copy(deep=True)

2018-05-02 17:11:22,595 - memory_profile6_log - INFO -    131    141.9 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 17:11:22,595 - memory_profile6_log - INFO -    132                             

2018-05-02 17:11:22,596 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-05-02 17:11:22,596 - memory_profile6_log - INFO -    134    141.9 MiB      0.0 MiB       t0 = time.time()

2018-05-02 17:11:22,596 - memory_profile6_log - INFO -    135    141.9 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-05-02 17:11:22,598 - memory_profile6_log - INFO -    136    141.9 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-05-02 17:11:22,598 - memory_profile6_log - INFO -    137    141.9 MiB      0.0 MiB       logger.info("apply on: %d total history...)", len(df_hist))

2018-05-02 17:11:22,598 - memory_profile6_log - INFO -    138                             

2018-05-02 17:11:22,598 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-05-02 17:11:22,598 - memory_profile6_log - INFO -    140    141.9 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-05-02 17:11:22,599 - memory_profile6_log - INFO -    141                             

2018-05-02 17:11:22,599 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-05-02 17:11:22,601 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-05-02 17:11:22,601 - memory_profile6_log - INFO -    144    142.3 MiB      0.3 MiB       NB = BR.processX(df_dut)

2018-05-02 17:11:22,601 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-05-02 17:11:22,601 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-05-02 17:11:22,601 - memory_profile6_log - INFO -    147    142.4 MiB      0.1 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-05-02 17:11:22,602 - memory_profile6_log - INFO -    148                                 """

2018-05-02 17:11:22,607 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-05-02 17:11:22,608 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-05-02 17:11:22,608 - memory_profile6_log - INFO -    151                                 """

2018-05-02 17:11:22,608 - memory_profile6_log - INFO -    152    142.4 MiB      0.0 MiB       fitby_sigmant = False

2018-05-02 17:11:22,608 - memory_profile6_log - INFO -    153    142.4 MiB      0.0 MiB       uniques_fit_hist = None

2018-05-02 17:11:22,608 - memory_profile6_log - INFO -    154    142.4 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-05-02 17:11:22,609 - memory_profile6_log - INFO -    155    142.4 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-05-02 17:11:22,609 - memory_profile6_log - INFO -    156    142.4 MiB      0.0 MiB                            'is_general']]

2018-05-02 17:11:22,609 - memory_profile6_log - INFO -    157                             

2018-05-02 17:11:22,611 - memory_profile6_log - INFO -    158                                 # get sigma_Nt from fitted_models_hist

2018-05-02 17:11:22,611 - memory_profile6_log - INFO -    159    142.4 MiB      0.0 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-05-02 17:11:22,611 - memory_profile6_log - INFO -    160    142.4 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-05-02 17:11:22,611 - memory_profile6_log - INFO -    161    142.4 MiB      0.0 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-05-02 17:11:22,611 - memory_profile6_log - INFO -    162    142.4 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-05-02 17:11:22,612 - memory_profile6_log - INFO -    163                             

2018-05-02 17:11:22,612 - memory_profile6_log - INFO -    164                                 # begin fit

2018-05-02 17:11:22,612 - memory_profile6_log - INFO -    165    142.4 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-05-02 17:11:22,614 - memory_profile6_log - INFO -    166    142.4 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-05-02 17:11:22,614 - memory_profile6_log - INFO -    167    142.7 MiB      0.2 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-05-02 17:11:22,614 - memory_profile6_log - INFO -    168    142.7 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-05-02 17:11:22,615 - memory_profile6_log - INFO -    169    142.7 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-05-02 17:11:22,615 - memory_profile6_log - INFO -    170                                 # print model_fit[['num_x', 'num_x', 'sigma_Nt', 'date_all_click']].loc[model_fit['user_id']=="1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157"].head(5)

2018-05-02 17:11:22,615 - memory_profile6_log - INFO -    171                             

2018-05-02 17:11:22,621 - memory_profile6_log - INFO -    172                                 # ~~ and Transform ~~

2018-05-02 17:11:22,621 - memory_profile6_log - INFO -    173                                 #   handling current news interest == current date

2018-05-02 17:11:22,621 - memory_profile6_log - INFO -    174    142.7 MiB      0.0 MiB       if df_dt.empty:

2018-05-02 17:11:22,621 - memory_profile6_log - INFO -    175                                     print "Cek your df_dt, cannot be emtpy!!"

2018-05-02 17:11:22,621 - memory_profile6_log - INFO -    176                                     return None

2018-05-02 17:11:22,622 - memory_profile6_log - INFO -    177    143.4 MiB      0.8 MiB       NB = BR.processX(df_dt)

2018-05-02 17:11:22,622 - memory_profile6_log - INFO -    178    143.5 MiB      0.1 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-05-02 17:11:22,624 - memory_profile6_log - INFO -    179                             

2018-05-02 17:11:22,624 - memory_profile6_log - INFO -    180    143.5 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-05-02 17:11:22,625 - memory_profile6_log - INFO -    181    143.5 MiB      0.0 MiB                            'num_x', 'num_y', 'is_general']]

2018-05-02 17:11:22,625 - memory_profile6_log - INFO -    182                             

2018-05-02 17:11:22,625 - memory_profile6_log - INFO -    183                                 # print "model_fit dtypes:\n", model_fit.dtypes

2018-05-02 17:11:22,625 - memory_profile6_log - INFO -    184                                 # print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-05-02 17:11:22,627 - memory_profile6_log - INFO -    185    143.5 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-05-02 17:11:22,627 - memory_profile6_log - INFO -    186    143.5 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-05-02 17:11:22,627 - memory_profile6_log - INFO -    187    143.5 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-05-02 17:11:22,628 - memory_profile6_log - INFO -    188    143.5 MiB      0.0 MiB                                                                                           "topic_id", "user_id"]],

2018-05-02 17:11:22,628 - memory_profile6_log - INFO -    189    146.7 MiB      3.2 MiB                                                     verbose=False)

2018-05-02 17:11:22,628 - memory_profile6_log - INFO -    190                             

2018-05-02 17:11:22,632 - memory_profile6_log - INFO -    191                                 # ~~~ filter is general and specific topic ~~~

2018-05-02 17:11:22,632 - memory_profile6_log - INFO -    192                                 # the idea is just we need to rerank every topic according

2018-05-02 17:11:22,634 - memory_profile6_log - INFO -    193                                 #    to user_id and and is_general by p0_posterior

2018-05-02 17:11:22,634 - memory_profile6_log - INFO -    194    146.7 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-05-02 17:11:22,634 - memory_profile6_log - INFO -    195    146.7 MiB      0.0 MiB                                     'is_general']].groupby(['topic_id',

2018-05-02 17:11:22,635 - memory_profile6_log - INFO -    196    146.7 MiB      0.0 MiB                                                             'is_general']

2018-05-02 17:11:22,638 - memory_profile6_log - INFO -    197                                                                                      ).size().to_frame().reset_index()

2018-05-02 17:11:22,638 - memory_profile6_log - INFO -    198                             

2018-05-02 17:11:22,638 - memory_profile6_log - INFO -    199                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-05-02 17:11:22,640 - memory_profile6_log - INFO -    200                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-05-02 17:11:22,641 - memory_profile6_log - INFO -    201    146.7 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-05-02 17:11:22,644 - memory_profile6_log - INFO -    202                             

2018-05-02 17:11:22,644 - memory_profile6_log - INFO -    203                                 # ~ start by provide rank for each topic type ~

2018-05-02 17:11:22,644 - memory_profile6_log - INFO -    204    146.8 MiB      0.1 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-05-02 17:11:22,645 - memory_profile6_log - INFO -    205    146.8 MiB      0.0 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-05-02 17:11:22,647 - memory_profile6_log - INFO -    206                             

2018-05-02 17:11:22,647 - memory_profile6_log - INFO -    207                                 # ~ set threshold to filter output

2018-05-02 17:11:22,648 - memory_profile6_log - INFO -    208    146.8 MiB      0.0 MiB       if threshold > 0:

2018-05-02 17:11:22,648 - memory_profile6_log - INFO -    209    146.8 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-05-02 17:11:22,650 - memory_profile6_log - INFO -    210    146.8 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-05-02 17:11:22,651 - memory_profile6_log - INFO -    211    146.8 MiB      0.0 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-05-02 17:11:22,654 - memory_profile6_log - INFO -    212                             

2018-05-02 17:11:22,655 - memory_profile6_log - INFO -    213    146.8 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 17:11:22,657 - memory_profile6_log - INFO -    214    146.8 MiB      0.0 MiB       print model_transform[model_transform['user_id']=='1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157']

2018-05-02 17:11:22,657 - memory_profile6_log - INFO -    215    146.8 MiB      0.0 MiB       logger.info("Len of model_transform: %d", len(model_transform))

2018-05-02 17:11:22,657 - memory_profile6_log - INFO -    216    146.8 MiB      0.0 MiB       logger.info("Len of df_dt: %d", len(df_dt))

2018-05-02 17:11:22,658 - memory_profile6_log - INFO -    217    146.8 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-05-02 17:11:22,658 - memory_profile6_log - INFO -    218                             

2018-05-02 17:11:22,660 - memory_profile6_log - INFO -    219    146.8 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-05-02 17:11:22,660 - memory_profile6_log - INFO -    220                             

2018-05-02 17:11:22,661 - memory_profile6_log - INFO -    221    146.8 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-05-02 17:11:22,661 - memory_profile6_log - INFO -    222    146.8 MiB      0.0 MiB       del df_dut

2018-05-02 17:11:22,661 - memory_profile6_log - INFO -    223    146.8 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-05-02 17:11:22,663 - memory_profile6_log - INFO -    224    146.8 MiB      0.0 MiB       del df_dt

2018-05-02 17:11:22,667 - memory_profile6_log - INFO -    225    146.8 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-05-02 17:11:22,667 - memory_profile6_log - INFO -    226    146.8 MiB      0.0 MiB       del df_input

2018-05-02 17:11:22,670 - memory_profile6_log - INFO -    227    146.8 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-05-02 17:11:22,670 - memory_profile6_log - INFO -    228    146.8 MiB      0.0 MiB       del df_input_X

2018-05-02 17:11:22,671 - memory_profile6_log - INFO -    229    146.8 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-05-02 17:11:22,674 - memory_profile6_log - INFO -    230    146.8 MiB      0.0 MiB       del df_current

2018-05-02 17:11:22,677 - memory_profile6_log - INFO -    231    146.8 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-05-02 17:11:22,677 - memory_profile6_log - INFO -    232    146.8 MiB      0.0 MiB       del map_topic_isgeneral

2018-05-02 17:11:22,680 - memory_profile6_log - INFO -    233    146.8 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-05-02 17:11:22,680 - memory_profile6_log - INFO -    234    146.8 MiB      0.0 MiB       del model_fit

2018-05-02 17:11:22,680 - memory_profile6_log - INFO -    235    146.8 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-05-02 17:11:22,681 - memory_profile6_log - INFO -    236    146.8 MiB      0.0 MiB       del result

2018-05-02 17:11:22,683 - memory_profile6_log - INFO -    237    146.8 MiB      0.0 MiB       logger.info("deleting result...")

2018-05-02 17:11:22,683 - memory_profile6_log - INFO -    238                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 17:11:22,686 - memory_profile6_log - INFO -    239                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 17:11:22,686 - memory_profile6_log - INFO -    240                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 17:11:22,687 - memory_profile6_log - INFO -    241    146.8 MiB      0.0 MiB       if savetrain:

2018-05-02 17:11:22,687 - memory_profile6_log - INFO -    242    146.8 MiB      0.0 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'p0_posterior', 'rank']].copy(deep=True)

2018-05-02 17:11:22,687 - memory_profile6_log - INFO -    243    146.8 MiB      0.0 MiB           model_transformsv['date'] = current_date  # we need manually adding date, because table not support

2018-05-02 17:11:22,688 - memory_profile6_log - INFO -    244    146.8 MiB      0.0 MiB           model_transformsv['date'] = pd.to_datetime(model_transformsv['date'],

2018-05-02 17:11:22,690 - memory_profile6_log - INFO -    245    146.8 MiB      0.0 MiB                                                      format='%Y-%m-%d', errors='coerce')

2018-05-02 17:11:22,690 - memory_profile6_log - INFO -    246    146.8 MiB      0.0 MiB           print "list(model_transformsv) before rename: ", list(model_transformsv)       

2018-05-02 17:11:22,690 - memory_profile6_log - INFO -    247    146.8 MiB      0.0 MiB           print len(model_transformsv)                

2018-05-02 17:11:22,690 - memory_profile6_log - INFO -    248    146.8 MiB      0.0 MiB           model_transformsv = model_transformsv.rename(columns={'is_general': 'topic_is_general', 'p0_posterior': 'interest_score',

2018-05-02 17:11:22,691 - memory_profile6_log - INFO -    249    146.8 MiB      0.0 MiB                                                                 'rank':'interest_rank' , 'date':'interest_score_created_at'})

2018-05-02 17:11:22,693 - memory_profile6_log - INFO -    250    146.8 MiB      0.0 MiB           print "list(model_transformsv) after rename: ", list(model_transformsv)    

2018-05-02 17:11:22,693 - memory_profile6_log - INFO -    251    146.8 MiB      0.0 MiB           print len(model_transformsv) 

2018-05-02 17:11:22,694 - memory_profile6_log - INFO -    252    146.8 MiB      0.0 MiB           del model_transform

2018-05-02 17:11:22,694 - memory_profile6_log - INFO -    253    146.8 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-05-02 17:11:22,697 - memory_profile6_log - INFO -    254    146.8 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-05-02 17:11:22,698 - memory_profile6_log - INFO -    255                             

2018-05-02 17:11:22,700 - memory_profile6_log - INFO -    256    146.8 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-05-02 17:11:22,701 - memory_profile6_log - INFO -    257                                     # ~ Place your code to save the training model here ~

2018-05-02 17:11:22,703 - memory_profile6_log - INFO -    258    146.8 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-05-02 17:11:22,703 - memory_profile6_log - INFO -    259                                         logger.info("Using google datastore as storage...")

2018-05-02 17:11:22,703 - memory_profile6_log - INFO -    260                                         if multproc:

2018-05-02 17:11:22,703 - memory_profile6_log - INFO -    261                                             # ~ save transform models ~

2018-05-02 17:11:22,703 - memory_profile6_log - INFO -    262                                             logger.info("Saving main Transform model to Google DataStore...")

2018-05-02 17:11:22,703 - memory_profile6_log - INFO -    263                                             logger.info("Saving total data: %d", len(model_transformsv))

2018-05-02 17:11:22,704 - memory_profile6_log - INFO -    264                                             mh.saveDataStorePutMulti(model_transformsv)

2018-05-02 17:11:22,704 - memory_profile6_log - INFO -    265                             

2018-05-02 17:11:22,706 - memory_profile6_log - INFO -    266                                             # ~ save fitted models ~

2018-05-02 17:11:22,706 - memory_profile6_log - INFO -    267                                             logger.info("Saving fitted_models as history to Google DataStore...")

2018-05-02 17:11:22,710 - memory_profile6_log - INFO -    268                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 17:11:22,710 - memory_profile6_log - INFO -    269                                             fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-05-02 17:11:22,711 - memory_profile6_log - INFO -    270                                             logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-05-02 17:11:22,711 - memory_profile6_log - INFO -    271                                             X_split = np.array_split(fitted_models_sigmant, 10)

2018-05-02 17:11:22,711 - memory_profile6_log - INFO -    272                                             logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-05-02 17:11:22,713 - memory_profile6_log - INFO -    273                                             for ix in range(len(X_split)):

2018-05-02 17:11:22,713 - memory_profile6_log - INFO -    274                                                 logger.info("processing batch-%d", ix)

2018-05-02 17:11:22,713 - memory_profile6_log - INFO -    275                                                 mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-05-02 17:11:22,713 - memory_profile6_log - INFO -    276                             

2018-05-02 17:11:22,714 - memory_profile6_log - INFO -    277                                             del X_split

2018-05-02 17:11:22,714 - memory_profile6_log - INFO -    278                                             logger.info("deleting X_split...")

2018-05-02 17:11:22,716 - memory_profile6_log - INFO -    279                                             del save_sigma_nt

2018-05-02 17:11:22,716 - memory_profile6_log - INFO -    280                                             logger.info("deleting save_sigma_nt...")

2018-05-02 17:11:22,717 - memory_profile6_log - INFO -    281                             

2018-05-02 17:11:22,717 - memory_profile6_log - INFO -    282                                             del BR

2018-05-02 17:11:22,717 - memory_profile6_log - INFO -    283                                             logger.info("deleting BR...")

2018-05-02 17:11:22,717 - memory_profile6_log - INFO -    284                             

2018-05-02 17:11:22,717 - memory_profile6_log - INFO -    285    146.8 MiB      0.0 MiB           elif str(saveto).lower() == "elastic":

2018-05-02 17:11:22,723 - memory_profile6_log - INFO -    286    146.8 MiB      0.0 MiB               logger.info("Using ElasticSearch as storage...")

2018-05-02 17:11:22,723 - memory_profile6_log - INFO -    287                                         """logging.info("Saving main Transform model to Elasticsearch...")

2018-05-02 17:11:22,724 - memory_profile6_log - INFO -    288                                         # print model_transformsv.head(5)

2018-05-02 17:11:22,726 - memory_profile6_log - INFO -    289                                         

2018-05-02 17:11:22,726 - memory_profile6_log - INFO -    290                                         X_split = np.array_split(model_transformsv, 5)

2018-05-02 17:11:22,726 - memory_profile6_log - INFO -    291                                         logger.info("Saving total data: %d", len(model_transformsv))

2018-05-02 17:11:22,727 - memory_profile6_log - INFO -    292                                         logger.info("Len of X_split for batch save model_transformsv: %d", len(X_split))

2018-05-02 17:11:22,727 - memory_profile6_log - INFO -    293                                         for ix in range(len(X_split)):

2018-05-02 17:11:22,729 - memory_profile6_log - INFO -    294                                             logger.info("processing batch-%d", ix)

2018-05-02 17:11:22,730 - memory_profile6_log - INFO -    295                                             mh.saveElasticS(X_split[ix])

2018-05-02 17:11:22,730 - memory_profile6_log - INFO -    296                                         del X_split

2018-05-02 17:11:22,730 - memory_profile6_log - INFO -    297                                         """

2018-05-02 17:11:22,730 - memory_profile6_log - INFO -    298                             

2018-05-02 17:11:22,733 - memory_profile6_log - INFO -    299    146.8 MiB      0.0 MiB               logger.info("Saving fitted_models as history to Elasticsearch...")

2018-05-02 17:11:22,733 - memory_profile6_log - INFO -    300    146.8 MiB      0.0 MiB               save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 17:11:22,734 - memory_profile6_log - INFO -    301    146.8 MiB      0.0 MiB               fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-05-02 17:11:22,734 - memory_profile6_log - INFO -    302                             

2018-05-02 17:11:22,736 - memory_profile6_log - INFO -    303    146.8 MiB      0.0 MiB               fitted_models_sigmant['uid_topid'] = fitted_models_sigmant["user_id"].map(str) + "_" + fitted_models_sigmant["topic_id"].map(str)

2018-05-02 17:11:22,736 - memory_profile6_log - INFO -    304    146.8 MiB      0.0 MiB               fitted_models_sigmant = fitted_models_sigmant[["uid_topid", "pt_posterior_x_Nt", "smoothed_pt_posterior", "p0_cat_ci", "sigma_Nt"]]

2018-05-02 17:11:22,736 - memory_profile6_log - INFO -    305    146.8 MiB      0.0 MiB               print fitted_models_sigmant.head(5)

2018-05-02 17:11:22,736 - memory_profile6_log - INFO -    306    146.8 MiB      0.0 MiB               print fitted_models_sigmant['uid_topid'].tolist()[1]

2018-05-02 17:11:22,736 - memory_profile6_log - INFO -    307    146.8 MiB      0.0 MiB               X_split = np.array_split(fitted_models_sigmant, 5)

2018-05-02 17:11:22,737 - memory_profile6_log - INFO -    308    146.8 MiB      0.0 MiB               logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-05-02 17:11:22,737 - memory_profile6_log - INFO -    309    146.8 MiB      0.0 MiB               logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-05-02 17:11:22,739 - memory_profile6_log - INFO -    310    148.9 MiB     -9.8 MiB               for ix in range(len(X_split)):

2018-05-02 17:11:22,739 - memory_profile6_log - INFO -    311    148.9 MiB     -8.5 MiB                   logger.info("processing batch-%d", ix)

2018-05-02 17:11:22,740 - memory_profile6_log - INFO -    312    148.9 MiB     -7.6 MiB                   mh.saveElasticS(X_split[ix], esindex_name="fitted_hist_index",  estype_name='fitted_hist_type')

2018-05-02 17:11:22,740 - memory_profile6_log - INFO -    313    147.7 MiB     -1.3 MiB               del X_split

2018-05-02 17:11:22,740 - memory_profile6_log - INFO -    314                                         

2018-05-02 17:11:22,742 - memory_profile6_log - INFO -    315    147.7 MiB      0.0 MiB               del BR

2018-05-02 17:11:22,746 - memory_profile6_log - INFO -    316    147.7 MiB      0.0 MiB               logger.info("deleting BR...")

2018-05-02 17:11:22,746 - memory_profile6_log - INFO -    317    147.7 MiB      0.0 MiB               del save_sigma_nt

2018-05-02 17:11:22,746 - memory_profile6_log - INFO -    318    147.7 MiB      0.0 MiB               logger.info("deleting save_sigma_nt...")

2018-05-02 17:11:22,746 - memory_profile6_log - INFO -    319    147.7 MiB      0.0 MiB               del fitted_models_sigmant

2018-05-02 17:11:22,747 - memory_profile6_log - INFO -    320    147.7 MiB      0.0 MiB               logger.info("deleting fitted_models_sigmant...")

2018-05-02 17:11:22,749 - memory_profile6_log - INFO -    321                             

2018-05-02 17:11:22,749 - memory_profile6_log - INFO -    322                                     # need save sigma_nt for daily train

2018-05-02 17:11:22,750 - memory_profile6_log - INFO -    323                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-05-02 17:11:22,750 - memory_profile6_log - INFO -    324                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-05-02 17:11:22,750 - memory_profile6_log - INFO -    325    147.7 MiB      0.0 MiB           if start_date and end_date:

2018-05-02 17:11:22,750 - memory_profile6_log - INFO -    326                                         if not fitby_sigmant:

2018-05-02 17:11:22,752 - memory_profile6_log - INFO -    327                                             logging.info("Saving sigma Nt...")

2018-05-02 17:11:22,752 - memory_profile6_log - INFO -    328                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 17:11:22,752 - memory_profile6_log - INFO -    329                                             save_sigma_nt['start_date'] = start_date

2018-05-02 17:11:22,753 - memory_profile6_log - INFO -    330                                             save_sigma_nt['end_date'] = end_date

2018-05-02 17:11:22,753 - memory_profile6_log - INFO -    331                                             print save_sigma_nt.head(5)

2018-05-02 17:11:22,756 - memory_profile6_log - INFO -    332                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-05-02 17:11:22,759 - memory_profile6_log - INFO -    333                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-05-02 17:11:22,759 - memory_profile6_log - INFO -    334    147.7 MiB      0.0 MiB       return

2018-05-02 17:11:22,760 - memory_profile6_log - INFO - 


2018-05-02 17:11:22,760 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-05-02 17:19:19,092 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 17:19:19,095 - memory_profile6_log - INFO - date_generated: 
2018-05-02 17:19:19,095 - memory_profile6_log - INFO -  
2018-05-02 17:19:19,095 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 17, 19, 19, 93000)]
2018-05-02 17:19:19,095 - memory_profile6_log - INFO - 

2018-05-02 17:19:19,096 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 17:19:19,096 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 17:19:19,096 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 17:19:19,234 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 17:19:19,237 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 17:19:29,428 - memory_profile6_log - INFO - size of df: 5.05 MB
2018-05-02 17:19:29,430 - memory_profile6_log - INFO - getting total: 20000 training data(genuine interest) for date: 2018-05-01
2018-05-02 17:19:29,447 - memory_profile6_log - INFO - loading history data from datastore...
2018-05-02 17:19:29,447 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-05-02 17:19:29,450 - memory_profile6_log - INFO - Appending history data...
2018-05-02 17:19:29,450 - memory_profile6_log - INFO - processing batch-0
2018-05-02 17:19:29,451 - memory_profile6_log - INFO - creating list history data...
2018-05-02 17:19:29,493 - memory_profile6_log - INFO - call history data...
2018-05-02 17:20:06,815 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 17:20:07,486 - memory_profile6_log - INFO - processing batch-1
2018-05-02 17:20:07,487 - memory_profile6_log - INFO - creating list history data...
2018-05-02 17:20:07,496 - memory_profile6_log - INFO - call history data...
2018-05-02 17:20:46,571 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 17:20:47,263 - memory_profile6_log - INFO - processing batch-2
2018-05-02 17:20:47,265 - memory_profile6_log - INFO - creating list history data...
2018-05-02 17:20:47,273 - memory_profile6_log - INFO - call history data...
2018-05-02 17:21:24,569 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 17:21:25,157 - memory_profile6_log - INFO - processing batch-3
2018-05-02 17:21:25,158 - memory_profile6_log - INFO - creating list history data...
2018-05-02 17:21:25,167 - memory_profile6_log - INFO - call history data...
2018-05-02 17:22:00,576 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 17:22:01,121 - memory_profile6_log - INFO - processing batch-4
2018-05-02 17:22:01,124 - memory_profile6_log - INFO - creating list history data...
2018-05-02 17:22:01,134 - memory_profile6_log - INFO - call history data...
2018-05-02 17:22:36,684 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-05-02 17:22:36,894 - memory_profile6_log - INFO - Appending training data...
2018-05-02 17:22:36,894 - memory_profile6_log - INFO - len datalist: 1
2018-05-02 17:22:36,895 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-05-02 17:22:36,898 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 17:22:36,900 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 17:22:36,900 - memory_profile6_log - INFO - ================================================

2018-05-02 17:22:36,901 - memory_profile6_log - INFO -    359     86.8 MiB     86.8 MiB   @profile

2018-05-02 17:22:36,901 - memory_profile6_log - INFO -    360                             def BQPreprocess(cpu, date_generated, client, query_fit, loadfrom="datastore"):

2018-05-02 17:22:36,901 - memory_profile6_log - INFO -    361     86.8 MiB      0.0 MiB       bq_client = client

2018-05-02 17:22:36,903 - memory_profile6_log - INFO -    362     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 17:22:36,903 - memory_profile6_log - INFO -    363                             

2018-05-02 17:22:36,905 - memory_profile6_log - INFO -    364     86.8 MiB      0.0 MiB       datalist = []

2018-05-02 17:22:36,907 - memory_profile6_log - INFO -    365     86.8 MiB      0.0 MiB       datalist_hist = []

2018-05-02 17:22:36,907 - memory_profile6_log - INFO -    366                             

2018-05-02 17:22:36,908 - memory_profile6_log - INFO -    367     86.8 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-05-02 17:22:36,910 - memory_profile6_log - INFO -    368    142.5 MiB      0.0 MiB       for ndate in date_generated:

2018-05-02 17:22:36,910 - memory_profile6_log - INFO -    369    141.4 MiB     54.6 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-05-02 17:22:36,911 - memory_profile6_log - INFO -    370    141.4 MiB      0.0 MiB           if tframe is not None:

2018-05-02 17:22:36,911 - memory_profile6_log - INFO -    371    141.4 MiB      0.0 MiB               if not tframe.empty:

2018-05-02 17:22:36,911 - memory_profile6_log - INFO -    372    141.4 MiB      0.0 MiB                   if loadfrom.strip().lower() == 'datastore':

2018-05-02 17:22:36,913 - memory_profile6_log - INFO -    373    141.5 MiB      0.1 MiB                       X_split = np.array_split(tframe, 5)

2018-05-02 17:22:36,915 - memory_profile6_log - INFO -    374    141.5 MiB      0.0 MiB                       logger.info("loading history data from datastore...")

2018-05-02 17:22:36,917 - memory_profile6_log - INFO -    375    141.5 MiB      0.0 MiB                       logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 17:22:36,918 - memory_profile6_log - INFO -    376    141.5 MiB      0.0 MiB                       logger.info("Appending history data...")

2018-05-02 17:22:36,920 - memory_profile6_log - INFO -    377    142.5 MiB      0.0 MiB                       for ix in range(len(X_split)):

2018-05-02 17:22:36,920 - memory_profile6_log - INFO -    378                                                     # ~ loading history

2018-05-02 17:22:36,921 - memory_profile6_log - INFO -    379                                                     """

2018-05-02 17:22:36,923 - memory_profile6_log - INFO -    380                                                         disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-05-02 17:22:36,924 - memory_profile6_log - INFO -    381                                                     """

2018-05-02 17:22:36,924 - memory_profile6_log - INFO -    382    142.5 MiB      0.0 MiB                           logger.info("processing batch-%d", ix)

2018-05-02 17:22:36,927 - memory_profile6_log - INFO -    383                                                     # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-05-02 17:22:36,927 - memory_profile6_log - INFO -    384    142.5 MiB      0.0 MiB                           logger.info("creating list history data...")

2018-05-02 17:22:36,928 - memory_profile6_log - INFO -    385                                                     #lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-05-02 17:22:36,930 - memory_profile6_log - INFO -    386    142.5 MiB      0.4 MiB                           lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 17:22:36,931 - memory_profile6_log - INFO -    387                             

2018-05-02 17:22:36,933 - memory_profile6_log - INFO -    388    142.5 MiB      0.0 MiB                           logger.info("call history data...")

2018-05-02 17:22:36,933 - memory_profile6_log - INFO -    389    142.5 MiB      0.4 MiB                           h_frame = mh.loadDSHistory(lhistory)

2018-05-02 17:22:36,934 - memory_profile6_log - INFO -    390                             

2018-05-02 17:22:36,937 - memory_profile6_log - INFO -    391                                                     # me = os.getpid()

2018-05-02 17:22:36,937 - memory_profile6_log - INFO -    392                                                     # kill_proc_tree(me)

2018-05-02 17:22:36,940 - memory_profile6_log - INFO -    393                             

2018-05-02 17:22:36,940 - memory_profile6_log - INFO -    394    142.5 MiB      0.0 MiB                           logger.info("done collecting history data, appending now...")

2018-05-02 17:22:36,941 - memory_profile6_log - INFO -    395    142.5 MiB     -3.8 MiB                           for m in h_frame:

2018-05-02 17:22:36,943 - memory_profile6_log - INFO -    396    142.5 MiB     -3.8 MiB                               if m is not None:

2018-05-02 17:22:36,944 - memory_profile6_log - INFO -    397    142.5 MiB     -3.8 MiB                                   if len(m) > 0:

2018-05-02 17:22:36,944 - memory_profile6_log - INFO -    398    142.5 MiB     -3.6 MiB                                       datalist_hist.append(pd.DataFrame(m))

2018-05-02 17:22:36,947 - memory_profile6_log - INFO -    399    142.5 MiB     -0.0 MiB                           del h_frame

2018-05-02 17:22:36,947 - memory_profile6_log - INFO -    400    142.5 MiB      0.0 MiB                           del lhistory

2018-05-02 17:22:36,950 - memory_profile6_log - INFO -    401                             

2018-05-02 17:22:36,950 - memory_profile6_log - INFO -    402    142.5 MiB      0.0 MiB                       logger.info("Appending training data...")

2018-05-02 17:22:36,951 - memory_profile6_log - INFO -    403    142.5 MiB      0.0 MiB                       datalist.append(tframe)

2018-05-02 17:22:36,951 - memory_profile6_log - INFO -    404                                             elif loadfrom.strip().lower() == 'elastic':

2018-05-02 17:22:36,953 - memory_profile6_log - INFO -    405                                                 X_split = np.array_split(tframe, 5)

2018-05-02 17:22:36,953 - memory_profile6_log - INFO -    406                                                 logger.info("loading history data from elastic...")

2018-05-02 17:22:36,957 - memory_profile6_log - INFO -    407                                                 logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 17:22:36,957 - memory_profile6_log - INFO -    408                                                 logger.info("Appending history data...")

2018-05-02 17:22:36,959 - memory_profile6_log - INFO -    409                                                 for ix in range(len(X_split)):

2018-05-02 17:22:36,960 - memory_profile6_log - INFO -    410                                                     for framedata in X_split[ix].itertuples():

2018-05-02 17:22:36,960 - memory_profile6_log - INFO -    411                                                         inside_data = mh.loadESHistory(framedata.user_id, framedata.topic_id ,

2018-05-02 17:22:36,960 - memory_profile6_log - INFO -    412                                                                                        esindex_name='fitted_hist_index',

2018-05-02 17:22:36,961 - memory_profile6_log - INFO -    413                                                                                        estype_name='fitted_hist_type')

2018-05-02 17:22:36,963 - memory_profile6_log - INFO -    414                             

2018-05-02 17:22:36,963 - memory_profile6_log - INFO -    415                                                         if not inside_data.empty:

2018-05-02 17:22:36,964 - memory_profile6_log - INFO -    416                                                             datalist_hist.append(inside_data)

2018-05-02 17:22:36,967 - memory_profile6_log - INFO -    417                                                             del inside_data

2018-05-02 17:22:36,970 - memory_profile6_log - INFO -    418                                                         else:

2018-05-02 17:22:36,970 - memory_profile6_log - INFO -    419                                                             logger.info("uid: %s with topic_id: %s is empty!" % (framedata.user_id, framedata.topic_id))

2018-05-02 17:22:36,973 - memory_profile6_log - INFO -    420                                             else:

2018-05-02 17:22:36,973 - memory_profile6_log - INFO -    421                                                 logger.info("Unknows source is selected !")

2018-05-02 17:22:36,973 - memory_profile6_log - INFO -    422                                                 break

2018-05-02 17:22:36,974 - memory_profile6_log - INFO -    423                                     else: 

2018-05-02 17:22:36,976 - memory_profile6_log - INFO -    424                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-05-02 17:22:36,980 - memory_profile6_log - INFO -    425    142.5 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-05-02 17:22:36,980 - memory_profile6_log - INFO -    426    142.5 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-05-02 17:22:36,982 - memory_profile6_log - INFO -    427                             

2018-05-02 17:22:36,983 - memory_profile6_log - INFO -    428    142.5 MiB      0.0 MiB       return datalist, datalist_hist

2018-05-02 17:22:36,983 - memory_profile6_log - INFO - 


2018-05-02 17:22:37,903 - memory_profile6_log - INFO - size of big_frame_hist: 2.34 MB
2018-05-02 17:22:37,915 - memory_profile6_log - INFO - size of big_frame: 5.05 MB
2018-05-02 17:22:37,920 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-05-02 17:22:46,255 - memory_profile6_log - INFO - size of df: 4.93 MB
2018-05-02 17:22:46,256 - memory_profile6_log - INFO - getting total: 20000 training data(current date interest)
2018-05-02 17:22:46,270 - memory_profile6_log - INFO - size of current_frame: 5.09 MB
2018-05-02 17:22:46,273 - memory_profile6_log - INFO - loading time of: 40000 total genuine-current interest data ~ take 207.071s
2018-05-02 17:22:46,279 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 17:22:46,279 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 17:22:46,280 - memory_profile6_log - INFO - ================================================

2018-05-02 17:22:46,280 - memory_profile6_log - INFO -    430     86.6 MiB     86.6 MiB   @profile

2018-05-02 17:22:46,282 - memory_profile6_log - INFO -    431                             def preprocess(cpu, cd, query_fit, date_generated):

2018-05-02 17:22:46,285 - memory_profile6_log - INFO -    432     86.8 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-05-02 17:22:46,289 - memory_profile6_log - INFO -    433     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 17:22:46,289 - memory_profile6_log - INFO -    434                             

2018-05-02 17:22:46,290 - memory_profile6_log - INFO -    435                                 # ~~~ Begin collecting data ~~~

2018-05-02 17:22:46,292 - memory_profile6_log - INFO -    436     86.8 MiB      0.0 MiB       t0 = time.time()

2018-05-02 17:22:46,292 - memory_profile6_log - INFO -    437    142.5 MiB     55.7 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-05-02 17:22:46,292 - memory_profile6_log - INFO -    438    142.5 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-05-02 17:22:46,292 - memory_profile6_log - INFO -    439                                     logger.info("Training cannot be empty..")

2018-05-02 17:22:46,292 - memory_profile6_log - INFO -    440                                     return False

2018-05-02 17:22:46,296 - memory_profile6_log - INFO -    441    142.5 MiB      0.1 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-05-02 17:22:46,299 - memory_profile6_log - INFO -    442                                 # print "big_frame_hist:\n", big_frame_hist.head(20)

2018-05-02 17:22:46,299 - memory_profile6_log - INFO -    443    142.5 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-05-02 17:22:46,299 - memory_profile6_log - INFO -    444                             

2018-05-02 17:22:46,299 - memory_profile6_log - INFO -    445    142.5 MiB      0.0 MiB       big_frame = pd.concat(datalist)

2018-05-02 17:22:46,301 - memory_profile6_log - INFO -    446    142.5 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-05-02 17:22:46,301 - memory_profile6_log - INFO -    447    142.5 MiB      0.0 MiB       del datalist

2018-05-02 17:22:46,302 - memory_profile6_log - INFO -    448                             

2018-05-02 17:22:46,303 - memory_profile6_log - INFO -    449    142.5 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 17:22:46,308 - memory_profile6_log - INFO -    450                             

2018-05-02 17:22:46,311 - memory_profile6_log - INFO -    451                                 # ~ get current news interest ~

2018-05-02 17:22:46,311 - memory_profile6_log - INFO -    452    142.5 MiB      0.0 MiB       if not cd:

2018-05-02 17:22:46,315 - memory_profile6_log - INFO -    453    142.5 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-05-02 17:22:46,315 - memory_profile6_log - INFO -    454    148.0 MiB      5.5 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-05-02 17:22:46,316 - memory_profile6_log - INFO -    455    148.0 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-05-02 17:22:46,319 - memory_profile6_log - INFO -    456                                 else:

2018-05-02 17:22:46,319 - memory_profile6_log - INFO -    457                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-05-02 17:22:46,322 - memory_profile6_log - INFO -    458                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date) limit 20000"

2018-05-02 17:22:46,323 - memory_profile6_log - INFO -    459                             

2018-05-02 17:22:46,326 - memory_profile6_log - INFO -    460                                     # safe handling of query parameter

2018-05-02 17:22:46,326 - memory_profile6_log - INFO -    461                                     query_params = [

2018-05-02 17:22:46,328 - memory_profile6_log - INFO -    462                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-05-02 17:22:46,334 - memory_profile6_log - INFO -    463                                     ]

2018-05-02 17:22:46,335 - memory_profile6_log - INFO -    464                             

2018-05-02 17:22:46,341 - memory_profile6_log - INFO -    465                                     job_config.query_parameters = query_params

2018-05-02 17:22:46,341 - memory_profile6_log - INFO -    466                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-05-02 17:22:46,342 - memory_profile6_log - INFO -    467                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-05-02 17:22:46,344 - memory_profile6_log - INFO -    468                             

2018-05-02 17:22:46,348 - memory_profile6_log - INFO -    469    148.0 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-05-02 17:22:46,349 - memory_profile6_log - INFO -    470    148.0 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-05-02 17:22:46,351 - memory_profile6_log - INFO -    471    148.0 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-05-02 17:22:46,351 - memory_profile6_log - INFO -    472    148.0 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-05-02 17:22:46,351 - memory_profile6_log - INFO -    473    148.0 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 17:22:46,355 - memory_profile6_log - INFO -    474    148.0 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-05-02 17:22:46,355 - memory_profile6_log - INFO -    475                             

2018-05-02 17:22:46,358 - memory_profile6_log - INFO -    476    148.0 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-05-02 17:22:46,358 - memory_profile6_log - INFO - 


2018-05-02 17:22:46,362 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-05-02 17:22:46,371 - memory_profile6_log - INFO - train on: 20000 total genuine interest data(D(u, t))
2018-05-02 17:22:46,372 - memory_profile6_log - INFO - transform on: 20000 total current data(D(t))
2018-05-02 17:22:46,374 - memory_profile6_log - INFO - apply on: 8587 total history...)
2018-05-02 17:22:46,428 - memory_profile6_log - INFO - len of uniques_fit_hist:8587
2018-05-02 17:22:46,437 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:4891
2018-05-02 17:22:46,529 - memory_profile6_log - INFO - self.sum_all_nt before concate:

2018-05-02 17:22:46,549 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 17:22:46,549 - memory_profile6_log - INFO - 

2018-05-02 17:22:46,566 - memory_profile6_log - INFO - self.sum_all_nt after concate:

2018-05-02 17:22:46,576 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 17:22:46,578 - memory_profile6_log - INFO - 

2018-05-02 17:22:46,638 - memory_profile6_log - INFO - Len of model_fit: 20000
2018-05-02 17:22:46,638 - memory_profile6_log - INFO - Len of df_dut: 20000
2018-05-02 17:22:46,786 - memory_profile6_log - INFO - Len of fitted_models on main class: 20000
2018-05-02 17:22:46,786 - memory_profile6_log - INFO - 

2018-05-02 17:22:46,789 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-05-02 17:22:46,789 - memory_profile6_log - INFO - 

2018-05-02 17:22:46,790 - memory_profile6_log - INFO - fitted_model_hist:

2018-05-02 17:22:46,792 - memory_profile6_log - INFO - 

2018-05-02 17:22:46,802 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 17:22:46,803 - memory_profile6_log - INFO - 

2018-05-02 17:22:46,812 - memory_profile6_log - INFO - len of fitted models after concat: 28587
2018-05-02 17:22:46,813 - memory_profile6_log - INFO - 

2018-05-02 17:22:46,815 - memory_profile6_log - INFO - Recalculating fitted models...
2018-05-02 17:22:46,816 - memory_profile6_log - INFO - 

2018-05-02 17:22:46,848 - memory_profile6_log - INFO - fitted_models after cobine:

2018-05-02 17:22:46,849 - memory_profile6_log - INFO - 

2018-05-02 17:22:46,861 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 17:22:46,861 - memory_profile6_log - INFO - 

2018-05-02 17:22:46,861 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 20000
2018-05-02 17:22:46,862 - memory_profile6_log - INFO - 

2018-05-02 17:22:51,825 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-05-02 17:22:51,845 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, topic_id, pt_posterior_x_Nt, smoothed_pt_posterior, p0_cat_ci, sigma_Nt, p0_posterior, is_general, rank]
Index: []
2018-05-02 17:22:51,846 - memory_profile6_log - INFO - 

2018-05-02 17:22:51,848 - memory_profile6_log - INFO - Len of model_transform: 9072
2018-05-02 17:22:51,849 - memory_profile6_log - INFO - Len of df_dt: 20000
2018-05-02 17:22:51,851 - memory_profile6_log - INFO - Total train time: 5.463s
2018-05-02 17:22:51,852 - memory_profile6_log - INFO - memory left before cleaning: 83.900 percent memory...
2018-05-02 17:22:51,852 - memory_profile6_log - INFO - cleaning up some objects...
2018-05-02 17:22:51,854 - memory_profile6_log - INFO - deleting df_dut...
2018-05-02 17:22:51,855 - memory_profile6_log - INFO - deleting df_dt...
2018-05-02 17:22:51,858 - memory_profile6_log - INFO - deleting df_input...
2018-05-02 17:22:51,859 - memory_profile6_log - INFO - deleting df_input_X...
2018-05-02 17:22:51,861 - memory_profile6_log - INFO - deleting df_current...
2018-05-02 17:22:51,861 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-05-02 17:22:51,865 - memory_profile6_log - INFO - deleting model_fit...
2018-05-02 17:22:51,865 - memory_profile6_log - INFO - deleting result...
2018-05-02 17:22:51,875 - memory_profile6_log - INFO - list(model_transformsv) before rename: 
2018-05-02 17:22:51,875 - memory_profile6_log - INFO -  
2018-05-02 17:22:51,878 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'is_general', 'p0_posterior', 'rank', 'date']
2018-05-02 17:22:51,878 - memory_profile6_log - INFO - 

2018-05-02 17:22:51,881 - memory_profile6_log - INFO - 9072
2018-05-02 17:22:51,881 - memory_profile6_log - INFO - 

2018-05-02 17:22:51,887 - memory_profile6_log - INFO - list(model_transformsv) after rename: 
2018-05-02 17:22:51,888 - memory_profile6_log - INFO -  
2018-05-02 17:22:51,891 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'topic_is_general', 'interest_score', 'interest_rank', 'interest_score_created_at']
2018-05-02 17:22:51,891 - memory_profile6_log - INFO - 

2018-05-02 17:22:51,892 - memory_profile6_log - INFO - 9072
2018-05-02 17:22:51,894 - memory_profile6_log - INFO - 

2018-05-02 17:22:51,894 - memory_profile6_log - INFO - deleting model_transform...
2018-05-02 17:22:51,894 - memory_profile6_log - INFO - memory left after cleaning: 83.800 percent memory...
2018-05-02 17:22:51,895 - memory_profile6_log - INFO - Begin saving trained data...
2018-05-02 17:22:51,898 - memory_profile6_log - INFO - Using ElasticSearch as storage...
2018-05-02 17:22:51,900 - memory_profile6_log - INFO - Saving fitted_models as history to Elasticsearch...
2018-05-02 17:22:51,941 - memory_profile6_log - INFO - Saving total data: 20000
2018-05-02 17:22:51,944 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 5
2018-05-02 17:22:51,944 - memory_profile6_log - INFO - processing batch-0
2018-05-02 17:23:23,910 - memory_profile6_log - INFO - processing batch-1
2018-05-02 17:23:44,642 - memory_profile6_log - INFO - processing batch-2
2018-05-02 17:24:14,757 - memory_profile6_log - INFO - processing batch-3
2018-05-02 17:24:45,395 - memory_profile6_log - INFO - processing batch-4
2018-05-02 17:25:06,746 - memory_profile6_log - INFO - deleting BR...
2018-05-02 17:25:06,746 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-05-02 17:25:06,747 - memory_profile6_log - INFO - deleting fitted_models_sigmant...
2018-05-02 17:25:06,750 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 17:25:06,750 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 17:25:06,753 - memory_profile6_log - INFO - ================================================

2018-05-02 17:25:06,756 - memory_profile6_log - INFO -    113    148.0 MiB    148.0 MiB   @profile

2018-05-02 17:25:06,759 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-05-02 17:25:06,759 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-05-02 17:25:06,760 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-05-02 17:25:06,762 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-05-02 17:25:06,765 - memory_profile6_log - INFO -    118                                      saveto="datastore", fitted_models_hist=None):

2018-05-02 17:25:06,766 - memory_profile6_log - INFO -    119                                 """

2018-05-02 17:25:06,766 - memory_profile6_log - INFO -    120                                     Main Process

2018-05-02 17:25:06,767 - memory_profile6_log - INFO -    121                                 """

2018-05-02 17:25:06,767 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-05-02 17:25:06,769 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-05-02 17:25:06,769 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-05-02 17:25:06,769 - memory_profile6_log - INFO -    125    148.0 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-05-02 17:25:06,770 - memory_profile6_log - INFO -    126    148.0 MiB      0.0 MiB       df_dut = df_input.copy(deep=True)

2018-05-02 17:25:06,770 - memory_profile6_log - INFO -    127    148.0 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 17:25:06,772 - memory_profile6_log - INFO -    128                             

2018-05-02 17:25:06,772 - memory_profile6_log - INFO -    129                                 # D(t)

2018-05-02 17:25:06,773 - memory_profile6_log - INFO -    130    148.0 MiB      0.0 MiB       df_dt = df_current.copy(deep=True)

2018-05-02 17:25:06,773 - memory_profile6_log - INFO -    131    148.0 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 17:25:06,773 - memory_profile6_log - INFO -    132                             

2018-05-02 17:25:06,776 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-05-02 17:25:06,779 - memory_profile6_log - INFO -    134    148.0 MiB      0.0 MiB       t0 = time.time()

2018-05-02 17:25:06,779 - memory_profile6_log - INFO -    135    148.0 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-05-02 17:25:06,779 - memory_profile6_log - INFO -    136    148.0 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-05-02 17:25:06,780 - memory_profile6_log - INFO -    137    148.0 MiB      0.0 MiB       logger.info("apply on: %d total history...)", len(df_hist))

2018-05-02 17:25:06,780 - memory_profile6_log - INFO -    138                             

2018-05-02 17:25:06,782 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-05-02 17:25:06,782 - memory_profile6_log - INFO -    140    148.0 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-05-02 17:25:06,782 - memory_profile6_log - INFO -    141                             

2018-05-02 17:25:06,782 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-05-02 17:25:06,783 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-05-02 17:25:06,783 - memory_profile6_log - INFO -    144    148.4 MiB      0.4 MiB       NB = BR.processX(df_dut)

2018-05-02 17:25:06,785 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-05-02 17:25:06,785 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-05-02 17:25:06,789 - memory_profile6_log - INFO -    147    148.5 MiB      0.1 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-05-02 17:25:06,789 - memory_profile6_log - INFO -    148                                 """

2018-05-02 17:25:06,792 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-05-02 17:25:06,792 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-05-02 17:25:06,792 - memory_profile6_log - INFO -    151                                 """

2018-05-02 17:25:06,793 - memory_profile6_log - INFO -    152    148.5 MiB      0.0 MiB       fitby_sigmant = False

2018-05-02 17:25:06,793 - memory_profile6_log - INFO -    153    148.5 MiB      0.0 MiB       uniques_fit_hist = None

2018-05-02 17:25:06,795 - memory_profile6_log - INFO -    154    148.5 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-05-02 17:25:06,795 - memory_profile6_log - INFO -    155    148.5 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-05-02 17:25:06,796 - memory_profile6_log - INFO -    156    148.5 MiB      0.0 MiB                            'is_general']]

2018-05-02 17:25:06,796 - memory_profile6_log - INFO -    157                             

2018-05-02 17:25:06,796 - memory_profile6_log - INFO -    158                                 # get sigma_Nt from fitted_models_hist

2018-05-02 17:25:06,796 - memory_profile6_log - INFO -    159    148.5 MiB      0.0 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-05-02 17:25:06,799 - memory_profile6_log - INFO -    160    148.5 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-05-02 17:25:06,802 - memory_profile6_log - INFO -    161    148.6 MiB      0.0 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-05-02 17:25:06,802 - memory_profile6_log - INFO -    162    148.6 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-05-02 17:25:06,802 - memory_profile6_log - INFO -    163                             

2018-05-02 17:25:06,802 - memory_profile6_log - INFO -    164                                 # begin fit

2018-05-02 17:25:06,803 - memory_profile6_log - INFO -    165    148.6 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-05-02 17:25:06,805 - memory_profile6_log - INFO -    166    148.6 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-05-02 17:25:06,805 - memory_profile6_log - INFO -    167    151.2 MiB      2.6 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-05-02 17:25:06,805 - memory_profile6_log - INFO -    168    151.2 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-05-02 17:25:06,805 - memory_profile6_log - INFO -    169    151.2 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-05-02 17:25:06,805 - memory_profile6_log - INFO -    170                                 # print model_fit[['num_x', 'num_x', 'sigma_Nt', 'date_all_click']].loc[model_fit['user_id']=="1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157"].head(5)

2018-05-02 17:25:06,806 - memory_profile6_log - INFO -    171                             

2018-05-02 17:25:06,806 - memory_profile6_log - INFO -    172                                 # ~~ and Transform ~~

2018-05-02 17:25:06,806 - memory_profile6_log - INFO -    173                                 #   handling current news interest == current date

2018-05-02 17:25:06,808 - memory_profile6_log - INFO -    174    151.2 MiB      0.0 MiB       if df_dt.empty:

2018-05-02 17:25:06,808 - memory_profile6_log - INFO -    175                                     print "Cek your df_dt, cannot be emtpy!!"

2018-05-02 17:25:06,808 - memory_profile6_log - INFO -    176                                     return None

2018-05-02 17:25:06,812 - memory_profile6_log - INFO -    177    152.0 MiB      0.8 MiB       NB = BR.processX(df_dt)

2018-05-02 17:25:06,812 - memory_profile6_log - INFO -    178    152.0 MiB      0.0 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-05-02 17:25:06,815 - memory_profile6_log - INFO -    179                             

2018-05-02 17:25:06,815 - memory_profile6_log - INFO -    180    152.0 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-05-02 17:25:06,815 - memory_profile6_log - INFO -    181    152.0 MiB      0.0 MiB                            'num_x', 'num_y', 'is_general']]

2018-05-02 17:25:06,815 - memory_profile6_log - INFO -    182                             

2018-05-02 17:25:06,815 - memory_profile6_log - INFO -    183                                 # print "model_fit dtypes:\n", model_fit.dtypes

2018-05-02 17:25:06,816 - memory_profile6_log - INFO -    184                                 # print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-05-02 17:25:06,816 - memory_profile6_log - INFO -    185    152.0 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-05-02 17:25:06,818 - memory_profile6_log - INFO -    186    152.0 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-05-02 17:25:06,818 - memory_profile6_log - INFO -    187    152.0 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-05-02 17:25:06,819 - memory_profile6_log - INFO -    188    152.1 MiB      0.1 MiB                                                                                           "topic_id", "user_id"]],

2018-05-02 17:25:06,819 - memory_profile6_log - INFO -    189    155.6 MiB      3.4 MiB                                                     verbose=False)

2018-05-02 17:25:06,819 - memory_profile6_log - INFO -    190                             

2018-05-02 17:25:06,819 - memory_profile6_log - INFO -    191                                 # ~~~ filter is general and specific topic ~~~

2018-05-02 17:25:06,825 - memory_profile6_log - INFO -    192                                 # the idea is just we need to rerank every topic according

2018-05-02 17:25:06,825 - memory_profile6_log - INFO -    193                                 #    to user_id and and is_general by p0_posterior

2018-05-02 17:25:06,826 - memory_profile6_log - INFO -    194    155.6 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-05-02 17:25:06,826 - memory_profile6_log - INFO -    195    155.6 MiB      0.0 MiB                                     'is_general']].groupby(['topic_id',

2018-05-02 17:25:06,828 - memory_profile6_log - INFO -    196    155.6 MiB      0.0 MiB                                                             'is_general']

2018-05-02 17:25:06,828 - memory_profile6_log - INFO -    197                                                                                      ).size().to_frame().reset_index()

2018-05-02 17:25:06,828 - memory_profile6_log - INFO -    198                             

2018-05-02 17:25:06,829 - memory_profile6_log - INFO -    199                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-05-02 17:25:06,829 - memory_profile6_log - INFO -    200                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-05-02 17:25:06,831 - memory_profile6_log - INFO -    201    155.6 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-05-02 17:25:06,832 - memory_profile6_log - INFO -    202                             

2018-05-02 17:25:06,832 - memory_profile6_log - INFO -    203                                 # ~ start by provide rank for each topic type ~

2018-05-02 17:25:06,832 - memory_profile6_log - INFO -    204    160.7 MiB      5.1 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-05-02 17:25:06,835 - memory_profile6_log - INFO -    205    159.5 MiB     -1.2 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-05-02 17:25:06,836 - memory_profile6_log - INFO -    206                             

2018-05-02 17:25:06,838 - memory_profile6_log - INFO -    207                                 # ~ set threshold to filter output

2018-05-02 17:25:06,838 - memory_profile6_log - INFO -    208    159.5 MiB      0.0 MiB       if threshold > 0:

2018-05-02 17:25:06,838 - memory_profile6_log - INFO -    209    159.5 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-05-02 17:25:06,838 - memory_profile6_log - INFO -    210    159.5 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-05-02 17:25:06,839 - memory_profile6_log - INFO -    211    159.5 MiB      0.0 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-05-02 17:25:06,839 - memory_profile6_log - INFO -    212                             

2018-05-02 17:25:06,841 - memory_profile6_log - INFO -    213    159.5 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 17:25:06,841 - memory_profile6_log - INFO -    214    159.5 MiB      0.0 MiB       print model_transform[model_transform['user_id']=='1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157']

2018-05-02 17:25:06,841 - memory_profile6_log - INFO -    215    159.5 MiB      0.0 MiB       logger.info("Len of model_transform: %d", len(model_transform))

2018-05-02 17:25:06,842 - memory_profile6_log - INFO -    216    159.5 MiB      0.0 MiB       logger.info("Len of df_dt: %d", len(df_dt))

2018-05-02 17:25:06,842 - memory_profile6_log - INFO -    217    159.5 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-05-02 17:25:06,842 - memory_profile6_log - INFO -    218                             

2018-05-02 17:25:06,842 - memory_profile6_log - INFO -    219    159.5 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-05-02 17:25:06,842 - memory_profile6_log - INFO -    220                             

2018-05-02 17:25:06,842 - memory_profile6_log - INFO -    221    159.5 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-05-02 17:25:06,844 - memory_profile6_log - INFO -    222    159.5 MiB      0.0 MiB       del df_dut

2018-05-02 17:25:06,844 - memory_profile6_log - INFO -    223    159.5 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-05-02 17:25:06,844 - memory_profile6_log - INFO -    224    159.5 MiB      0.0 MiB       del df_dt

2018-05-02 17:25:06,848 - memory_profile6_log - INFO -    225    159.5 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-05-02 17:25:06,851 - memory_profile6_log - INFO -    226    159.5 MiB      0.0 MiB       del df_input

2018-05-02 17:25:06,851 - memory_profile6_log - INFO -    227    159.5 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-05-02 17:25:06,851 - memory_profile6_log - INFO -    228    151.3 MiB     -8.1 MiB       del df_input_X

2018-05-02 17:25:06,851 - memory_profile6_log - INFO -    229    151.3 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-05-02 17:25:06,852 - memory_profile6_log - INFO -    230    151.3 MiB      0.0 MiB       del df_current

2018-05-02 17:25:06,852 - memory_profile6_log - INFO -    231    151.3 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-05-02 17:25:06,854 - memory_profile6_log - INFO -    232    151.3 MiB      0.0 MiB       del map_topic_isgeneral

2018-05-02 17:25:06,855 - memory_profile6_log - INFO -    233    151.3 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-05-02 17:25:06,855 - memory_profile6_log - INFO -    234    150.7 MiB     -0.6 MiB       del model_fit

2018-05-02 17:25:06,857 - memory_profile6_log - INFO -    235    150.7 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-05-02 17:25:06,857 - memory_profile6_log - INFO -    236    150.7 MiB      0.0 MiB       del result

2018-05-02 17:25:06,857 - memory_profile6_log - INFO -    237    150.7 MiB      0.0 MiB       logger.info("deleting result...")

2018-05-02 17:25:06,861 - memory_profile6_log - INFO -    238                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 17:25:06,862 - memory_profile6_log - INFO -    239                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 17:25:06,862 - memory_profile6_log - INFO -    240                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 17:25:06,864 - memory_profile6_log - INFO -    241    150.7 MiB      0.0 MiB       if savetrain:

2018-05-02 17:25:06,865 - memory_profile6_log - INFO -    242    151.1 MiB      0.4 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'p0_posterior', 'rank']].copy(deep=True)

2018-05-02 17:25:06,865 - memory_profile6_log - INFO -    243    151.2 MiB      0.1 MiB           model_transformsv['date'] = current_date  # we need manually adding date, because table not support

2018-05-02 17:25:06,865 - memory_profile6_log - INFO -    244    151.2 MiB      0.0 MiB           model_transformsv['date'] = pd.to_datetime(model_transformsv['date'],

2018-05-02 17:25:06,865 - memory_profile6_log - INFO -    245    151.4 MiB      0.2 MiB                                                      format='%Y-%m-%d', errors='coerce')

2018-05-02 17:25:06,867 - memory_profile6_log - INFO -    246    151.4 MiB      0.0 MiB           print "list(model_transformsv) before rename: ", list(model_transformsv)       

2018-05-02 17:25:06,867 - memory_profile6_log - INFO -    247    151.4 MiB      0.0 MiB           print len(model_transformsv)                

2018-05-02 17:25:06,868 - memory_profile6_log - INFO -    248    151.4 MiB      0.0 MiB           model_transformsv = model_transformsv.rename(columns={'is_general': 'topic_is_general', 'p0_posterior': 'interest_score',

2018-05-02 17:25:06,868 - memory_profile6_log - INFO -    249    151.7 MiB      0.3 MiB                                                                 'rank':'interest_rank' , 'date':'interest_score_created_at'})

2018-05-02 17:25:06,868 - memory_profile6_log - INFO -    250    151.7 MiB      0.0 MiB           print "list(model_transformsv) after rename: ", list(model_transformsv)    

2018-05-02 17:25:06,868 - memory_profile6_log - INFO -    251    151.7 MiB      0.0 MiB           print len(model_transformsv) 

2018-05-02 17:25:06,872 - memory_profile6_log - INFO -    252    151.7 MiB      0.0 MiB           del model_transform

2018-05-02 17:25:06,875 - memory_profile6_log - INFO -    253    151.7 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-05-02 17:25:06,875 - memory_profile6_log - INFO -    254    151.7 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-05-02 17:25:06,875 - memory_profile6_log - INFO -    255                             

2018-05-02 17:25:06,875 - memory_profile6_log - INFO -    256    151.7 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-05-02 17:25:06,875 - memory_profile6_log - INFO -    257                                     # ~ Place your code to save the training model here ~

2018-05-02 17:25:06,877 - memory_profile6_log - INFO -    258    151.7 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-05-02 17:25:06,877 - memory_profile6_log - INFO -    259                                         logger.info("Using google datastore as storage...")

2018-05-02 17:25:06,878 - memory_profile6_log - INFO -    260                                         if multproc:

2018-05-02 17:25:06,878 - memory_profile6_log - INFO -    261                                             # ~ save transform models ~

2018-05-02 17:25:06,878 - memory_profile6_log - INFO -    262                                             logger.info("Saving main Transform model to Google DataStore...")

2018-05-02 17:25:06,880 - memory_profile6_log - INFO -    263                                             logger.info("Saving total data: %d", len(model_transformsv))

2018-05-02 17:25:06,880 - memory_profile6_log - INFO -    264                                             mh.saveDataStorePutMulti(model_transformsv)

2018-05-02 17:25:06,880 - memory_profile6_log - INFO -    265                             

2018-05-02 17:25:06,881 - memory_profile6_log - INFO -    266                                             # ~ save fitted models ~

2018-05-02 17:25:06,881 - memory_profile6_log - INFO -    267                                             logger.info("Saving fitted_models as history to Google DataStore...")

2018-05-02 17:25:06,884 - memory_profile6_log - INFO -    268                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 17:25:06,885 - memory_profile6_log - INFO -    269                                             fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-05-02 17:25:06,888 - memory_profile6_log - INFO -    270                                             logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-05-02 17:25:06,888 - memory_profile6_log - INFO -    271                                             X_split = np.array_split(fitted_models_sigmant, 10)

2018-05-02 17:25:06,888 - memory_profile6_log - INFO -    272                                             logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-05-02 17:25:06,888 - memory_profile6_log - INFO -    273                                             for ix in range(len(X_split)):

2018-05-02 17:25:06,888 - memory_profile6_log - INFO -    274                                                 logger.info("processing batch-%d", ix)

2018-05-02 17:25:06,890 - memory_profile6_log - INFO -    275                                                 mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-05-02 17:25:06,890 - memory_profile6_log - INFO -    276                             

2018-05-02 17:25:06,891 - memory_profile6_log - INFO -    277                                             del X_split

2018-05-02 17:25:06,891 - memory_profile6_log - INFO -    278                                             logger.info("deleting X_split...")

2018-05-02 17:25:06,891 - memory_profile6_log - INFO -    279                                             del save_sigma_nt

2018-05-02 17:25:06,891 - memory_profile6_log - INFO -    280                                             logger.info("deleting save_sigma_nt...")

2018-05-02 17:25:06,892 - memory_profile6_log - INFO -    281                             

2018-05-02 17:25:06,892 - memory_profile6_log - INFO -    282                                             del BR

2018-05-02 17:25:06,898 - memory_profile6_log - INFO -    283                                             logger.info("deleting BR...")

2018-05-02 17:25:06,898 - memory_profile6_log - INFO -    284                             

2018-05-02 17:25:06,900 - memory_profile6_log - INFO -    285    151.7 MiB      0.0 MiB           elif str(saveto).lower() == "elastic":

2018-05-02 17:25:06,901 - memory_profile6_log - INFO -    286    151.7 MiB      0.0 MiB               logger.info("Using ElasticSearch as storage...")

2018-05-02 17:25:06,901 - memory_profile6_log - INFO -    287                                         """logging.info("Saving main Transform model to Elasticsearch...")

2018-05-02 17:25:06,901 - memory_profile6_log - INFO -    288                                         # print model_transformsv.head(5)

2018-05-02 17:25:06,901 - memory_profile6_log - INFO -    289                                         

2018-05-02 17:25:06,901 - memory_profile6_log - INFO -    290                                         X_split = np.array_split(model_transformsv, 5)

2018-05-02 17:25:06,903 - memory_profile6_log - INFO -    291                                         logger.info("Saving total data: %d", len(model_transformsv))

2018-05-02 17:25:06,903 - memory_profile6_log - INFO -    292                                         logger.info("Len of X_split for batch save model_transformsv: %d", len(X_split))

2018-05-02 17:25:06,904 - memory_profile6_log - INFO -    293                                         for ix in range(len(X_split)):

2018-05-02 17:25:06,904 - memory_profile6_log - INFO -    294                                             logger.info("processing batch-%d", ix)

2018-05-02 17:25:06,905 - memory_profile6_log - INFO -    295                                             mh.saveElasticS(X_split[ix])

2018-05-02 17:25:06,905 - memory_profile6_log - INFO -    296                                         del X_split

2018-05-02 17:25:06,905 - memory_profile6_log - INFO -    297                                         """

2018-05-02 17:25:06,911 - memory_profile6_log - INFO -    298                             

2018-05-02 17:25:06,911 - memory_profile6_log - INFO -    299    151.7 MiB      0.0 MiB               logger.info("Saving fitted_models as history to Elasticsearch...")

2018-05-02 17:25:06,911 - memory_profile6_log - INFO -    300    151.8 MiB      0.1 MiB               save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 17:25:06,913 - memory_profile6_log - INFO -    301    152.8 MiB      1.0 MiB               fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-05-02 17:25:06,914 - memory_profile6_log - INFO -    302                             

2018-05-02 17:25:06,914 - memory_profile6_log - INFO -    303    152.8 MiB      0.0 MiB               fitted_models_sigmant['uid_topid'] = fitted_models_sigmant["user_id"].map(str) + "_" + fitted_models_sigmant["topic_id"].map(str)

2018-05-02 17:25:06,914 - memory_profile6_log - INFO -    304    153.8 MiB      1.1 MiB               fitted_models_sigmant = fitted_models_sigmant[["uid_topid", "pt_posterior_x_Nt", "smoothed_pt_posterior", "p0_cat_ci", "sigma_Nt"]]

2018-05-02 17:25:06,914 - memory_profile6_log - INFO -    305                               

2018-05-02 17:25:06,915 - memory_profile6_log - INFO -    306    154.9 MiB      1.1 MiB               X_split = np.array_split(fitted_models_sigmant, 5)

2018-05-02 17:25:06,915 - memory_profile6_log - INFO -    307    154.9 MiB      0.0 MiB               logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-05-02 17:25:06,915 - memory_profile6_log - INFO -    308    154.9 MiB      0.0 MiB               logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-05-02 17:25:06,917 - memory_profile6_log - INFO -    309    155.7 MiB     -5.0 MiB               for ix in range(len(X_split)):

2018-05-02 17:25:06,917 - memory_profile6_log - INFO -    310    155.7 MiB     -8.0 MiB                   logger.info("processing batch-%d", ix)

2018-05-02 17:25:06,917 - memory_profile6_log - INFO -    311    155.7 MiB    -10.4 MiB                   mh.saveElasticS(X_split[ix], esindex_name="fitted_hist_index",  estype_name='fitted_hist_type')

2018-05-02 17:25:06,917 - memory_profile6_log - INFO -    312    152.5 MiB     -3.2 MiB               del X_split

2018-05-02 17:25:06,923 - memory_profile6_log - INFO -    313                                         

2018-05-02 17:25:06,923 - memory_profile6_log - INFO -    314    152.5 MiB      0.0 MiB               del BR

2018-05-02 17:25:06,924 - memory_profile6_log - INFO -    315    152.5 MiB      0.0 MiB               logger.info("deleting BR...")

2018-05-02 17:25:06,924 - memory_profile6_log - INFO -    316    152.5 MiB      0.0 MiB               del save_sigma_nt

2018-05-02 17:25:06,926 - memory_profile6_log - INFO -    317    152.5 MiB      0.0 MiB               logger.info("deleting save_sigma_nt...")

2018-05-02 17:25:06,926 - memory_profile6_log - INFO -    318    152.5 MiB      0.0 MiB               del fitted_models_sigmant

2018-05-02 17:25:06,927 - memory_profile6_log - INFO -    319    152.5 MiB      0.0 MiB               logger.info("deleting fitted_models_sigmant...")

2018-05-02 17:25:06,927 - memory_profile6_log - INFO -    320                             

2018-05-02 17:25:06,927 - memory_profile6_log - INFO -    321                                     # need save sigma_nt for daily train

2018-05-02 17:25:06,928 - memory_profile6_log - INFO -    322                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-05-02 17:25:06,928 - memory_profile6_log - INFO -    323                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-05-02 17:25:06,930 - memory_profile6_log - INFO -    324    152.5 MiB      0.0 MiB           if start_date and end_date:

2018-05-02 17:25:06,934 - memory_profile6_log - INFO -    325                                         if not fitby_sigmant:

2018-05-02 17:25:06,934 - memory_profile6_log - INFO -    326                                             logging.info("Saving sigma Nt...")

2018-05-02 17:25:06,936 - memory_profile6_log - INFO -    327                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 17:25:06,937 - memory_profile6_log - INFO -    328                                             save_sigma_nt['start_date'] = start_date

2018-05-02 17:25:06,937 - memory_profile6_log - INFO -    329                                             save_sigma_nt['end_date'] = end_date

2018-05-02 17:25:06,938 - memory_profile6_log - INFO -    330                                             print save_sigma_nt.head(5)

2018-05-02 17:25:06,938 - memory_profile6_log - INFO -    331                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-05-02 17:25:06,940 - memory_profile6_log - INFO -    332                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-05-02 17:25:06,940 - memory_profile6_log - INFO -    333    152.5 MiB      0.0 MiB       return

2018-05-02 17:25:06,940 - memory_profile6_log - INFO - 


2018-05-02 17:25:06,940 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-05-02 17:31:31,450 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 17:31:31,451 - memory_profile6_log - INFO - date_generated: 
2018-05-02 17:31:31,451 - memory_profile6_log - INFO -  
2018-05-02 17:31:31,451 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 17, 31, 31, 450000)]
2018-05-02 17:31:31,453 - memory_profile6_log - INFO - 

2018-05-02 17:31:31,453 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 17:31:31,453 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 17:31:31,453 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 17:31:31,595 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 17:31:31,598 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 17:31:41,858 - memory_profile6_log - INFO - size of df: 5.05 MB
2018-05-02 17:31:41,859 - memory_profile6_log - INFO - getting total: 20000 training data(genuine interest) for date: 2018-05-01
2018-05-02 17:31:41,904 - memory_profile6_log - INFO - loading history data from elastic...
2018-05-02 17:31:41,905 - memory_profile6_log - INFO - Len of X_split for batch load: 10
2018-05-02 17:31:41,907 - memory_profile6_log - INFO - Appending history data...
2018-05-02 17:31:41,946 - memory_profile6_log - INFO - call 2000 history data...
2018-05-02 17:34:12,289 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 17:34:12,290 - memory_profile6_log - INFO - date_generated: 
2018-05-02 17:34:12,292 - memory_profile6_log - INFO -  
2018-05-02 17:34:12,292 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 17, 34, 12, 289000)]
2018-05-02 17:34:12,292 - memory_profile6_log - INFO - 

2018-05-02 17:34:12,292 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 17:34:12,292 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 17:34:12,293 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 17:34:12,438 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 17:34:12,443 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 17:34:21,976 - memory_profile6_log - INFO - size of df: 5.05 MB
2018-05-02 17:34:21,979 - memory_profile6_log - INFO - getting total: 20000 training data(genuine interest) for date: 2018-05-01
2018-05-02 17:34:22,009 - memory_profile6_log - INFO - loading history data from elastic...
2018-05-02 17:34:22,010 - memory_profile6_log - INFO - Len of X_split for batch load: 10
2018-05-02 17:34:22,012 - memory_profile6_log - INFO - Appending history data...
2018-05-02 17:34:22,051 - memory_profile6_log - INFO - call 2000 history data...
2018-05-02 17:34:25,289 - memory_profile6_log - INFO -                                            uid_topid  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  16130e77ab1ae-0e200dfc009b5b-16457644-38400-16...         227.678634             237.678634   0.110964       227
1  16130e8bf71104-001943f03c4b79-585d68-38400-161...         331.423702             341.423702   0.006561       251
2  16130e8bf71104-001943f03c4b79-585d68-38400-161...         331.018407             341.018407   0.006664       251
3  16130e91ce5f1-071af767bfbd58-4c33197a-38400-16...          67.390934              77.390934   0.110964        81
4  16130ee125610c-03aa3303ee081a-e46181e-38400-16...          51.003231              61.003231   0.110964        74
2018-05-02 17:34:25,298 - memory_profile6_log - INFO - 

2018-05-02 17:34:25,305 - memory_profile6_log - INFO - call 2000 history data...
2018-05-02 17:34:30,209 - memory_profile6_log - INFO -                                            uid_topid  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  16130ea976fb7-046bf699a445a1-4b476b3b-38400-16...         195.418685             205.418685   0.110964       142
1  16130ebcc1995-089b5acdc39f35-716a1530-38400-16...         134.137879             144.137879   0.110964       190
2  16130eddcf312d-042f4e2fa42d57-5c01624f-38400-1...          40.120904              50.120904   0.110964        45
3  16130f4124cf1-0898267e3f80d2-282b543f-38400-16...          39.273657              49.273657   0.110964        31
4  16130f53ef03d-086697dfd-450d4f0d-2c880-16130f5...         103.388945             113.388945   0.110964        74
2018-05-02 17:34:30,210 - memory_profile6_log - INFO - 

2018-05-02 17:34:30,217 - memory_profile6_log - INFO - call 2000 history data...
2018-05-02 17:34:34,651 - memory_profile6_log - INFO -                                            uid_topid  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  16130e8bf71104-001943f03c4b79-585d68-38400-161...         291.458674             301.458674   0.110964       251
1  16130e8bf71104-001943f03c4b79-585d68-38400-161...         204.700947             214.700947   0.034659       251
2  16130e91ce5f1-071af767bfbd58-4c33197a-38400-16...         111.180287             121.180287   0.075163        81
3  16130e982d911b-05318b7ae00a1a-c0e0044-38400-16...         132.653211             142.653211   0.110964       100
4  16130e982d911b-05318b7ae00a1a-c0e0044-38400-16...         225.874064             235.874064   0.034659       100
2018-05-02 17:34:34,651 - memory_profile6_log - INFO - 

2018-05-02 17:34:34,658 - memory_profile6_log - INFO - call 2000 history data...
2018-05-02 17:34:39,743 - memory_profile6_log - INFO -                                            uid_topid  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  16130e77ab1ae-0e200dfc009b5b-16457644-38400-16...         543.187160             553.187160   0.000590       227
1  16130e8bf71104-001943f03c4b79-585d68-38400-161...          99.857574             109.857574   0.005467       251
2  16130eb903239-0b97b26455b808-7065430a-38400-16...         210.815038             220.815038   0.075163       103
3  16130ebcc1995-089b5acdc39f35-716a1530-38400-16...         385.283534             395.283534   0.004464       190
4  16130ee9f3651-0c7b790e00b1e6-292a5039-38400-16...         552.310683             562.310683   0.075163       136
2018-05-02 17:34:39,743 - memory_profile6_log - INFO - 

2018-05-02 17:34:39,750 - memory_profile6_log - INFO - call 2000 history data...
2018-05-02 17:34:43,194 - memory_profile6_log - INFO -                                            uid_topid  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  16130e91ce5f1-071af767bfbd58-4c33197a-38400-16...          51.346328              61.346328   0.005467        81
1  16130e91ce5f1-071af767bfbd58-4c33197a-38400-16...         115.513008             125.513008   0.067631        81
2  16130e91ce5f1-071af767bfbd58-4c33197a-38400-16...          39.925538              49.925538   0.004738        81
3  16130e982d911b-05318b7ae00a1a-c0e0044-38400-16...          39.925538              49.925538   0.004738       100
4  16130fdafc746-02b8e228aad61d-4e645706-38400-16...         748.769600             758.769600   0.025705       135
2018-05-02 17:34:43,196 - memory_profile6_log - INFO - 

2018-05-02 17:34:43,203 - memory_profile6_log - INFO - call 2000 history data...
2018-05-02 17:34:45,999 - memory_profile6_log - INFO -                                            uid_topid  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  16130e8bf71104-001943f03c4b79-585d68-38400-161...       14262.542857           14272.542857   0.000100       251
1  16130e8bf71104-001943f03c4b79-585d68-38400-161...         153.454965             163.454965   0.000236       251
2  16130e8bf71104-001943f03c4b79-585d68-38400-161...         170.758830             180.758830   0.004917       251
3  16130e8bf71104-001943f03c4b79-585d68-38400-161...         265.328611             275.328611   0.012182       251
4  16130e982d911b-05318b7ae00a1a-c0e0044-38400-16...         306.909929             316.909929   0.000236       100
2018-05-02 17:34:46,000 - memory_profile6_log - INFO - 

2018-05-02 17:34:46,007 - memory_profile6_log - INFO - call 2000 history data...
2018-05-02 17:34:48,253 - memory_profile6_log - INFO -                                            uid_topid  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  16130e8bf71104-001943f03c4b79-585d68-38400-161...         258.647150             268.647150   0.001652       251
1  16130e8bf71104-001943f03c4b79-585d68-38400-161...       11609.046512           11619.046512   0.000067       251
2  16130e8bf71104-001943f03c4b79-585d68-38400-161...         821.422669             831.422669   0.001133       251
3  16130e8bf71104-001943f03c4b79-585d68-38400-161...         538.474913             548.474913   0.030802       251
4  16130e8bf71104-001943f03c4b79-585d68-38400-161...         579.778165             589.778165   0.000743       251
2018-05-02 17:34:48,253 - memory_profile6_log - INFO - 

2018-05-02 17:34:48,260 - memory_profile6_log - INFO - call 2000 history data...
2018-05-02 17:34:52,085 - memory_profile6_log - INFO -                                            uid_topid  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  16130e8bf71104-001943f03c4b79-585d68-38400-161...        2641.211640            2651.211640   0.000862       251
1  16130e8bf71104-001943f03c4b79-585d68-38400-161...         309.525334             319.525334   0.003680       251
2  16130e91ce5f1-071af767bfbd58-4c33197a-38400-16...        1306.777487            1316.777487   0.002090        81
3  16130e982d911b-05318b7ae00a1a-c0e0044-38400-16...          86.156196              96.156196   0.003499       100
4  16130e982d911b-05318b7ae00a1a-c0e0044-38400-16...         149.437754             159.437754   0.018864       100
2018-05-02 17:34:52,085 - memory_profile6_log - INFO - 

2018-05-02 17:34:52,095 - memory_profile6_log - INFO - call 2000 history data...
2018-05-02 17:34:54,323 - memory_profile6_log - INFO -                                            uid_topid  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  16130e8bf71104-001943f03c4b79-585d68-38400-161...         889.029269             899.029269   0.003729       251
1  16130e8bf71104-001943f03c4b79-585d68-38400-161...         132.217984             142.217984   0.001942       251
2  16130e8bf71104-001943f03c4b79-585d68-38400-161...         170.062117             180.062117   0.003543       251
3  16130e982d911b-05318b7ae00a1a-c0e0044-38400-16...         104.258354             114.258354   0.008433       100
4  16130e982d911b-05318b7ae00a1a-c0e0044-38400-16...          71.312714              81.312714   0.001702       100
2018-05-02 17:34:54,325 - memory_profile6_log - INFO - 

2018-05-02 17:34:54,332 - memory_profile6_log - INFO - call 2000 history data...
2018-05-02 17:34:57,964 - memory_profile6_log - INFO -                                            uid_topid  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  16130e8bf71104-001943f03c4b79-585d68-38400-161...         809.058347             819.058347   0.003553       251
1  16130e8bf71104-001943f03c4b79-585d68-38400-161...          66.915416              76.915416   0.004074       251
2  16130e8bf71104-001943f03c4b79-585d68-38400-161...          90.106318             100.106318   0.013512       251
3  16130e8bf71104-001943f03c4b79-585d68-38400-161...         204.753486             214.753486   0.004592       251
4  16130e91ce5f1-071af767bfbd58-4c33197a-38400-16...         271.853234             281.853234   0.000444        81
2018-05-02 17:34:57,967 - memory_profile6_log - INFO - 

2018-05-02 17:34:57,967 - memory_profile6_log - INFO - len datalist: 0
2018-05-02 17:34:57,970 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-05-02 17:34:57,979 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 17:34:57,980 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 17:34:57,980 - memory_profile6_log - INFO - ================================================

2018-05-02 17:34:57,982 - memory_profile6_log - INFO -    359     86.7 MiB     86.7 MiB   @profile

2018-05-02 17:34:57,983 - memory_profile6_log - INFO -    360                             def BQPreprocess(cpu, date_generated, client, query_fit, loadfrom="elastic"):

2018-05-02 17:34:57,983 - memory_profile6_log - INFO -    361     86.7 MiB      0.0 MiB       bq_client = client

2018-05-02 17:34:57,986 - memory_profile6_log - INFO -    362     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 17:34:57,987 - memory_profile6_log - INFO -    363                             

2018-05-02 17:34:57,989 - memory_profile6_log - INFO -    364     86.7 MiB      0.0 MiB       datalist = []

2018-05-02 17:34:57,990 - memory_profile6_log - INFO -    365     86.7 MiB      0.0 MiB       datalist_hist = []

2018-05-02 17:34:57,992 - memory_profile6_log - INFO -    366                             

2018-05-02 17:34:57,993 - memory_profile6_log - INFO -    367     86.7 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-05-02 17:34:57,996 - memory_profile6_log - INFO -    368    141.6 MiB      0.0 MiB       for ndate in date_generated:

2018-05-02 17:34:57,997 - memory_profile6_log - INFO -    369    139.1 MiB     52.4 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-05-02 17:34:57,999 - memory_profile6_log - INFO -    370    139.1 MiB      0.0 MiB           if tframe is not None:

2018-05-02 17:34:58,000 - memory_profile6_log - INFO -    371    139.1 MiB      0.0 MiB               if not tframe.empty:

2018-05-02 17:34:58,000 - memory_profile6_log - INFO -    372    139.1 MiB      0.0 MiB                   if loadfrom.strip().lower() == 'datastore':

2018-05-02 17:34:58,000 - memory_profile6_log - INFO -    373                                                 X_split = np.array_split(tframe, 5)

2018-05-02 17:34:58,002 - memory_profile6_log - INFO -    374                                                 logger.info("loading history data from datastore...")

2018-05-02 17:34:58,003 - memory_profile6_log - INFO -    375                                                 logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 17:34:58,003 - memory_profile6_log - INFO -    376                                                 logger.info("Appending history data...")

2018-05-02 17:34:58,005 - memory_profile6_log - INFO -    377                                                 for ix in range(len(X_split)):

2018-05-02 17:34:58,007 - memory_profile6_log - INFO -    378                                                     # ~ loading history

2018-05-02 17:34:58,009 - memory_profile6_log - INFO -    379                                                     """

2018-05-02 17:34:58,010 - memory_profile6_log - INFO -    380                                                         disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-05-02 17:34:58,010 - memory_profile6_log - INFO -    381                                                     """

2018-05-02 17:34:58,012 - memory_profile6_log - INFO -    382                                                     logger.info("processing batch-%d", ix)

2018-05-02 17:34:58,013 - memory_profile6_log - INFO -    383                                                     # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-05-02 17:34:58,013 - memory_profile6_log - INFO -    384                                                     logger.info("creating list history data...")

2018-05-02 17:34:58,015 - memory_profile6_log - INFO -    385                                                     #lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-05-02 17:34:58,015 - memory_profile6_log - INFO -    386                                                     lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 17:34:58,016 - memory_profile6_log - INFO -    387                             

2018-05-02 17:34:58,019 - memory_profile6_log - INFO -    388                                                     logger.info("call history data...")

2018-05-02 17:34:58,020 - memory_profile6_log - INFO -    389                                                     h_frame = mh.loadDSHistory(lhistory)

2018-05-02 17:34:58,023 - memory_profile6_log - INFO -    390                             

2018-05-02 17:34:58,023 - memory_profile6_log - INFO -    391                                                     # me = os.getpid()

2018-05-02 17:34:58,023 - memory_profile6_log - INFO -    392                                                     # kill_proc_tree(me)

2018-05-02 17:34:58,025 - memory_profile6_log - INFO -    393                             

2018-05-02 17:34:58,025 - memory_profile6_log - INFO -    394                                                     logger.info("done collecting history data, appending now...")

2018-05-02 17:34:58,026 - memory_profile6_log - INFO -    395                                                     for m in h_frame:

2018-05-02 17:34:58,026 - memory_profile6_log - INFO -    396                                                         if m is not None:

2018-05-02 17:34:58,026 - memory_profile6_log - INFO -    397                                                             if len(m) > 0:

2018-05-02 17:34:58,028 - memory_profile6_log - INFO -    398                                                                 datalist_hist.append(pd.DataFrame(m))

2018-05-02 17:34:58,029 - memory_profile6_log - INFO -    399                                                     del h_frame

2018-05-02 17:34:58,032 - memory_profile6_log - INFO -    400                                                     del lhistory

2018-05-02 17:34:58,033 - memory_profile6_log - INFO -    401                             

2018-05-02 17:34:58,035 - memory_profile6_log - INFO -    402                                                 logger.info("Appending training data...")

2018-05-02 17:34:58,036 - memory_profile6_log - INFO -    403                                                 datalist.append(tframe)

2018-05-02 17:34:58,036 - memory_profile6_log - INFO -    404    139.1 MiB      0.0 MiB                   elif loadfrom.strip().lower() == 'elastic':

2018-05-02 17:34:58,038 - memory_profile6_log - INFO -    405    139.5 MiB      0.4 MiB                       X_split = np.array_split(tframe, 10)

2018-05-02 17:34:58,039 - memory_profile6_log - INFO -    406    139.5 MiB      0.0 MiB                       logger.info("loading history data from elastic...")

2018-05-02 17:34:58,039 - memory_profile6_log - INFO -    407    139.5 MiB      0.0 MiB                       logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 17:34:58,042 - memory_profile6_log - INFO -    408    139.5 MiB      0.0 MiB                       logger.info("Appending history data...")

2018-05-02 17:34:58,042 - memory_profile6_log - INFO -    409    141.6 MiB     -0.1 MiB                       for ix in range(len(X_split)):

2018-05-02 17:34:58,043 - memory_profile6_log - INFO -    410    141.5 MiB      0.6 MiB                           lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 17:34:58,045 - memory_profile6_log - INFO -    411    141.5 MiB     -0.1 MiB                           logger.info("call %d history data...", len(lhistory))

2018-05-02 17:34:58,046 - memory_profile6_log - INFO -    412    141.5 MiB     -0.1 MiB                           inside_data = mh.loadESHistory(lhistory,

2018-05-02 17:34:58,046 - memory_profile6_log - INFO -    413    141.5 MiB     -0.1 MiB                                                          esindex_name='fitted_hist_index',

2018-05-02 17:34:58,048 - memory_profile6_log - INFO -    414    141.6 MiB      1.4 MiB                                                          estype_name='fitted_hist_type')

2018-05-02 17:34:58,049 - memory_profile6_log - INFO -    415                             

2018-05-02 17:34:58,049 - memory_profile6_log - INFO -    416    141.6 MiB     -0.1 MiB                           if not inside_data.empty:

2018-05-02 17:34:58,049 - memory_profile6_log - INFO -    417    141.6 MiB      0.0 MiB                               print inside_data.head(5)

2018-05-02 17:34:58,051 - memory_profile6_log - INFO -    418    141.6 MiB     -0.1 MiB                               datalist_hist.append(inside_data)

2018-05-02 17:34:58,053 - memory_profile6_log - INFO -    419    141.6 MiB     -0.1 MiB                               del inside_data

2018-05-02 17:34:58,053 - memory_profile6_log - INFO -    420                                                     else:

2018-05-02 17:34:58,055 - memory_profile6_log - INFO -    421                                                         logger.info("uid: %s with topic_id: %s is empty!" % (framedata.user_id, framedata.topic_id))

2018-05-02 17:34:58,055 - memory_profile6_log - INFO -    422                                             else:

2018-05-02 17:34:58,056 - memory_profile6_log - INFO -    423                                                 logger.info("Unknows source is selected !")

2018-05-02 17:34:58,058 - memory_profile6_log - INFO -    424                                                 break

2018-05-02 17:34:58,058 - memory_profile6_log - INFO -    425                                     else: 

2018-05-02 17:34:58,059 - memory_profile6_log - INFO -    426                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-05-02 17:34:58,059 - memory_profile6_log - INFO -    427    141.6 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-05-02 17:34:58,059 - memory_profile6_log - INFO -    428    141.6 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-05-02 17:34:58,061 - memory_profile6_log - INFO -    429                             

2018-05-02 17:34:58,061 - memory_profile6_log - INFO -    430    141.6 MiB      0.0 MiB       return datalist, datalist_hist

2018-05-02 17:34:58,065 - memory_profile6_log - INFO - 


2018-05-02 17:34:58,082 - memory_profile6_log - INFO - size of big_frame_hist: 22.79 KB
2018-05-02 17:40:29,732 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 17:40:29,736 - memory_profile6_log - INFO - date_generated: 
2018-05-02 17:40:29,736 - memory_profile6_log - INFO -  
2018-05-02 17:40:29,736 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 17, 40, 29, 733000)]
2018-05-02 17:40:29,736 - memory_profile6_log - INFO - 

2018-05-02 17:40:29,736 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 17:40:29,736 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 17:40:29,737 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 17:40:29,877 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 17:40:29,881 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 17:40:39,831 - memory_profile6_log - INFO - size of df: 5.05 MB
2018-05-02 17:40:39,832 - memory_profile6_log - INFO - getting total: 20000 training data(genuine interest) for date: 2018-05-01
2018-05-02 17:40:39,973 - memory_profile6_log - INFO - loading history data from elastic...
2018-05-02 17:40:39,974 - memory_profile6_log - INFO - Len of X_split for batch load: 50
2018-05-02 17:40:39,976 - memory_profile6_log - INFO - Appending history data...
2018-05-02 17:40:40,013 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:40:42,696 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:40:44,237 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:40:45,956 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:40:47,993 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:40:49,867 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:40:51,592 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:40:53,490 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:40:55,302 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:41:06,168 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:41:07,974 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:41:09,657 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:41:12,217 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:41:13,898 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:41:15,737 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:41:17,391 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:41:18,905 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:41:20,615 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:41:22,519 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:41:24,066 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:41:25,910 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:41:27,730 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:41:29,878 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:41:31,657 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:41:33,502 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:41:35,816 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:41:37,496 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:41:39,335 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:41:41,177 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:41:43,181 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:41:44,864 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:41:46,698 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:41:48,476 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:41:50,217 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:41:51,976 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:41:53,680 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:41:55,405 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:41:57,345 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:41:59,496 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:42:01,312 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:42:03,043 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:42:04,861 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:42:06,671 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:42:08,786 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:42:10,625 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:42:12,619 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:42:14,546 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:42:16,398 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:42:18,220 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:42:20,578 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 17:42:22,220 - memory_profile6_log - INFO - Appending training data...
2018-05-02 17:42:22,220 - memory_profile6_log - INFO - len datalist: 1
2018-05-02 17:42:22,223 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-05-02 17:42:22,226 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 17:42:22,227 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 17:42:22,227 - memory_profile6_log - INFO - ================================================

2018-05-02 17:42:22,229 - memory_profile6_log - INFO -    359     86.6 MiB     86.6 MiB   @profile

2018-05-02 17:42:22,230 - memory_profile6_log - INFO -    360                             def BQPreprocess(cpu, date_generated, client, query_fit, loadfrom="elastic"):

2018-05-02 17:42:22,233 - memory_profile6_log - INFO -    361     86.6 MiB      0.0 MiB       bq_client = client

2018-05-02 17:42:22,233 - memory_profile6_log - INFO -    362     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 17:42:22,234 - memory_profile6_log - INFO -    363                             

2018-05-02 17:42:22,236 - memory_profile6_log - INFO -    364     86.6 MiB      0.0 MiB       datalist = []

2018-05-02 17:42:22,236 - memory_profile6_log - INFO -    365     86.6 MiB      0.0 MiB       datalist_hist = []

2018-05-02 17:42:22,236 - memory_profile6_log - INFO -    366                             

2018-05-02 17:42:22,237 - memory_profile6_log - INFO -    367     86.6 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-05-02 17:42:22,239 - memory_profile6_log - INFO -    368    147.9 MiB      0.0 MiB       for ndate in date_generated:

2018-05-02 17:42:22,239 - memory_profile6_log - INFO -    369    140.0 MiB     53.4 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-05-02 17:42:22,240 - memory_profile6_log - INFO -    370    140.0 MiB      0.0 MiB           if tframe is not None:

2018-05-02 17:42:22,240 - memory_profile6_log - INFO -    371    140.0 MiB      0.0 MiB               if not tframe.empty:

2018-05-02 17:42:22,243 - memory_profile6_log - INFO -    372    140.0 MiB      0.0 MiB                   if loadfrom.strip().lower() == 'datastore':

2018-05-02 17:42:22,244 - memory_profile6_log - INFO -    373                                                 X_split = np.array_split(tframe, 5)

2018-05-02 17:42:22,246 - memory_profile6_log - INFO -    374                                                 logger.info("loading history data from datastore...")

2018-05-02 17:42:22,246 - memory_profile6_log - INFO -    375                                                 logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 17:42:22,247 - memory_profile6_log - INFO -    376                                                 logger.info("Appending history data...")

2018-05-02 17:42:22,249 - memory_profile6_log - INFO -    377                                                 for ix in range(len(X_split)):

2018-05-02 17:42:22,250 - memory_profile6_log - INFO -    378                                                     # ~ loading history

2018-05-02 17:42:22,250 - memory_profile6_log - INFO -    379                                                     """

2018-05-02 17:42:22,250 - memory_profile6_log - INFO -    380                                                         disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-05-02 17:42:22,252 - memory_profile6_log - INFO -    381                                                     """

2018-05-02 17:42:22,255 - memory_profile6_log - INFO -    382                                                     logger.info("processing batch-%d", ix)

2018-05-02 17:42:22,256 - memory_profile6_log - INFO -    383                                                     # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-05-02 17:42:22,257 - memory_profile6_log - INFO -    384                                                     logger.info("creating list history data...")

2018-05-02 17:42:22,259 - memory_profile6_log - INFO -    385                                                     #lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-05-02 17:42:22,260 - memory_profile6_log - INFO -    386                                                     lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 17:42:22,262 - memory_profile6_log - INFO -    387                             

2018-05-02 17:42:22,263 - memory_profile6_log - INFO -    388                                                     logger.info("call history data...")

2018-05-02 17:42:22,265 - memory_profile6_log - INFO -    389                                                     h_frame = mh.loadDSHistory(lhistory)

2018-05-02 17:42:22,266 - memory_profile6_log - INFO -    390                             

2018-05-02 17:42:22,267 - memory_profile6_log - INFO -    391                                                     # me = os.getpid()

2018-05-02 17:42:22,269 - memory_profile6_log - INFO -    392                                                     # kill_proc_tree(me)

2018-05-02 17:42:22,269 - memory_profile6_log - INFO -    393                             

2018-05-02 17:42:22,269 - memory_profile6_log - INFO -    394                                                     logger.info("done collecting history data, appending now...")

2018-05-02 17:42:22,270 - memory_profile6_log - INFO -    395                                                     for m in h_frame:

2018-05-02 17:42:22,272 - memory_profile6_log - INFO -    396                                                         if m is not None:

2018-05-02 17:42:22,273 - memory_profile6_log - INFO -    397                                                             if len(m) > 0:

2018-05-02 17:42:22,273 - memory_profile6_log - INFO -    398                                                                 datalist_hist.append(pd.DataFrame(m))

2018-05-02 17:42:22,276 - memory_profile6_log - INFO -    399                                                     del h_frame

2018-05-02 17:42:22,279 - memory_profile6_log - INFO -    400                                                     del lhistory

2018-05-02 17:42:22,279 - memory_profile6_log - INFO -    401                             

2018-05-02 17:42:22,280 - memory_profile6_log - INFO -    402                                                 logger.info("Appending training data...")

2018-05-02 17:42:22,280 - memory_profile6_log - INFO -    403                                                 datalist.append(tframe)

2018-05-02 17:42:22,282 - memory_profile6_log - INFO -    404    140.0 MiB      0.0 MiB                   elif loadfrom.strip().lower() == 'elastic':

2018-05-02 17:42:22,282 - memory_profile6_log - INFO -    405    140.1 MiB      0.1 MiB                       X_split = np.array_split(tframe, 50)

2018-05-02 17:42:22,283 - memory_profile6_log - INFO -    406    140.1 MiB      0.0 MiB                       logger.info("loading history data from elastic...")

2018-05-02 17:42:22,285 - memory_profile6_log - INFO -    407    140.1 MiB      0.0 MiB                       logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 17:42:22,288 - memory_profile6_log - INFO -    408    140.1 MiB      0.0 MiB                       logger.info("Appending history data...")

2018-05-02 17:42:22,289 - memory_profile6_log - INFO -    409    147.9 MiB     -0.2 MiB                       for ix in range(len(X_split)):

2018-05-02 17:42:22,289 - memory_profile6_log - INFO -    410    147.9 MiB      0.2 MiB                           lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 17:42:22,290 - memory_profile6_log - INFO -    411    147.9 MiB     -0.2 MiB                           logger.info("call %d history data...", len(lhistory))

2018-05-02 17:42:22,292 - memory_profile6_log - INFO -    412    147.9 MiB     -0.2 MiB                           inside_data = mh.loadESHistory(lhistory,

2018-05-02 17:42:22,292 - memory_profile6_log - INFO -    413    147.9 MiB     -0.2 MiB                                                          esindex_name='fitted_hist_index',

2018-05-02 17:42:22,292 - memory_profile6_log - INFO -    414    147.9 MiB      7.2 MiB                                                          estype_name='fitted_hist_type')

2018-05-02 17:42:22,293 - memory_profile6_log - INFO -    415                             

2018-05-02 17:42:22,293 - memory_profile6_log - INFO -    416    147.9 MiB     -0.2 MiB                           if not inside_data.empty:

2018-05-02 17:42:22,295 - memory_profile6_log - INFO -    417    147.9 MiB     -0.2 MiB                               datalist_hist.append(inside_data)

2018-05-02 17:42:22,296 - memory_profile6_log - INFO -    418    147.9 MiB     -0.2 MiB                               del inside_data

2018-05-02 17:42:22,296 - memory_profile6_log - INFO -    419                                                     else:

2018-05-02 17:42:22,299 - memory_profile6_log - INFO -    420                                                         logger.info("uid: %s with topic_id: %s is empty!" % (framedata.user_id, framedata.topic_id))

2018-05-02 17:42:22,301 - memory_profile6_log - INFO -    421                                                 

2018-05-02 17:42:22,302 - memory_profile6_log - INFO -    422    147.9 MiB      0.0 MiB                       logger.info("Appending training data...")

2018-05-02 17:42:22,302 - memory_profile6_log - INFO -    423    147.9 MiB      0.0 MiB                       datalist.append(tframe)

2018-05-02 17:42:22,305 - memory_profile6_log - INFO -    424                                             else:

2018-05-02 17:42:22,305 - memory_profile6_log - INFO -    425                                                 logger.info("Unknows source is selected !")

2018-05-02 17:42:22,305 - memory_profile6_log - INFO -    426                                                 break

2018-05-02 17:42:22,306 - memory_profile6_log - INFO -    427                                     else: 

2018-05-02 17:42:22,308 - memory_profile6_log - INFO -    428                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-05-02 17:42:22,309 - memory_profile6_log - INFO -    429    147.9 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-05-02 17:42:22,312 - memory_profile6_log - INFO -    430    147.9 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-05-02 17:42:22,313 - memory_profile6_log - INFO -    431                             

2018-05-02 17:42:22,315 - memory_profile6_log - INFO -    432    147.9 MiB      0.0 MiB       return datalist, datalist_hist

2018-05-02 17:42:22,315 - memory_profile6_log - INFO - 


2018-05-02 17:42:22,371 - memory_profile6_log - INFO - size of big_frame_hist: 113.14 KB
2018-05-02 17:42:22,385 - memory_profile6_log - INFO - size of big_frame: 5.05 MB
2018-05-02 17:42:22,390 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-05-02 17:42:30,359 - memory_profile6_log - INFO - size of df: 4.93 MB
2018-05-02 17:42:30,361 - memory_profile6_log - INFO - getting total: 20000 training data(current date interest)
2018-05-02 17:42:30,375 - memory_profile6_log - INFO - size of current_frame: 5.09 MB
2018-05-02 17:42:30,378 - memory_profile6_log - INFO - loading time of: 40000 total genuine-current interest data ~ take 120.535s
2018-05-02 17:42:30,378 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 17:42:30,381 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 17:42:30,381 - memory_profile6_log - INFO - ================================================

2018-05-02 17:42:30,382 - memory_profile6_log - INFO -    434     86.5 MiB     86.5 MiB   @profile

2018-05-02 17:42:30,384 - memory_profile6_log - INFO -    435                             def preprocess(cpu, cd, query_fit, date_generated):

2018-05-02 17:42:30,384 - memory_profile6_log - INFO -    436     86.6 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-05-02 17:42:30,384 - memory_profile6_log - INFO -    437     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 17:42:30,384 - memory_profile6_log - INFO -    438                             

2018-05-02 17:42:30,384 - memory_profile6_log - INFO -    439                                 # ~~~ Begin collecting data ~~~

2018-05-02 17:42:30,385 - memory_profile6_log - INFO -    440     86.6 MiB      0.0 MiB       t0 = time.time()

2018-05-02 17:42:30,385 - memory_profile6_log - INFO -    441    147.9 MiB     61.3 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-05-02 17:42:30,385 - memory_profile6_log - INFO -    442    147.9 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-05-02 17:42:30,387 - memory_profile6_log - INFO -    443                                     logger.info("Training cannot be empty..")

2018-05-02 17:42:30,387 - memory_profile6_log - INFO -    444                                     return False

2018-05-02 17:42:30,387 - memory_profile6_log - INFO -    445    140.8 MiB     -7.1 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-05-02 17:42:30,388 - memory_profile6_log - INFO -    446                                 # print "big_frame_hist:\n", big_frame_hist.head(20)

2018-05-02 17:42:30,388 - memory_profile6_log - INFO -    447    140.8 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-05-02 17:42:30,388 - memory_profile6_log - INFO -    448                             

2018-05-02 17:42:30,392 - memory_profile6_log - INFO -    449    141.3 MiB      0.4 MiB       big_frame = pd.concat(datalist)

2018-05-02 17:42:30,394 - memory_profile6_log - INFO -    450    141.3 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-05-02 17:42:30,394 - memory_profile6_log - INFO -    451    141.3 MiB      0.0 MiB       del datalist

2018-05-02 17:42:30,394 - memory_profile6_log - INFO -    452                             

2018-05-02 17:42:30,394 - memory_profile6_log - INFO -    453    141.6 MiB      0.3 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 17:42:30,395 - memory_profile6_log - INFO -    454                             

2018-05-02 17:42:30,395 - memory_profile6_log - INFO -    455                                 # ~ get current news interest ~

2018-05-02 17:42:30,397 - memory_profile6_log - INFO -    456    141.6 MiB      0.0 MiB       if not cd:

2018-05-02 17:42:30,397 - memory_profile6_log - INFO -    457    141.6 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-05-02 17:42:30,397 - memory_profile6_log - INFO -    458    147.1 MiB      5.5 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-05-02 17:42:30,398 - memory_profile6_log - INFO -    459    147.1 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-05-02 17:42:30,398 - memory_profile6_log - INFO -    460                                 else:

2018-05-02 17:42:30,398 - memory_profile6_log - INFO -    461                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-05-02 17:42:30,398 - memory_profile6_log - INFO -    462                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date) limit 20000"

2018-05-02 17:42:30,398 - memory_profile6_log - INFO -    463                             

2018-05-02 17:42:30,398 - memory_profile6_log - INFO -    464                                     # safe handling of query parameter

2018-05-02 17:42:30,400 - memory_profile6_log - INFO -    465                                     query_params = [

2018-05-02 17:42:30,400 - memory_profile6_log - INFO -    466                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-05-02 17:42:30,400 - memory_profile6_log - INFO -    467                                     ]

2018-05-02 17:42:30,404 - memory_profile6_log - INFO -    468                             

2018-05-02 17:42:30,404 - memory_profile6_log - INFO -    469                                     job_config.query_parameters = query_params

2018-05-02 17:42:30,405 - memory_profile6_log - INFO -    470                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-05-02 17:42:30,405 - memory_profile6_log - INFO -    471                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-05-02 17:42:30,407 - memory_profile6_log - INFO -    472                             

2018-05-02 17:42:30,407 - memory_profile6_log - INFO -    473    147.1 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-05-02 17:42:30,407 - memory_profile6_log - INFO -    474    147.1 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-05-02 17:42:30,407 - memory_profile6_log - INFO -    475    147.1 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-05-02 17:42:30,408 - memory_profile6_log - INFO -    476    147.1 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-05-02 17:42:30,408 - memory_profile6_log - INFO -    477    147.1 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 17:42:30,408 - memory_profile6_log - INFO -    478    147.1 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-05-02 17:42:30,410 - memory_profile6_log - INFO -    479                             

2018-05-02 17:42:30,410 - memory_profile6_log - INFO -    480    147.1 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-05-02 17:42:30,411 - memory_profile6_log - INFO - 


2018-05-02 17:42:30,415 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-05-02 17:42:30,424 - memory_profile6_log - INFO - train on: 20000 total genuine interest data(D(u, t))
2018-05-02 17:42:30,426 - memory_profile6_log - INFO - transform on: 20000 total current data(D(t))
2018-05-02 17:42:30,427 - memory_profile6_log - INFO - apply on: 500 total history...)
2018-05-02 17:47:10,289 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 17:47:10,292 - memory_profile6_log - INFO - date_generated: 
2018-05-02 17:47:10,292 - memory_profile6_log - INFO -  
2018-05-02 17:47:10,293 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 17, 47, 10, 291000)]
2018-05-02 17:47:10,293 - memory_profile6_log - INFO - 

2018-05-02 17:47:10,295 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 17:47:10,295 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 17:47:10,295 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 17:47:10,438 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 17:47:10,443 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 17:47:19,505 - memory_profile6_log - INFO - size of df: 5.05 MB
2018-05-02 17:47:19,506 - memory_profile6_log - INFO - getting total: 20000 training data(genuine interest) for date: 2018-05-01
2018-05-02 17:47:19,536 - memory_profile6_log - INFO - loading history data from elastic...
2018-05-02 17:47:19,538 - memory_profile6_log - INFO - Len of X_split for batch load: 10
2018-05-02 17:47:19,539 - memory_profile6_log - INFO - Appending history data...
2018-05-02 17:47:19,578 - memory_profile6_log - INFO - call 2000 history data...
2018-05-02 17:47:22,888 - memory_profile6_log - INFO -                                              user_id  topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  16130e77ab1ae-0e200dfc009b5b-16457644-38400-16...  22553543         227.678634             237.678634   0.110964       227
1  16130e8bf71104-001943f03c4b79-585d68-38400-161...  22552186         331.423702             341.423702   0.006561       251
2  16130e8bf71104-001943f03c4b79-585d68-38400-161...  22553321         331.018407             341.018407   0.006664       251
3  16130e91ce5f1-071af767bfbd58-4c33197a-38400-16...  22553543          67.390934              77.390934   0.110964        81
4  16130ee125610c-03aa3303ee081a-e46181e-38400-16...  22553543          51.003231              61.003231   0.110964        74
2018-05-02 17:47:22,891 - memory_profile6_log - INFO - 

2018-05-02 17:47:22,897 - memory_profile6_log - INFO - call 2000 history data...
2018-05-02 17:47:26,430 - memory_profile6_log - INFO -                                              user_id  topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  16130ea976fb7-046bf699a445a1-4b476b3b-38400-16...  22553543         195.418685             205.418685   0.110964       142
1  16130ebcc1995-089b5acdc39f35-716a1530-38400-16...  22553543         134.137879             144.137879   0.110964       190
2  16130eddcf312d-042f4e2fa42d57-5c01624f-38400-1...  22553543          40.120904              50.120904   0.110964        45
3  16130f4124cf1-0898267e3f80d2-282b543f-38400-16...  22553543          39.273657              49.273657   0.110964        31
4  16130f53ef03d-086697dfd-450d4f0d-2c880-16130f5...  22553543         103.388945             113.388945   0.110964        74
2018-05-02 17:47:26,431 - memory_profile6_log - INFO - 

2018-05-02 17:47:26,437 - memory_profile6_log - INFO - call 2000 history data...
2018-05-02 17:47:28,785 - memory_profile6_log - INFO -                                              user_id  topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  16130e8bf71104-001943f03c4b79-585d68-38400-161...  22553543         291.458674             301.458674   0.110964       251
1  16130e8bf71104-001943f03c4b79-585d68-38400-161...  22596701         204.700947             214.700947   0.034659       251
2  16130e91ce5f1-071af767bfbd58-4c33197a-38400-16...  22601470         111.180287             121.180287   0.075163        81
3  16130e982d911b-05318b7ae00a1a-c0e0044-38400-16...  22553543         132.653211             142.653211   0.110964       100
4  16130e982d911b-05318b7ae00a1a-c0e0044-38400-16...  22596701         225.874064             235.874064   0.034659       100
2018-05-02 17:47:28,786 - memory_profile6_log - INFO - 

2018-05-02 17:47:28,793 - memory_profile6_log - INFO - call 2000 history data...
2018-05-02 17:47:33,276 - memory_profile6_log - INFO -                                              user_id  topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  16130e77ab1ae-0e200dfc009b5b-16457644-38400-16...  27313197         543.187160             553.187160   0.000590       227
1  16130e8bf71104-001943f03c4b79-585d68-38400-161...  27430823          99.857574             109.857574   0.005467       251
2  16130eb903239-0b97b26455b808-7065430a-38400-16...  22601470         210.815038             220.815038   0.075163       103
3  16130ebcc1995-089b5acdc39f35-716a1530-38400-16...  22661796         385.283534             395.283534   0.004464       190
4  16130ee9f3651-0c7b790e00b1e6-292a5039-38400-16...  22601470         552.310683             562.310683   0.075163       136
2018-05-02 17:47:33,278 - memory_profile6_log - INFO - 

2018-05-02 17:47:33,285 - memory_profile6_log - INFO - call 2000 history data...
2018-05-02 17:47:35,506 - memory_profile6_log - INFO -                                              user_id  topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  16130e91ce5f1-071af767bfbd58-4c33197a-38400-16...  27430823          51.346328              61.346328   0.005467        81
1  16130e91ce5f1-071af767bfbd58-4c33197a-38400-16...  27431099         115.513008             125.513008   0.067631        81
2  16130e91ce5f1-071af767bfbd58-4c33197a-38400-16...  27435908          39.925538              49.925538   0.004738        81
3  16130e982d911b-05318b7ae00a1a-c0e0044-38400-16...  27435908          39.925538              49.925538   0.004738       100
4  16130fdafc746-02b8e228aad61d-4e645706-38400-16...  36060446         748.769600             758.769600   0.025705       135
2018-05-02 17:47:35,507 - memory_profile6_log - INFO - 

2018-05-02 17:47:35,516 - memory_profile6_log - INFO - call 2000 history data...
2018-05-02 17:47:37,907 - memory_profile6_log - INFO -                                              user_id  topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  16130e8bf71104-001943f03c4b79-585d68-38400-161...  38682224       14262.542857           14272.542857   0.000100       251
1  16130e8bf71104-001943f03c4b79-585d68-38400-161...  39321129         153.454965             163.454965   0.000236       251
2  16130e8bf71104-001943f03c4b79-585d68-38400-161...  40710244         170.758830             180.758830   0.004917       251
3  16130e8bf71104-001943f03c4b79-585d68-38400-161...  41566010         265.328611             275.328611   0.012182       251
4  16130e982d911b-05318b7ae00a1a-c0e0044-38400-16...  39321129         306.909929             316.909929   0.000236       100
2018-05-02 17:47:37,908 - memory_profile6_log - INFO - 

2018-05-02 17:47:37,917 - memory_profile6_log - INFO - call 2000 history data...
2018-05-02 17:47:43,598 - memory_profile6_log - INFO -                                              user_id   topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  16130e8bf71104-001943f03c4b79-585d68-38400-161...  107156920         258.647150             268.647150   0.001652       251
1  16130e8bf71104-001943f03c4b79-585d68-38400-161...   43682517       11609.046512           11619.046512   0.000067       251
2  16130e8bf71104-001943f03c4b79-585d68-38400-161...   46104963         821.422669             831.422669   0.001133       251
3  16130e8bf71104-001943f03c4b79-585d68-38400-161...   47085613         538.474913             548.474913   0.030802       251
4  16130e8bf71104-001943f03c4b79-585d68-38400-161...   47478784         579.778165             589.778165   0.000743       251
2018-05-02 17:47:43,598 - memory_profile6_log - INFO - 

2018-05-02 17:47:43,608 - memory_profile6_log - INFO - call 2000 history data...
2018-05-02 17:47:47,877 - memory_profile6_log - INFO -                                              user_id    topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  16130e8bf71104-001943f03c4b79-585d68-38400-161...  1034952471        2641.211640            2651.211640   0.000862       251
1  16130e8bf71104-001943f03c4b79-585d68-38400-161...   273342551         309.525334             319.525334   0.003680       251
2  16130e91ce5f1-071af767bfbd58-4c33197a-38400-16...   819469507        1306.777487            1316.777487   0.002090        81
3  16130e982d911b-05318b7ae00a1a-c0e0044-38400-16...  1019283820          86.156196              96.156196   0.003499       100
4  16130e982d911b-05318b7ae00a1a-c0e0044-38400-16...   188400148         149.437754             159.437754   0.018864       100
2018-05-02 17:47:47,878 - memory_profile6_log - INFO - 

2018-05-02 17:47:47,885 - memory_profile6_log - INFO - call 2000 history data...
2018-05-02 17:47:50,900 - memory_profile6_log - INFO -                                              user_id    topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  16130e8bf71104-001943f03c4b79-585d68-38400-161...  1065124711         889.029269             899.029269   0.003729       251
1  16130e8bf71104-001943f03c4b79-585d68-38400-161...  1157057159         132.217984             142.217984   0.001942       251
2  16130e8bf71104-001943f03c4b79-585d68-38400-161...  1161375937         170.062117             180.062117   0.003543       251
3  16130e982d911b-05318b7ae00a1a-c0e0044-38400-16...  1037574949         104.258354             114.258354   0.008433       100
4  16130e982d911b-05318b7ae00a1a-c0e0044-38400-16...  1038512181          71.312714              81.312714   0.001702       100
2018-05-02 17:47:50,901 - memory_profile6_log - INFO - 

2018-05-02 17:47:50,908 - memory_profile6_log - INFO - call 2000 history data...
2018-05-02 17:47:53,148 - memory_profile6_log - INFO -                                              user_id           topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  16130e8bf71104-001943f03c4b79-585d68-38400-161...  27431110790314246         809.058347             819.058347   0.003553       251
1  16130e8bf71104-001943f03c4b79-585d68-38400-161...  27431110790314255          66.915416              76.915416   0.004074       251
2  16130e8bf71104-001943f03c4b79-585d68-38400-161...  27431110790314280          90.106318             100.106318   0.013512       251
3  16130e8bf71104-001943f03c4b79-585d68-38400-161...  27431110790314285         204.753486             214.753486   0.004592       251
4  16130e91ce5f1-071af767bfbd58-4c33197a-38400-16...  27431110790314278         271.853234             281.853234   0.000444        81
2018-05-02 17:47:53,148 - memory_profile6_log - INFO - 

2018-05-02 17:47:53,150 - memory_profile6_log - INFO - Appending training data...
2018-05-02 17:47:53,151 - memory_profile6_log - INFO - len datalist: 1
2018-05-02 17:47:53,151 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-05-02 17:47:53,154 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 17:47:53,154 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 17:47:53,154 - memory_profile6_log - INFO - ================================================

2018-05-02 17:47:53,155 - memory_profile6_log - INFO -    359     86.6 MiB     86.6 MiB   @profile

2018-05-02 17:47:53,157 - memory_profile6_log - INFO -    360                             def BQPreprocess(cpu, date_generated, client, query_fit, loadfrom="elastic"):

2018-05-02 17:47:53,160 - memory_profile6_log - INFO -    361     86.6 MiB      0.0 MiB       bq_client = client

2018-05-02 17:47:53,161 - memory_profile6_log - INFO -    362     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 17:47:53,164 - memory_profile6_log - INFO -    363                             

2018-05-02 17:47:53,164 - memory_profile6_log - INFO -    364     86.6 MiB      0.0 MiB       datalist = []

2018-05-02 17:47:53,164 - memory_profile6_log - INFO -    365     86.6 MiB      0.0 MiB       datalist_hist = []

2018-05-02 17:47:53,165 - memory_profile6_log - INFO -    366                             

2018-05-02 17:47:53,165 - memory_profile6_log - INFO -    367     86.6 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-05-02 17:47:53,167 - memory_profile6_log - INFO -    368    141.4 MiB      0.0 MiB       for ndate in date_generated:

2018-05-02 17:47:53,167 - memory_profile6_log - INFO -    369    139.9 MiB     53.3 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-05-02 17:47:53,167 - memory_profile6_log - INFO -    370    139.9 MiB      0.0 MiB           if tframe is not None:

2018-05-02 17:47:53,171 - memory_profile6_log - INFO -    371    139.9 MiB      0.0 MiB               if not tframe.empty:

2018-05-02 17:47:53,173 - memory_profile6_log - INFO -    372    139.9 MiB      0.0 MiB                   if loadfrom.strip().lower() == 'datastore':

2018-05-02 17:47:53,174 - memory_profile6_log - INFO -    373                                                 X_split = np.array_split(tframe, 5)

2018-05-02 17:47:53,174 - memory_profile6_log - INFO -    374                                                 logger.info("loading history data from datastore...")

2018-05-02 17:47:53,174 - memory_profile6_log - INFO -    375                                                 logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 17:47:53,176 - memory_profile6_log - INFO -    376                                                 logger.info("Appending history data...")

2018-05-02 17:47:53,177 - memory_profile6_log - INFO -    377                                                 for ix in range(len(X_split)):

2018-05-02 17:47:53,177 - memory_profile6_log - INFO -    378                                                     # ~ loading history

2018-05-02 17:47:53,178 - memory_profile6_log - INFO -    379                                                     """

2018-05-02 17:47:53,178 - memory_profile6_log - INFO -    380                                                         disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-05-02 17:47:53,183 - memory_profile6_log - INFO -    381                                                     """

2018-05-02 17:47:53,184 - memory_profile6_log - INFO -    382                                                     logger.info("processing batch-%d", ix)

2018-05-02 17:47:53,186 - memory_profile6_log - INFO -    383                                                     # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-05-02 17:47:53,187 - memory_profile6_log - INFO -    384                                                     logger.info("creating list history data...")

2018-05-02 17:47:53,187 - memory_profile6_log - INFO -    385                                                     #lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-05-02 17:47:53,190 - memory_profile6_log - INFO -    386                                                     lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 17:47:53,191 - memory_profile6_log - INFO -    387                             

2018-05-02 17:47:53,196 - memory_profile6_log - INFO -    388                                                     logger.info("call history data...")

2018-05-02 17:47:53,197 - memory_profile6_log - INFO -    389                                                     h_frame = mh.loadDSHistory(lhistory)

2018-05-02 17:47:53,198 - memory_profile6_log - INFO -    390                             

2018-05-02 17:47:53,200 - memory_profile6_log - INFO -    391                                                     # me = os.getpid()

2018-05-02 17:47:53,200 - memory_profile6_log - INFO -    392                                                     # kill_proc_tree(me)

2018-05-02 17:47:53,200 - memory_profile6_log - INFO -    393                             

2018-05-02 17:47:53,201 - memory_profile6_log - INFO -    394                                                     logger.info("done collecting history data, appending now...")

2018-05-02 17:47:53,201 - memory_profile6_log - INFO -    395                                                     for m in h_frame:

2018-05-02 17:47:53,203 - memory_profile6_log - INFO -    396                                                         if m is not None:

2018-05-02 17:47:53,207 - memory_profile6_log - INFO -    397                                                             if len(m) > 0:

2018-05-02 17:47:53,207 - memory_profile6_log - INFO -    398                                                                 datalist_hist.append(pd.DataFrame(m))

2018-05-02 17:47:53,209 - memory_profile6_log - INFO -    399                                                     del h_frame

2018-05-02 17:47:53,210 - memory_profile6_log - INFO -    400                                                     del lhistory

2018-05-02 17:47:53,210 - memory_profile6_log - INFO -    401                             

2018-05-02 17:47:53,211 - memory_profile6_log - INFO -    402                                                 logger.info("Appending training data...")

2018-05-02 17:47:53,211 - memory_profile6_log - INFO -    403                                                 datalist.append(tframe)

2018-05-02 17:47:53,213 - memory_profile6_log - INFO -    404    139.9 MiB      0.0 MiB                   elif loadfrom.strip().lower() == 'elastic':

2018-05-02 17:47:53,214 - memory_profile6_log - INFO -    405    140.3 MiB      0.4 MiB                       X_split = np.array_split(tframe, 10)

2018-05-02 17:47:53,217 - memory_profile6_log - INFO -    406    140.3 MiB      0.0 MiB                       logger.info("loading history data from elastic...")

2018-05-02 17:47:53,220 - memory_profile6_log - INFO -    407    140.3 MiB      0.0 MiB                       logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 17:47:53,220 - memory_profile6_log - INFO -    408    140.3 MiB      0.0 MiB                       logger.info("Appending history data...")

2018-05-02 17:47:53,221 - memory_profile6_log - INFO -    409                                                 

2018-05-02 17:47:53,223 - memory_profile6_log - INFO -    410    141.4 MiB     -0.1 MiB                       for ix in range(len(X_split)):

2018-05-02 17:47:53,224 - memory_profile6_log - INFO -    411    141.4 MiB      0.4 MiB                           lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 17:47:53,226 - memory_profile6_log - INFO -    412    141.4 MiB     -0.0 MiB                           logger.info("call %d history data...", len(lhistory))

2018-05-02 17:47:53,229 - memory_profile6_log - INFO -    413    141.4 MiB     -0.0 MiB                           inside_data = mh.loadESHistory(lhistory,

2018-05-02 17:47:53,230 - memory_profile6_log - INFO -    414    141.4 MiB     -0.0 MiB                                                          esindex_name='fitted_hist_index',

2018-05-02 17:47:53,232 - memory_profile6_log - INFO -    415    141.4 MiB      0.4 MiB                                                          estype_name='fitted_hist_type')

2018-05-02 17:47:53,232 - memory_profile6_log - INFO -    416                                                     # split back the user_id and topic_id

2018-05-02 17:47:53,233 - memory_profile6_log - INFO -    417    141.4 MiB     -0.0 MiB                           inside_data[['user_id','topic_id']] = inside_data.uid_topid.str.split('_', expand=True)

2018-05-02 17:47:53,233 - memory_profile6_log - INFO -    418    141.4 MiB      0.1 MiB                           inside_data = inside_data[["user_id","topic_id", "pt_posterior_x_Nt", "smoothed_pt_posterior", "p0_cat_ci", "sigma_Nt"]]

2018-05-02 17:47:53,234 - memory_profile6_log - INFO -    419    141.4 MiB     -0.1 MiB                           if not inside_data.empty:

2018-05-02 17:47:53,234 - memory_profile6_log - INFO -    420    141.4 MiB     -0.0 MiB                               print inside_data.head(5)

2018-05-02 17:47:53,236 - memory_profile6_log - INFO -    421    141.4 MiB     -0.1 MiB                               datalist_hist.append(inside_data)

2018-05-02 17:47:53,239 - memory_profile6_log - INFO -    422    141.4 MiB     -0.1 MiB                               del inside_data

2018-05-02 17:47:53,240 - memory_profile6_log - INFO -    423                                                     else:

2018-05-02 17:47:53,242 - memory_profile6_log - INFO -    424                                                         logger.info("uid: %s with topic_id: %s is empty!" % (framedata.user_id, framedata.topic_id))

2018-05-02 17:47:53,243 - memory_profile6_log - INFO -    425                                                 

2018-05-02 17:47:53,243 - memory_profile6_log - INFO -    426    141.4 MiB     -0.0 MiB                       logger.info("Appending training data...")

2018-05-02 17:47:53,244 - memory_profile6_log - INFO -    427    141.4 MiB      0.0 MiB                       datalist.append(tframe)

2018-05-02 17:47:53,246 - memory_profile6_log - INFO -    428                                             else:

2018-05-02 17:47:53,247 - memory_profile6_log - INFO -    429                                                 logger.info("Unknows source is selected !")

2018-05-02 17:47:53,250 - memory_profile6_log - INFO -    430                                                 break

2018-05-02 17:47:53,250 - memory_profile6_log - INFO -    431                                     else: 

2018-05-02 17:47:53,253 - memory_profile6_log - INFO -    432                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-05-02 17:47:53,253 - memory_profile6_log - INFO -    433    141.4 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-05-02 17:47:53,256 - memory_profile6_log - INFO -    434    141.4 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-05-02 17:47:53,256 - memory_profile6_log - INFO -    435                             

2018-05-02 17:47:53,257 - memory_profile6_log - INFO -    436    141.4 MiB      0.0 MiB       return datalist, datalist_hist

2018-05-02 17:47:53,262 - memory_profile6_log - INFO - 


2018-05-02 17:47:53,276 - memory_profile6_log - INFO - size of big_frame_hist: 28.26 KB
2018-05-02 17:47:53,289 - memory_profile6_log - INFO - size of big_frame: 5.05 MB
2018-05-02 17:47:53,292 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-05-02 17:48:00,984 - memory_profile6_log - INFO - size of df: 4.93 MB
2018-05-02 17:48:00,986 - memory_profile6_log - INFO - getting total: 20000 training data(current date interest)
2018-05-02 17:48:01,000 - memory_profile6_log - INFO - size of current_frame: 5.09 MB
2018-05-02 17:48:01,002 - memory_profile6_log - INFO - loading time of: 40000 total genuine-current interest data ~ take 50.601s
2018-05-02 17:48:01,003 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 17:48:01,003 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 17:48:01,003 - memory_profile6_log - INFO - ================================================

2018-05-02 17:48:01,005 - memory_profile6_log - INFO -    438     86.5 MiB     86.5 MiB   @profile

2018-05-02 17:48:01,007 - memory_profile6_log - INFO -    439                             def preprocess(cpu, cd, query_fit, date_generated):

2018-05-02 17:48:01,009 - memory_profile6_log - INFO -    440     86.6 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-05-02 17:48:01,009 - memory_profile6_log - INFO -    441     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 17:48:01,010 - memory_profile6_log - INFO -    442                             

2018-05-02 17:48:01,010 - memory_profile6_log - INFO -    443                                 # ~~~ Begin collecting data ~~~

2018-05-02 17:48:01,012 - memory_profile6_log - INFO -    444     86.6 MiB      0.0 MiB       t0 = time.time()

2018-05-02 17:48:01,012 - memory_profile6_log - INFO -    445    141.4 MiB     54.7 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-05-02 17:48:01,015 - memory_profile6_log - INFO -    446    141.4 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-05-02 17:48:01,015 - memory_profile6_log - INFO -    447                                     logger.info("Training cannot be empty..")

2018-05-02 17:48:01,017 - memory_profile6_log - INFO -    448                                     return False

2018-05-02 17:48:01,019 - memory_profile6_log - INFO -    449    141.4 MiB      0.0 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-05-02 17:48:01,019 - memory_profile6_log - INFO -    450                                 # print "big_frame_hist:\n", big_frame_hist.head(20)

2018-05-02 17:48:01,020 - memory_profile6_log - INFO -    451    141.4 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-05-02 17:48:01,022 - memory_profile6_log - INFO -    452                             

2018-05-02 17:48:01,023 - memory_profile6_log - INFO -    453    141.4 MiB      0.0 MiB       big_frame = pd.concat(datalist)

2018-05-02 17:48:01,025 - memory_profile6_log - INFO -    454    141.4 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-05-02 17:48:01,025 - memory_profile6_log - INFO -    455    141.4 MiB      0.0 MiB       del datalist

2018-05-02 17:48:01,028 - memory_profile6_log - INFO -    456                             

2018-05-02 17:48:01,029 - memory_profile6_log - INFO -    457    141.4 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 17:48:01,030 - memory_profile6_log - INFO -    458                             

2018-05-02 17:48:01,032 - memory_profile6_log - INFO -    459                                 # ~ get current news interest ~

2018-05-02 17:48:01,032 - memory_profile6_log - INFO -    460    141.4 MiB      0.0 MiB       if not cd:

2018-05-02 17:48:01,033 - memory_profile6_log - INFO -    461    141.4 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-05-02 17:48:01,033 - memory_profile6_log - INFO -    462    145.1 MiB      3.7 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-05-02 17:48:01,035 - memory_profile6_log - INFO -    463    145.1 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-05-02 17:48:01,035 - memory_profile6_log - INFO -    464                                 else:

2018-05-02 17:48:01,036 - memory_profile6_log - INFO -    465                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-05-02 17:48:01,036 - memory_profile6_log - INFO -    466                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date) limit 20000"

2018-05-02 17:48:01,039 - memory_profile6_log - INFO -    467                             

2018-05-02 17:48:01,042 - memory_profile6_log - INFO -    468                                     # safe handling of query parameter

2018-05-02 17:48:01,042 - memory_profile6_log - INFO -    469                                     query_params = [

2018-05-02 17:48:01,042 - memory_profile6_log - INFO -    470                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-05-02 17:48:01,042 - memory_profile6_log - INFO -    471                                     ]

2018-05-02 17:48:01,043 - memory_profile6_log - INFO -    472                             

2018-05-02 17:48:01,043 - memory_profile6_log - INFO -    473                                     job_config.query_parameters = query_params

2018-05-02 17:48:01,045 - memory_profile6_log - INFO -    474                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-05-02 17:48:01,046 - memory_profile6_log - INFO -    475                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-05-02 17:48:01,049 - memory_profile6_log - INFO -    476                             

2018-05-02 17:48:01,052 - memory_profile6_log - INFO -    477    145.1 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-05-02 17:48:01,052 - memory_profile6_log - INFO -    478    145.1 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-05-02 17:48:01,052 - memory_profile6_log - INFO -    479    145.1 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-05-02 17:48:01,052 - memory_profile6_log - INFO -    480    145.1 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-05-02 17:48:01,053 - memory_profile6_log - INFO -    481    145.1 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 17:48:01,055 - memory_profile6_log - INFO -    482    145.1 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-05-02 17:48:01,056 - memory_profile6_log - INFO -    483                             

2018-05-02 17:48:01,056 - memory_profile6_log - INFO -    484    145.1 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-05-02 17:48:01,059 - memory_profile6_log - INFO - 


2018-05-02 17:48:01,065 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-05-02 17:48:01,072 - memory_profile6_log - INFO - train on: 20000 total genuine interest data(D(u, t))
2018-05-02 17:48:01,073 - memory_profile6_log - INFO - transform on: 20000 total current data(D(t))
2018-05-02 17:48:01,075 - memory_profile6_log - INFO - apply on: 100 total history...)
2018-05-02 17:48:01,124 - memory_profile6_log - INFO - len of uniques_fit_hist:100
2018-05-02 17:48:01,128 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:26
2018-05-02 17:48:01,201 - memory_profile6_log - INFO - self.sum_all_nt before concate:

2018-05-02 17:48:01,211 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 17:48:01,213 - memory_profile6_log - INFO - 

2018-05-02 17:48:01,229 - memory_profile6_log - INFO - self.sum_all_nt after concate:

2018-05-02 17:48:01,237 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 17:48:01,239 - memory_profile6_log - INFO - 

2018-05-02 17:48:01,288 - memory_profile6_log - INFO - Len of model_fit: 20000
2018-05-02 17:48:01,289 - memory_profile6_log - INFO - Len of df_dut: 20000
2018-05-02 17:48:01,433 - memory_profile6_log - INFO - Len of fitted_models on main class: 20000
2018-05-02 17:48:01,434 - memory_profile6_log - INFO - 

2018-05-02 17:48:01,436 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-05-02 17:48:01,437 - memory_profile6_log - INFO - 

2018-05-02 17:48:01,437 - memory_profile6_log - INFO - fitted_model_hist:

2018-05-02 17:48:01,438 - memory_profile6_log - INFO - 

2018-05-02 17:48:01,447 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 17:48:01,448 - memory_profile6_log - INFO - 

2018-05-02 17:48:01,457 - memory_profile6_log - INFO - len of fitted models after concat: 20100
2018-05-02 17:48:01,457 - memory_profile6_log - INFO - 

2018-05-02 17:48:01,459 - memory_profile6_log - INFO - Recalculating fitted models...
2018-05-02 17:48:01,460 - memory_profile6_log - INFO - 

2018-05-02 17:48:01,490 - memory_profile6_log - INFO - fitted_models after cobine:

2018-05-02 17:48:01,490 - memory_profile6_log - INFO - 

2018-05-02 17:48:01,499 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 17:48:01,500 - memory_profile6_log - INFO - 

2018-05-02 17:48:01,502 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 20000
2018-05-02 17:48:01,506 - memory_profile6_log - INFO - 

2018-05-02 17:48:06,598 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-05-02 17:48:06,618 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, topic_id, pt_posterior_x_Nt, smoothed_pt_posterior, p0_cat_ci, sigma_Nt, p0_posterior, is_general, rank]
Index: []
2018-05-02 17:48:06,619 - memory_profile6_log - INFO - 

2018-05-02 17:48:06,621 - memory_profile6_log - INFO - Len of model_transform: 9071
2018-05-02 17:48:06,621 - memory_profile6_log - INFO - Len of df_dt: 20000
2018-05-02 17:48:06,622 - memory_profile6_log - INFO - Total train time: 5.535s
2018-05-02 17:48:06,625 - memory_profile6_log - INFO - memory left before cleaning: 88.500 percent memory...
2018-05-02 17:48:06,625 - memory_profile6_log - INFO - cleaning up some objects...
2018-05-02 17:48:06,627 - memory_profile6_log - INFO - deleting df_dut...
2018-05-02 17:48:06,628 - memory_profile6_log - INFO - deleting df_dt...
2018-05-02 17:48:06,628 - memory_profile6_log - INFO - deleting df_input...
2018-05-02 17:48:06,632 - memory_profile6_log - INFO - deleting df_input_X...
2018-05-02 17:48:06,634 - memory_profile6_log - INFO - deleting df_current...
2018-05-02 17:48:06,634 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-05-02 17:48:06,638 - memory_profile6_log - INFO - deleting model_fit...
2018-05-02 17:48:06,638 - memory_profile6_log - INFO - deleting result...
2018-05-02 17:48:06,648 - memory_profile6_log - INFO - list(model_transformsv) before rename: 
2018-05-02 17:48:06,650 - memory_profile6_log - INFO -  
2018-05-02 17:48:06,651 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'is_general', 'p0_posterior', 'rank', 'date']
2018-05-02 17:48:06,654 - memory_profile6_log - INFO - 

2018-05-02 17:48:06,654 - memory_profile6_log - INFO - 9071
2018-05-02 17:48:06,655 - memory_profile6_log - INFO - 

2018-05-02 17:48:06,658 - memory_profile6_log - INFO - list(model_transformsv) after rename: 
2018-05-02 17:48:06,661 - memory_profile6_log - INFO -  
2018-05-02 17:48:06,664 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'topic_is_general', 'interest_score', 'interest_rank', 'interest_score_created_at']
2018-05-02 17:48:06,665 - memory_profile6_log - INFO - 

2018-05-02 17:48:06,667 - memory_profile6_log - INFO - 9071
2018-05-02 17:48:06,667 - memory_profile6_log - INFO - 

2018-05-02 17:48:06,670 - memory_profile6_log - INFO - deleting model_transform...
2018-05-02 17:48:06,673 - memory_profile6_log - INFO - memory left after cleaning: 88.500 percent memory...
2018-05-02 17:48:06,674 - memory_profile6_log - INFO - Begin saving trained data...
2018-05-02 17:48:06,677 - memory_profile6_log - INFO - Using ElasticSearch as storage...
2018-05-02 17:48:06,677 - memory_profile6_log - INFO - Saving fitted_models as history to Elasticsearch...
2018-05-02 17:48:06,723 - memory_profile6_log - INFO - Saving total data: 20000
2018-05-02 17:48:06,726 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 5
2018-05-02 17:48:06,726 - memory_profile6_log - INFO - processing batch-0
2018-05-02 17:48:28,930 - memory_profile6_log - INFO - processing batch-1
2018-05-02 17:49:10,608 - memory_profile6_log - INFO - processing batch-2
2018-05-02 17:49:38,315 - memory_profile6_log - INFO - processing batch-3
2018-05-02 17:50:01,806 - memory_profile6_log - INFO - processing batch-4
2018-05-02 17:50:29,367 - memory_profile6_log - INFO - deleting BR...
2018-05-02 17:50:29,368 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-05-02 17:50:29,369 - memory_profile6_log - INFO - deleting fitted_models_sigmant...
2018-05-02 17:50:29,371 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 17:50:29,371 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 17:50:29,372 - memory_profile6_log - INFO - ================================================

2018-05-02 17:50:29,375 - memory_profile6_log - INFO -    113    145.1 MiB    145.1 MiB   @profile

2018-05-02 17:50:29,375 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-05-02 17:50:29,377 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-05-02 17:50:29,378 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-05-02 17:50:29,378 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-05-02 17:50:29,378 - memory_profile6_log - INFO -    118                                      saveto="datastore", fitted_models_hist=None):

2018-05-02 17:50:29,378 - memory_profile6_log - INFO -    119                                 """

2018-05-02 17:50:29,378 - memory_profile6_log - INFO -    120                                     Main Process

2018-05-02 17:50:29,380 - memory_profile6_log - INFO -    121                                 """

2018-05-02 17:50:29,380 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-05-02 17:50:29,381 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-05-02 17:50:29,381 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-05-02 17:50:29,382 - memory_profile6_log - INFO -    125    145.1 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-05-02 17:50:29,384 - memory_profile6_log - INFO -    126    145.1 MiB      0.0 MiB       df_dut = df_input.copy(deep=True)

2018-05-02 17:50:29,387 - memory_profile6_log - INFO -    127    145.1 MiB      0.1 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 17:50:29,388 - memory_profile6_log - INFO -    128                             

2018-05-02 17:50:29,388 - memory_profile6_log - INFO -    129                                 # D(t)

2018-05-02 17:50:29,388 - memory_profile6_log - INFO -    130    145.1 MiB      0.0 MiB       df_dt = df_current.copy(deep=True)

2018-05-02 17:50:29,391 - memory_profile6_log - INFO -    131    145.1 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 17:50:29,391 - memory_profile6_log - INFO -    132                             

2018-05-02 17:50:29,392 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-05-02 17:50:29,392 - memory_profile6_log - INFO -    134    145.1 MiB      0.0 MiB       t0 = time.time()

2018-05-02 17:50:29,394 - memory_profile6_log - INFO -    135    145.1 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-05-02 17:50:29,394 - memory_profile6_log - INFO -    136    145.1 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-05-02 17:50:29,394 - memory_profile6_log - INFO -    137    145.1 MiB      0.0 MiB       logger.info("apply on: %d total history...)", len(df_hist))

2018-05-02 17:50:29,394 - memory_profile6_log - INFO -    138                             

2018-05-02 17:50:29,398 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-05-02 17:50:29,400 - memory_profile6_log - INFO -    140    145.1 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-05-02 17:50:29,401 - memory_profile6_log - INFO -    141                             

2018-05-02 17:50:29,401 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-05-02 17:50:29,401 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-05-02 17:50:29,403 - memory_profile6_log - INFO -    144    145.3 MiB      0.2 MiB       NB = BR.processX(df_dut)

2018-05-02 17:50:29,404 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-05-02 17:50:29,404 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-05-02 17:50:29,404 - memory_profile6_log - INFO -    147    145.4 MiB      0.1 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-05-02 17:50:29,404 - memory_profile6_log - INFO -    148                                 """

2018-05-02 17:50:29,404 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-05-02 17:50:29,405 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-05-02 17:50:29,411 - memory_profile6_log - INFO -    151                                 """

2018-05-02 17:50:29,413 - memory_profile6_log - INFO -    152    145.4 MiB      0.0 MiB       fitby_sigmant = False

2018-05-02 17:50:29,413 - memory_profile6_log - INFO -    153    145.4 MiB      0.0 MiB       uniques_fit_hist = None

2018-05-02 17:50:29,414 - memory_profile6_log - INFO -    154    145.4 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-05-02 17:50:29,414 - memory_profile6_log - INFO -    155    145.4 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-05-02 17:50:29,417 - memory_profile6_log - INFO -    156    145.4 MiB      0.0 MiB                            'is_general']]

2018-05-02 17:50:29,417 - memory_profile6_log - INFO -    157                             

2018-05-02 17:50:29,418 - memory_profile6_log - INFO -    158                                 # get sigma_Nt from fitted_models_hist

2018-05-02 17:50:29,418 - memory_profile6_log - INFO -    159    145.4 MiB      0.0 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-05-02 17:50:29,423 - memory_profile6_log - INFO -    160    145.4 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-05-02 17:50:29,424 - memory_profile6_log - INFO -    161    145.4 MiB      0.0 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-05-02 17:50:29,426 - memory_profile6_log - INFO -    162    145.4 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-05-02 17:50:29,427 - memory_profile6_log - INFO -    163                             

2018-05-02 17:50:29,427 - memory_profile6_log - INFO -    164                                 # begin fit

2018-05-02 17:50:29,427 - memory_profile6_log - INFO -    165    145.4 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-05-02 17:50:29,428 - memory_profile6_log - INFO -    166    145.4 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-05-02 17:50:29,428 - memory_profile6_log - INFO -    167    147.9 MiB      2.4 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-05-02 17:50:29,430 - memory_profile6_log - INFO -    168    147.9 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-05-02 17:50:29,430 - memory_profile6_log - INFO -    169    147.9 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-05-02 17:50:29,430 - memory_profile6_log - INFO -    170                                 # print model_fit[['num_x', 'num_x', 'sigma_Nt', 'date_all_click']].loc[model_fit['user_id']=="1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157"].head(5)

2018-05-02 17:50:29,436 - memory_profile6_log - INFO -    171                             

2018-05-02 17:50:29,437 - memory_profile6_log - INFO -    172                                 # ~~ and Transform ~~

2018-05-02 17:50:29,437 - memory_profile6_log - INFO -    173                                 #   handling current news interest == current date

2018-05-02 17:50:29,438 - memory_profile6_log - INFO -    174    147.9 MiB      0.0 MiB       if df_dt.empty:

2018-05-02 17:50:29,438 - memory_profile6_log - INFO -    175                                     print "Cek your df_dt, cannot be emtpy!!"

2018-05-02 17:50:29,441 - memory_profile6_log - INFO -    176                                     return None

2018-05-02 17:50:29,441 - memory_profile6_log - INFO -    177    148.6 MiB      0.7 MiB       NB = BR.processX(df_dt)

2018-05-02 17:50:29,443 - memory_profile6_log - INFO -    178    148.6 MiB      0.0 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-05-02 17:50:29,443 - memory_profile6_log - INFO -    179                             

2018-05-02 17:50:29,446 - memory_profile6_log - INFO -    180    148.6 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-05-02 17:50:29,447 - memory_profile6_log - INFO -    181    148.8 MiB      0.2 MiB                            'num_x', 'num_y', 'is_general']]

2018-05-02 17:50:29,447 - memory_profile6_log - INFO -    182                             

2018-05-02 17:50:29,450 - memory_profile6_log - INFO -    183                                 # print "model_fit dtypes:\n", model_fit.dtypes

2018-05-02 17:50:29,450 - memory_profile6_log - INFO -    184                                 # print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-05-02 17:50:29,450 - memory_profile6_log - INFO -    185    148.8 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-05-02 17:50:29,451 - memory_profile6_log - INFO -    186    148.8 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-05-02 17:50:29,451 - memory_profile6_log - INFO -    187    148.8 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-05-02 17:50:29,451 - memory_profile6_log - INFO -    188    148.8 MiB      0.0 MiB                                                                                           "topic_id", "user_id"]],

2018-05-02 17:50:29,453 - memory_profile6_log - INFO -    189    152.2 MiB      3.4 MiB                                                     verbose=False)

2018-05-02 17:50:29,453 - memory_profile6_log - INFO -    190                             

2018-05-02 17:50:29,453 - memory_profile6_log - INFO -    191                                 # ~~~ filter is general and specific topic ~~~

2018-05-02 17:50:29,453 - memory_profile6_log - INFO -    192                                 # the idea is just we need to rerank every topic according

2018-05-02 17:50:29,453 - memory_profile6_log - INFO -    193                                 #    to user_id and and is_general by p0_posterior

2018-05-02 17:50:29,453 - memory_profile6_log - INFO -    194    152.2 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-05-02 17:50:29,459 - memory_profile6_log - INFO -    195    152.2 MiB      0.0 MiB                                     'is_general']].groupby(['topic_id',

2018-05-02 17:50:29,460 - memory_profile6_log - INFO -    196    152.2 MiB      0.0 MiB                                                             'is_general']

2018-05-02 17:50:29,461 - memory_profile6_log - INFO -    197                                                                                      ).size().to_frame().reset_index()

2018-05-02 17:50:29,461 - memory_profile6_log - INFO -    198                             

2018-05-02 17:50:29,463 - memory_profile6_log - INFO -    199                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-05-02 17:50:29,463 - memory_profile6_log - INFO -    200                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-05-02 17:50:29,463 - memory_profile6_log - INFO -    201    152.2 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-05-02 17:50:29,463 - memory_profile6_log - INFO -    202                             

2018-05-02 17:50:29,464 - memory_profile6_log - INFO -    203                                 # ~ start by provide rank for each topic type ~

2018-05-02 17:50:29,469 - memory_profile6_log - INFO -    204    156.5 MiB      4.4 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-05-02 17:50:29,470 - memory_profile6_log - INFO -    205    154.2 MiB     -2.3 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-05-02 17:50:29,470 - memory_profile6_log - INFO -    206                             

2018-05-02 17:50:29,470 - memory_profile6_log - INFO -    207                                 # ~ set threshold to filter output

2018-05-02 17:50:29,471 - memory_profile6_log - INFO -    208    154.2 MiB      0.0 MiB       if threshold > 0:

2018-05-02 17:50:29,471 - memory_profile6_log - INFO -    209    154.2 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-05-02 17:50:29,471 - memory_profile6_log - INFO -    210    154.2 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-05-02 17:50:29,474 - memory_profile6_log - INFO -    211    154.2 MiB      0.0 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-05-02 17:50:29,476 - memory_profile6_log - INFO -    212                             

2018-05-02 17:50:29,476 - memory_profile6_log - INFO -    213    154.2 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 17:50:29,476 - memory_profile6_log - INFO -    214    154.2 MiB      0.0 MiB       print model_transform[model_transform['user_id']=='1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157']

2018-05-02 17:50:29,476 - memory_profile6_log - INFO -    215    154.2 MiB      0.0 MiB       logger.info("Len of model_transform: %d", len(model_transform))

2018-05-02 17:50:29,476 - memory_profile6_log - INFO -    216    154.2 MiB      0.0 MiB       logger.info("Len of df_dt: %d", len(df_dt))

2018-05-02 17:50:29,482 - memory_profile6_log - INFO -    217    154.2 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-05-02 17:50:29,482 - memory_profile6_log - INFO -    218                             

2018-05-02 17:50:29,484 - memory_profile6_log - INFO -    219    154.2 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-05-02 17:50:29,484 - memory_profile6_log - INFO -    220                             

2018-05-02 17:50:29,486 - memory_profile6_log - INFO -    221    154.2 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-05-02 17:50:29,486 - memory_profile6_log - INFO -    222    154.2 MiB      0.0 MiB       del df_dut

2018-05-02 17:50:29,486 - memory_profile6_log - INFO -    223    154.2 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-05-02 17:50:29,486 - memory_profile6_log - INFO -    224    154.2 MiB      0.0 MiB       del df_dt

2018-05-02 17:50:29,486 - memory_profile6_log - INFO -    225    154.2 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-05-02 17:50:29,486 - memory_profile6_log - INFO -    226    154.2 MiB      0.0 MiB       del df_input

2018-05-02 17:50:29,487 - memory_profile6_log - INFO -    227    154.2 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-05-02 17:50:29,487 - memory_profile6_log - INFO -    228    146.7 MiB     -7.5 MiB       del df_input_X

2018-05-02 17:50:29,487 - memory_profile6_log - INFO -    229    146.7 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-05-02 17:50:29,489 - memory_profile6_log - INFO -    230    146.7 MiB      0.0 MiB       del df_current

2018-05-02 17:50:29,489 - memory_profile6_log - INFO -    231    146.7 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-05-02 17:50:29,494 - memory_profile6_log - INFO -    232    146.7 MiB      0.0 MiB       del map_topic_isgeneral

2018-05-02 17:50:29,496 - memory_profile6_log - INFO -    233    146.7 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-05-02 17:50:29,496 - memory_profile6_log - INFO -    234    145.7 MiB     -1.1 MiB       del model_fit

2018-05-02 17:50:29,497 - memory_profile6_log - INFO -    235    145.7 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-05-02 17:50:29,500 - memory_profile6_log - INFO -    236    145.7 MiB      0.0 MiB       del result

2018-05-02 17:50:29,500 - memory_profile6_log - INFO -    237    145.7 MiB      0.0 MiB       logger.info("deleting result...")

2018-05-02 17:50:29,500 - memory_profile6_log - INFO -    238                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 17:50:29,502 - memory_profile6_log - INFO -    239                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 17:50:29,502 - memory_profile6_log - INFO -    240                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 17:50:29,502 - memory_profile6_log - INFO -    241    145.7 MiB      0.0 MiB       if savetrain:

2018-05-02 17:50:29,505 - memory_profile6_log - INFO -    242    146.2 MiB      0.6 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'p0_posterior', 'rank']].copy(deep=True)

2018-05-02 17:50:29,509 - memory_profile6_log - INFO -    243    146.3 MiB      0.1 MiB           model_transformsv['date'] = current_date  # we need manually adding date, because table not support

2018-05-02 17:50:29,509 - memory_profile6_log - INFO -    244    146.3 MiB      0.0 MiB           model_transformsv['date'] = pd.to_datetime(model_transformsv['date'],

2018-05-02 17:50:29,509 - memory_profile6_log - INFO -    245    146.4 MiB      0.1 MiB                                                      format='%Y-%m-%d', errors='coerce')

2018-05-02 17:50:29,509 - memory_profile6_log - INFO -    246    146.4 MiB      0.0 MiB           print "list(model_transformsv) before rename: ", list(model_transformsv)       

2018-05-02 17:50:29,509 - memory_profile6_log - INFO -    247    146.4 MiB      0.0 MiB           print len(model_transformsv)                

2018-05-02 17:50:29,509 - memory_profile6_log - INFO -    248    146.4 MiB      0.0 MiB           model_transformsv = model_transformsv.rename(columns={'is_general': 'topic_is_general', 'p0_posterior': 'interest_score',

2018-05-02 17:50:29,510 - memory_profile6_log - INFO -    249    146.7 MiB      0.3 MiB                                                                 'rank':'interest_rank' , 'date':'interest_score_created_at'})

2018-05-02 17:50:29,510 - memory_profile6_log - INFO -    250    146.7 MiB      0.0 MiB           print "list(model_transformsv) after rename: ", list(model_transformsv)    

2018-05-02 17:50:29,510 - memory_profile6_log - INFO -    251    146.7 MiB      0.0 MiB           print len(model_transformsv) 

2018-05-02 17:50:29,512 - memory_profile6_log - INFO -    252    146.7 MiB      0.0 MiB           del model_transform

2018-05-02 17:50:29,512 - memory_profile6_log - INFO -    253    146.7 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-05-02 17:50:29,512 - memory_profile6_log - INFO -    254    146.7 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-05-02 17:50:29,513 - memory_profile6_log - INFO -    255                             

2018-05-02 17:50:29,513 - memory_profile6_log - INFO -    256    146.7 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-05-02 17:50:29,513 - memory_profile6_log - INFO -    257                                     # ~ Place your code to save the training model here ~

2018-05-02 17:50:29,516 - memory_profile6_log - INFO -    258    146.7 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-05-02 17:50:29,517 - memory_profile6_log - INFO -    259                                         logger.info("Using google datastore as storage...")

2018-05-02 17:50:29,519 - memory_profile6_log - INFO -    260                                         if multproc:

2018-05-02 17:50:29,519 - memory_profile6_log - INFO -    261                                             # ~ save transform models ~

2018-05-02 17:50:29,519 - memory_profile6_log - INFO -    262                                             logger.info("Saving main Transform model to Google DataStore...")

2018-05-02 17:50:29,519 - memory_profile6_log - INFO -    263                                             logger.info("Saving total data: %d", len(model_transformsv))

2018-05-02 17:50:29,520 - memory_profile6_log - INFO -    264                                             mh.saveDataStorePutMulti(model_transformsv)

2018-05-02 17:50:29,520 - memory_profile6_log - INFO -    265                             

2018-05-02 17:50:29,522 - memory_profile6_log - INFO -    266                                             # ~ save fitted models ~

2018-05-02 17:50:29,522 - memory_profile6_log - INFO -    267                                             logger.info("Saving fitted_models as history to Google DataStore...")

2018-05-02 17:50:29,522 - memory_profile6_log - INFO -    268                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 17:50:29,525 - memory_profile6_log - INFO -    269                                             fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-05-02 17:50:29,525 - memory_profile6_log - INFO -    270                                             logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-05-02 17:50:29,525 - memory_profile6_log - INFO -    271                                             X_split = np.array_split(fitted_models_sigmant, 10)

2018-05-02 17:50:29,526 - memory_profile6_log - INFO -    272                                             logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-05-02 17:50:29,530 - memory_profile6_log - INFO -    273                                             for ix in range(len(X_split)):

2018-05-02 17:50:29,530 - memory_profile6_log - INFO -    274                                                 logger.info("processing batch-%d", ix)

2018-05-02 17:50:29,532 - memory_profile6_log - INFO -    275                                                 mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-05-02 17:50:29,533 - memory_profile6_log - INFO -    276                             

2018-05-02 17:50:29,533 - memory_profile6_log - INFO -    277                                             del X_split

2018-05-02 17:50:29,535 - memory_profile6_log - INFO -    278                                             logger.info("deleting X_split...")

2018-05-02 17:50:29,536 - memory_profile6_log - INFO -    279                                             del save_sigma_nt

2018-05-02 17:50:29,536 - memory_profile6_log - INFO -    280                                             logger.info("deleting save_sigma_nt...")

2018-05-02 17:50:29,536 - memory_profile6_log - INFO -    281                             

2018-05-02 17:50:29,536 - memory_profile6_log - INFO -    282                                             del BR

2018-05-02 17:50:29,539 - memory_profile6_log - INFO -    283                                             logger.info("deleting BR...")

2018-05-02 17:50:29,542 - memory_profile6_log - INFO -    284                             

2018-05-02 17:50:29,542 - memory_profile6_log - INFO -    285    146.7 MiB      0.0 MiB           elif str(saveto).lower() == "elastic":

2018-05-02 17:50:29,542 - memory_profile6_log - INFO -    286    146.7 MiB      0.0 MiB               logger.info("Using ElasticSearch as storage...")

2018-05-02 17:50:29,543 - memory_profile6_log - INFO -    287                                         """logging.info("Saving main Transform model to Elasticsearch...")

2018-05-02 17:50:29,545 - memory_profile6_log - INFO -    288                                         # print model_transformsv.head(5)

2018-05-02 17:50:29,545 - memory_profile6_log - INFO -    289                                         

2018-05-02 17:50:29,546 - memory_profile6_log - INFO -    290                                         X_split = np.array_split(model_transformsv, 5)

2018-05-02 17:50:29,546 - memory_profile6_log - INFO -    291                                         logger.info("Saving total data: %d", len(model_transformsv))

2018-05-02 17:50:29,546 - memory_profile6_log - INFO -    292                                         logger.info("Len of X_split for batch save model_transformsv: %d", len(X_split))

2018-05-02 17:50:29,546 - memory_profile6_log - INFO -    293                                         for ix in range(len(X_split)):

2018-05-02 17:50:29,552 - memory_profile6_log - INFO -    294                                             logger.info("processing batch-%d", ix)

2018-05-02 17:50:29,552 - memory_profile6_log - INFO -    295                                             mh.saveElasticS(X_split[ix])

2018-05-02 17:50:29,555 - memory_profile6_log - INFO -    296                                         del X_split

2018-05-02 17:50:29,555 - memory_profile6_log - INFO -    297                                         """

2018-05-02 17:50:29,555 - memory_profile6_log - INFO -    298                             

2018-05-02 17:50:29,558 - memory_profile6_log - INFO -    299    146.7 MiB      0.0 MiB               logger.info("Saving fitted_models as history to Elasticsearch...")

2018-05-02 17:50:29,559 - memory_profile6_log - INFO -    300    146.8 MiB      0.1 MiB               save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 17:50:29,559 - memory_profile6_log - INFO -    301    148.0 MiB      1.2 MiB               fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-05-02 17:50:29,559 - memory_profile6_log - INFO -    302                             

2018-05-02 17:50:29,559 - memory_profile6_log - INFO -    303    148.0 MiB      0.0 MiB               fitted_models_sigmant['uid_topid'] = fitted_models_sigmant["user_id"].map(str) + "_" + fitted_models_sigmant["topic_id"].map(str)

2018-05-02 17:50:29,559 - memory_profile6_log - INFO -    304    149.4 MiB      1.4 MiB               fitted_models_sigmant = fitted_models_sigmant[["uid_topid", "pt_posterior_x_Nt", "smoothed_pt_posterior", "p0_cat_ci", "sigma_Nt"]]

2018-05-02 17:50:29,559 - memory_profile6_log - INFO -    305                               

2018-05-02 17:50:29,561 - memory_profile6_log - INFO -    306    149.7 MiB      0.3 MiB               X_split = np.array_split(fitted_models_sigmant, 5)

2018-05-02 17:50:29,565 - memory_profile6_log - INFO -    307    149.7 MiB      0.0 MiB               logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-05-02 17:50:29,565 - memory_profile6_log - INFO -    308    149.7 MiB      0.0 MiB               logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-05-02 17:50:29,568 - memory_profile6_log - INFO -    309    151.1 MiB    -14.0 MiB               for ix in range(len(X_split)):

2018-05-02 17:50:29,568 - memory_profile6_log - INFO -    310    151.1 MiB    -13.6 MiB                   logger.info("processing batch-%d", ix)

2018-05-02 17:50:29,569 - memory_profile6_log - INFO -    311    151.1 MiB    -16.6 MiB                   mh.saveElasticS(X_split[ix], esindex_name="fitted_hist_index",  estype_name='fitted_hist_type')

2018-05-02 17:50:29,569 - memory_profile6_log - INFO -    312    146.5 MiB     -4.6 MiB               del X_split

2018-05-02 17:50:29,569 - memory_profile6_log - INFO -    313                                         

2018-05-02 17:50:29,569 - memory_profile6_log - INFO -    314    146.5 MiB      0.0 MiB               del BR

2018-05-02 17:50:29,569 - memory_profile6_log - INFO -    315    146.5 MiB      0.0 MiB               logger.info("deleting BR...")

2018-05-02 17:50:29,571 - memory_profile6_log - INFO -    316    146.5 MiB      0.0 MiB               del save_sigma_nt

2018-05-02 17:50:29,571 - memory_profile6_log - INFO -    317    146.5 MiB      0.0 MiB               logger.info("deleting save_sigma_nt...")

2018-05-02 17:50:29,571 - memory_profile6_log - INFO -    318    145.8 MiB     -0.7 MiB               del fitted_models_sigmant

2018-05-02 17:50:29,572 - memory_profile6_log - INFO -    319    145.8 MiB      0.0 MiB               logger.info("deleting fitted_models_sigmant...")

2018-05-02 17:50:29,575 - memory_profile6_log - INFO -    320                             

2018-05-02 17:50:29,576 - memory_profile6_log - INFO -    321                                     # need save sigma_nt for daily train

2018-05-02 17:50:29,578 - memory_profile6_log - INFO -    322                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-05-02 17:50:29,578 - memory_profile6_log - INFO -    323                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-05-02 17:50:29,578 - memory_profile6_log - INFO -    324    145.8 MiB      0.0 MiB           if start_date and end_date:

2018-05-02 17:50:29,579 - memory_profile6_log - INFO -    325                                         if not fitby_sigmant:

2018-05-02 17:50:29,579 - memory_profile6_log - INFO -    326                                             logging.info("Saving sigma Nt...")

2018-05-02 17:50:29,581 - memory_profile6_log - INFO -    327                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 17:50:29,582 - memory_profile6_log - INFO -    328                                             save_sigma_nt['start_date'] = start_date

2018-05-02 17:50:29,592 - memory_profile6_log - INFO -    329                                             save_sigma_nt['end_date'] = end_date

2018-05-02 17:50:29,595 - memory_profile6_log - INFO -    330                                             print save_sigma_nt.head(5)

2018-05-02 17:50:29,595 - memory_profile6_log - INFO -    331                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-05-02 17:50:29,595 - memory_profile6_log - INFO -    332                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-05-02 17:50:29,596 - memory_profile6_log - INFO -    333    145.8 MiB      0.0 MiB       return

2018-05-02 17:50:29,596 - memory_profile6_log - INFO - 


2018-05-02 17:50:29,596 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-05-02 18:00:39,832 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 18:00:39,835 - memory_profile6_log - INFO - date_generated: 
2018-05-02 18:00:39,836 - memory_profile6_log - INFO -  
2018-05-02 18:00:39,838 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 18, 0, 39, 833000)]
2018-05-02 18:00:39,838 - memory_profile6_log - INFO - 

2018-05-02 18:00:39,838 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 18:00:39,838 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 18:00:39,839 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 18:00:40,010 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 18:00:40,015 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 18:00:50,413 - memory_profile6_log - INFO - size of df: 5.05 MB
2018-05-02 18:00:50,414 - memory_profile6_log - INFO - getting total: 20000 training data(genuine interest) for date: 2018-05-01
2018-05-02 18:00:50,447 - memory_profile6_log - INFO - loading history data from elastic...
2018-05-02 18:00:50,448 - memory_profile6_log - INFO - Len of X_split for batch load: 10
2018-05-02 18:00:50,450 - memory_profile6_log - INFO - Appending history data...
2018-05-02 18:00:50,496 - memory_profile6_log - INFO - call 2000 history data...
2018-05-02 18:00:53,890 - memory_profile6_log - INFO -                                              user_id  topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  16130e77ab1ae-0e200dfc009b5b-16457644-38400-16...  22553543         227.678634             237.678634   0.110964       227
1  16130e8bf71104-001943f03c4b79-585d68-38400-161...  22552186         331.423702             341.423702   0.006561       251
2  16130e8bf71104-001943f03c4b79-585d68-38400-161...  22553321         331.018407             341.018407   0.006664       251
3  16130e91ce5f1-071af767bfbd58-4c33197a-38400-16...  22553543          67.390934              77.390934   0.110964        81
4  16130ee125610c-03aa3303ee081a-e46181e-38400-16...  22553543          51.003231              61.003231   0.110964        74
2018-05-02 18:00:53,891 - memory_profile6_log - INFO - 

2018-05-02 18:00:53,898 - memory_profile6_log - INFO - call 2000 history data...
2018-05-02 18:00:59,045 - memory_profile6_log - INFO -                                              user_id  topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  16130ea976fb7-046bf699a445a1-4b476b3b-38400-16...  22553543         195.418685             205.418685   0.110964       142
1  16130ebcc1995-089b5acdc39f35-716a1530-38400-16...  22553543         134.137879             144.137879   0.110964       190
2  16130eddcf312d-042f4e2fa42d57-5c01624f-38400-1...  22553543          40.120904              50.120904   0.110964        45
3  16130f4124cf1-0898267e3f80d2-282b543f-38400-16...  22553543          39.273657              49.273657   0.110964        31
4  16130f53ef03d-086697dfd-450d4f0d-2c880-16130f5...  22553543         103.388945             113.388945   0.110964        74
2018-05-02 18:00:59,046 - memory_profile6_log - INFO - 

2018-05-02 18:00:59,053 - memory_profile6_log - INFO - call 2000 history data...
2018-05-02 18:01:02,276 - memory_profile6_log - INFO -                                              user_id  topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  16130e8bf71104-001943f03c4b79-585d68-38400-161...  22553543         291.458674             301.458674   0.110964       251
1  16130e8bf71104-001943f03c4b79-585d68-38400-161...  22596701         204.700947             214.700947   0.034659       251
2  16130e91ce5f1-071af767bfbd58-4c33197a-38400-16...  22601470         111.180287             121.180287   0.075163        81
3  16130e982d911b-05318b7ae00a1a-c0e0044-38400-16...  22553543         132.653211             142.653211   0.110964       100
4  16130e982d911b-05318b7ae00a1a-c0e0044-38400-16...  22596701         225.874064             235.874064   0.034659       100
2018-05-02 18:01:02,276 - memory_profile6_log - INFO - 

2018-05-02 18:01:02,283 - memory_profile6_log - INFO - call 2000 history data...
2018-05-02 18:01:05,755 - memory_profile6_log - INFO -                                              user_id  topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  16130e77ab1ae-0e200dfc009b5b-16457644-38400-16...  27313197         543.187160             553.187160   0.000590       227
1  16130e8bf71104-001943f03c4b79-585d68-38400-161...  27430823          99.857574             109.857574   0.005467       251
2  16130eb903239-0b97b26455b808-7065430a-38400-16...  22601470         210.815038             220.815038   0.075163       103
3  16130ebcc1995-089b5acdc39f35-716a1530-38400-16...  22661796         385.283534             395.283534   0.004464       190
4  16130ee9f3651-0c7b790e00b1e6-292a5039-38400-16...  22601470         552.310683             562.310683   0.075163       136
2018-05-02 18:01:05,756 - memory_profile6_log - INFO - 

2018-05-02 18:01:05,763 - memory_profile6_log - INFO - call 2000 history data...
2018-05-02 18:01:08,871 - memory_profile6_log - INFO -                                              user_id  topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  16130e91ce5f1-071af767bfbd58-4c33197a-38400-16...  27430823          51.346328              61.346328   0.005467        81
1  16130e91ce5f1-071af767bfbd58-4c33197a-38400-16...  27431099         115.513008             125.513008   0.067631        81
2  16130e91ce5f1-071af767bfbd58-4c33197a-38400-16...  27435908          39.925538              49.925538   0.004738        81
3  16130e982d911b-05318b7ae00a1a-c0e0044-38400-16...  27435908          39.925538              49.925538   0.004738       100
4  16130fdafc746-02b8e228aad61d-4e645706-38400-16...  36060446         748.769600             758.769600   0.025705       135
2018-05-02 18:01:08,874 - memory_profile6_log - INFO - 

2018-05-02 18:01:08,881 - memory_profile6_log - INFO - call 2000 history data...
2018-05-02 18:01:12,305 - memory_profile6_log - INFO -                                              user_id  topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  16130e8bf71104-001943f03c4b79-585d68-38400-161...  38682224       14262.542857           14272.542857   0.000100       251
1  16130e8bf71104-001943f03c4b79-585d68-38400-161...  39321129         153.454965             163.454965   0.000236       251
2  16130e8bf71104-001943f03c4b79-585d68-38400-161...  40710244         170.758830             180.758830   0.004917       251
3  16130e8bf71104-001943f03c4b79-585d68-38400-161...  41566010         265.328611             275.328611   0.012182       251
4  16130e982d911b-05318b7ae00a1a-c0e0044-38400-16...  39321129         306.909929             316.909929   0.000236       100
2018-05-02 18:01:12,306 - memory_profile6_log - INFO - 

2018-05-02 18:01:12,315 - memory_profile6_log - INFO - call 2000 history data...
2018-05-02 18:01:19,654 - memory_profile6_log - INFO -                                              user_id   topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  16130e8bf71104-001943f03c4b79-585d68-38400-161...  107156920         258.647150             268.647150   0.001652       251
1  16130e8bf71104-001943f03c4b79-585d68-38400-161...   43682517       11609.046512           11619.046512   0.000067       251
2  16130e8bf71104-001943f03c4b79-585d68-38400-161...   46104963         821.422669             831.422669   0.001133       251
3  16130e8bf71104-001943f03c4b79-585d68-38400-161...   47085613         538.474913             548.474913   0.030802       251
4  16130e8bf71104-001943f03c4b79-585d68-38400-161...   47478784         579.778165             589.778165   0.000743       251
2018-05-02 18:01:19,657 - memory_profile6_log - INFO - 

2018-05-02 18:01:19,667 - memory_profile6_log - INFO - call 2000 history data...
2018-05-02 18:01:23,769 - memory_profile6_log - INFO -                                              user_id    topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  16130e8bf71104-001943f03c4b79-585d68-38400-161...  1034952471        2641.211640            2651.211640   0.000862       251
1  16130e8bf71104-001943f03c4b79-585d68-38400-161...   273342551         309.525334             319.525334   0.003680       251
2  16130e91ce5f1-071af767bfbd58-4c33197a-38400-16...   819469507        1306.777487            1316.777487   0.002090        81
3  16130e982d911b-05318b7ae00a1a-c0e0044-38400-16...  1019283820          86.156196              96.156196   0.003499       100
4  16130e982d911b-05318b7ae00a1a-c0e0044-38400-16...   188400148         149.437754             159.437754   0.018864       100
2018-05-02 18:01:23,769 - memory_profile6_log - INFO - 

2018-05-02 18:01:23,782 - memory_profile6_log - INFO - call 2000 history data...
2018-05-02 18:01:27,017 - memory_profile6_log - INFO -                                              user_id    topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  16130e8bf71104-001943f03c4b79-585d68-38400-161...  1065124711         889.029269             899.029269   0.003729       251
1  16130e8bf71104-001943f03c4b79-585d68-38400-161...  1157057159         132.217984             142.217984   0.001942       251
2  16130e8bf71104-001943f03c4b79-585d68-38400-161...  1161375937         170.062117             180.062117   0.003543       251
3  16130e982d911b-05318b7ae00a1a-c0e0044-38400-16...  1037574949         104.258354             114.258354   0.008433       100
4  16130e982d911b-05318b7ae00a1a-c0e0044-38400-16...  1038512181          71.312714              81.312714   0.001702       100
2018-05-02 18:01:27,019 - memory_profile6_log - INFO - 

2018-05-02 18:01:27,028 - memory_profile6_log - INFO - call 2000 history data...
2018-05-02 18:01:31,969 - memory_profile6_log - INFO -                                              user_id           topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt
0  16130e8bf71104-001943f03c4b79-585d68-38400-161...  27431110790314246         809.058347             819.058347   0.003553       251
1  16130e8bf71104-001943f03c4b79-585d68-38400-161...  27431110790314255          66.915416              76.915416   0.004074       251
2  16130e8bf71104-001943f03c4b79-585d68-38400-161...  27431110790314280          90.106318             100.106318   0.013512       251
3  16130e8bf71104-001943f03c4b79-585d68-38400-161...  27431110790314285         204.753486             214.753486   0.004592       251
4  16130e91ce5f1-071af767bfbd58-4c33197a-38400-16...  27431110790314278         271.853234             281.853234   0.000444        81
2018-05-02 18:01:31,970 - memory_profile6_log - INFO - 

2018-05-02 18:01:31,971 - memory_profile6_log - INFO - Appending training data...
2018-05-02 18:01:31,973 - memory_profile6_log - INFO - len datalist: 1
2018-05-02 18:01:31,974 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-05-02 18:01:31,977 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 18:01:31,979 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 18:01:31,982 - memory_profile6_log - INFO - ================================================

2018-05-02 18:01:31,983 - memory_profile6_log - INFO -    360     86.6 MiB     86.6 MiB   @profile

2018-05-02 18:01:31,983 - memory_profile6_log - INFO -    361                             def BQPreprocess(cpu, date_generated, client, query_fit, loadfrom="elastic"):

2018-05-02 18:01:31,984 - memory_profile6_log - INFO -    362     86.6 MiB      0.0 MiB       bq_client = client

2018-05-02 18:01:31,986 - memory_profile6_log - INFO -    363     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 18:01:31,986 - memory_profile6_log - INFO -    364                             

2018-05-02 18:01:31,986 - memory_profile6_log - INFO -    365     86.6 MiB      0.0 MiB       datalist = []

2018-05-02 18:01:31,987 - memory_profile6_log - INFO -    366     86.6 MiB      0.0 MiB       datalist_hist = []

2018-05-02 18:01:31,990 - memory_profile6_log - INFO -    367                             

2018-05-02 18:01:31,993 - memory_profile6_log - INFO -    368     86.6 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-05-02 18:01:31,993 - memory_profile6_log - INFO -    369    141.9 MiB      0.0 MiB       for ndate in date_generated:

2018-05-02 18:01:31,994 - memory_profile6_log - INFO -    370    140.2 MiB     53.6 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-05-02 18:01:31,996 - memory_profile6_log - INFO -    371    140.2 MiB      0.0 MiB           if tframe is not None:

2018-05-02 18:01:31,996 - memory_profile6_log - INFO -    372    140.2 MiB      0.0 MiB               if not tframe.empty:

2018-05-02 18:01:31,997 - memory_profile6_log - INFO -    373    140.2 MiB      0.0 MiB                   if loadfrom.strip().lower() == 'datastore':

2018-05-02 18:01:31,999 - memory_profile6_log - INFO -    374                                                 X_split = np.array_split(tframe, 5)

2018-05-02 18:01:32,000 - memory_profile6_log - INFO -    375                                                 logger.info("loading history data from datastore...")

2018-05-02 18:01:32,003 - memory_profile6_log - INFO -    376                                                 logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 18:01:32,003 - memory_profile6_log - INFO -    377                                                 logger.info("Appending history data...")

2018-05-02 18:01:32,005 - memory_profile6_log - INFO -    378                                                 for ix in range(len(X_split)):

2018-05-02 18:01:32,006 - memory_profile6_log - INFO -    379                                                     # ~ loading history

2018-05-02 18:01:32,006 - memory_profile6_log - INFO -    380                                                     """

2018-05-02 18:01:32,007 - memory_profile6_log - INFO -    381                                                         disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-05-02 18:01:32,007 - memory_profile6_log - INFO -    382                                                     """

2018-05-02 18:01:32,009 - memory_profile6_log - INFO -    383                                                     logger.info("processing batch-%d", ix)

2018-05-02 18:01:32,009 - memory_profile6_log - INFO -    384                                                     # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-05-02 18:01:32,013 - memory_profile6_log - INFO -    385                                                     logger.info("creating list history data...")

2018-05-02 18:01:32,013 - memory_profile6_log - INFO -    386                                                     #lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-05-02 18:01:32,015 - memory_profile6_log - INFO -    387                                                     lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 18:01:32,016 - memory_profile6_log - INFO -    388                             

2018-05-02 18:01:32,016 - memory_profile6_log - INFO -    389                                                     logger.info("call history data...")

2018-05-02 18:01:32,017 - memory_profile6_log - INFO -    390                                                     h_frame = mh.loadDSHistory(lhistory)

2018-05-02 18:01:32,019 - memory_profile6_log - INFO -    391                             

2018-05-02 18:01:32,019 - memory_profile6_log - INFO -    392                                                     # me = os.getpid()

2018-05-02 18:01:32,019 - memory_profile6_log - INFO -    393                                                     # kill_proc_tree(me)

2018-05-02 18:01:32,020 - memory_profile6_log - INFO -    394                             

2018-05-02 18:01:32,023 - memory_profile6_log - INFO -    395                                                     logger.info("done collecting history data, appending now...")

2018-05-02 18:01:32,025 - memory_profile6_log - INFO -    396                                                     for m in h_frame:

2018-05-02 18:01:32,026 - memory_profile6_log - INFO -    397                                                         if m is not None:

2018-05-02 18:01:32,026 - memory_profile6_log - INFO -    398                                                             if len(m) > 0:

2018-05-02 18:01:32,028 - memory_profile6_log - INFO -    399                                                                 datalist_hist.append(pd.DataFrame(m))

2018-05-02 18:01:32,029 - memory_profile6_log - INFO -    400                                                     del h_frame

2018-05-02 18:01:32,029 - memory_profile6_log - INFO -    401                                                     del lhistory

2018-05-02 18:01:32,029 - memory_profile6_log - INFO -    402                             

2018-05-02 18:01:32,030 - memory_profile6_log - INFO -    403                                                 logger.info("Appending training data...")

2018-05-02 18:01:32,032 - memory_profile6_log - INFO -    404                                                 datalist.append(tframe)

2018-05-02 18:01:32,035 - memory_profile6_log - INFO -    405    140.2 MiB      0.0 MiB                   elif loadfrom.strip().lower() == 'elastic':

2018-05-02 18:01:32,036 - memory_profile6_log - INFO -    406    140.8 MiB      0.5 MiB                       X_split = np.array_split(tframe, 10)

2018-05-02 18:01:32,036 - memory_profile6_log - INFO -    407    140.8 MiB      0.0 MiB                       logger.info("loading history data from elastic...")

2018-05-02 18:01:32,038 - memory_profile6_log - INFO -    408    140.8 MiB      0.0 MiB                       logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 18:01:32,039 - memory_profile6_log - INFO -    409    140.8 MiB      0.0 MiB                       logger.info("Appending history data...")

2018-05-02 18:01:32,039 - memory_profile6_log - INFO -    410                                                 

2018-05-02 18:01:32,040 - memory_profile6_log - INFO -    411    141.9 MiB     -0.0 MiB                       for ix in range(len(X_split)):

2018-05-02 18:01:32,042 - memory_profile6_log - INFO -    412    141.9 MiB      0.5 MiB                           lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 18:01:32,042 - memory_profile6_log - INFO -    413    141.9 MiB     -0.0 MiB                           logger.info("call %d history data...", len(lhistory))

2018-05-02 18:01:32,046 - memory_profile6_log - INFO -    414    141.9 MiB     -0.0 MiB                           inside_data = mh.loadESHistory(lhistory,

2018-05-02 18:01:32,046 - memory_profile6_log - INFO -    415    141.9 MiB     -0.0 MiB                                                          esindex_name='fitted_hist_index',

2018-05-02 18:01:32,048 - memory_profile6_log - INFO -    416    141.9 MiB      0.3 MiB                                                          estype_name='fitted_hist_type')

2018-05-02 18:01:32,049 - memory_profile6_log - INFO -    417                                                     # split back the user_id and topic_id

2018-05-02 18:01:32,049 - memory_profile6_log - INFO -    418    141.9 MiB      0.0 MiB                           inside_data[['user_id','topic_id']] = inside_data.uid_topid.str.split('_', expand=True)

2018-05-02 18:01:32,051 - memory_profile6_log - INFO -    419    141.9 MiB      0.2 MiB                           inside_data = inside_data[["user_id","topic_id", "pt_posterior_x_Nt", "smoothed_pt_posterior", "p0_cat_ci", "sigma_Nt"]]

2018-05-02 18:01:32,052 - memory_profile6_log - INFO -    420    141.9 MiB     -0.0 MiB                           if not inside_data.empty:

2018-05-02 18:01:32,052 - memory_profile6_log - INFO -    421    141.9 MiB      0.0 MiB                               print inside_data.head(5)

2018-05-02 18:01:32,053 - memory_profile6_log - INFO -    422    141.9 MiB     -0.0 MiB                               datalist_hist.append(inside_data)

2018-05-02 18:01:32,058 - memory_profile6_log - INFO -    423    141.9 MiB     -0.0 MiB                               del inside_data

2018-05-02 18:01:32,059 - memory_profile6_log - INFO -    424                                                     else:

2018-05-02 18:01:32,059 - memory_profile6_log - INFO -    425                                                         logger.info("uid: %s with topic_id: %s is empty!" % (framedata.user_id, framedata.topic_id))

2018-05-02 18:01:32,061 - memory_profile6_log - INFO -    426                                                 

2018-05-02 18:01:32,062 - memory_profile6_log - INFO -    427    141.9 MiB     -0.0 MiB                       logger.info("Appending training data...")

2018-05-02 18:01:32,062 - memory_profile6_log - INFO -    428    141.9 MiB      0.0 MiB                       datalist.append(tframe)

2018-05-02 18:01:32,063 - memory_profile6_log - INFO -    429                                             else:

2018-05-02 18:01:32,065 - memory_profile6_log - INFO -    430                                                 logger.info("Unknows source is selected !")

2018-05-02 18:01:32,065 - memory_profile6_log - INFO -    431                                                 break

2018-05-02 18:01:32,069 - memory_profile6_log - INFO -    432                                     else: 

2018-05-02 18:01:32,069 - memory_profile6_log - INFO -    433                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-05-02 18:01:32,069 - memory_profile6_log - INFO -    434    141.9 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-05-02 18:01:32,071 - memory_profile6_log - INFO -    435    141.9 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-05-02 18:01:32,072 - memory_profile6_log - INFO -    436                             

2018-05-02 18:01:32,072 - memory_profile6_log - INFO -    437    141.9 MiB      0.0 MiB       return datalist, datalist_hist

2018-05-02 18:01:32,073 - memory_profile6_log - INFO - 


2018-05-02 18:01:32,095 - memory_profile6_log - INFO - size of big_frame_hist: 28.26 KB
2018-05-02 18:01:32,117 - memory_profile6_log - INFO - size of big_frame: 5.05 MB
2018-05-02 18:01:32,122 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-05-02 18:01:39,900 - memory_profile6_log - INFO - size of df: 4.93 MB
2018-05-02 18:01:39,901 - memory_profile6_log - INFO - getting total: 20000 training data(current date interest)
2018-05-02 18:01:39,914 - memory_profile6_log - INFO - size of current_frame: 5.09 MB
2018-05-02 18:01:39,917 - memory_profile6_log - INFO - loading time of: 40000 total genuine-current interest data ~ take 59.949s
2018-05-02 18:01:39,917 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 18:01:39,917 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 18:01:39,920 - memory_profile6_log - INFO - ================================================

2018-05-02 18:01:39,921 - memory_profile6_log - INFO -    439     86.4 MiB     86.4 MiB   @profile

2018-05-02 18:01:39,921 - memory_profile6_log - INFO -    440                             def preprocess(cpu, cd, query_fit, date_generated):

2018-05-02 18:01:39,921 - memory_profile6_log - INFO -    441     86.6 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-05-02 18:01:39,923 - memory_profile6_log - INFO -    442     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 18:01:39,923 - memory_profile6_log - INFO -    443                             

2018-05-02 18:01:39,924 - memory_profile6_log - INFO -    444                                 # ~~~ Begin collecting data ~~~

2018-05-02 18:01:39,924 - memory_profile6_log - INFO -    445     86.6 MiB      0.0 MiB       t0 = time.time()

2018-05-02 18:01:39,924 - memory_profile6_log - INFO -    446    141.9 MiB     55.3 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-05-02 18:01:39,924 - memory_profile6_log - INFO -    447    141.9 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-05-02 18:01:39,924 - memory_profile6_log - INFO -    448                                     logger.info("Training cannot be empty..")

2018-05-02 18:01:39,926 - memory_profile6_log - INFO -    449                                     return False

2018-05-02 18:01:39,926 - memory_profile6_log - INFO -    450    141.9 MiB      0.0 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-05-02 18:01:39,926 - memory_profile6_log - INFO -    451                                 # print "big_frame_hist:\n", big_frame_hist.head(20)

2018-05-02 18:01:39,926 - memory_profile6_log - INFO -    452    141.9 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-05-02 18:01:39,927 - memory_profile6_log - INFO -    453                             

2018-05-02 18:01:39,927 - memory_profile6_log - INFO -    454    141.9 MiB      0.0 MiB       big_frame = pd.concat(datalist)

2018-05-02 18:01:39,927 - memory_profile6_log - INFO -    455    141.9 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-05-02 18:01:39,927 - memory_profile6_log - INFO -    456    141.9 MiB      0.0 MiB       del datalist

2018-05-02 18:01:39,931 - memory_profile6_log - INFO -    457                             

2018-05-02 18:01:39,933 - memory_profile6_log - INFO -    458    141.9 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 18:01:39,933 - memory_profile6_log - INFO -    459                             

2018-05-02 18:01:39,934 - memory_profile6_log - INFO -    460                                 # ~ get current news interest ~

2018-05-02 18:01:39,934 - memory_profile6_log - INFO -    461    141.9 MiB      0.0 MiB       if not cd:

2018-05-02 18:01:39,934 - memory_profile6_log - INFO -    462    141.9 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-05-02 18:01:39,934 - memory_profile6_log - INFO -    463    145.4 MiB      3.5 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-05-02 18:01:39,936 - memory_profile6_log - INFO -    464    145.4 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-05-02 18:01:39,936 - memory_profile6_log - INFO -    465                                 else:

2018-05-02 18:01:39,937 - memory_profile6_log - INFO -    466                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-05-02 18:01:39,937 - memory_profile6_log - INFO -    467                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date) limit 20000"

2018-05-02 18:01:39,937 - memory_profile6_log - INFO -    468                             

2018-05-02 18:01:39,937 - memory_profile6_log - INFO -    469                                     # safe handling of query parameter

2018-05-02 18:01:39,938 - memory_profile6_log - INFO -    470                                     query_params = [

2018-05-02 18:01:39,938 - memory_profile6_log - INFO -    471                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-05-02 18:01:39,940 - memory_profile6_log - INFO -    472                                     ]

2018-05-02 18:01:39,940 - memory_profile6_log - INFO -    473                             

2018-05-02 18:01:39,940 - memory_profile6_log - INFO -    474                                     job_config.query_parameters = query_params

2018-05-02 18:01:39,940 - memory_profile6_log - INFO -    475                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-05-02 18:01:39,944 - memory_profile6_log - INFO -    476                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-05-02 18:01:39,946 - memory_profile6_log - INFO -    477                             

2018-05-02 18:01:39,946 - memory_profile6_log - INFO -    478    145.4 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-05-02 18:01:39,947 - memory_profile6_log - INFO -    479    145.4 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-05-02 18:01:39,947 - memory_profile6_log - INFO -    480    145.4 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-05-02 18:01:39,947 - memory_profile6_log - INFO -    481    145.4 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-05-02 18:01:39,948 - memory_profile6_log - INFO -    482    145.4 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 18:01:39,948 - memory_profile6_log - INFO -    483    145.4 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-05-02 18:01:39,948 - memory_profile6_log - INFO -    484                             

2018-05-02 18:01:39,950 - memory_profile6_log - INFO -    485    145.4 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-05-02 18:01:39,950 - memory_profile6_log - INFO - 


2018-05-02 18:01:39,954 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-05-02 18:01:39,963 - memory_profile6_log - INFO - train on: 20000 total genuine interest data(D(u, t))
2018-05-02 18:01:39,964 - memory_profile6_log - INFO - transform on: 20000 total current data(D(t))
2018-05-02 18:01:39,966 - memory_profile6_log - INFO - apply on: 100 total history...)
2018-05-02 18:01:40,020 - memory_profile6_log - INFO - len of uniques_fit_hist:100
2018-05-02 18:01:40,026 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:26
2018-05-02 18:01:40,105 - memory_profile6_log - INFO - self.sum_all_nt before concate:

2018-05-02 18:01:40,121 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 18:01:40,122 - memory_profile6_log - INFO - 

2018-05-02 18:01:40,145 - memory_profile6_log - INFO - self.sum_all_nt after concate:

2018-05-02 18:01:40,160 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 18:01:40,161 - memory_profile6_log - INFO - 

2018-05-02 18:01:40,220 - memory_profile6_log - INFO - Len of model_fit: 20000
2018-05-02 18:01:40,223 - memory_profile6_log - INFO - Len of df_dut: 20000
2018-05-02 18:01:40,391 - memory_profile6_log - INFO - Len of fitted_models on main class: 20000
2018-05-02 18:01:40,392 - memory_profile6_log - INFO - 

2018-05-02 18:01:40,394 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-05-02 18:01:40,394 - memory_profile6_log - INFO - 

2018-05-02 18:01:40,397 - memory_profile6_log - INFO - fitted_model_hist:

2018-05-02 18:01:40,397 - memory_profile6_log - INFO - 

2018-05-02 18:01:40,407 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 18:01:40,407 - memory_profile6_log - INFO - 

2018-05-02 18:01:40,421 - memory_profile6_log - INFO - len of fitted models after concat: 20100
2018-05-02 18:01:40,421 - memory_profile6_log - INFO - 

2018-05-02 18:01:40,424 - memory_profile6_log - INFO - Recalculating fitted models...
2018-05-02 18:01:40,424 - memory_profile6_log - INFO - 

2018-05-02 18:01:40,463 - memory_profile6_log - INFO - fitted_models after cobine:

2018-05-02 18:01:40,466 - memory_profile6_log - INFO - 

2018-05-02 18:01:40,473 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 18:01:40,474 - memory_profile6_log - INFO - 

2018-05-02 18:01:40,476 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 20000
2018-05-02 18:01:40,476 - memory_profile6_log - INFO - 

2018-05-02 18:01:45,716 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-05-02 18:01:45,734 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, topic_id, pt_posterior_x_Nt, smoothed_pt_posterior, p0_cat_ci, sigma_Nt, p0_posterior, is_general, rank]
Index: []
2018-05-02 18:01:45,736 - memory_profile6_log - INFO - 

2018-05-02 18:01:45,736 - memory_profile6_log - INFO - Len of model_transform: 9071
2018-05-02 18:01:45,737 - memory_profile6_log - INFO - Len of df_dt: 20000
2018-05-02 18:01:45,739 - memory_profile6_log - INFO - Total train time: 5.760s
2018-05-02 18:01:45,740 - memory_profile6_log - INFO - memory left before cleaning: 93.200 percent memory...
2018-05-02 18:01:45,742 - memory_profile6_log - INFO - cleaning up some objects...
2018-05-02 18:01:45,743 - memory_profile6_log - INFO - deleting df_dut...
2018-05-02 18:01:45,743 - memory_profile6_log - INFO - deleting df_dt...
2018-05-02 18:01:45,744 - memory_profile6_log - INFO - deleting df_input...
2018-05-02 18:01:45,746 - memory_profile6_log - INFO - deleting df_input_X...
2018-05-02 18:01:45,747 - memory_profile6_log - INFO - deleting df_current...
2018-05-02 18:01:45,749 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-05-02 18:01:45,752 - memory_profile6_log - INFO - deleting model_fit...
2018-05-02 18:01:45,752 - memory_profile6_log - INFO - deleting result...
2018-05-02 18:01:45,759 - memory_profile6_log - INFO - list(model_transformsv) before rename: 
2018-05-02 18:01:45,760 - memory_profile6_log - INFO -  
2018-05-02 18:01:45,762 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'is_general', 'p0_posterior', 'rank', 'date']
2018-05-02 18:01:45,763 - memory_profile6_log - INFO - 

2018-05-02 18:01:45,763 - memory_profile6_log - INFO - 9071
2018-05-02 18:01:45,766 - memory_profile6_log - INFO - 

2018-05-02 18:01:45,769 - memory_profile6_log - INFO - list(model_transformsv) after rename: 
2018-05-02 18:01:45,770 - memory_profile6_log - INFO -  
2018-05-02 18:01:45,770 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'topic_is_general', 'interest_score', 'interest_rank', 'interest_score_created_at']
2018-05-02 18:01:45,773 - memory_profile6_log - INFO - 

2018-05-02 18:01:45,773 - memory_profile6_log - INFO - 9071
2018-05-02 18:01:45,776 - memory_profile6_log - INFO - 

2018-05-02 18:01:45,776 - memory_profile6_log - INFO - deleting model_transform...
2018-05-02 18:01:45,778 - memory_profile6_log - INFO - memory left after cleaning: 93.100 percent memory...
2018-05-02 18:01:45,779 - memory_profile6_log - INFO - Begin saving trained data...
2018-05-02 18:01:45,779 - memory_profile6_log - INFO - Using ElasticSearch as storage...
2018-05-02 18:01:45,788 - memory_profile6_log - INFO - Saving total data: 9071
2018-05-02 18:01:45,789 - memory_profile6_log - INFO - Len of X_split for batch save model_transformsv: 5
2018-05-02 18:01:45,789 - memory_profile6_log - INFO - processing batch-0
2018-05-02 18:01:45,819 - memory_profile6_log - INFO -                                                user_id    topic_id  topic_is_general  interest_score  interest_rank interest_score_created_at
50   16120973230c3-065c9d7d55d544-5f6c3a73-100200-1...    22661796              True        0.046700            1.0   2018-05-02 18:00:39.833
56   161285256c03f5-065f4919abd206-7c2d6751-1fa400-...  1065124711              True        0.241304            1.0   2018-05-02 18:00:39.833
82   1612cf2dca04e-04537d4ab97b66-3c660c68-38400-16...    27428824              True        0.423427            1.0   2018-05-02 18:00:39.833
102  1612cf401744e-0189b26ff41373-6b70473f-38400-16...    27435908              True        0.036791            1.0   2018-05-02 18:00:39.833
110  1612cf5f25d0-097a7f71d-9665763-38400-1612cf5f2...    39328335              True        0.042882            1.0   2018-05-02 18:00:39.833
2018-05-02 18:01:45,819 - memory_profile6_log - INFO - 

2018-05-02 18:01:55,974 - memory_profile6_log - INFO - processing batch-1
2018-05-02 18:01:56,003 - memory_profile6_log - INFO -                                                  user_id  topic_id  topic_is_general  interest_score  interest_rank interest_score_created_at
15130  162118de9628b-08f2cd5d8932ac-32d6205-38400-162...  27435908              True        0.028448            2.0   2018-05-02 18:00:39.833
15162  162128cfdf23c5-0ab5509c2b73d7-547d0e1b-3d10d-1...  22552349              True        0.013341            2.0   2018-05-02 18:00:39.833
15265  16215bbee53b8-0df5c556416575-5b10605c-38400-16...  39328335              True        0.018378            2.0   2018-05-02 18:00:39.833
15277  1621724a7d51c9-0f78379a4d0be5-78221560-38400-1...  27435908              True        0.036791            2.0   2018-05-02 18:00:39.833
15506  162276184301b8-01c809b75d55ac-3e3d5f01-100200-...  22552349              True        0.013341            2.0   2018-05-02 18:00:39.833
2018-05-02 18:01:56,006 - memory_profile6_log - INFO - 

2018-05-02 18:02:08,411 - memory_profile6_log - INFO - processing batch-2
2018-05-02 18:02:08,443 - memory_profile6_log - INFO -                                                  user_id    topic_id  topic_is_general  interest_score  interest_rank interest_score_created_at
19807  16317f154c24fa-0bba5398e5d4d2-314c5569-100200-...    27311838             False        0.232909            1.0   2018-05-02 18:00:39.833
19808  16318997ed2161-0d05ff2b9-3d7b4c1e-34080-163189...   136538686             False        0.042655            1.0   2018-05-02 18:00:39.833
19818  1631a28ae2df-08b008fc7-55061743-38400-1631a28a...  1038512181             False        0.063789            1.0   2018-05-02 18:00:39.833
19833  1631b49b82a3e-01ffbc93eed6bf-173a0c7d-38400-16...   189391506             False        0.166099            1.0   2018-05-02 18:00:39.833
19880  1631b56d1a480-0c4c33692-1e6b600f-38400-1631b56...    92218899             False        0.755380            1.0   2018-05-02 18:00:39.833
2018-05-02 18:02:08,444 - memory_profile6_log - INFO - 

2018-05-02 18:02:22,325 - memory_profile6_log - INFO - processing batch-3
2018-05-02 18:02:22,355 - memory_profile6_log - INFO -                                                  user_id           topic_id  topic_is_general  interest_score  interest_rank interest_score_created_at
10985  16170e688c51e-06c521613982f-25453036-38400-161...          273342551             False        0.035628            3.0   2018-05-02 18:00:39.833
11010  16172544cda48-05c62fe67a5c56-45625e3a-38400-16...           39321129             False        0.011035            3.0   2018-05-02 18:00:39.833
11018  16172703e7e9e-092f0b20faab8b-a0e38-38400-16172...  27431110790314285             False        0.177511            3.0   2018-05-02 18:00:39.833
11028  16172abe24022-014ca8cef28987-574f603f-410a0-16...           41522104             False        0.051459            3.0   2018-05-02 18:00:39.833
11035  16173192628af-0024dedf90f297-4568692c-38400-16...  27431110790314255             False        0.027668            3.0   2018-05-02 18:00:39.833
2018-05-02 18:02:22,357 - memory_profile6_log - INFO - 

2018-05-02 18:02:37,630 - memory_profile6_log - INFO - processing batch-4
2018-05-02 18:02:37,664 - memory_profile6_log - INFO -                                                 user_id    topic_id  topic_is_general  interest_score  interest_rank interest_score_created_at
5155  16132b0217b54-091e93789a25fd-1850044f-38400-16...   338797555             False        0.018080            6.0   2018-05-02 18:00:39.833
5187  16132bd3c8f45-0a0eac671cdfe9-493f1f77-38400-16...   249083327             False        0.028993            6.0   2018-05-02 18:00:39.833
5264  161332c158cdd-07f8c66d061a8d-7703023f-38400-16...   249083327             False        0.046834            6.0   2018-05-02 18:00:39.833
5312  161337b3bcb2eb-082e747c3298d28-547e0e18-4a640-...  1161375937             False        0.030850            6.0   2018-05-02 18:00:39.833
5334  161348ead56b5-03f6d2c1d340cd-5d640103-29b80-16...  1157057159             False        0.018348            6.0   2018-05-02 18:00:39.833
2018-05-02 18:02:37,664 - memory_profile6_log - INFO - 

2018-05-02 18:02:54,055 - memory_profile6_log - INFO - Saving fitted_models as history to Elasticsearch...
2018-05-02 18:02:54,112 - memory_profile6_log - INFO - Saving total data: 20000
2018-05-02 18:02:54,114 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 5
2018-05-02 18:02:54,115 - memory_profile6_log - INFO - processing batch-0
2018-05-02 18:03:30,075 - memory_profile6_log - INFO - processing batch-1
2018-05-02 18:03:56,757 - memory_profile6_log - INFO - processing batch-2
2018-05-02 18:04:23,549 - memory_profile6_log - INFO - processing batch-3
2018-05-02 18:04:46,059 - memory_profile6_log - INFO - processing batch-4
2018-05-02 18:05:08,026 - memory_profile6_log - INFO - deleting BR...
2018-05-02 18:05:08,029 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-05-02 18:05:08,029 - memory_profile6_log - INFO - deleting fitted_models_sigmant...
2018-05-02 18:05:08,030 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 18:05:08,032 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 18:05:08,032 - memory_profile6_log - INFO - ================================================

2018-05-02 18:05:08,036 - memory_profile6_log - INFO -    113    145.4 MiB    145.4 MiB   @profile

2018-05-02 18:05:08,036 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-05-02 18:05:08,036 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-05-02 18:05:08,038 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-05-02 18:05:08,039 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-05-02 18:05:08,039 - memory_profile6_log - INFO -    118                                      saveto="datastore", fitted_models_hist=None):

2018-05-02 18:05:08,039 - memory_profile6_log - INFO -    119                                 """

2018-05-02 18:05:08,040 - memory_profile6_log - INFO -    120                                     Main Process

2018-05-02 18:05:08,040 - memory_profile6_log - INFO -    121                                 """

2018-05-02 18:05:08,042 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-05-02 18:05:08,042 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-05-02 18:05:08,042 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-05-02 18:05:08,042 - memory_profile6_log - INFO -    125    145.4 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-05-02 18:05:08,042 - memory_profile6_log - INFO -    126    145.4 MiB      0.0 MiB       df_dut = df_input.copy(deep=True)

2018-05-02 18:05:08,046 - memory_profile6_log - INFO -    127    145.4 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 18:05:08,048 - memory_profile6_log - INFO -    128                             

2018-05-02 18:05:08,048 - memory_profile6_log - INFO -    129                                 # D(t)

2018-05-02 18:05:08,049 - memory_profile6_log - INFO -    130    145.4 MiB      0.0 MiB       df_dt = df_current.copy(deep=True)

2018-05-02 18:05:08,049 - memory_profile6_log - INFO -    131    145.4 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 18:05:08,049 - memory_profile6_log - INFO -    132                             

2018-05-02 18:05:08,051 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-05-02 18:05:08,051 - memory_profile6_log - INFO -    134    145.4 MiB      0.0 MiB       t0 = time.time()

2018-05-02 18:05:08,052 - memory_profile6_log - INFO -    135    145.4 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-05-02 18:05:08,052 - memory_profile6_log - INFO -    136    145.4 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-05-02 18:05:08,052 - memory_profile6_log - INFO -    137    145.4 MiB      0.0 MiB       logger.info("apply on: %d total history...)", len(df_hist))

2018-05-02 18:05:08,053 - memory_profile6_log - INFO -    138                             

2018-05-02 18:05:08,053 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-05-02 18:05:08,058 - memory_profile6_log - INFO -    140    145.4 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-05-02 18:05:08,059 - memory_profile6_log - INFO -    141                             

2018-05-02 18:05:08,059 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-05-02 18:05:08,059 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-05-02 18:05:08,061 - memory_profile6_log - INFO -    144    145.6 MiB      0.2 MiB       NB = BR.processX(df_dut)

2018-05-02 18:05:08,061 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-05-02 18:05:08,062 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-05-02 18:05:08,062 - memory_profile6_log - INFO -    147    145.7 MiB      0.1 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-05-02 18:05:08,062 - memory_profile6_log - INFO -    148                                 """

2018-05-02 18:05:08,062 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-05-02 18:05:08,063 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-05-02 18:05:08,063 - memory_profile6_log - INFO -    151                                 """

2018-05-02 18:05:08,063 - memory_profile6_log - INFO -    152    145.7 MiB      0.0 MiB       fitby_sigmant = False

2018-05-02 18:05:08,065 - memory_profile6_log - INFO -    153    145.7 MiB      0.0 MiB       uniques_fit_hist = None

2018-05-02 18:05:08,065 - memory_profile6_log - INFO -    154    145.7 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-05-02 18:05:08,065 - memory_profile6_log - INFO -    155    145.7 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-05-02 18:05:08,069 - memory_profile6_log - INFO -    156    145.7 MiB      0.0 MiB                            'is_general']]

2018-05-02 18:05:08,072 - memory_profile6_log - INFO -    157                             

2018-05-02 18:05:08,072 - memory_profile6_log - INFO -    158                                 # get sigma_Nt from fitted_models_hist

2018-05-02 18:05:08,072 - memory_profile6_log - INFO -    159    145.7 MiB      0.0 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-05-02 18:05:08,073 - memory_profile6_log - INFO -    160    145.7 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-05-02 18:05:08,073 - memory_profile6_log - INFO -    161    145.7 MiB      0.0 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-05-02 18:05:08,075 - memory_profile6_log - INFO -    162    145.7 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-05-02 18:05:08,075 - memory_profile6_log - INFO -    163                             

2018-05-02 18:05:08,075 - memory_profile6_log - INFO -    164                                 # begin fit

2018-05-02 18:05:08,075 - memory_profile6_log - INFO -    165    145.7 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-05-02 18:05:08,076 - memory_profile6_log - INFO -    166    145.7 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-05-02 18:05:08,076 - memory_profile6_log - INFO -    167    147.8 MiB      2.1 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-05-02 18:05:08,078 - memory_profile6_log - INFO -    168    147.8 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-05-02 18:05:08,078 - memory_profile6_log - INFO -    169    147.8 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-05-02 18:05:08,082 - memory_profile6_log - INFO -    170                                 # print model_fit[['num_x', 'num_x', 'sigma_Nt', 'date_all_click']].loc[model_fit['user_id']=="1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157"].head(5)

2018-05-02 18:05:08,084 - memory_profile6_log - INFO -    171                             

2018-05-02 18:05:08,086 - memory_profile6_log - INFO -    172                                 # ~~ and Transform ~~

2018-05-02 18:05:08,086 - memory_profile6_log - INFO -    173                                 #   handling current news interest == current date

2018-05-02 18:05:08,088 - memory_profile6_log - INFO -    174    147.8 MiB      0.0 MiB       if df_dt.empty:

2018-05-02 18:05:08,088 - memory_profile6_log - INFO -    175                                     print "Cek your df_dt, cannot be emtpy!!"

2018-05-02 18:05:08,088 - memory_profile6_log - INFO -    176                                     return None

2018-05-02 18:05:08,088 - memory_profile6_log - INFO -    177    148.3 MiB      0.5 MiB       NB = BR.processX(df_dt)

2018-05-02 18:05:08,088 - memory_profile6_log - INFO -    178    148.3 MiB      0.0 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-05-02 18:05:08,088 - memory_profile6_log - INFO -    179                             

2018-05-02 18:05:08,091 - memory_profile6_log - INFO -    180    148.3 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-05-02 18:05:08,091 - memory_profile6_log - INFO -    181    148.3 MiB      0.0 MiB                            'num_x', 'num_y', 'is_general']]

2018-05-02 18:05:08,096 - memory_profile6_log - INFO -    182                             

2018-05-02 18:05:08,098 - memory_profile6_log - INFO -    183                                 # print "model_fit dtypes:\n", model_fit.dtypes

2018-05-02 18:05:08,098 - memory_profile6_log - INFO -    184                                 # print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-05-02 18:05:08,099 - memory_profile6_log - INFO -    185    148.3 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-05-02 18:05:08,101 - memory_profile6_log - INFO -    186    148.3 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-05-02 18:05:08,102 - memory_profile6_log - INFO -    187    148.3 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-05-02 18:05:08,104 - memory_profile6_log - INFO -    188    148.3 MiB      0.0 MiB                                                                                           "topic_id", "user_id"]],

2018-05-02 18:05:08,105 - memory_profile6_log - INFO -    189    152.1 MiB      3.8 MiB                                                     verbose=False)

2018-05-02 18:05:08,105 - memory_profile6_log - INFO -    190                             

2018-05-02 18:05:08,105 - memory_profile6_log - INFO -    191                                 # ~~~ filter is general and specific topic ~~~

2018-05-02 18:05:08,108 - memory_profile6_log - INFO -    192                                 # the idea is just we need to rerank every topic according

2018-05-02 18:05:08,108 - memory_profile6_log - INFO -    193                                 #    to user_id and and is_general by p0_posterior

2018-05-02 18:05:08,111 - memory_profile6_log - INFO -    194    152.1 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-05-02 18:05:08,111 - memory_profile6_log - INFO -    195    152.1 MiB      0.0 MiB                                     'is_general']].groupby(['topic_id',

2018-05-02 18:05:08,111 - memory_profile6_log - INFO -    196    152.1 MiB      0.0 MiB                                                             'is_general']

2018-05-02 18:05:08,112 - memory_profile6_log - INFO -    197                                                                                      ).size().to_frame().reset_index()

2018-05-02 18:05:08,112 - memory_profile6_log - INFO -    198                             

2018-05-02 18:05:08,114 - memory_profile6_log - INFO -    199                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-05-02 18:05:08,114 - memory_profile6_log - INFO -    200                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-05-02 18:05:08,114 - memory_profile6_log - INFO -    201    152.1 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-05-02 18:05:08,115 - memory_profile6_log - INFO -    202                             

2018-05-02 18:05:08,118 - memory_profile6_log - INFO -    203                                 # ~ start by provide rank for each topic type ~

2018-05-02 18:05:08,121 - memory_profile6_log - INFO -    204    157.2 MiB      5.2 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-05-02 18:05:08,121 - memory_profile6_log - INFO -    205    155.2 MiB     -2.0 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-05-02 18:05:08,122 - memory_profile6_log - INFO -    206                             

2018-05-02 18:05:08,124 - memory_profile6_log - INFO -    207                                 # ~ set threshold to filter output

2018-05-02 18:05:08,124 - memory_profile6_log - INFO -    208    155.2 MiB      0.0 MiB       if threshold > 0:

2018-05-02 18:05:08,125 - memory_profile6_log - INFO -    209    155.2 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-05-02 18:05:08,125 - memory_profile6_log - INFO -    210    155.2 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-05-02 18:05:08,125 - memory_profile6_log - INFO -    211    155.2 MiB      0.0 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-05-02 18:05:08,127 - memory_profile6_log - INFO -    212                             

2018-05-02 18:05:08,127 - memory_profile6_log - INFO -    213    155.2 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 18:05:08,128 - memory_profile6_log - INFO -    214    155.2 MiB      0.0 MiB       print model_transform[model_transform['user_id']=='1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157']

2018-05-02 18:05:08,130 - memory_profile6_log - INFO -    215    155.2 MiB      0.0 MiB       logger.info("Len of model_transform: %d", len(model_transform))

2018-05-02 18:05:08,131 - memory_profile6_log - INFO -    216    155.2 MiB      0.0 MiB       logger.info("Len of df_dt: %d", len(df_dt))

2018-05-02 18:05:08,131 - memory_profile6_log - INFO -    217    155.2 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-05-02 18:05:08,134 - memory_profile6_log - INFO -    218                             

2018-05-02 18:05:08,134 - memory_profile6_log - INFO -    219    155.2 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-05-02 18:05:08,134 - memory_profile6_log - INFO -    220                             

2018-05-02 18:05:08,134 - memory_profile6_log - INFO -    221    155.2 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-05-02 18:05:08,134 - memory_profile6_log - INFO -    222    155.2 MiB      0.0 MiB       del df_dut

2018-05-02 18:05:08,135 - memory_profile6_log - INFO -    223    155.2 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-05-02 18:05:08,135 - memory_profile6_log - INFO -    224    155.2 MiB      0.0 MiB       del df_dt

2018-05-02 18:05:08,135 - memory_profile6_log - INFO -    225    155.2 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-05-02 18:05:08,137 - memory_profile6_log - INFO -    226    155.2 MiB      0.0 MiB       del df_input

2018-05-02 18:05:08,137 - memory_profile6_log - INFO -    227    155.2 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-05-02 18:05:08,137 - memory_profile6_log - INFO -    228    154.1 MiB     -1.2 MiB       del df_input_X

2018-05-02 18:05:08,138 - memory_profile6_log - INFO -    229    154.1 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-05-02 18:05:08,138 - memory_profile6_log - INFO -    230    154.1 MiB      0.0 MiB       del df_current

2018-05-02 18:05:08,138 - memory_profile6_log - INFO -    231    154.1 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-05-02 18:05:08,142 - memory_profile6_log - INFO -    232    154.1 MiB      0.0 MiB       del map_topic_isgeneral

2018-05-02 18:05:08,142 - memory_profile6_log - INFO -    233    154.1 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-05-02 18:05:08,144 - memory_profile6_log - INFO -    234    146.0 MiB     -8.1 MiB       del model_fit

2018-05-02 18:05:08,147 - memory_profile6_log - INFO -    235    146.0 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-05-02 18:05:08,148 - memory_profile6_log - INFO -    236    146.0 MiB      0.0 MiB       del result

2018-05-02 18:05:08,148 - memory_profile6_log - INFO -    237    146.0 MiB      0.0 MiB       logger.info("deleting result...")

2018-05-02 18:05:08,148 - memory_profile6_log - INFO -    238                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 18:05:08,150 - memory_profile6_log - INFO -    239                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 18:05:08,153 - memory_profile6_log - INFO -    240                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 18:05:08,153 - memory_profile6_log - INFO -    241    146.0 MiB      0.0 MiB       if savetrain:

2018-05-02 18:05:08,154 - memory_profile6_log - INFO -    242    146.4 MiB      0.5 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'p0_posterior', 'rank']].copy(deep=True)

2018-05-02 18:05:08,154 - memory_profile6_log - INFO -    243    146.5 MiB      0.1 MiB           model_transformsv['date'] = current_date  # we need manually adding date, because table not support

2018-05-02 18:05:08,155 - memory_profile6_log - INFO -    244    146.5 MiB      0.0 MiB           model_transformsv['date'] = pd.to_datetime(model_transformsv['date'],

2018-05-02 18:05:08,157 - memory_profile6_log - INFO -    245    146.7 MiB      0.2 MiB                                                      format='%Y-%m-%d', errors='coerce')

2018-05-02 18:05:08,157 - memory_profile6_log - INFO -    246    146.7 MiB      0.0 MiB           print "list(model_transformsv) before rename: ", list(model_transformsv)       

2018-05-02 18:05:08,157 - memory_profile6_log - INFO -    247    146.7 MiB      0.0 MiB           print len(model_transformsv)                

2018-05-02 18:05:08,158 - memory_profile6_log - INFO -    248    146.7 MiB      0.0 MiB           model_transformsv = model_transformsv.rename(columns={'is_general': 'topic_is_general', 'p0_posterior': 'interest_score',

2018-05-02 18:05:08,158 - memory_profile6_log - INFO -    249    147.0 MiB      0.3 MiB                                                                 'rank':'interest_rank' , 'date':'interest_score_created_at'})

2018-05-02 18:05:08,160 - memory_profile6_log - INFO -    250    147.0 MiB      0.0 MiB           print "list(model_transformsv) after rename: ", list(model_transformsv)    

2018-05-02 18:05:08,160 - memory_profile6_log - INFO -    251    147.0 MiB      0.0 MiB           print len(model_transformsv) 

2018-05-02 18:05:08,160 - memory_profile6_log - INFO -    252    147.0 MiB      0.0 MiB           del model_transform

2018-05-02 18:05:08,163 - memory_profile6_log - INFO -    253    147.0 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-05-02 18:05:08,165 - memory_profile6_log - INFO -    254    147.0 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-05-02 18:05:08,167 - memory_profile6_log - INFO -    255                             

2018-05-02 18:05:08,167 - memory_profile6_log - INFO -    256    147.0 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-05-02 18:05:08,167 - memory_profile6_log - INFO -    257                                     # ~ Place your code to save the training model here ~

2018-05-02 18:05:08,168 - memory_profile6_log - INFO -    258    147.0 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-05-02 18:05:08,168 - memory_profile6_log - INFO -    259                                         logger.info("Using google datastore as storage...")

2018-05-02 18:05:08,168 - memory_profile6_log - INFO -    260                                         if multproc:

2018-05-02 18:05:08,170 - memory_profile6_log - INFO -    261                                             # ~ save transform models ~

2018-05-02 18:05:08,170 - memory_profile6_log - INFO -    262                                             logger.info("Saving main Transform model to Google DataStore...")

2018-05-02 18:05:08,171 - memory_profile6_log - INFO -    263                                             logger.info("Saving total data: %d", len(model_transformsv))

2018-05-02 18:05:08,171 - memory_profile6_log - INFO -    264                                             mh.saveDataStorePutMulti(model_transformsv)

2018-05-02 18:05:08,171 - memory_profile6_log - INFO -    265                             

2018-05-02 18:05:08,174 - memory_profile6_log - INFO -    266                                             # ~ save fitted models ~

2018-05-02 18:05:08,174 - memory_profile6_log - INFO -    267                                             logger.info("Saving fitted_models as history to Google DataStore...")

2018-05-02 18:05:08,177 - memory_profile6_log - INFO -    268                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 18:05:08,177 - memory_profile6_log - INFO -    269                                             fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-05-02 18:05:08,177 - memory_profile6_log - INFO -    270                                             logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-05-02 18:05:08,177 - memory_profile6_log - INFO -    271                                             X_split = np.array_split(fitted_models_sigmant, 10)

2018-05-02 18:05:08,177 - memory_profile6_log - INFO -    272                                             logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-05-02 18:05:08,177 - memory_profile6_log - INFO -    273                                             for ix in range(len(X_split)):

2018-05-02 18:05:08,178 - memory_profile6_log - INFO -    274                                                 logger.info("processing batch-%d", ix)

2018-05-02 18:05:08,180 - memory_profile6_log - INFO -    275                                                 mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-05-02 18:05:08,180 - memory_profile6_log - INFO -    276                             

2018-05-02 18:05:08,180 - memory_profile6_log - INFO -    277                                             del X_split

2018-05-02 18:05:08,180 - memory_profile6_log - INFO -    278                                             logger.info("deleting X_split...")

2018-05-02 18:05:08,181 - memory_profile6_log - INFO -    279                                             del save_sigma_nt

2018-05-02 18:05:08,181 - memory_profile6_log - INFO -    280                                             logger.info("deleting save_sigma_nt...")

2018-05-02 18:05:08,181 - memory_profile6_log - INFO -    281                             

2018-05-02 18:05:08,184 - memory_profile6_log - INFO -    282                                             del BR

2018-05-02 18:05:08,188 - memory_profile6_log - INFO -    283                                             logger.info("deleting BR...")

2018-05-02 18:05:08,190 - memory_profile6_log - INFO -    284                             

2018-05-02 18:05:08,190 - memory_profile6_log - INFO -    285    147.0 MiB      0.0 MiB           elif str(saveto).lower() == "elastic":

2018-05-02 18:05:08,190 - memory_profile6_log - INFO -    286    147.0 MiB      0.0 MiB               logger.info("Using ElasticSearch as storage...")

2018-05-02 18:05:08,191 - memory_profile6_log - INFO -    287    147.0 MiB      0.0 MiB               logging.info("Saving main Transform model to Elasticsearch...")

2018-05-02 18:05:08,193 - memory_profile6_log - INFO -    288                                         # print model_transformsv.head(5)

2018-05-02 18:05:08,194 - memory_profile6_log - INFO -    289                                         

2018-05-02 18:05:08,194 - memory_profile6_log - INFO -    290    147.3 MiB      0.3 MiB               X_split = np.array_split(model_transformsv, 5)

2018-05-02 18:05:08,194 - memory_profile6_log - INFO -    291    147.3 MiB      0.0 MiB               logger.info("Saving total data: %d", len(model_transformsv))

2018-05-02 18:05:08,194 - memory_profile6_log - INFO -    292    147.3 MiB      0.0 MiB               logger.info("Len of X_split for batch save model_transformsv: %d", len(X_split))

2018-05-02 18:05:08,194 - memory_profile6_log - INFO -    293    151.2 MiB     -5.1 MiB               for ix in range(len(X_split)):

2018-05-02 18:05:08,200 - memory_profile6_log - INFO -    294    151.2 MiB     -3.7 MiB                   logger.info("processing batch-%d", ix)

2018-05-02 18:05:08,200 - memory_profile6_log - INFO -    295    151.2 MiB     -3.7 MiB                   print X_split[ix].head(5)

2018-05-02 18:05:08,201 - memory_profile6_log - INFO -    296    151.2 MiB     -1.2 MiB                   mh.saveElasticS(X_split[ix])

2018-05-02 18:05:08,201 - memory_profile6_log - INFO -    297    149.8 MiB     -1.5 MiB               del X_split

2018-05-02 18:05:08,203 - memory_profile6_log - INFO -    298                                         

2018-05-02 18:05:08,203 - memory_profile6_log - INFO -    299                             

2018-05-02 18:05:08,203 - memory_profile6_log - INFO -    300    149.8 MiB      0.0 MiB               logger.info("Saving fitted_models as history to Elasticsearch...")

2018-05-02 18:05:08,203 - memory_profile6_log - INFO -    301    149.8 MiB      0.0 MiB               save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 18:05:08,204 - memory_profile6_log - INFO -    302    149.9 MiB      0.1 MiB               fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-05-02 18:05:08,204 - memory_profile6_log - INFO -    303                             

2018-05-02 18:05:08,206 - memory_profile6_log - INFO -    304    149.9 MiB      0.0 MiB               fitted_models_sigmant['uid_topid'] = fitted_models_sigmant["user_id"].map(str) + "_" + fitted_models_sigmant["topic_id"].map(str)

2018-05-02 18:05:08,206 - memory_profile6_log - INFO -    305    149.9 MiB      0.0 MiB               fitted_models_sigmant = fitted_models_sigmant[["uid_topid", "pt_posterior_x_Nt", "smoothed_pt_posterior", "p0_cat_ci", "sigma_Nt"]]

2018-05-02 18:05:08,207 - memory_profile6_log - INFO -    306                               

2018-05-02 18:05:08,207 - memory_profile6_log - INFO -    307    149.9 MiB      0.0 MiB               X_split = np.array_split(fitted_models_sigmant, 5)

2018-05-02 18:05:08,207 - memory_profile6_log - INFO -    308    149.9 MiB      0.0 MiB               logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-05-02 18:05:08,211 - memory_profile6_log - INFO -    309    149.9 MiB      0.0 MiB               logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-05-02 18:05:08,213 - memory_profile6_log - INFO -    310    149.9 MiB     -0.2 MiB               for ix in range(len(X_split)):

2018-05-02 18:05:08,213 - memory_profile6_log - INFO -    311    149.9 MiB     -8.6 MiB                   logger.info("processing batch-%d", ix)

2018-05-02 18:05:08,213 - memory_profile6_log - INFO -    312    147.9 MiB    -10.7 MiB                   mh.saveElasticS(X_split[ix], esindex_name="fitted_hist_index",  estype_name='fitted_hist_type')

2018-05-02 18:05:08,214 - memory_profile6_log - INFO -    313    147.8 MiB     -2.1 MiB               del X_split

2018-05-02 18:05:08,214 - memory_profile6_log - INFO -    314                                         

2018-05-02 18:05:08,216 - memory_profile6_log - INFO -    315    147.8 MiB      0.0 MiB               del BR

2018-05-02 18:05:08,217 - memory_profile6_log - INFO -    316    147.8 MiB      0.0 MiB               logger.info("deleting BR...")

2018-05-02 18:05:08,217 - memory_profile6_log - INFO -    317    147.8 MiB      0.0 MiB               del save_sigma_nt

2018-05-02 18:05:08,217 - memory_profile6_log - INFO -    318    147.8 MiB      0.0 MiB               logger.info("deleting save_sigma_nt...")

2018-05-02 18:05:08,217 - memory_profile6_log - INFO -    319    147.8 MiB      0.0 MiB               del fitted_models_sigmant

2018-05-02 18:05:08,217 - memory_profile6_log - INFO -    320    147.8 MiB      0.0 MiB               logger.info("deleting fitted_models_sigmant...")

2018-05-02 18:05:08,219 - memory_profile6_log - INFO -    321                             

2018-05-02 18:05:08,219 - memory_profile6_log - INFO -    322                                     # need save sigma_nt for daily train

2018-05-02 18:05:08,219 - memory_profile6_log - INFO -    323                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-05-02 18:05:08,220 - memory_profile6_log - INFO -    324                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-05-02 18:05:08,220 - memory_profile6_log - INFO -    325    147.8 MiB      0.0 MiB           if start_date and end_date:

2018-05-02 18:05:08,220 - memory_profile6_log - INFO -    326                                         if not fitby_sigmant:

2018-05-02 18:05:08,220 - memory_profile6_log - INFO -    327                                             logging.info("Saving sigma Nt...")

2018-05-02 18:05:08,224 - memory_profile6_log - INFO -    328                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 18:05:08,226 - memory_profile6_log - INFO -    329                                             save_sigma_nt['start_date'] = start_date

2018-05-02 18:05:08,227 - memory_profile6_log - INFO -    330                                             save_sigma_nt['end_date'] = end_date

2018-05-02 18:05:08,227 - memory_profile6_log - INFO -    331                                             print save_sigma_nt.head(5)

2018-05-02 18:05:08,229 - memory_profile6_log - INFO -    332                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-05-02 18:05:08,229 - memory_profile6_log - INFO -    333                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-05-02 18:05:08,230 - memory_profile6_log - INFO -    334    147.8 MiB      0.0 MiB       return

2018-05-02 18:05:08,230 - memory_profile6_log - INFO - 


2018-05-02 18:05:08,230 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-05-02 18:47:51,341 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 18:47:51,345 - memory_profile6_log - INFO - date_generated: 
2018-05-02 18:47:51,345 - memory_profile6_log - INFO -  
2018-05-02 18:47:51,345 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 18, 47, 51, 342000)]
2018-05-02 18:47:51,345 - memory_profile6_log - INFO - 

2018-05-02 18:47:51,346 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 18:47:51,348 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 18:47:51,348 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 18:47:51,516 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 18:47:51,519 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 18:49:28,905 - memory_profile6_log - INFO - size of df: 82.28 MB
2018-05-02 18:49:28,907 - memory_profile6_log - INFO - getting total: 324874 training data(genuine interest) for date: 2018-05-01
2018-05-02 18:49:28,974 - memory_profile6_log - INFO - loading history data from elastic...
2018-05-02 18:49:28,976 - memory_profile6_log - INFO - Len of X_split for batch load: 10
2018-05-02 18:49:28,977 - memory_profile6_log - INFO - Appending history data...
2018-05-02 18:49:29,062 - memory_profile6_log - INFO - call 32488 history data...
2018-05-02 18:50:20,388 - memory_profile6_log - INFO - call 32488 history data...
2018-05-02 18:51:18,433 - memory_profile6_log - INFO - call 32488 history data...
2018-05-02 18:51:42,874 - memory_profile6_log - INFO - call 32488 history data...
2018-05-02 18:52:34,871 - memory_profile6_log - INFO - call 32487 history data...
2018-05-02 18:53:36,081 - memory_profile6_log - INFO - call 32487 history data...
2018-05-02 18:54:13,984 - memory_profile6_log - INFO - call 32487 history data...
2018-05-02 18:55:07,378 - memory_profile6_log - INFO - call 32487 history data...
2018-05-02 18:55:54,270 - memory_profile6_log - INFO - call 32487 history data...
2018-05-02 18:56:48,729 - memory_profile6_log - INFO - call 32487 history data...
2018-05-02 18:57:40,598 - memory_profile6_log - INFO - Appending training data...
2018-05-02 18:57:40,599 - memory_profile6_log - INFO - len datalist: 1
2018-05-02 18:57:40,601 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-05-02 18:57:40,617 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 18:57:40,618 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 18:57:40,621 - memory_profile6_log - INFO - ================================================

2018-05-02 18:57:40,621 - memory_profile6_log - INFO -    359     86.3 MiB     86.3 MiB   @profile

2018-05-02 18:57:40,622 - memory_profile6_log - INFO -    360                             def BQPreprocess(cpu, date_generated, client, query_fit, loadfrom="elastic"):

2018-05-02 18:57:40,625 - memory_profile6_log - INFO -    361     86.3 MiB      0.0 MiB       bq_client = client

2018-05-02 18:57:40,625 - memory_profile6_log - INFO -    362     86.3 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 18:57:40,627 - memory_profile6_log - INFO -    363                             

2018-05-02 18:57:40,628 - memory_profile6_log - INFO -    364     86.3 MiB      0.0 MiB       datalist = []

2018-05-02 18:57:40,631 - memory_profile6_log - INFO -    365     86.3 MiB      0.0 MiB       datalist_hist = []

2018-05-02 18:57:40,632 - memory_profile6_log - INFO -    366                             

2018-05-02 18:57:40,634 - memory_profile6_log - INFO -    367     86.3 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-05-02 18:57:40,634 - memory_profile6_log - INFO -    368    385.2 MiB      0.0 MiB       for ndate in date_generated:

2018-05-02 18:57:40,635 - memory_profile6_log - INFO -    369    367.0 MiB    280.6 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-05-02 18:57:40,638 - memory_profile6_log - INFO -    370    367.0 MiB      0.0 MiB           if tframe is not None:

2018-05-02 18:57:40,638 - memory_profile6_log - INFO -    371    367.0 MiB      0.0 MiB               if not tframe.empty:

2018-05-02 18:57:40,640 - memory_profile6_log - INFO -    372    367.0 MiB      0.0 MiB                   if loadfrom.strip().lower() == 'datastore':

2018-05-02 18:57:40,641 - memory_profile6_log - INFO -    373                                                 X_split = np.array_split(tframe, 5)

2018-05-02 18:57:40,644 - memory_profile6_log - INFO -    374                                                 logger.info("loading history data from datastore...")

2018-05-02 18:57:40,645 - memory_profile6_log - INFO -    375                                                 logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 18:57:40,647 - memory_profile6_log - INFO -    376                                                 logger.info("Appending history data...")

2018-05-02 18:57:40,648 - memory_profile6_log - INFO -    377                                                 for ix in range(len(X_split)):

2018-05-02 18:57:40,650 - memory_profile6_log - INFO -    378                                                     # ~ loading history

2018-05-02 18:57:40,651 - memory_profile6_log - INFO -    379                                                     """

2018-05-02 18:57:40,654 - memory_profile6_log - INFO -    380                                                         disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-05-02 18:57:40,654 - memory_profile6_log - INFO -    381                                                     """

2018-05-02 18:57:40,655 - memory_profile6_log - INFO -    382                                                     logger.info("processing batch-%d", ix)

2018-05-02 18:57:40,657 - memory_profile6_log - INFO -    383                                                     # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-05-02 18:57:40,657 - memory_profile6_log - INFO -    384                                                     logger.info("creating list history data...")

2018-05-02 18:57:40,658 - memory_profile6_log - INFO -    385                                                     #lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-05-02 18:57:40,660 - memory_profile6_log - INFO -    386                                                     lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 18:57:40,661 - memory_profile6_log - INFO -    387                             

2018-05-02 18:57:40,664 - memory_profile6_log - INFO -    388                                                     logger.info("call history data...")

2018-05-02 18:57:40,664 - memory_profile6_log - INFO -    389                                                     h_frame = mh.loadDSHistory(lhistory)

2018-05-02 18:57:40,665 - memory_profile6_log - INFO -    390                             

2018-05-02 18:57:40,667 - memory_profile6_log - INFO -    391                                                     # me = os.getpid()

2018-05-02 18:57:40,668 - memory_profile6_log - INFO -    392                                                     # kill_proc_tree(me)

2018-05-02 18:57:40,671 - memory_profile6_log - INFO -    393                             

2018-05-02 18:57:40,671 - memory_profile6_log - INFO -    394                                                     logger.info("done collecting history data, appending now...")

2018-05-02 18:57:40,673 - memory_profile6_log - INFO -    395                                                     for m in h_frame:

2018-05-02 18:57:40,676 - memory_profile6_log - INFO -    396                                                         if m is not None:

2018-05-02 18:57:40,678 - memory_profile6_log - INFO -    397                                                             if len(m) > 0:

2018-05-02 18:57:40,680 - memory_profile6_log - INFO -    398                                                                 datalist_hist.append(pd.DataFrame(m))

2018-05-02 18:57:40,681 - memory_profile6_log - INFO -    399                                                     del h_frame

2018-05-02 18:57:40,684 - memory_profile6_log - INFO -    400                                                     del lhistory

2018-05-02 18:57:40,687 - memory_profile6_log - INFO -    401                             

2018-05-02 18:57:40,687 - memory_profile6_log - INFO -    402                                                 logger.info("Appending training data...")

2018-05-02 18:57:40,688 - memory_profile6_log - INFO -    403                                                 datalist.append(tframe)

2018-05-02 18:57:40,690 - memory_profile6_log - INFO -    404    367.0 MiB      0.0 MiB                   elif loadfrom.strip().lower() == 'elastic':

2018-05-02 18:57:40,690 - memory_profile6_log - INFO -    405    376.4 MiB      9.4 MiB                       X_split = np.array_split(tframe, 10)

2018-05-02 18:57:40,691 - memory_profile6_log - INFO -    406    376.4 MiB      0.0 MiB                       logger.info("loading history data from elastic...")

2018-05-02 18:57:40,693 - memory_profile6_log - INFO -    407    376.4 MiB      0.0 MiB                       logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 18:57:40,694 - memory_profile6_log - INFO -    408    376.4 MiB      0.0 MiB                       logger.info("Appending history data...")

2018-05-02 18:57:40,697 - memory_profile6_log - INFO -    409                                                 

2018-05-02 18:57:40,697 - memory_profile6_log - INFO -    410    385.2 MiB      0.0 MiB                       for ix in range(len(X_split)):

2018-05-02 18:57:40,698 - memory_profile6_log - INFO -    411    384.9 MiB      1.1 MiB                           lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 18:57:40,700 - memory_profile6_log - INFO -    412    384.9 MiB      0.0 MiB                           logger.info("call %d history data...", len(lhistory))

2018-05-02 18:57:40,700 - memory_profile6_log - INFO -    413    384.9 MiB      0.0 MiB                           inside_data = mh.loadESHistory(lhistory,

2018-05-02 18:57:40,701 - memory_profile6_log - INFO -    414    384.9 MiB      0.0 MiB                                                          esindex_name='fitted_hist_index',

2018-05-02 18:57:40,703 - memory_profile6_log - INFO -    415    385.2 MiB      7.4 MiB                                                          estype_name='fitted_hist_type')

2018-05-02 18:57:40,703 - memory_profile6_log - INFO -    416                                                     # split back the user_id and topic_id

2018-05-02 18:57:40,703 - memory_profile6_log - INFO -    417    385.2 MiB      0.1 MiB                           inside_data[['user_id','topic_id']] = inside_data.uid_topid.str.split('_', expand=True)

2018-05-02 18:57:40,704 - memory_profile6_log - INFO -    418    385.2 MiB      0.2 MiB                           inside_data = inside_data[["user_id","topic_id", "pt_posterior_x_Nt", "smoothed_pt_posterior", "p0_cat_ci", "sigma_Nt"]]

2018-05-02 18:57:40,706 - memory_profile6_log - INFO -    419    385.2 MiB      0.0 MiB                           if not inside_data.empty:

2018-05-02 18:57:40,709 - memory_profile6_log - INFO -    420    385.2 MiB      0.0 MiB                               datalist_hist.append(inside_data)

2018-05-02 18:57:40,710 - memory_profile6_log - INFO -    421    385.2 MiB      0.0 MiB                               del inside_data

2018-05-02 18:57:40,710 - memory_profile6_log - INFO -    422                                                     else:

2018-05-02 18:57:40,711 - memory_profile6_log - INFO -    423                                                         logger.info("uid: %s with topic_id: %s is empty!" % (framedata.user_id, framedata.topic_id))

2018-05-02 18:57:40,711 - memory_profile6_log - INFO -    424                                                 

2018-05-02 18:57:40,713 - memory_profile6_log - INFO -    425    385.2 MiB      0.0 MiB                       logger.info("Appending training data...")

2018-05-02 18:57:40,713 - memory_profile6_log - INFO -    426    385.2 MiB      0.0 MiB                       datalist.append(tframe)

2018-05-02 18:57:40,714 - memory_profile6_log - INFO -    427                                             else:

2018-05-02 18:57:40,716 - memory_profile6_log - INFO -    428                                                 logger.info("Unknows source is selected !")

2018-05-02 18:57:40,717 - memory_profile6_log - INFO -    429                                                 break

2018-05-02 18:57:40,720 - memory_profile6_log - INFO -    430                                     else: 

2018-05-02 18:57:40,720 - memory_profile6_log - INFO -    431                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-05-02 18:57:40,721 - memory_profile6_log - INFO -    432    385.2 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-05-02 18:57:40,723 - memory_profile6_log - INFO -    433    385.2 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-05-02 18:57:40,723 - memory_profile6_log - INFO -    434                             

2018-05-02 18:57:40,723 - memory_profile6_log - INFO -    435    385.2 MiB      0.0 MiB       return datalist, datalist_hist

2018-05-02 18:57:40,724 - memory_profile6_log - INFO - 


2018-05-02 18:57:40,740 - memory_profile6_log - INFO - size of big_frame_hist: 28.29 KB
2018-05-02 18:57:40,835 - memory_profile6_log - INFO - size of big_frame: 82.28 MB
2018-05-02 18:57:40,855 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-05-02 19:04:01,651 - memory_profile6_log - INFO - size of df: 424.92 MB
2018-05-02 19:04:01,653 - memory_profile6_log - INFO - getting total: 1740031 training data(current date interest)
2018-05-02 19:04:02,084 - memory_profile6_log - INFO - size of current_frame: 438.20 MB
2018-05-02 19:04:02,085 - memory_profile6_log - INFO - loading time of: 2064905 total genuine-current interest data ~ take 970.607s
2018-05-02 19:04:02,086 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 19:04:02,086 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 19:04:02,088 - memory_profile6_log - INFO - ================================================

2018-05-02 19:04:02,088 - memory_profile6_log - INFO -    437     86.2 MiB     86.2 MiB   @profile

2018-05-02 19:04:02,088 - memory_profile6_log - INFO -    438                             def preprocess(cpu, cd, query_fit, date_generated):

2018-05-02 19:04:02,089 - memory_profile6_log - INFO -    439     86.3 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-05-02 19:04:02,091 - memory_profile6_log - INFO -    440     86.3 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 19:04:02,092 - memory_profile6_log - INFO -    441                             

2018-05-02 19:04:02,096 - memory_profile6_log - INFO -    442                                 # ~~~ Begin collecting data ~~~

2018-05-02 19:04:02,098 - memory_profile6_log - INFO -    443     86.3 MiB      0.0 MiB       t0 = time.time()

2018-05-02 19:04:02,098 - memory_profile6_log - INFO -    444    385.2 MiB    298.9 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-05-02 19:04:02,098 - memory_profile6_log - INFO -    445    385.2 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-05-02 19:04:02,099 - memory_profile6_log - INFO -    446                                     logger.info("Training cannot be empty..")

2018-05-02 19:04:02,101 - memory_profile6_log - INFO -    447                                     return False

2018-05-02 19:04:02,102 - memory_profile6_log - INFO -    448    385.2 MiB      0.0 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-05-02 19:04:02,104 - memory_profile6_log - INFO -    449                                 # print "big_frame_hist:\n", big_frame_hist.head(20)

2018-05-02 19:04:02,105 - memory_profile6_log - INFO -    450    385.2 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-05-02 19:04:02,108 - memory_profile6_log - INFO -    451                             

2018-05-02 19:04:02,108 - memory_profile6_log - INFO -    452    392.7 MiB      7.4 MiB       big_frame = pd.concat(datalist)

2018-05-02 19:04:02,111 - memory_profile6_log - INFO -    453    392.7 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-05-02 19:04:02,111 - memory_profile6_log - INFO -    454    385.2 MiB     -7.4 MiB       del datalist

2018-05-02 19:04:02,111 - memory_profile6_log - INFO -    455                             

2018-05-02 19:04:02,112 - memory_profile6_log - INFO -    456    385.2 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 19:04:02,112 - memory_profile6_log - INFO -    457                             

2018-05-02 19:04:02,114 - memory_profile6_log - INFO -    458                                 # ~ get current news interest ~

2018-05-02 19:04:02,114 - memory_profile6_log - INFO -    459    385.2 MiB      0.0 MiB       if not cd:

2018-05-02 19:04:02,115 - memory_profile6_log - INFO -    460    385.2 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-05-02 19:04:02,115 - memory_profile6_log - INFO -    461    791.6 MiB    406.4 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-05-02 19:04:02,115 - memory_profile6_log - INFO -    462    791.6 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-05-02 19:04:02,121 - memory_profile6_log - INFO -    463                                 else:

2018-05-02 19:04:02,122 - memory_profile6_log - INFO -    464                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-05-02 19:04:02,122 - memory_profile6_log - INFO -    465                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-05-02 19:04:02,124 - memory_profile6_log - INFO -    466                             

2018-05-02 19:04:02,125 - memory_profile6_log - INFO -    467                                     # safe handling of query parameter

2018-05-02 19:04:02,127 - memory_profile6_log - INFO -    468                                     query_params = [

2018-05-02 19:04:02,128 - memory_profile6_log - INFO -    469                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-05-02 19:04:02,128 - memory_profile6_log - INFO -    470                                     ]

2018-05-02 19:04:02,128 - memory_profile6_log - INFO -    471                             

2018-05-02 19:04:02,132 - memory_profile6_log - INFO -    472                                     job_config.query_parameters = query_params

2018-05-02 19:04:02,134 - memory_profile6_log - INFO -    473                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-05-02 19:04:02,135 - memory_profile6_log - INFO -    474                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-05-02 19:04:02,137 - memory_profile6_log - INFO -    475                             

2018-05-02 19:04:02,137 - memory_profile6_log - INFO -    476    805.0 MiB     13.3 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-05-02 19:04:02,138 - memory_profile6_log - INFO -    477    805.0 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-05-02 19:04:02,138 - memory_profile6_log - INFO -    478    805.0 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-05-02 19:04:02,138 - memory_profile6_log - INFO -    479    805.0 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-05-02 19:04:02,138 - memory_profile6_log - INFO -    480    805.0 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 19:04:02,138 - memory_profile6_log - INFO -    481    805.0 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-05-02 19:04:02,140 - memory_profile6_log - INFO -    482                             

2018-05-02 19:04:02,141 - memory_profile6_log - INFO -    483    805.0 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-05-02 19:04:02,145 - memory_profile6_log - INFO - 


2018-05-02 19:04:02,153 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-05-02 19:04:02,289 - memory_profile6_log - INFO - train on: 324874 total genuine interest data(D(u, t))
2018-05-02 19:04:02,289 - memory_profile6_log - INFO - transform on: 1740031 total current data(D(t))
2018-05-02 19:04:02,292 - memory_profile6_log - INFO - apply on: 100 total history...)
2018-05-02 19:04:02,651 - memory_profile6_log - INFO - len of uniques_fit_hist:100
2018-05-02 19:04:02,657 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:12
2018-05-02 19:04:03,283 - memory_profile6_log - INFO - self.sum_all_nt before concate:

2018-05-02 19:04:03,332 - memory_profile6_log - INFO -                                                  user_id  sigma_Nt
15052  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        27
2018-05-02 19:04:03,334 - memory_profile6_log - INFO - 

2018-05-02 19:04:03,398 - memory_profile6_log - INFO - self.sum_all_nt after concate:

2018-05-02 19:04:03,423 - memory_profile6_log - INFO -                                                  user_id  sigma_Nt
15052  1616f009d96b1-0285d8288a5bce-70217860-38400-16...        27
2018-05-02 19:04:03,424 - memory_profile6_log - INFO - 

2018-05-02 19:04:03,756 - memory_profile6_log - INFO - Len of model_fit: 324874
2018-05-02 19:04:03,759 - memory_profile6_log - INFO - Len of df_dut: 324874
2018-05-02 19:04:07,984 - memory_profile6_log - INFO - Len of fitted_models on main class: 324874
2018-05-02 19:04:07,986 - memory_profile6_log - INFO - 

2018-05-02 19:04:07,986 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-05-02 19:04:07,987 - memory_profile6_log - INFO - 

2018-05-02 19:04:07,987 - memory_profile6_log - INFO - fitted_model_hist:

2018-05-02 19:04:07,989 - memory_profile6_log - INFO - 

2018-05-02 19:04:08,003 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 19:04:08,003 - memory_profile6_log - INFO - 

2018-05-02 19:04:08,056 - memory_profile6_log - INFO - len of fitted models after concat: 324974
2018-05-02 19:04:08,059 - memory_profile6_log - INFO - 

2018-05-02 19:04:08,059 - memory_profile6_log - INFO - Recalculating fitted models...
2018-05-02 19:04:08,061 - memory_profile6_log - INFO - 

2018-05-02 19:04:08,279 - memory_profile6_log - INFO - fitted_models after cobine:

2018-05-02 19:04:08,279 - memory_profile6_log - INFO - 

2018-05-02 19:04:08,289 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 19:04:08,292 - memory_profile6_log - INFO - 

2018-05-02 19:04:08,292 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 324874
2018-05-02 19:04:08,296 - memory_profile6_log - INFO - 

2018-05-02 19:04:54,006 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-05-02 19:05:01,651 - memory_profile6_log - INFO -                                                   user_id           topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci  sigma_Nt  p0_posterior  is_general  rank
168616  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22601470          96.864073             106.864073   0.075156        27      0.217066        True   1.0
168618  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27431099         103.095622             113.095622   0.068024        27      0.207926        True   2.0
168615  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           22553543          22.395451              32.395451   0.106228        27      0.093009        True   3.0
168623  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           33020303          64.754054              74.754054   0.012933        27      0.026130        True   4.0
168617  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           27313197         543.187160             553.187160   0.000637        27      0.009522        True   5.0
168624  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           39301645         256.652442             266.652442   0.026996        27      0.194554       False   1.0
168619  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790312980        5254.621053            5264.621053   0.000289        27      0.041117       False   2.0
168611  1616f009d96b1-0285d8288a5bce-70217860-38400-16...          107151123          91.409815             101.409815   0.009773        27      0.026787       False   3.0
168612  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1149084290        3395.843537            3405.843537   0.000222        27      0.020439       False   4.0
168622  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790314285         102.376743             112.376743   0.005171        27      0.015704       False   5.0
168625  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           39825972          33.653947              43.653947   0.011061        27      0.013050       False   6.0
168620  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790313523        2655.260638            2665.260638   0.000121        27      0.008719       False   7.0
168609  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1037574949          20.851671              30.851671   0.007401        27      0.006171       False   8.0
168608  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1023751122         771.544049             781.544049   0.000291        27      0.006155       False   9.0
168627  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           62323193         776.343701             786.343701   0.000287        27      0.006090       False  10.0
168614  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1161375937          56.687372              66.687372   0.002697        27      0.004860       False  11.0
168613  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1157057159          66.108992              76.108992   0.001596        27      0.003284       False  12.0
168610  1616f009d96b1-0285d8288a5bce-70217860-38400-16...         1038512181          71.312714              81.312714   0.000970        27      0.002132       False  13.0
168626  1616f009d96b1-0285d8288a5bce-70217860-38400-16...           41639090        1000.378758            1010.378758   0.000061        27      0.001675       False  14.0
168621  1616f009d96b1-0285d8288a5bce-70217860-38400-16...  27431110790314267         597.116029             607.116029   0.000024        27      0.000395       False  15.0
2018-05-02 19:05:01,653 - memory_profile6_log - INFO - 

2018-05-02 19:05:01,654 - memory_profile6_log - INFO - Len of model_transform: 304911
2018-05-02 19:05:01,654 - memory_profile6_log - INFO - Len of df_dt: 1740031
2018-05-02 19:05:01,657 - memory_profile6_log - INFO - Total train time: 59.148s
2018-05-02 19:05:01,657 - memory_profile6_log - INFO - memory left before cleaning: 92.400 percent memory...
2018-05-02 19:05:01,658 - memory_profile6_log - INFO - cleaning up some objects...
2018-05-02 19:05:01,661 - memory_profile6_log - INFO - deleting df_dut...
2018-05-02 19:05:01,664 - memory_profile6_log - INFO - deleting df_dt...
2018-05-02 19:05:01,665 - memory_profile6_log - INFO - deleting df_input...
2018-05-02 19:05:01,743 - memory_profile6_log - INFO - deleting df_input_X...
2018-05-02 19:05:01,746 - memory_profile6_log - INFO - deleting df_current...
2018-05-02 19:05:01,746 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-05-02 19:05:01,776 - memory_profile6_log - INFO - deleting model_fit...
2018-05-02 19:05:01,778 - memory_profile6_log - INFO - deleting result...
2018-05-02 19:05:01,819 - memory_profile6_log - INFO - list(model_transformsv) before rename: 
2018-05-02 19:05:01,821 - memory_profile6_log - INFO -  
2018-05-02 19:05:01,822 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'is_general', 'p0_posterior', 'rank', 'date']
2018-05-02 19:05:01,823 - memory_profile6_log - INFO - 

2018-05-02 19:05:01,825 - memory_profile6_log - INFO - 304911
2018-05-02 19:05:01,825 - memory_profile6_log - INFO - 

2018-05-02 19:05:01,855 - memory_profile6_log - INFO - list(model_transformsv) after rename: 
2018-05-02 19:05:01,858 - memory_profile6_log - INFO -  
2018-05-02 19:05:01,858 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'topic_is_general', 'interest_score', 'interest_rank', 'interest_score_created_at']
2018-05-02 19:05:01,861 - memory_profile6_log - INFO - 

2018-05-02 19:05:01,861 - memory_profile6_log - INFO - 304911
2018-05-02 19:05:01,862 - memory_profile6_log - INFO - 

2018-05-02 19:05:01,865 - memory_profile6_log - INFO - deleting model_transform...
2018-05-02 19:05:01,868 - memory_profile6_log - INFO - memory left after cleaning: 92.100 percent memory...
2018-05-02 19:05:01,871 - memory_profile6_log - INFO - Begin saving trained data...
2018-05-02 19:05:01,872 - memory_profile6_log - INFO - Using ElasticSearch as storage...
2018-05-02 19:05:01,904 - memory_profile6_log - INFO - Saving total data: 304911
2018-05-02 19:05:01,905 - memory_profile6_log - INFO - Len of X_split for batch save model_transformsv: 5
2018-05-02 19:05:01,907 - memory_profile6_log - INFO - processing batch-0
2018-05-02 19:07:34,438 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 19:07:34,443 - memory_profile6_log - INFO - date_generated: 
2018-05-02 19:07:34,444 - memory_profile6_log - INFO -  
2018-05-02 19:07:34,444 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 19, 7, 34, 440000)]
2018-05-02 19:07:34,444 - memory_profile6_log - INFO - 

2018-05-02 19:07:34,444 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 19:07:34,446 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 19:07:34,446 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 19:07:34,674 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 19:07:34,683 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 19:07:47,220 - memory_profile6_log - INFO - size of df: 5.05 MB
2018-05-02 19:07:47,223 - memory_profile6_log - INFO - getting total: 20000 training data(genuine interest) for date: 2018-05-01
2018-05-02 19:07:47,303 - memory_profile6_log - INFO - loading history data from elastic...
2018-05-02 19:07:47,305 - memory_profile6_log - INFO - Len of X_split for batch load: 25
2018-05-02 19:07:47,306 - memory_profile6_log - INFO - Appending history data...
2018-05-02 19:07:47,364 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:07:50,536 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:07:54,118 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:07:57,390 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:08:00,234 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:08:02,316 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:08:06,107 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:08:09,375 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:08:12,157 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:08:14,963 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:08:18,081 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:08:21,161 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:08:24,411 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:08:26,891 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:08:29,355 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:08:32,812 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:08:35,365 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:08:37,648 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:08:39,799 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:08:41,812 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:08:44,697 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:08:47,457 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:08:49,933 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:08:52,095 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:08:54,845 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:08:57,595 - memory_profile6_log - INFO - Appending training data...
2018-05-02 19:08:57,598 - memory_profile6_log - INFO - len datalist: 1
2018-05-02 19:08:57,598 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-05-02 19:08:57,601 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 19:08:57,602 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 19:08:57,604 - memory_profile6_log - INFO - ================================================

2018-05-02 19:08:57,605 - memory_profile6_log - INFO -    359     86.8 MiB     86.8 MiB   @profile

2018-05-02 19:08:57,608 - memory_profile6_log - INFO -    360                             def BQPreprocess(cpu, date_generated, client, query_fit, loadfrom="elastic"):

2018-05-02 19:08:57,609 - memory_profile6_log - INFO -    361     86.8 MiB      0.0 MiB       bq_client = client

2018-05-02 19:08:57,611 - memory_profile6_log - INFO -    362     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 19:08:57,611 - memory_profile6_log - INFO -    363                             

2018-05-02 19:08:57,612 - memory_profile6_log - INFO -    364     86.8 MiB      0.0 MiB       datalist = []

2018-05-02 19:08:57,614 - memory_profile6_log - INFO -    365     86.8 MiB      0.0 MiB       datalist_hist = []

2018-05-02 19:08:57,615 - memory_profile6_log - INFO -    366                             

2018-05-02 19:08:57,615 - memory_profile6_log - INFO -    367     86.8 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-05-02 19:08:57,617 - memory_profile6_log - INFO -    368    144.4 MiB      0.0 MiB       for ndate in date_generated:

2018-05-02 19:08:57,618 - memory_profile6_log - INFO -    369    139.6 MiB     52.8 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-05-02 19:08:57,622 - memory_profile6_log - INFO -    370    139.6 MiB      0.0 MiB           if tframe is not None:

2018-05-02 19:08:57,624 - memory_profile6_log - INFO -    371    139.6 MiB      0.0 MiB               if not tframe.empty:

2018-05-02 19:08:57,625 - memory_profile6_log - INFO -    372    139.6 MiB      0.0 MiB                   if loadfrom.strip().lower() == 'datastore':

2018-05-02 19:08:57,625 - memory_profile6_log - INFO -    373                                                 X_split = np.array_split(tframe, 5)

2018-05-02 19:08:57,625 - memory_profile6_log - INFO -    374                                                 logger.info("loading history data from datastore...")

2018-05-02 19:08:57,627 - memory_profile6_log - INFO -    375                                                 logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 19:08:57,630 - memory_profile6_log - INFO -    376                                                 logger.info("Appending history data...")

2018-05-02 19:08:57,630 - memory_profile6_log - INFO -    377                                                 for ix in range(len(X_split)):

2018-05-02 19:08:57,631 - memory_profile6_log - INFO -    378                                                     # ~ loading history

2018-05-02 19:08:57,632 - memory_profile6_log - INFO -    379                                                     """

2018-05-02 19:08:57,634 - memory_profile6_log - INFO -    380                                                         disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-05-02 19:08:57,634 - memory_profile6_log - INFO -    381                                                     """

2018-05-02 19:08:57,635 - memory_profile6_log - INFO -    382                                                     logger.info("processing batch-%d", ix)

2018-05-02 19:08:57,635 - memory_profile6_log - INFO -    383                                                     # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-05-02 19:08:57,637 - memory_profile6_log - INFO -    384                                                     logger.info("creating list history data...")

2018-05-02 19:08:57,641 - memory_profile6_log - INFO -    385                                                     #lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-05-02 19:08:57,641 - memory_profile6_log - INFO -    386                                                     lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 19:08:57,642 - memory_profile6_log - INFO -    387                             

2018-05-02 19:08:57,644 - memory_profile6_log - INFO -    388                                                     logger.info("call history data...")

2018-05-02 19:08:57,644 - memory_profile6_log - INFO -    389                                                     h_frame = mh.loadDSHistory(lhistory)

2018-05-02 19:08:57,645 - memory_profile6_log - INFO -    390                             

2018-05-02 19:08:57,647 - memory_profile6_log - INFO -    391                                                     # me = os.getpid()

2018-05-02 19:08:57,648 - memory_profile6_log - INFO -    392                                                     # kill_proc_tree(me)

2018-05-02 19:08:57,651 - memory_profile6_log - INFO -    393                             

2018-05-02 19:08:57,651 - memory_profile6_log - INFO -    394                                                     logger.info("done collecting history data, appending now...")

2018-05-02 19:08:57,653 - memory_profile6_log - INFO -    395                                                     for m in h_frame:

2018-05-02 19:08:57,654 - memory_profile6_log - INFO -    396                                                         if m is not None:

2018-05-02 19:08:57,655 - memory_profile6_log - INFO -    397                                                             if len(m) > 0:

2018-05-02 19:08:57,657 - memory_profile6_log - INFO -    398                                                                 datalist_hist.append(pd.DataFrame(m))

2018-05-02 19:08:57,657 - memory_profile6_log - INFO -    399                                                     del h_frame

2018-05-02 19:08:57,657 - memory_profile6_log - INFO -    400                                                     del lhistory

2018-05-02 19:08:57,661 - memory_profile6_log - INFO -    401                             

2018-05-02 19:08:57,664 - memory_profile6_log - INFO -    402                                                 logger.info("Appending training data...")

2018-05-02 19:08:57,665 - memory_profile6_log - INFO -    403                                                 datalist.append(tframe)

2018-05-02 19:08:57,667 - memory_profile6_log - INFO -    404    139.6 MiB      0.0 MiB                   elif loadfrom.strip().lower() == 'elastic':

2018-05-02 19:08:57,667 - memory_profile6_log - INFO -    405    139.6 MiB      0.1 MiB                       X_split = np.array_split(tframe, 25)

2018-05-02 19:08:57,668 - memory_profile6_log - INFO -    406    139.6 MiB      0.0 MiB                       logger.info("loading history data from elastic...")

2018-05-02 19:08:57,671 - memory_profile6_log - INFO -    407    139.6 MiB      0.0 MiB                       logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 19:08:57,673 - memory_profile6_log - INFO -    408    139.6 MiB      0.0 MiB                       logger.info("Appending history data...")

2018-05-02 19:08:57,674 - memory_profile6_log - INFO -    409                                                 

2018-05-02 19:08:57,676 - memory_profile6_log - INFO -    410    144.4 MiB     -0.1 MiB                       for ix in range(len(X_split)):

2018-05-02 19:08:57,677 - memory_profile6_log - INFO -    411    144.4 MiB      0.2 MiB                           lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 19:08:57,677 - memory_profile6_log - INFO -    412    144.4 MiB     -0.1 MiB                           logger.info("call %d history data...", len(lhistory))

2018-05-02 19:08:57,680 - memory_profile6_log - INFO -    413    144.4 MiB     -0.1 MiB                           inside_data = mh.loadESHistory(lhistory,

2018-05-02 19:08:57,684 - memory_profile6_log - INFO -    414    144.4 MiB     -0.1 MiB                                                          esindex_name='fitted_hist_index',

2018-05-02 19:08:57,686 - memory_profile6_log - INFO -    415    144.4 MiB      4.1 MiB                                                          estype_name='fitted_hist_type')

2018-05-02 19:08:57,687 - memory_profile6_log - INFO -    416                                                     # split back the user_id and topic_id

2018-05-02 19:08:57,687 - memory_profile6_log - INFO -    417    144.4 MiB      0.0 MiB                           inside_data[['user_id','topic_id']] = inside_data.uid_topid.str.split('_', expand=True)

2018-05-02 19:08:57,688 - memory_profile6_log - INFO -    418    144.4 MiB      0.1 MiB                           inside_data = inside_data[["user_id","topic_id", "pt_posterior_x_Nt", "smoothed_pt_posterior", "p0_cat_ci", "sigma_Nt"]]

2018-05-02 19:08:57,690 - memory_profile6_log - INFO -    419    144.4 MiB     -0.1 MiB                           if not inside_data.empty:

2018-05-02 19:08:57,691 - memory_profile6_log - INFO -    420    144.4 MiB     -0.1 MiB                               datalist_hist.append(inside_data)

2018-05-02 19:08:57,694 - memory_profile6_log - INFO -    421    144.4 MiB     -0.1 MiB                               del inside_data

2018-05-02 19:08:57,694 - memory_profile6_log - INFO -    422                                                 

2018-05-02 19:08:57,698 - memory_profile6_log - INFO -    423    144.4 MiB      0.0 MiB                       logger.info("Appending training data...")

2018-05-02 19:08:57,700 - memory_profile6_log - INFO -    424    144.4 MiB      0.0 MiB                       datalist.append(tframe)

2018-05-02 19:08:57,700 - memory_profile6_log - INFO -    425                                             else:

2018-05-02 19:08:57,703 - memory_profile6_log - INFO -    426                                                 logger.info("Unknows source is selected !")

2018-05-02 19:08:57,704 - memory_profile6_log - INFO -    427                                                 break

2018-05-02 19:08:57,707 - memory_profile6_log - INFO -    428                                     else: 

2018-05-02 19:08:57,707 - memory_profile6_log - INFO -    429                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-05-02 19:08:57,709 - memory_profile6_log - INFO -    430    144.4 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-05-02 19:08:57,710 - memory_profile6_log - INFO -    431    144.4 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-05-02 19:08:57,710 - memory_profile6_log - INFO -    432                             

2018-05-02 19:08:57,713 - memory_profile6_log - INFO -    433    144.4 MiB      0.0 MiB       return datalist, datalist_hist

2018-05-02 19:08:57,714 - memory_profile6_log - INFO - 


2018-05-02 19:08:57,734 - memory_profile6_log - INFO - len of big_frame_hist: 250
2018-05-02 19:08:57,742 - memory_profile6_log - INFO - size of big_frame_hist: 70.30 KB
2018-05-02 19:08:57,756 - memory_profile6_log - INFO - size of big_frame: 5.05 MB
2018-05-02 19:08:57,759 - memory_profile6_log - INFO - len of big_frame: 20000
2018-05-02 19:08:57,763 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-05-02 19:09:06,951 - memory_profile6_log - INFO - size of df: 4.93 MB
2018-05-02 19:09:06,953 - memory_profile6_log - INFO - getting total: 20000 training data(current date interest)
2018-05-02 19:09:06,974 - memory_profile6_log - INFO - size of current_frame: 5.09 MB
2018-05-02 19:09:06,976 - memory_profile6_log - INFO - loading time of: 40000 total genuine-current interest data ~ take 92.392s
2018-05-02 19:09:06,977 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 19:09:06,977 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 19:09:06,979 - memory_profile6_log - INFO - ================================================

2018-05-02 19:09:06,980 - memory_profile6_log - INFO -    435     86.7 MiB     86.7 MiB   @profile

2018-05-02 19:09:06,980 - memory_profile6_log - INFO -    436                             def preprocess(cpu, cd, query_fit, date_generated):

2018-05-02 19:09:06,980 - memory_profile6_log - INFO -    437     86.8 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-05-02 19:09:06,982 - memory_profile6_log - INFO -    438     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 19:09:06,982 - memory_profile6_log - INFO -    439                             

2018-05-02 19:09:06,983 - memory_profile6_log - INFO -    440                                 # ~~~ Begin collecting data ~~~

2018-05-02 19:09:06,983 - memory_profile6_log - INFO -    441     86.8 MiB      0.0 MiB       t0 = time.time()

2018-05-02 19:09:06,983 - memory_profile6_log - INFO -    442    144.4 MiB     57.6 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-05-02 19:09:06,983 - memory_profile6_log - INFO -    443    144.4 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-05-02 19:09:06,983 - memory_profile6_log - INFO -    444                                     logger.info("Training cannot be empty..")

2018-05-02 19:09:06,983 - memory_profile6_log - INFO -    445                                     return False

2018-05-02 19:09:06,987 - memory_profile6_log - INFO -    446    144.4 MiB      0.0 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-05-02 19:09:06,989 - memory_profile6_log - INFO -    447    144.4 MiB      0.0 MiB       logger.info("len of big_frame_hist: %s", len(big_frame_hist))

2018-05-02 19:09:06,990 - memory_profile6_log - INFO -    448    144.4 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-05-02 19:09:06,990 - memory_profile6_log - INFO -    449                             

2018-05-02 19:09:06,992 - memory_profile6_log - INFO -    450    144.8 MiB      0.3 MiB       big_frame = pd.concat(datalist)

2018-05-02 19:09:06,992 - memory_profile6_log - INFO -    451    144.8 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-05-02 19:09:06,993 - memory_profile6_log - INFO -    452    144.8 MiB      0.0 MiB       logger.info("len of big_frame: %s", len(big_frame))

2018-05-02 19:09:06,993 - memory_profile6_log - INFO -    453    144.8 MiB      0.0 MiB       del datalist

2018-05-02 19:09:06,993 - memory_profile6_log - INFO -    454                             

2018-05-02 19:09:06,993 - memory_profile6_log - INFO -    455    144.8 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 19:09:06,993 - memory_profile6_log - INFO -    456                             

2018-05-02 19:09:06,994 - memory_profile6_log - INFO -    457                                 # ~ get current news interest ~

2018-05-02 19:09:07,000 - memory_profile6_log - INFO -    458    144.8 MiB      0.0 MiB       if not cd:

2018-05-02 19:09:07,000 - memory_profile6_log - INFO -    459    144.8 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-05-02 19:09:07,002 - memory_profile6_log - INFO -    460    144.8 MiB      0.0 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-05-02 19:09:07,003 - memory_profile6_log - INFO -    461    144.8 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-05-02 19:09:07,003 - memory_profile6_log - INFO -    462                                 else:

2018-05-02 19:09:07,005 - memory_profile6_log - INFO -    463                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-05-02 19:09:07,006 - memory_profile6_log - INFO -    464                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date) LIMIT 20000"

2018-05-02 19:09:07,006 - memory_profile6_log - INFO -    465                             

2018-05-02 19:09:07,006 - memory_profile6_log - INFO -    466                                     # safe handling of query parameter

2018-05-02 19:09:07,007 - memory_profile6_log - INFO -    467                                     query_params = [

2018-05-02 19:09:07,007 - memory_profile6_log - INFO -    468                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-05-02 19:09:07,012 - memory_profile6_log - INFO -    469                                     ]

2018-05-02 19:09:07,013 - memory_profile6_log - INFO -    470                             

2018-05-02 19:09:07,015 - memory_profile6_log - INFO -    471                                     job_config.query_parameters = query_params

2018-05-02 19:09:07,015 - memory_profile6_log - INFO -    472                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-05-02 19:09:07,016 - memory_profile6_log - INFO -    473                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-05-02 19:09:07,016 - memory_profile6_log - INFO -    474                             

2018-05-02 19:09:07,016 - memory_profile6_log - INFO -    475    144.8 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-05-02 19:09:07,017 - memory_profile6_log - INFO -    476    144.8 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-05-02 19:09:07,017 - memory_profile6_log - INFO -    477    144.9 MiB      0.1 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-05-02 19:09:07,019 - memory_profile6_log - INFO -    478    144.9 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-05-02 19:09:07,019 - memory_profile6_log - INFO -    479    144.9 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 19:09:07,019 - memory_profile6_log - INFO -    480    144.9 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-05-02 19:09:07,025 - memory_profile6_log - INFO -    481                             

2018-05-02 19:09:07,026 - memory_profile6_log - INFO -    482    144.9 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-05-02 19:09:07,026 - memory_profile6_log - INFO - 


2018-05-02 19:09:07,032 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-05-02 19:09:07,046 - memory_profile6_log - INFO - train on: 20000 total genuine interest data(D(u, t))
2018-05-02 19:09:07,046 - memory_profile6_log - INFO - transform on: 20000 total current data(D(t))
2018-05-02 19:09:07,049 - memory_profile6_log - INFO - apply on: 250 total history...
2018-05-02 19:09:07,118 - memory_profile6_log - INFO - len of uniques_fit_hist:250
2018-05-02 19:09:07,124 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:81
2018-05-02 19:09:07,216 - memory_profile6_log - INFO - self.sum_all_nt before concate:

2018-05-02 19:09:07,237 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 19:09:07,240 - memory_profile6_log - INFO - 

2018-05-02 19:09:07,263 - memory_profile6_log - INFO - self.sum_all_nt after concate:

2018-05-02 19:09:07,273 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 19:09:07,275 - memory_profile6_log - INFO - 

2018-05-02 19:09:07,334 - memory_profile6_log - INFO - Len of model_fit: 20000
2018-05-02 19:09:07,335 - memory_profile6_log - INFO - Len of df_dut: 20000
2018-05-02 19:09:07,502 - memory_profile6_log - INFO - Len of fitted_models on main class: 20000
2018-05-02 19:09:07,503 - memory_profile6_log - INFO - 

2018-05-02 19:09:07,503 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-05-02 19:09:07,506 - memory_profile6_log - INFO - 

2018-05-02 19:09:07,506 - memory_profile6_log - INFO - fitted_model_hist:

2018-05-02 19:09:07,507 - memory_profile6_log - INFO - 

2018-05-02 19:09:07,516 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 19:09:07,517 - memory_profile6_log - INFO - 

2018-05-02 19:09:07,526 - memory_profile6_log - INFO - len of fitted models after concat: 20250
2018-05-02 19:09:07,528 - memory_profile6_log - INFO - 

2018-05-02 19:09:07,529 - memory_profile6_log - INFO - Recalculating fitted models...
2018-05-02 19:09:07,529 - memory_profile6_log - INFO - 

2018-05-02 19:09:07,562 - memory_profile6_log - INFO - fitted_models after cobine:

2018-05-02 19:09:07,565 - memory_profile6_log - INFO - 

2018-05-02 19:09:07,572 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 19:09:07,573 - memory_profile6_log - INFO - 

2018-05-02 19:09:07,575 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 20000
2018-05-02 19:09:07,576 - memory_profile6_log - INFO - 

2018-05-02 19:10:13,536 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 19:10:13,539 - memory_profile6_log - INFO - date_generated: 
2018-05-02 19:10:13,539 - memory_profile6_log - INFO -  
2018-05-02 19:10:13,540 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 19, 10, 13, 537000)]
2018-05-02 19:10:13,540 - memory_profile6_log - INFO - 

2018-05-02 19:10:13,542 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 19:10:13,542 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 19:10:13,542 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 19:10:13,693 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 19:10:13,697 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 19:10:23,140 - memory_profile6_log - INFO - size of df: 5.05 MB
2018-05-02 19:10:23,141 - memory_profile6_log - INFO - getting total: 20000 training data(genuine interest) for date: 2018-05-01
2018-05-02 19:10:23,210 - memory_profile6_log - INFO - loading history data from elastic...
2018-05-02 19:10:23,210 - memory_profile6_log - INFO - Len of X_split for batch load: 25
2018-05-02 19:10:23,211 - memory_profile6_log - INFO - Appending history data...
2018-05-02 19:10:23,250 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:10:25,707 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:10:29,213 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:10:31,342 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:10:33,453 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:11:05,542 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 19:11:05,545 - memory_profile6_log - INFO - date_generated: 
2018-05-02 19:11:05,546 - memory_profile6_log - INFO -  
2018-05-02 19:11:05,546 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 19, 11, 5, 543000)]
2018-05-02 19:11:05,546 - memory_profile6_log - INFO - 

2018-05-02 19:11:05,546 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 19:11:05,546 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 19:11:05,546 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 19:11:05,717 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 19:11:05,721 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 19:11:15,743 - memory_profile6_log - INFO - size of df: 5.05 MB
2018-05-02 19:11:15,746 - memory_profile6_log - INFO - getting total: 20000 training data(genuine interest) for date: 2018-05-01
2018-05-02 19:11:15,828 - memory_profile6_log - INFO - loading history data from elastic...
2018-05-02 19:11:15,829 - memory_profile6_log - INFO - Len of X_split for batch load: 25
2018-05-02 19:11:15,829 - memory_profile6_log - INFO - Appending history data...
2018-05-02 19:11:15,874 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:11:18,618 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:11:20,726 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:11:23,433 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:11:25,687 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:13:25,134 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 19:13:25,140 - memory_profile6_log - INFO - date_generated: 
2018-05-02 19:13:25,140 - memory_profile6_log - INFO -  
2018-05-02 19:13:25,141 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 19, 13, 25, 136000)]
2018-05-02 19:13:25,141 - memory_profile6_log - INFO - 

2018-05-02 19:13:25,141 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 19:13:25,142 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 19:13:25,142 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 19:13:25,321 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 19:13:25,325 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 19:13:35,065 - memory_profile6_log - INFO - size of df: 5.05 MB
2018-05-02 19:13:35,065 - memory_profile6_log - INFO - getting total: 20000 training data(genuine interest) for date: 2018-05-01
2018-05-02 19:13:35,145 - memory_profile6_log - INFO - loading history data from elastic...
2018-05-02 19:13:35,148 - memory_profile6_log - INFO - Len of X_split for batch load: 25
2018-05-02 19:13:35,148 - memory_profile6_log - INFO - Appending history data...
2018-05-02 19:13:35,203 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:13:39,582 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:13:42,841 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:13:45,832 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:13:51,046 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:13:55,265 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:13:59,556 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:14:03,454 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:14:06,397 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:14:10,536 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:14:13,875 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:14:16,970 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:14:20,617 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:14:23,792 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:14:29,003 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:14:32,934 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:14:36,658 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:14:40,002 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:14:44,760 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:14:47,878 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:14:51,755 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:14:55,834 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:14:59,535 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:15:03,865 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:15:07,114 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:15:10,874 - memory_profile6_log - INFO - Appending training data...
2018-05-02 19:15:10,875 - memory_profile6_log - INFO - len datalist: 1
2018-05-02 19:15:10,877 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-05-02 19:15:10,878 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 19:15:10,881 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 19:15:10,881 - memory_profile6_log - INFO - ================================================

2018-05-02 19:15:10,882 - memory_profile6_log - INFO -    359     86.8 MiB     86.8 MiB   @profile

2018-05-02 19:15:10,884 - memory_profile6_log - INFO -    360                             def BQPreprocess(cpu, date_generated, client, query_fit, loadfrom="elastic"):

2018-05-02 19:15:10,885 - memory_profile6_log - INFO -    361     86.8 MiB      0.0 MiB       bq_client = client

2018-05-02 19:15:10,888 - memory_profile6_log - INFO -    362     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 19:15:10,890 - memory_profile6_log - INFO -    363                             

2018-05-02 19:15:10,891 - memory_profile6_log - INFO -    364     86.8 MiB      0.0 MiB       datalist = []

2018-05-02 19:15:10,891 - memory_profile6_log - INFO -    365     86.8 MiB      0.0 MiB       datalist_hist = []

2018-05-02 19:15:10,892 - memory_profile6_log - INFO -    366                             

2018-05-02 19:15:10,895 - memory_profile6_log - INFO -    367     86.8 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-05-02 19:15:10,897 - memory_profile6_log - INFO -    368    143.0 MiB      0.0 MiB       for ndate in date_generated:

2018-05-02 19:15:10,898 - memory_profile6_log - INFO -    369    139.2 MiB     52.4 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-05-02 19:15:10,898 - memory_profile6_log - INFO -    370    139.2 MiB      0.0 MiB           if tframe is not None:

2018-05-02 19:15:10,900 - memory_profile6_log - INFO -    371    139.2 MiB      0.0 MiB               if not tframe.empty:

2018-05-02 19:15:10,901 - memory_profile6_log - INFO -    372    139.2 MiB      0.0 MiB                   if loadfrom.strip().lower() == 'datastore':

2018-05-02 19:15:10,903 - memory_profile6_log - INFO -    373                                                 X_split = np.array_split(tframe, 5)

2018-05-02 19:15:10,904 - memory_profile6_log - INFO -    374                                                 logger.info("loading history data from datastore...")

2018-05-02 19:15:10,904 - memory_profile6_log - INFO -    375                                                 logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 19:15:10,907 - memory_profile6_log - INFO -    376                                                 logger.info("Appending history data...")

2018-05-02 19:15:10,913 - memory_profile6_log - INFO -    377                                                 for ix in range(len(X_split)):

2018-05-02 19:15:10,914 - memory_profile6_log - INFO -    378                                                     # ~ loading history

2018-05-02 19:15:10,914 - memory_profile6_log - INFO -    379                                                     """

2018-05-02 19:15:10,917 - memory_profile6_log - INFO -    380                                                         disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-05-02 19:15:10,918 - memory_profile6_log - INFO -    381                                                     """

2018-05-02 19:15:10,921 - memory_profile6_log - INFO -    382                                                     logger.info("processing batch-%d", ix)

2018-05-02 19:15:10,921 - memory_profile6_log - INFO -    383                                                     # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-05-02 19:15:10,921 - memory_profile6_log - INFO -    384                                                     logger.info("creating list history data...")

2018-05-02 19:15:10,923 - memory_profile6_log - INFO -    385                                                     #lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-05-02 19:15:10,924 - memory_profile6_log - INFO -    386                                                     lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 19:15:10,924 - memory_profile6_log - INFO -    387                             

2018-05-02 19:15:10,927 - memory_profile6_log - INFO -    388                                                     logger.info("call history data...")

2018-05-02 19:15:10,928 - memory_profile6_log - INFO -    389                                                     h_frame = mh.loadDSHistory(lhistory)

2018-05-02 19:15:10,930 - memory_profile6_log - INFO -    390                             

2018-05-02 19:15:10,930 - memory_profile6_log - INFO -    391                                                     # me = os.getpid()

2018-05-02 19:15:10,931 - memory_profile6_log - INFO -    392                                                     # kill_proc_tree(me)

2018-05-02 19:15:10,933 - memory_profile6_log - INFO -    393                             

2018-05-02 19:15:10,934 - memory_profile6_log - INFO -    394                                                     logger.info("done collecting history data, appending now...")

2018-05-02 19:15:10,934 - memory_profile6_log - INFO -    395                                                     for m in h_frame:

2018-05-02 19:15:10,937 - memory_profile6_log - INFO -    396                                                         if m is not None:

2018-05-02 19:15:10,938 - memory_profile6_log - INFO -    397                                                             if len(m) > 0:

2018-05-02 19:15:10,940 - memory_profile6_log - INFO -    398                                                                 datalist_hist.append(pd.DataFrame(m))

2018-05-02 19:15:10,941 - memory_profile6_log - INFO -    399                                                     del h_frame

2018-05-02 19:15:10,943 - memory_profile6_log - INFO -    400                                                     del lhistory

2018-05-02 19:15:10,944 - memory_profile6_log - INFO -    401                             

2018-05-02 19:15:10,946 - memory_profile6_log - INFO -    402                                                 logger.info("Appending training data...")

2018-05-02 19:15:10,948 - memory_profile6_log - INFO -    403                                                 datalist.append(tframe)

2018-05-02 19:15:10,950 - memory_profile6_log - INFO -    404    139.2 MiB      0.0 MiB                   elif loadfrom.strip().lower() == 'elastic':

2018-05-02 19:15:10,953 - memory_profile6_log - INFO -    405    139.4 MiB      0.2 MiB                       X_split = np.array_split(tframe, 25)

2018-05-02 19:15:10,954 - memory_profile6_log - INFO -    406    139.4 MiB      0.0 MiB                       logger.info("loading history data from elastic...")

2018-05-02 19:15:10,956 - memory_profile6_log - INFO -    407    139.4 MiB      0.0 MiB                       logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 19:15:10,959 - memory_profile6_log - INFO -    408    139.4 MiB      0.0 MiB                       logger.info("Appending history data...")

2018-05-02 19:15:10,961 - memory_profile6_log - INFO -    409                                                 

2018-05-02 19:15:10,963 - memory_profile6_log - INFO -    410    143.0 MiB     -0.0 MiB                       for ix in range(len(X_split)):

2018-05-02 19:15:10,963 - memory_profile6_log - INFO -    411    143.0 MiB      0.4 MiB                           lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 19:15:10,963 - memory_profile6_log - INFO -    412    143.0 MiB     -0.0 MiB                           logger.info("call %d history data...", len(lhistory))

2018-05-02 19:15:10,964 - memory_profile6_log - INFO -    413    143.0 MiB     -0.0 MiB                           inside_data = mh.loadESHistory(lhistory,

2018-05-02 19:15:10,966 - memory_profile6_log - INFO -    414    143.0 MiB     -0.0 MiB                                                          esindex_name='fitted_hist_index',

2018-05-02 19:15:10,967 - memory_profile6_log - INFO -    415    143.0 MiB      2.9 MiB                                                          estype_name='fitted_hist_type')

2018-05-02 19:15:10,969 - memory_profile6_log - INFO -    416                                                     # split back the user_id and topic_id

2018-05-02 19:15:10,970 - memory_profile6_log - INFO -    417    143.0 MiB      0.0 MiB                           inside_data[['user_id','topic_id']] = inside_data.uid_topid.str.split('_', expand=True)

2018-05-02 19:15:10,971 - memory_profile6_log - INFO -    418    143.0 MiB      0.1 MiB                           inside_data = inside_data[["user_id","topic_id", "pt_posterior_x_Nt", "smoothed_pt_posterior", "p0_cat_ci", "sigma_Nt"]]

2018-05-02 19:15:10,973 - memory_profile6_log - INFO -    419    143.0 MiB     -0.0 MiB                           if not inside_data.empty:

2018-05-02 19:15:10,973 - memory_profile6_log - INFO -    420    143.0 MiB     -0.0 MiB                               datalist_hist.append(inside_data)

2018-05-02 19:15:10,974 - memory_profile6_log - INFO -    421    143.0 MiB     -0.0 MiB                               del inside_data

2018-05-02 19:15:10,976 - memory_profile6_log - INFO -    422                                                 

2018-05-02 19:15:10,976 - memory_profile6_log - INFO -    423    143.0 MiB      0.0 MiB                       logger.info("Appending training data...")

2018-05-02 19:15:10,979 - memory_profile6_log - INFO -    424    143.0 MiB      0.0 MiB                       datalist.append(tframe)

2018-05-02 19:15:10,982 - memory_profile6_log - INFO -    425                                             else:

2018-05-02 19:15:10,983 - memory_profile6_log - INFO -    426                                                 logger.info("Unknows source is selected !")

2018-05-02 19:15:10,983 - memory_profile6_log - INFO -    427                                                 break

2018-05-02 19:15:10,984 - memory_profile6_log - INFO -    428                                     else: 

2018-05-02 19:15:10,986 - memory_profile6_log - INFO -    429                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-05-02 19:15:10,986 - memory_profile6_log - INFO -    430    143.0 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-05-02 19:15:10,990 - memory_profile6_log - INFO -    431    143.0 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-05-02 19:15:10,990 - memory_profile6_log - INFO -    432                             

2018-05-02 19:15:10,994 - memory_profile6_log - INFO -    433    143.0 MiB      0.0 MiB       return datalist, datalist_hist

2018-05-02 19:15:10,996 - memory_profile6_log - INFO - 


2018-05-02 19:15:11,019 - memory_profile6_log - INFO - len of big_frame_hist: 20000
2018-05-02 19:15:11,035 - memory_profile6_log - INFO - size of big_frame_hist: 5.49 MB
2018-05-02 19:15:11,049 - memory_profile6_log - INFO - size of big_frame: 5.05 MB
2018-05-02 19:15:11,052 - memory_profile6_log - INFO - len of big_frame: 20000
2018-05-02 19:15:11,055 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-05-02 19:15:19,016 - memory_profile6_log - INFO - size of df: 4.93 MB
2018-05-02 19:15:19,019 - memory_profile6_log - INFO - getting total: 20000 training data(current date interest)
2018-05-02 19:15:19,036 - memory_profile6_log - INFO - size of current_frame: 5.09 MB
2018-05-02 19:15:19,036 - memory_profile6_log - INFO - loading time of: 40000 total genuine-current interest data ~ take 113.753s
2018-05-02 19:15:19,038 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 19:15:19,042 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 19:15:19,042 - memory_profile6_log - INFO - ================================================

2018-05-02 19:15:19,042 - memory_profile6_log - INFO -    435     86.7 MiB     86.7 MiB   @profile

2018-05-02 19:15:19,046 - memory_profile6_log - INFO -    436                             def preprocess(cpu, cd, query_fit, date_generated):

2018-05-02 19:15:19,048 - memory_profile6_log - INFO -    437     86.8 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-05-02 19:15:19,049 - memory_profile6_log - INFO -    438     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 19:15:19,049 - memory_profile6_log - INFO -    439                             

2018-05-02 19:15:19,051 - memory_profile6_log - INFO -    440                                 # ~~~ Begin collecting data ~~~

2018-05-02 19:15:19,052 - memory_profile6_log - INFO -    441     86.8 MiB      0.0 MiB       t0 = time.time()

2018-05-02 19:15:19,053 - memory_profile6_log - INFO -    442    143.0 MiB     56.2 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-05-02 19:15:19,055 - memory_profile6_log - INFO -    443    143.0 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-05-02 19:15:19,056 - memory_profile6_log - INFO -    444                                     logger.info("Training cannot be empty..")

2018-05-02 19:15:19,059 - memory_profile6_log - INFO -    445                                     return False

2018-05-02 19:15:19,059 - memory_profile6_log - INFO -    446    143.0 MiB      0.0 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-05-02 19:15:19,062 - memory_profile6_log - INFO -    447    143.0 MiB      0.0 MiB       logger.info("len of big_frame_hist: %s", len(big_frame_hist))

2018-05-02 19:15:19,062 - memory_profile6_log - INFO -    448    143.0 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-05-02 19:15:19,063 - memory_profile6_log - INFO -    449                             

2018-05-02 19:15:19,063 - memory_profile6_log - INFO -    450    143.1 MiB      0.1 MiB       big_frame = pd.concat(datalist)

2018-05-02 19:15:19,066 - memory_profile6_log - INFO -    451    143.1 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-05-02 19:15:19,071 - memory_profile6_log - INFO -    452    143.1 MiB      0.0 MiB       logger.info("len of big_frame: %s", len(big_frame))

2018-05-02 19:15:19,072 - memory_profile6_log - INFO -    453    143.1 MiB      0.0 MiB       del datalist

2018-05-02 19:15:19,072 - memory_profile6_log - INFO -    454                             

2018-05-02 19:15:19,072 - memory_profile6_log - INFO -    455    143.1 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 19:15:19,073 - memory_profile6_log - INFO -    456                             

2018-05-02 19:15:19,073 - memory_profile6_log - INFO -    457                                 # ~ get current news interest ~

2018-05-02 19:15:19,076 - memory_profile6_log - INFO -    458    143.1 MiB      0.0 MiB       if not cd:

2018-05-02 19:15:19,076 - memory_profile6_log - INFO -    459    143.1 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-05-02 19:15:19,078 - memory_profile6_log - INFO -    460    149.3 MiB      6.2 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-05-02 19:15:19,079 - memory_profile6_log - INFO -    461    149.3 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-05-02 19:15:19,079 - memory_profile6_log - INFO -    462                                 else:

2018-05-02 19:15:19,081 - memory_profile6_log - INFO -    463                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-05-02 19:15:19,081 - memory_profile6_log - INFO -    464                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date) LIMIT 20000"

2018-05-02 19:15:19,082 - memory_profile6_log - INFO -    465                             

2018-05-02 19:15:19,082 - memory_profile6_log - INFO -    466                                     # safe handling of query parameter

2018-05-02 19:15:19,082 - memory_profile6_log - INFO -    467                                     query_params = [

2018-05-02 19:15:19,082 - memory_profile6_log - INFO -    468                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-05-02 19:15:19,084 - memory_profile6_log - INFO -    469                                     ]

2018-05-02 19:15:19,085 - memory_profile6_log - INFO -    470                             

2018-05-02 19:15:19,088 - memory_profile6_log - INFO -    471                                     job_config.query_parameters = query_params

2018-05-02 19:15:19,091 - memory_profile6_log - INFO -    472                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-05-02 19:15:19,092 - memory_profile6_log - INFO -    473                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-05-02 19:15:19,092 - memory_profile6_log - INFO -    474                             

2018-05-02 19:15:19,094 - memory_profile6_log - INFO -    475    149.3 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-05-02 19:15:19,095 - memory_profile6_log - INFO -    476    149.3 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-05-02 19:15:19,095 - memory_profile6_log - INFO -    477    149.4 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-05-02 19:15:19,096 - memory_profile6_log - INFO -    478    149.4 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-05-02 19:15:19,098 - memory_profile6_log - INFO -    479    149.4 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 19:15:19,099 - memory_profile6_log - INFO -    480    149.4 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-05-02 19:15:19,105 - memory_profile6_log - INFO -    481                             

2018-05-02 19:15:19,105 - memory_profile6_log - INFO -    482    149.4 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-05-02 19:15:19,107 - memory_profile6_log - INFO - 


2018-05-02 19:15:19,111 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-05-02 19:15:19,121 - memory_profile6_log - INFO - train on: 20000 total genuine interest data(D(u, t))
2018-05-02 19:15:19,121 - memory_profile6_log - INFO - transform on: 20000 total current data(D(t))
2018-05-02 19:15:19,124 - memory_profile6_log - INFO - apply on: 20000 total history...
2018-05-02 19:15:19,171 - memory_profile6_log - INFO - len of uniques_fit_hist:20000
2018-05-02 19:15:19,183 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:5392
2018-05-02 19:15:19,269 - memory_profile6_log - INFO - self.sum_all_nt before concate:

2018-05-02 19:15:19,289 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 19:15:19,290 - memory_profile6_log - INFO - 

2018-05-02 19:15:19,308 - memory_profile6_log - INFO - self.sum_all_nt after concate:

2018-05-02 19:15:19,319 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 19:15:19,322 - memory_profile6_log - INFO - 

2018-05-02 19:15:19,374 - memory_profile6_log - INFO - Len of model_fit: 20000
2018-05-02 19:15:19,375 - memory_profile6_log - INFO - Len of df_dut: 20000
2018-05-02 19:15:19,540 - memory_profile6_log - INFO - Len of fitted_models on main class: 20000
2018-05-02 19:15:19,542 - memory_profile6_log - INFO - 

2018-05-02 19:15:19,542 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-05-02 19:15:19,543 - memory_profile6_log - INFO - 

2018-05-02 19:15:19,545 - memory_profile6_log - INFO - fitted_model_hist:

2018-05-02 19:15:19,546 - memory_profile6_log - INFO - 

2018-05-02 19:15:19,562 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 19:15:19,563 - memory_profile6_log - INFO - 

2018-05-02 19:15:19,575 - memory_profile6_log - INFO - len of fitted models after concat: 40000
2018-05-02 19:15:19,578 - memory_profile6_log - INFO - 

2018-05-02 19:15:19,578 - memory_profile6_log - INFO - Recalculating fitted models...
2018-05-02 19:15:19,579 - memory_profile6_log - INFO - 

2018-05-02 19:15:19,630 - memory_profile6_log - INFO - fitted_models after cobine:

2018-05-02 19:15:19,631 - memory_profile6_log - INFO - 

2018-05-02 19:15:19,648 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 19:15:19,648 - memory_profile6_log - INFO - 

2018-05-02 19:15:19,651 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 20000
2018-05-02 19:15:19,653 - memory_profile6_log - INFO - 

2018-05-02 19:15:24,752 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-05-02 19:15:24,772 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, topic_id, pt_posterior_x_Nt, smoothed_pt_posterior, p0_cat_ci, sigma_Nt, p0_posterior, is_general, rank]
Index: []
2018-05-02 19:15:24,773 - memory_profile6_log - INFO - 

2018-05-02 19:15:24,773 - memory_profile6_log - INFO - Len of model_transform: 8553
2018-05-02 19:15:24,776 - memory_profile6_log - INFO - Len of df_dt: 20000
2018-05-02 19:15:24,778 - memory_profile6_log - INFO - Total train time: 5.639s
2018-05-02 19:15:24,779 - memory_profile6_log - INFO - memory left before cleaning: 87.900 percent memory...
2018-05-02 19:15:24,779 - memory_profile6_log - INFO - cleaning up some objects...
2018-05-02 19:15:24,780 - memory_profile6_log - INFO - deleting df_dut...
2018-05-02 19:15:24,783 - memory_profile6_log - INFO - deleting df_dt...
2018-05-02 19:15:24,785 - memory_profile6_log - INFO - deleting df_input...
2018-05-02 19:15:24,786 - memory_profile6_log - INFO - deleting df_input_X...
2018-05-02 19:15:24,788 - memory_profile6_log - INFO - deleting df_current...
2018-05-02 19:15:24,789 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-05-02 19:15:24,789 - memory_profile6_log - INFO - deleting model_fit...
2018-05-02 19:15:24,792 - memory_profile6_log - INFO - deleting result...
2018-05-02 19:15:24,799 - memory_profile6_log - INFO - list(model_transformsv) before rename: 
2018-05-02 19:15:24,801 - memory_profile6_log - INFO -  
2018-05-02 19:15:24,802 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'is_general', 'p0_posterior', 'rank', 'date']
2018-05-02 19:15:24,803 - memory_profile6_log - INFO - 

2018-05-02 19:15:24,808 - memory_profile6_log - INFO - 8553
2018-05-02 19:15:24,809 - memory_profile6_log - INFO - 

2018-05-02 19:15:24,812 - memory_profile6_log - INFO - list(model_transformsv) after rename: 
2018-05-02 19:15:24,813 - memory_profile6_log - INFO -  
2018-05-02 19:15:24,815 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'topic_is_general', 'interest_score', 'interest_rank', 'interest_score_created_at']
2018-05-02 19:15:24,816 - memory_profile6_log - INFO - 

2018-05-02 19:15:24,818 - memory_profile6_log - INFO - 8553
2018-05-02 19:15:24,819 - memory_profile6_log - INFO - 

2018-05-02 19:15:24,819 - memory_profile6_log - INFO - deleting model_transform...
2018-05-02 19:15:24,821 - memory_profile6_log - INFO - memory left after cleaning: 87.900 percent memory...
2018-05-02 19:15:24,822 - memory_profile6_log - INFO - Begin saving trained data...
2018-05-02 19:15:24,825 - memory_profile6_log - INFO - Using ElasticSearch as storage...
2018-05-02 19:15:24,844 - memory_profile6_log - INFO - Saving total data: 8553
2018-05-02 19:15:24,845 - memory_profile6_log - INFO - Len of X_split for batch save model_transformsv: 15
2018-05-02 19:15:24,846 - memory_profile6_log - INFO - processing batch-0
2018-05-02 19:15:32,298 - memory_profile6_log - INFO - processing batch-1
2018-05-02 19:15:38,032 - memory_profile6_log - INFO - processing batch-2
2018-05-02 19:15:43,545 - memory_profile6_log - INFO - processing batch-3
2018-05-02 19:15:50,799 - memory_profile6_log - INFO - processing batch-4
2018-05-02 19:15:59,105 - memory_profile6_log - INFO - processing batch-5
2018-05-02 19:16:05,276 - memory_profile6_log - INFO - processing batch-6
2018-05-02 19:16:09,125 - memory_profile6_log - INFO - processing batch-7
2018-05-02 19:16:14,318 - memory_profile6_log - INFO - processing batch-8
2018-05-02 19:16:19,391 - memory_profile6_log - INFO - processing batch-9
2018-05-02 19:16:27,980 - memory_profile6_log - INFO - processing batch-10
2018-05-02 19:16:32,266 - memory_profile6_log - INFO - processing batch-11
2018-05-02 19:16:37,186 - memory_profile6_log - INFO - processing batch-12
2018-05-02 19:16:45,361 - memory_profile6_log - INFO - processing batch-13
2018-05-02 19:16:49,736 - memory_profile6_log - INFO - processing batch-14
2018-05-02 19:16:55,579 - memory_profile6_log - INFO - Saving fitted_models as history to Elasticsearch...
2018-05-02 19:16:55,650 - memory_profile6_log - INFO - Saving total data: 20000
2018-05-02 19:16:55,651 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 25
2018-05-02 19:16:55,653 - memory_profile6_log - INFO - processing batch-0
2018-05-02 19:17:04,716 - memory_profile6_log - INFO - processing batch-1
2018-05-02 19:17:09,917 - memory_profile6_log - INFO - processing batch-2
2018-05-02 19:17:16,038 - memory_profile6_log - INFO - processing batch-3
2018-05-02 19:17:32,927 - memory_profile6_log - INFO - processing batch-4
2018-05-02 19:17:48,839 - memory_profile6_log - INFO - processing batch-5
2018-05-02 19:17:58,351 - memory_profile6_log - INFO - processing batch-6
2018-05-02 19:18:02,536 - memory_profile6_log - INFO - processing batch-7
2018-05-02 19:18:11,727 - memory_profile6_log - INFO - processing batch-8
2018-05-02 19:18:21,403 - memory_profile6_log - INFO - processing batch-9
2018-05-02 19:18:29,279 - memory_profile6_log - INFO - processing batch-10
2018-05-02 19:18:37,408 - memory_profile6_log - INFO - processing batch-11
2018-05-02 19:18:44,943 - memory_profile6_log - INFO - processing batch-12
2018-05-02 19:18:49,276 - memory_profile6_log - INFO - processing batch-13
2018-05-02 19:18:56,517 - memory_profile6_log - INFO - processing batch-14
2018-05-02 19:19:04,740 - memory_profile6_log - INFO - processing batch-15
2018-05-02 19:19:12,789 - memory_profile6_log - INFO - processing batch-16
2018-05-02 19:19:17,155 - memory_profile6_log - INFO - processing batch-17
2018-05-02 19:19:22,430 - memory_profile6_log - INFO - processing batch-18
2018-05-02 19:19:31,642 - memory_profile6_log - INFO - processing batch-19
2018-05-02 19:19:42,071 - memory_profile6_log - INFO - processing batch-20
2018-05-02 19:19:48,657 - memory_profile6_log - INFO - processing batch-21
2018-05-02 19:19:54,336 - memory_profile6_log - INFO - processing batch-22
2018-05-02 19:20:03,029 - memory_profile6_log - INFO - processing batch-23
2018-05-02 19:20:10,592 - memory_profile6_log - INFO - processing batch-24
2018-05-02 19:20:19,316 - memory_profile6_log - INFO - deleting BR...
2018-05-02 19:20:19,318 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-05-02 19:20:19,319 - memory_profile6_log - INFO - deleting fitted_models_sigmant...
2018-05-02 19:20:19,322 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 19:20:19,322 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 19:20:19,325 - memory_profile6_log - INFO - ================================================

2018-05-02 19:20:19,325 - memory_profile6_log - INFO -    113    149.4 MiB    149.4 MiB   @profile

2018-05-02 19:20:19,326 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-05-02 19:20:19,328 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-05-02 19:20:19,328 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-05-02 19:20:19,328 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-05-02 19:20:19,328 - memory_profile6_log - INFO -    118                                      saveto="datastore", fitted_models_hist=None):

2018-05-02 19:20:19,329 - memory_profile6_log - INFO -    119                                 """

2018-05-02 19:20:19,329 - memory_profile6_log - INFO -    120                                     Main Process

2018-05-02 19:20:19,331 - memory_profile6_log - INFO -    121                                 """

2018-05-02 19:20:19,331 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-05-02 19:20:19,332 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-05-02 19:20:19,332 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-05-02 19:20:19,336 - memory_profile6_log - INFO -    125    149.4 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-05-02 19:20:19,338 - memory_profile6_log - INFO -    126    149.4 MiB      0.0 MiB       df_dut = df_input.copy(deep=True)

2018-05-02 19:20:19,338 - memory_profile6_log - INFO -    127    149.4 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 19:20:19,339 - memory_profile6_log - INFO -    128                             

2018-05-02 19:20:19,339 - memory_profile6_log - INFO -    129                                 # D(t)

2018-05-02 19:20:19,341 - memory_profile6_log - INFO -    130    149.4 MiB      0.0 MiB       df_dt = df_current.copy(deep=True)

2018-05-02 19:20:19,341 - memory_profile6_log - INFO -    131    149.4 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 19:20:19,342 - memory_profile6_log - INFO -    132                             

2018-05-02 19:20:19,342 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-05-02 19:20:19,342 - memory_profile6_log - INFO -    134    149.4 MiB      0.0 MiB       t0 = time.time()

2018-05-02 19:20:19,344 - memory_profile6_log - INFO -    135    149.4 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-05-02 19:20:19,344 - memory_profile6_log - INFO -    136    149.4 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-05-02 19:20:19,348 - memory_profile6_log - INFO -    137    149.4 MiB      0.0 MiB       logger.info("apply on: %d total history...", len(df_hist))

2018-05-02 19:20:19,349 - memory_profile6_log - INFO -    138                             

2018-05-02 19:20:19,355 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-05-02 19:20:19,355 - memory_profile6_log - INFO -    140    149.4 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-05-02 19:20:19,355 - memory_profile6_log - INFO -    141                             

2018-05-02 19:20:19,358 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-05-02 19:20:19,358 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-05-02 19:20:19,361 - memory_profile6_log - INFO -    144    149.6 MiB      0.2 MiB       NB = BR.processX(df_dut)

2018-05-02 19:20:19,361 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-05-02 19:20:19,361 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-05-02 19:20:19,361 - memory_profile6_log - INFO -    147    149.7 MiB      0.1 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-05-02 19:20:19,362 - memory_profile6_log - INFO -    148                                 """

2018-05-02 19:20:19,362 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-05-02 19:20:19,364 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-05-02 19:20:19,364 - memory_profile6_log - INFO -    151                                 """

2018-05-02 19:20:19,365 - memory_profile6_log - INFO -    152    149.7 MiB      0.0 MiB       fitby_sigmant = False

2018-05-02 19:20:19,365 - memory_profile6_log - INFO -    153    149.7 MiB      0.0 MiB       uniques_fit_hist = None

2018-05-02 19:20:19,365 - memory_profile6_log - INFO -    154    149.7 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-05-02 19:20:19,365 - memory_profile6_log - INFO -    155    149.7 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-05-02 19:20:19,365 - memory_profile6_log - INFO -    156    149.7 MiB      0.0 MiB                            'is_general']]

2018-05-02 19:20:19,367 - memory_profile6_log - INFO -    157                             

2018-05-02 19:20:19,371 - memory_profile6_log - INFO -    158                                 # get sigma_Nt from fitted_models_hist

2018-05-02 19:20:19,374 - memory_profile6_log - INFO -    159    149.7 MiB      0.0 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-05-02 19:20:19,374 - memory_profile6_log - INFO -    160    149.7 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-05-02 19:20:19,375 - memory_profile6_log - INFO -    161    149.9 MiB      0.2 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-05-02 19:20:19,375 - memory_profile6_log - INFO -    162    149.9 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-05-02 19:20:19,375 - memory_profile6_log - INFO -    163                             

2018-05-02 19:20:19,377 - memory_profile6_log - INFO -    164                                 # begin fit

2018-05-02 19:20:19,377 - memory_profile6_log - INFO -    165    149.9 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-05-02 19:20:19,377 - memory_profile6_log - INFO -    166    149.9 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-05-02 19:20:19,378 - memory_profile6_log - INFO -    167    152.1 MiB      2.2 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-05-02 19:20:19,378 - memory_profile6_log - INFO -    168    152.1 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-05-02 19:20:19,378 - memory_profile6_log - INFO -    169    152.1 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-05-02 19:20:19,378 - memory_profile6_log - INFO -    170                                 # print model_fit[['num_x', 'num_x', 'sigma_Nt', 'date_all_click']].loc[model_fit['user_id']=="1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157"].head(5)

2018-05-02 19:20:19,380 - memory_profile6_log - INFO -    171                             

2018-05-02 19:20:19,381 - memory_profile6_log - INFO -    172                                 # ~~ and Transform ~~

2018-05-02 19:20:19,384 - memory_profile6_log - INFO -    173                                 #   handling current news interest == current date

2018-05-02 19:20:19,385 - memory_profile6_log - INFO -    174    152.1 MiB      0.0 MiB       if df_dt.empty:

2018-05-02 19:20:19,387 - memory_profile6_log - INFO -    175                                     print "Cek your df_dt, cannot be emtpy!!"

2018-05-02 19:20:19,388 - memory_profile6_log - INFO -    176                                     return None

2018-05-02 19:20:19,388 - memory_profile6_log - INFO -    177    153.1 MiB      1.0 MiB       NB = BR.processX(df_dt)

2018-05-02 19:20:19,388 - memory_profile6_log - INFO -    178    153.1 MiB      0.0 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-05-02 19:20:19,388 - memory_profile6_log - INFO -    179                             

2018-05-02 19:20:19,390 - memory_profile6_log - INFO -    180    153.1 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-05-02 19:20:19,390 - memory_profile6_log - INFO -    181    153.1 MiB      0.0 MiB                            'num_x', 'num_y', 'is_general']]

2018-05-02 19:20:19,391 - memory_profile6_log - INFO -    182                             

2018-05-02 19:20:19,391 - memory_profile6_log - INFO -    183                                 # print "model_fit dtypes:\n", model_fit.dtypes

2018-05-02 19:20:19,391 - memory_profile6_log - INFO -    184                                 # print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-05-02 19:20:19,392 - memory_profile6_log - INFO -    185    153.1 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-05-02 19:20:19,394 - memory_profile6_log - INFO -    186    153.1 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-05-02 19:20:19,398 - memory_profile6_log - INFO -    187    153.1 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-05-02 19:20:19,398 - memory_profile6_log - INFO -    188    153.1 MiB      0.0 MiB                                                                                           "topic_id", "user_id"]],

2018-05-02 19:20:19,400 - memory_profile6_log - INFO -    189    156.7 MiB      3.6 MiB                                                     verbose=False)

2018-05-02 19:20:19,401 - memory_profile6_log - INFO -    190                             

2018-05-02 19:20:19,401 - memory_profile6_log - INFO -    191                                 # ~~~ filter is general and specific topic ~~~

2018-05-02 19:20:19,403 - memory_profile6_log - INFO -    192                                 # the idea is just we need to rerank every topic according

2018-05-02 19:20:19,404 - memory_profile6_log - INFO -    193                                 #    to user_id and and is_general by p0_posterior

2018-05-02 19:20:19,404 - memory_profile6_log - INFO -    194    156.7 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-05-02 19:20:19,404 - memory_profile6_log - INFO -    195    156.7 MiB      0.0 MiB                                     'is_general']].groupby(['topic_id',

2018-05-02 19:20:19,408 - memory_profile6_log - INFO -    196    156.7 MiB      0.0 MiB                                                             'is_general']

2018-05-02 19:20:19,410 - memory_profile6_log - INFO -    197                                                                                      ).size().to_frame().reset_index()

2018-05-02 19:20:19,411 - memory_profile6_log - INFO -    198                             

2018-05-02 19:20:19,411 - memory_profile6_log - INFO -    199                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-05-02 19:20:19,413 - memory_profile6_log - INFO -    200                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-05-02 19:20:19,414 - memory_profile6_log - INFO -    201    156.7 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-05-02 19:20:19,414 - memory_profile6_log - INFO -    202                             

2018-05-02 19:20:19,417 - memory_profile6_log - INFO -    203                                 # ~ start by provide rank for each topic type ~

2018-05-02 19:20:19,421 - memory_profile6_log - INFO -    204    160.8 MiB      4.1 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-05-02 19:20:19,423 - memory_profile6_log - INFO -    205    158.2 MiB     -2.6 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-05-02 19:20:19,424 - memory_profile6_log - INFO -    206                             

2018-05-02 19:20:19,426 - memory_profile6_log - INFO -    207                                 # ~ set threshold to filter output

2018-05-02 19:20:19,426 - memory_profile6_log - INFO -    208    158.2 MiB      0.0 MiB       if threshold > 0:

2018-05-02 19:20:19,427 - memory_profile6_log - INFO -    209    158.2 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-05-02 19:20:19,427 - memory_profile6_log - INFO -    210    158.2 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-05-02 19:20:19,428 - memory_profile6_log - INFO -    211    158.2 MiB      0.0 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-05-02 19:20:19,436 - memory_profile6_log - INFO -    212                             

2018-05-02 19:20:19,437 - memory_profile6_log - INFO -    213    158.2 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 19:20:19,438 - memory_profile6_log - INFO -    214    158.2 MiB      0.0 MiB       print model_transform[model_transform['user_id']=='1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157']

2018-05-02 19:20:19,440 - memory_profile6_log - INFO -    215    158.2 MiB      0.0 MiB       logger.info("Len of model_transform: %d", len(model_transform))

2018-05-02 19:20:19,441 - memory_profile6_log - INFO -    216    158.2 MiB      0.0 MiB       logger.info("Len of df_dt: %d", len(df_dt))

2018-05-02 19:20:19,443 - memory_profile6_log - INFO -    217    158.2 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-05-02 19:20:19,450 - memory_profile6_log - INFO -    218                             

2018-05-02 19:20:19,456 - memory_profile6_log - INFO -    219    158.2 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-05-02 19:20:19,457 - memory_profile6_log - INFO -    220                             

2018-05-02 19:20:19,459 - memory_profile6_log - INFO -    221    158.2 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-05-02 19:20:19,463 - memory_profile6_log - INFO -    222    158.2 MiB      0.0 MiB       del df_dut

2018-05-02 19:20:19,467 - memory_profile6_log - INFO -    223    158.2 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-05-02 19:20:19,471 - memory_profile6_log - INFO -    224    158.2 MiB      0.0 MiB       del df_dt

2018-05-02 19:20:19,474 - memory_profile6_log - INFO -    225    158.2 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-05-02 19:20:19,480 - memory_profile6_log - INFO -    226    158.2 MiB      0.0 MiB       del df_input

2018-05-02 19:20:19,484 - memory_profile6_log - INFO -    227    158.2 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-05-02 19:20:19,487 - memory_profile6_log - INFO -    228    158.2 MiB      0.0 MiB       del df_input_X

2018-05-02 19:20:19,490 - memory_profile6_log - INFO -    229    158.2 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-05-02 19:20:19,493 - memory_profile6_log - INFO -    230    158.2 MiB      0.0 MiB       del df_current

2018-05-02 19:20:19,500 - memory_profile6_log - INFO -    231    158.2 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-05-02 19:20:19,505 - memory_profile6_log - INFO -    232    158.2 MiB      0.0 MiB       del map_topic_isgeneral

2018-05-02 19:20:19,512 - memory_profile6_log - INFO -    233    158.2 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-05-02 19:20:19,513 - memory_profile6_log - INFO -    234    156.5 MiB     -1.7 MiB       del model_fit

2018-05-02 19:20:19,516 - memory_profile6_log - INFO -    235    156.5 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-05-02 19:20:19,519 - memory_profile6_log - INFO -    236    156.5 MiB      0.0 MiB       del result

2018-05-02 19:20:19,522 - memory_profile6_log - INFO -    237    156.5 MiB      0.0 MiB       logger.info("deleting result...")

2018-05-02 19:20:19,523 - memory_profile6_log - INFO -    238                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 19:20:19,526 - memory_profile6_log - INFO -    239                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 19:20:19,529 - memory_profile6_log - INFO -    240                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 19:20:19,533 - memory_profile6_log - INFO -    241    156.5 MiB      0.0 MiB       if savetrain:

2018-05-02 19:20:19,536 - memory_profile6_log - INFO -    242    156.5 MiB      0.0 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'p0_posterior', 'rank']].copy(deep=True)

2018-05-02 19:20:19,538 - memory_profile6_log - INFO -    243    156.5 MiB      0.0 MiB           model_transformsv['date'] = current_date  # we need manually adding date, because table not support

2018-05-02 19:20:19,539 - memory_profile6_log - INFO -    244    156.5 MiB      0.0 MiB           model_transformsv['date'] = pd.to_datetime(model_transformsv['date'],

2018-05-02 19:20:19,546 - memory_profile6_log - INFO -    245    156.5 MiB      0.0 MiB                                                      format='%Y-%m-%d', errors='coerce')

2018-05-02 19:20:19,549 - memory_profile6_log - INFO -    246    156.5 MiB      0.0 MiB           print "list(model_transformsv) before rename: ", list(model_transformsv)       

2018-05-02 19:20:19,556 - memory_profile6_log - INFO -    247    156.5 MiB      0.0 MiB           print len(model_transformsv)                

2018-05-02 19:20:19,559 - memory_profile6_log - INFO -    248    156.5 MiB      0.0 MiB           model_transformsv = model_transformsv.rename(columns={'is_general': 'topic_is_general', 'p0_posterior': 'interest_score',

2018-05-02 19:20:19,561 - memory_profile6_log - INFO -    249    156.5 MiB      0.0 MiB                                                                 'rank':'interest_rank' , 'date':'interest_score_created_at'})

2018-05-02 19:20:19,563 - memory_profile6_log - INFO -    250    156.5 MiB      0.0 MiB           print "list(model_transformsv) after rename: ", list(model_transformsv)    

2018-05-02 19:20:19,568 - memory_profile6_log - INFO -    251    156.5 MiB      0.0 MiB           print len(model_transformsv) 

2018-05-02 19:20:19,572 - memory_profile6_log - INFO -    252    156.5 MiB      0.0 MiB           del model_transform

2018-05-02 19:20:19,572 - memory_profile6_log - INFO -    253    156.5 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-05-02 19:20:19,575 - memory_profile6_log - INFO -    254    156.5 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-05-02 19:20:19,578 - memory_profile6_log - INFO -    255                             

2018-05-02 19:20:19,578 - memory_profile6_log - INFO -    256    156.5 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-05-02 19:20:19,581 - memory_profile6_log - INFO -    257                                     # ~ Place your code to save the training model here ~

2018-05-02 19:20:19,582 - memory_profile6_log - INFO -    258    156.5 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-05-02 19:20:19,582 - memory_profile6_log - INFO -    259                                         logger.info("Using google datastore as storage...")

2018-05-02 19:20:19,584 - memory_profile6_log - INFO -    260                                         if multproc:

2018-05-02 19:20:19,592 - memory_profile6_log - INFO -    261                                             # ~ save transform models ~

2018-05-02 19:20:19,596 - memory_profile6_log - INFO -    262                                             logger.info("Saving main Transform model to Google DataStore...")

2018-05-02 19:20:19,598 - memory_profile6_log - INFO -    263                                             logger.info("Saving total data: %d", len(model_transformsv))

2018-05-02 19:20:19,599 - memory_profile6_log - INFO -    264                                             mh.saveDataStorePutMulti(model_transformsv)

2018-05-02 19:20:19,607 - memory_profile6_log - INFO -    265                             

2018-05-02 19:20:19,609 - memory_profile6_log - INFO -    266                                             # ~ save fitted models ~

2018-05-02 19:20:19,609 - memory_profile6_log - INFO -    267                                             logger.info("Saving fitted_models as history to Google DataStore...")

2018-05-02 19:20:19,611 - memory_profile6_log - INFO -    268                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 19:20:19,611 - memory_profile6_log - INFO -    269                                             fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-05-02 19:20:19,612 - memory_profile6_log - INFO -    270                                             logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-05-02 19:20:19,614 - memory_profile6_log - INFO -    271                                             X_split = np.array_split(fitted_models_sigmant, 10)

2018-05-02 19:20:19,615 - memory_profile6_log - INFO -    272                                             logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-05-02 19:20:19,615 - memory_profile6_log - INFO -    273                                             for ix in range(len(X_split)):

2018-05-02 19:20:19,618 - memory_profile6_log - INFO -    274                                                 logger.info("processing batch-%d", ix)

2018-05-02 19:20:19,621 - memory_profile6_log - INFO -    275                                                 mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-05-02 19:20:19,621 - memory_profile6_log - INFO -    276                             

2018-05-02 19:20:19,622 - memory_profile6_log - INFO -    277                                             del X_split

2018-05-02 19:20:19,624 - memory_profile6_log - INFO -    278                                             logger.info("deleting X_split...")

2018-05-02 19:20:19,625 - memory_profile6_log - INFO -    279                                             del save_sigma_nt

2018-05-02 19:20:19,625 - memory_profile6_log - INFO -    280                                             logger.info("deleting save_sigma_nt...")

2018-05-02 19:20:19,625 - memory_profile6_log - INFO -    281                             

2018-05-02 19:20:19,628 - memory_profile6_log - INFO -    282                                             del BR

2018-05-02 19:20:19,631 - memory_profile6_log - INFO -    283                                             logger.info("deleting BR...")

2018-05-02 19:20:19,631 - memory_profile6_log - INFO -    284                             

2018-05-02 19:20:19,632 - memory_profile6_log - INFO -    285    156.5 MiB      0.0 MiB           elif str(saveto).lower() == "elastic":

2018-05-02 19:20:19,634 - memory_profile6_log - INFO -    286    156.5 MiB      0.0 MiB               logger.info("Using ElasticSearch as storage...")

2018-05-02 19:20:19,634 - memory_profile6_log - INFO -    287    156.5 MiB      0.0 MiB               logging.info("Saving main Transform model to Elasticsearch...")

2018-05-02 19:20:19,635 - memory_profile6_log - INFO -    288                                         # print model_transformsv.head(5)

2018-05-02 19:20:19,637 - memory_profile6_log - INFO -    289                                         

2018-05-02 19:20:19,638 - memory_profile6_log - INFO -    290    156.5 MiB      0.0 MiB               X_split = np.array_split(model_transformsv, 15)

2018-05-02 19:20:19,638 - memory_profile6_log - INFO -    291    156.5 MiB      0.0 MiB               logger.info("Saving total data: %d", len(model_transformsv))

2018-05-02 19:20:19,642 - memory_profile6_log - INFO -    292    156.5 MiB      0.0 MiB               logger.info("Len of X_split for batch save model_transformsv: %d", len(X_split))

2018-05-02 19:20:19,644 - memory_profile6_log - INFO -    293    156.5 MiB      0.0 MiB               for ix in range(len(X_split)):

2018-05-02 19:20:19,644 - memory_profile6_log - INFO -    294    156.5 MiB    -61.9 MiB                   logger.info("processing batch-%d", ix)

2018-05-02 19:20:19,645 - memory_profile6_log - INFO -    295    154.3 MiB    -64.1 MiB                   mh.saveElasticS(X_split[ix])

2018-05-02 19:20:19,647 - memory_profile6_log - INFO -    296    154.3 MiB     -2.1 MiB               del X_split

2018-05-02 19:20:19,647 - memory_profile6_log - INFO -    297                                         

2018-05-02 19:20:19,648 - memory_profile6_log - INFO -    298                             

2018-05-02 19:20:19,648 - memory_profile6_log - INFO -    299    154.3 MiB      0.0 MiB               logger.info("Saving fitted_models as history to Elasticsearch...")

2018-05-02 19:20:19,648 - memory_profile6_log - INFO -    300    154.3 MiB      0.0 MiB               save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 19:20:19,650 - memory_profile6_log - INFO -    301    155.4 MiB      1.1 MiB               fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-05-02 19:20:19,651 - memory_profile6_log - INFO -    302                             

2018-05-02 19:20:19,654 - memory_profile6_log - INFO -    303    155.6 MiB      0.2 MiB               fitted_models_sigmant['uid_topid'] = fitted_models_sigmant["user_id"].map(str) + "_" + fitted_models_sigmant["topic_id"].map(str)

2018-05-02 19:20:19,657 - memory_profile6_log - INFO -    304    155.9 MiB      0.3 MiB               fitted_models_sigmant = fitted_models_sigmant[["uid_topid", "pt_posterior_x_Nt", "smoothed_pt_posterior", "p0_cat_ci", "sigma_Nt"]]

2018-05-02 19:20:19,657 - memory_profile6_log - INFO -    305                               

2018-05-02 19:20:19,661 - memory_profile6_log - INFO -    306    156.4 MiB      0.5 MiB               X_split = np.array_split(fitted_models_sigmant, 25)

2018-05-02 19:20:19,663 - memory_profile6_log - INFO -    307    156.4 MiB      0.0 MiB               logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-05-02 19:20:19,665 - memory_profile6_log - INFO -    308    156.4 MiB      0.0 MiB               logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-05-02 19:20:19,667 - memory_profile6_log - INFO -    309    160.3 MiB     -0.1 MiB               for ix in range(len(X_split)):

2018-05-02 19:20:19,668 - memory_profile6_log - INFO -    310    160.3 MiB     -0.1 MiB                   logger.info("processing batch-%d", ix)

2018-05-02 19:20:19,671 - memory_profile6_log - INFO -    311    160.3 MiB      3.8 MiB                   mh.saveElasticS(X_split[ix], esindex_name="fitted_hist_index",  estype_name='fitted_hist_type')

2018-05-02 19:20:19,671 - memory_profile6_log - INFO -    312    160.2 MiB     -0.0 MiB               del X_split

2018-05-02 19:20:19,671 - memory_profile6_log - INFO -    313                                         

2018-05-02 19:20:19,673 - memory_profile6_log - INFO -    314    160.2 MiB      0.0 MiB               del BR

2018-05-02 19:20:19,674 - memory_profile6_log - INFO -    315    160.2 MiB      0.0 MiB               logger.info("deleting BR...")

2018-05-02 19:20:19,677 - memory_profile6_log - INFO -    316    160.2 MiB      0.0 MiB               del save_sigma_nt

2018-05-02 19:20:19,678 - memory_profile6_log - INFO -    317    160.2 MiB      0.0 MiB               logger.info("deleting save_sigma_nt...")

2018-05-02 19:20:19,680 - memory_profile6_log - INFO -    318    160.2 MiB      0.0 MiB               del fitted_models_sigmant

2018-05-02 19:20:19,681 - memory_profile6_log - INFO -    319    160.2 MiB      0.0 MiB               logger.info("deleting fitted_models_sigmant...")

2018-05-02 19:20:19,681 - memory_profile6_log - INFO -    320                             

2018-05-02 19:20:19,683 - memory_profile6_log - INFO -    321                                     # need save sigma_nt for daily train

2018-05-02 19:20:19,684 - memory_profile6_log - INFO -    322                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-05-02 19:20:19,684 - memory_profile6_log - INFO -    323                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-05-02 19:20:19,684 - memory_profile6_log - INFO -    324    160.2 MiB      0.0 MiB           if start_date and end_date:

2018-05-02 19:20:19,686 - memory_profile6_log - INFO -    325                                         if not fitby_sigmant:

2018-05-02 19:20:19,686 - memory_profile6_log - INFO -    326                                             logging.info("Saving sigma Nt...")

2018-05-02 19:20:19,690 - memory_profile6_log - INFO -    327                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 19:20:19,690 - memory_profile6_log - INFO -    328                                             save_sigma_nt['start_date'] = start_date

2018-05-02 19:20:19,693 - memory_profile6_log - INFO -    329                                             save_sigma_nt['end_date'] = end_date

2018-05-02 19:20:19,693 - memory_profile6_log - INFO -    330                                             print save_sigma_nt.head(5)

2018-05-02 19:20:19,694 - memory_profile6_log - INFO -    331                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-05-02 19:20:19,694 - memory_profile6_log - INFO -    332                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-05-02 19:20:19,694 - memory_profile6_log - INFO -    333    160.2 MiB      0.0 MiB       return

2018-05-02 19:20:19,696 - memory_profile6_log - INFO - 


2018-05-02 19:20:19,696 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-05-02 19:22:00,040 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 19:22:00,043 - memory_profile6_log - INFO - date_generated: 
2018-05-02 19:22:00,043 - memory_profile6_log - INFO -  
2018-05-02 19:22:00,045 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 19, 22, 0, 42000)]
2018-05-02 19:22:00,045 - memory_profile6_log - INFO - 

2018-05-02 19:22:00,045 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 19:22:00,046 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 19:22:00,046 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 19:22:00,247 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 19:22:00,252 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 19:22:10,480 - memory_profile6_log - INFO - size of df: 5.05 MB
2018-05-02 19:22:10,482 - memory_profile6_log - INFO - getting total: 20000 training data(genuine interest) for date: 2018-05-01
2018-05-02 19:22:10,549 - memory_profile6_log - INFO - loading history data from elastic...
2018-05-02 19:22:10,549 - memory_profile6_log - INFO - Len of X_split for batch load: 25
2018-05-02 19:22:10,551 - memory_profile6_log - INFO - Appending history data...
2018-05-02 19:22:10,588 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:22:14,951 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:22:16,691 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:22:18,186 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:22:19,759 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:22:21,588 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:22:22,956 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:22:24,403 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:22:25,707 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:22:26,711 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:22:28,219 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:22:29,426 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:22:30,778 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:22:32,924 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:22:35,309 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:22:37,400 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:22:39,500 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:22:41,760 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:22:43,170 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:22:44,674 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:22:46,142 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:22:47,660 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:22:48,740 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:22:49,788 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:22:52,512 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:22:55,315 - memory_profile6_log - INFO - Appending training data...
2018-05-02 19:22:55,316 - memory_profile6_log - INFO - len datalist: 1
2018-05-02 19:22:55,318 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-05-02 19:22:55,319 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 19:22:55,321 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 19:22:55,322 - memory_profile6_log - INFO - ================================================

2018-05-02 19:22:55,322 - memory_profile6_log - INFO -    371     86.9 MiB     86.9 MiB   @profile

2018-05-02 19:22:55,323 - memory_profile6_log - INFO -    372                             def BQPreprocess(cpu, date_generated, client, query_fit, loadfrom="elastic"):

2018-05-02 19:22:55,326 - memory_profile6_log - INFO -    373     86.9 MiB      0.0 MiB       bq_client = client

2018-05-02 19:22:55,328 - memory_profile6_log - INFO -    374     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 19:22:55,332 - memory_profile6_log - INFO -    375                             

2018-05-02 19:22:55,332 - memory_profile6_log - INFO -    376     86.9 MiB      0.0 MiB       datalist = []

2018-05-02 19:22:55,334 - memory_profile6_log - INFO -    377     86.9 MiB      0.0 MiB       datalist_hist = []

2018-05-02 19:22:55,336 - memory_profile6_log - INFO -    378                             

2018-05-02 19:22:55,338 - memory_profile6_log - INFO -    379     86.9 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-05-02 19:22:55,341 - memory_profile6_log - INFO -    380    141.7 MiB      0.0 MiB       for ndate in date_generated:

2018-05-02 19:22:55,342 - memory_profile6_log - INFO -    381    140.6 MiB     53.7 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-05-02 19:22:55,344 - memory_profile6_log - INFO -    382    140.6 MiB      0.0 MiB           if tframe is not None:

2018-05-02 19:22:55,345 - memory_profile6_log - INFO -    383    140.6 MiB      0.0 MiB               if not tframe.empty:

2018-05-02 19:22:55,346 - memory_profile6_log - INFO -    384    140.6 MiB      0.0 MiB                   if loadfrom.strip().lower() == 'datastore':

2018-05-02 19:22:55,348 - memory_profile6_log - INFO -    385                                                 X_split = np.array_split(tframe, 5)

2018-05-02 19:22:55,349 - memory_profile6_log - INFO -    386                                                 logger.info("loading history data from datastore...")

2018-05-02 19:22:55,351 - memory_profile6_log - INFO -    387                                                 logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 19:22:55,352 - memory_profile6_log - INFO -    388                                                 logger.info("Appending history data...")

2018-05-02 19:22:55,354 - memory_profile6_log - INFO -    389                                                 for ix in range(len(X_split)):

2018-05-02 19:22:55,354 - memory_profile6_log - INFO -    390                                                     # ~ loading history

2018-05-02 19:22:55,361 - memory_profile6_log - INFO -    391                                                     """

2018-05-02 19:22:55,362 - memory_profile6_log - INFO -    392                                                         disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-05-02 19:22:55,365 - memory_profile6_log - INFO -    393                                                     """

2018-05-02 19:22:55,365 - memory_profile6_log - INFO -    394                                                     logger.info("processing batch-%d", ix)

2018-05-02 19:22:55,368 - memory_profile6_log - INFO -    395                                                     # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-05-02 19:22:55,368 - memory_profile6_log - INFO -    396                                                     logger.info("creating list history data...")

2018-05-02 19:22:55,371 - memory_profile6_log - INFO -    397                                                     #lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-05-02 19:22:55,372 - memory_profile6_log - INFO -    398                                                     lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 19:22:55,375 - memory_profile6_log - INFO -    399                             

2018-05-02 19:22:55,375 - memory_profile6_log - INFO -    400                                                     logger.info("call history data...")

2018-05-02 19:22:55,377 - memory_profile6_log - INFO -    401                                                     h_frame = mh.loadDSHistory(lhistory)

2018-05-02 19:22:55,378 - memory_profile6_log - INFO -    402                             

2018-05-02 19:22:55,378 - memory_profile6_log - INFO -    403                                                     # me = os.getpid()

2018-05-02 19:22:55,380 - memory_profile6_log - INFO -    404                                                     # kill_proc_tree(me)

2018-05-02 19:22:55,382 - memory_profile6_log - INFO -    405                             

2018-05-02 19:22:55,384 - memory_profile6_log - INFO -    406                                                     logger.info("done collecting history data, appending now...")

2018-05-02 19:22:55,384 - memory_profile6_log - INFO -    407                                                     for m in h_frame:

2018-05-02 19:22:55,385 - memory_profile6_log - INFO -    408                                                         if m is not None:

2018-05-02 19:22:55,387 - memory_profile6_log - INFO -    409                                                             if len(m) > 0:

2018-05-02 19:22:55,388 - memory_profile6_log - INFO -    410                                                                 datalist_hist.append(pd.DataFrame(m))

2018-05-02 19:22:55,388 - memory_profile6_log - INFO -    411                                                     del h_frame

2018-05-02 19:22:55,390 - memory_profile6_log - INFO -    412                                                     del lhistory

2018-05-02 19:22:55,392 - memory_profile6_log - INFO -    413                             

2018-05-02 19:22:55,394 - memory_profile6_log - INFO -    414                                                 logger.info("Appending training data...")

2018-05-02 19:22:55,398 - memory_profile6_log - INFO -    415                                                 datalist.append(tframe)

2018-05-02 19:22:55,398 - memory_profile6_log - INFO -    416    140.6 MiB      0.0 MiB                   elif loadfrom.strip().lower() == 'elastic':

2018-05-02 19:22:55,400 - memory_profile6_log - INFO -    417    140.7 MiB      0.1 MiB                       X_split = np.array_split(tframe, 25)

2018-05-02 19:22:55,401 - memory_profile6_log - INFO -    418    140.7 MiB      0.0 MiB                       logger.info("loading history data from elastic...")

2018-05-02 19:22:55,404 - memory_profile6_log - INFO -    419    140.7 MiB      0.0 MiB                       logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 19:22:55,404 - memory_profile6_log - INFO -    420    140.7 MiB      0.0 MiB                       logger.info("Appending history data...")

2018-05-02 19:22:55,407 - memory_profile6_log - INFO -    421                                                 

2018-05-02 19:22:55,407 - memory_profile6_log - INFO -    422    141.7 MiB     -0.0 MiB                       for ix in range(len(X_split)):

2018-05-02 19:22:55,408 - memory_profile6_log - INFO -    423    141.7 MiB      0.4 MiB                           lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 19:22:55,410 - memory_profile6_log - INFO -    424    141.7 MiB     -0.0 MiB                           logger.info("call %d history data...", len(lhistory))

2018-05-02 19:22:55,411 - memory_profile6_log - INFO -    425    141.7 MiB     -0.0 MiB                           inside_data = mh.loadESHistory(lhistory, es,

2018-05-02 19:22:55,414 - memory_profile6_log - INFO -    426    141.7 MiB     -0.0 MiB                                                          esindex_name='fitted_hist_index',

2018-05-02 19:22:55,417 - memory_profile6_log - INFO -    427    141.7 MiB      0.3 MiB                                                          estype_name='fitted_hist_type')

2018-05-02 19:22:55,417 - memory_profile6_log - INFO -    428                                                     # split back the user_id and topic_id

2018-05-02 19:22:55,418 - memory_profile6_log - INFO -    429    141.7 MiB      0.0 MiB                           inside_data[['user_id','topic_id']] = inside_data.uid_topid.str.split('_', expand=True)

2018-05-02 19:22:55,420 - memory_profile6_log - INFO -    430    141.7 MiB      0.2 MiB                           inside_data = inside_data[["user_id","topic_id", "pt_posterior_x_Nt", "smoothed_pt_posterior", "p0_cat_ci", "sigma_Nt"]]

2018-05-02 19:22:55,421 - memory_profile6_log - INFO -    431    141.7 MiB     -0.0 MiB                           if not inside_data.empty:

2018-05-02 19:22:55,421 - memory_profile6_log - INFO -    432    141.7 MiB     -0.0 MiB                               datalist_hist.append(inside_data)

2018-05-02 19:22:55,423 - memory_profile6_log - INFO -    433    141.7 MiB     -0.0 MiB                               del inside_data

2018-05-02 19:22:55,426 - memory_profile6_log - INFO -    434                                                 

2018-05-02 19:22:55,427 - memory_profile6_log - INFO -    435    141.7 MiB     -0.0 MiB                       logger.info("Appending training data...")

2018-05-02 19:22:55,430 - memory_profile6_log - INFO -    436    141.7 MiB      0.0 MiB                       datalist.append(tframe)

2018-05-02 19:22:55,433 - memory_profile6_log - INFO -    437                                             else:

2018-05-02 19:22:55,434 - memory_profile6_log - INFO -    438                                                 logger.info("Unknows source is selected !")

2018-05-02 19:22:55,437 - memory_profile6_log - INFO -    439                                                 break

2018-05-02 19:22:55,437 - memory_profile6_log - INFO -    440                                     else: 

2018-05-02 19:22:55,438 - memory_profile6_log - INFO -    441                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-05-02 19:22:55,440 - memory_profile6_log - INFO -    442    141.7 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-05-02 19:22:55,440 - memory_profile6_log - INFO -    443    141.7 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-05-02 19:22:55,441 - memory_profile6_log - INFO -    444                             

2018-05-02 19:22:55,443 - memory_profile6_log - INFO -    445    141.7 MiB      0.0 MiB       return datalist, datalist_hist

2018-05-02 19:22:55,444 - memory_profile6_log - INFO - 


2018-05-02 19:22:55,467 - memory_profile6_log - INFO - len of big_frame_hist: 20000
2018-05-02 19:22:55,479 - memory_profile6_log - INFO - size of big_frame_hist: 5.49 MB
2018-05-02 19:22:55,493 - memory_profile6_log - INFO - size of big_frame: 5.05 MB
2018-05-02 19:22:55,493 - memory_profile6_log - INFO - len of big_frame: 20000
2018-05-02 19:22:55,499 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-05-02 19:23:05,917 - memory_profile6_log - INFO - size of df: 4.93 MB
2018-05-02 19:23:05,918 - memory_profile6_log - INFO - getting total: 20000 training data(current date interest)
2018-05-02 19:23:05,931 - memory_profile6_log - INFO - size of current_frame: 5.08 MB
2018-05-02 19:23:05,933 - memory_profile6_log - INFO - loading time of: 40000 total genuine-current interest data ~ take 65.741s
2018-05-02 19:23:05,934 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 19:23:05,940 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 19:23:05,941 - memory_profile6_log - INFO - ================================================

2018-05-02 19:23:05,943 - memory_profile6_log - INFO -    447     86.8 MiB     86.8 MiB   @profile

2018-05-02 19:23:05,946 - memory_profile6_log - INFO -    448                             def preprocess(cpu, cd, query_fit, date_generated):

2018-05-02 19:23:05,946 - memory_profile6_log - INFO -    449     86.9 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-05-02 19:23:05,947 - memory_profile6_log - INFO -    450     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 19:23:05,947 - memory_profile6_log - INFO -    451                             

2018-05-02 19:23:05,950 - memory_profile6_log - INFO -    452                                 # ~~~ Begin collecting data ~~~

2018-05-02 19:23:05,953 - memory_profile6_log - INFO -    453     86.9 MiB      0.0 MiB       t0 = time.time()

2018-05-02 19:23:05,953 - memory_profile6_log - INFO -    454    141.7 MiB     54.8 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-05-02 19:23:05,956 - memory_profile6_log - INFO -    455    141.7 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-05-02 19:23:05,957 - memory_profile6_log - INFO -    456                                     logger.info("Training cannot be empty..")

2018-05-02 19:23:05,957 - memory_profile6_log - INFO -    457                                     return False

2018-05-02 19:23:05,960 - memory_profile6_log - INFO -    458    141.7 MiB      0.0 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-05-02 19:23:05,960 - memory_profile6_log - INFO -    459    141.7 MiB      0.0 MiB       logger.info("len of big_frame_hist: %s", len(big_frame_hist))

2018-05-02 19:23:05,963 - memory_profile6_log - INFO -    460    141.7 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-05-02 19:23:05,963 - memory_profile6_log - INFO -    461                             

2018-05-02 19:23:05,964 - memory_profile6_log - INFO -    462    141.7 MiB      0.0 MiB       big_frame = pd.concat(datalist)

2018-05-02 19:23:05,964 - memory_profile6_log - INFO -    463    141.7 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-05-02 19:23:05,967 - memory_profile6_log - INFO -    464    141.7 MiB      0.0 MiB       logger.info("len of big_frame: %s", len(big_frame))

2018-05-02 19:23:05,967 - memory_profile6_log - INFO -    465    141.7 MiB      0.0 MiB       del datalist

2018-05-02 19:23:05,967 - memory_profile6_log - INFO -    466                             

2018-05-02 19:23:05,969 - memory_profile6_log - INFO -    467    141.7 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 19:23:05,973 - memory_profile6_log - INFO -    468                             

2018-05-02 19:23:05,974 - memory_profile6_log - INFO -    469                                 # ~ get current news interest ~

2018-05-02 19:23:05,974 - memory_profile6_log - INFO -    470    141.7 MiB      0.0 MiB       if not cd:

2018-05-02 19:23:05,976 - memory_profile6_log - INFO -    471    141.7 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-05-02 19:23:05,976 - memory_profile6_log - INFO -    472    147.9 MiB      6.2 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-05-02 19:23:05,979 - memory_profile6_log - INFO -    473    147.9 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-05-02 19:23:05,980 - memory_profile6_log - INFO -    474                                 else:

2018-05-02 19:23:05,980 - memory_profile6_log - INFO -    475                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-05-02 19:23:05,980 - memory_profile6_log - INFO -    476                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date) LIMIT 20000"

2018-05-02 19:23:05,983 - memory_profile6_log - INFO -    477                             

2018-05-02 19:23:05,989 - memory_profile6_log - INFO -    478                                     # safe handling of query parameter

2018-05-02 19:23:05,990 - memory_profile6_log - INFO -    479                                     query_params = [

2018-05-02 19:23:05,994 - memory_profile6_log - INFO -    480                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-05-02 19:23:05,996 - memory_profile6_log - INFO -    481                                     ]

2018-05-02 19:23:05,996 - memory_profile6_log - INFO -    482                             

2018-05-02 19:23:05,997 - memory_profile6_log - INFO -    483                                     job_config.query_parameters = query_params

2018-05-02 19:23:05,999 - memory_profile6_log - INFO -    484                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-05-02 19:23:06,000 - memory_profile6_log - INFO -    485                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-05-02 19:23:06,002 - memory_profile6_log - INFO -    486                             

2018-05-02 19:23:06,006 - memory_profile6_log - INFO -    487    147.9 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-05-02 19:23:06,006 - memory_profile6_log - INFO -    488    147.9 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-05-02 19:23:06,009 - memory_profile6_log - INFO -    489    147.9 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-05-02 19:23:06,009 - memory_profile6_log - INFO -    490    147.9 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-05-02 19:23:06,010 - memory_profile6_log - INFO -    491    147.9 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 19:23:06,013 - memory_profile6_log - INFO -    492    147.9 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-05-02 19:23:06,013 - memory_profile6_log - INFO -    493                             

2018-05-02 19:23:06,019 - memory_profile6_log - INFO -    494    147.9 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-05-02 19:23:06,020 - memory_profile6_log - INFO - 


2018-05-02 19:23:06,030 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-05-02 19:23:06,046 - memory_profile6_log - INFO - train on: 20000 total genuine interest data(D(u, t))
2018-05-02 19:23:06,046 - memory_profile6_log - INFO - transform on: 20000 total current data(D(t))
2018-05-02 19:23:06,049 - memory_profile6_log - INFO - apply on: 20000 total history...
2018-05-02 19:23:06,111 - memory_profile6_log - INFO - len of uniques_fit_hist:20000
2018-05-02 19:23:06,121 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:5434
2018-05-02 19:23:06,193 - memory_profile6_log - INFO - self.sum_all_nt before concate:

2018-05-02 19:23:06,211 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 19:23:06,213 - memory_profile6_log - INFO - 

2018-05-02 19:23:06,230 - memory_profile6_log - INFO - self.sum_all_nt after concate:

2018-05-02 19:23:06,239 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 19:23:06,240 - memory_profile6_log - INFO - 

2018-05-02 19:23:06,283 - memory_profile6_log - INFO - Len of model_fit: 20000
2018-05-02 19:23:06,285 - memory_profile6_log - INFO - Len of df_dut: 20000
2018-05-02 19:23:06,436 - memory_profile6_log - INFO - Len of fitted_models on main class: 20000
2018-05-02 19:23:06,437 - memory_profile6_log - INFO - 

2018-05-02 19:23:06,437 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-05-02 19:23:06,438 - memory_profile6_log - INFO - 

2018-05-02 19:23:06,440 - memory_profile6_log - INFO - fitted_model_hist:

2018-05-02 19:23:06,440 - memory_profile6_log - INFO - 

2018-05-02 19:23:06,454 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 19:23:06,454 - memory_profile6_log - INFO - 

2018-05-02 19:23:06,466 - memory_profile6_log - INFO - len of fitted models after concat: 40000
2018-05-02 19:23:06,467 - memory_profile6_log - INFO - 

2018-05-02 19:23:06,471 - memory_profile6_log - INFO - Recalculating fitted models...
2018-05-02 19:23:06,473 - memory_profile6_log - INFO - 

2018-05-02 19:23:06,509 - memory_profile6_log - INFO - fitted_models after cobine:

2018-05-02 19:23:06,509 - memory_profile6_log - INFO - 

2018-05-02 19:23:06,523 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 19:23:06,523 - memory_profile6_log - INFO - 

2018-05-02 19:23:06,525 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 20000
2018-05-02 19:23:06,530 - memory_profile6_log - INFO - 

2018-05-02 19:23:11,684 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-05-02 19:23:11,703 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, topic_id, pt_posterior_x_Nt, smoothed_pt_posterior, p0_cat_ci, sigma_Nt, p0_posterior, is_general, rank]
Index: []
2018-05-02 19:23:11,704 - memory_profile6_log - INFO - 

2018-05-02 19:23:11,706 - memory_profile6_log - INFO - Len of model_transform: 7846
2018-05-02 19:23:11,707 - memory_profile6_log - INFO - Len of df_dt: 20000
2018-05-02 19:23:11,707 - memory_profile6_log - INFO - Total train time: 5.648s
2018-05-02 19:23:11,709 - memory_profile6_log - INFO - memory left before cleaning: 86.000 percent memory...
2018-05-02 19:23:11,710 - memory_profile6_log - INFO - cleaning up some objects...
2018-05-02 19:23:11,710 - memory_profile6_log - INFO - deleting df_dut...
2018-05-02 19:23:11,711 - memory_profile6_log - INFO - deleting df_dt...
2018-05-02 19:23:11,713 - memory_profile6_log - INFO - deleting df_input...
2018-05-02 19:23:11,713 - memory_profile6_log - INFO - deleting df_input_X...
2018-05-02 19:23:11,717 - memory_profile6_log - INFO - deleting df_current...
2018-05-02 19:23:11,717 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-05-02 19:23:11,720 - memory_profile6_log - INFO - deleting model_fit...
2018-05-02 19:23:11,721 - memory_profile6_log - INFO - deleting result...
2018-05-02 19:23:11,729 - memory_profile6_log - INFO - list(model_transformsv) before rename: 
2018-05-02 19:23:11,730 - memory_profile6_log - INFO -  
2018-05-02 19:23:11,730 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'is_general', 'p0_posterior', 'rank', 'date']
2018-05-02 19:23:11,733 - memory_profile6_log - INFO - 

2018-05-02 19:23:11,733 - memory_profile6_log - INFO - 7846
2018-05-02 19:23:11,736 - memory_profile6_log - INFO - 

2018-05-02 19:23:11,739 - memory_profile6_log - INFO - list(model_transformsv) after rename: 
2018-05-02 19:23:11,740 - memory_profile6_log - INFO -  
2018-05-02 19:23:11,742 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'topic_is_general', 'interest_score', 'interest_rank', 'interest_score_created_at']
2018-05-02 19:23:11,743 - memory_profile6_log - INFO - 

2018-05-02 19:23:11,744 - memory_profile6_log - INFO - 7846
2018-05-02 19:23:11,746 - memory_profile6_log - INFO - 

2018-05-02 19:23:11,750 - memory_profile6_log - INFO - deleting model_transform...
2018-05-02 19:23:11,750 - memory_profile6_log - INFO - memory left after cleaning: 86.000 percent memory...
2018-05-02 19:23:11,752 - memory_profile6_log - INFO - Begin saving trained data...
2018-05-02 19:23:11,753 - memory_profile6_log - INFO - Using ElasticSearch as storage...
2018-05-02 19:23:11,775 - memory_profile6_log - INFO - Saving total data: 7846
2018-05-02 19:23:11,776 - memory_profile6_log - INFO - Len of X_split for batch save model_transformsv: 15
2018-05-02 19:23:11,776 - memory_profile6_log - INFO - processing batch-0
2018-05-02 19:23:16,752 - memory_profile6_log - INFO - processing batch-1
2018-05-02 19:23:20,551 - memory_profile6_log - INFO - processing batch-2
2018-05-02 19:23:25,779 - memory_profile6_log - INFO - processing batch-3
2018-05-02 19:23:31,187 - memory_profile6_log - INFO - processing batch-4
2018-05-02 19:23:40,711 - memory_profile6_log - INFO - processing batch-5
2018-05-02 19:23:49,374 - memory_profile6_log - INFO - processing batch-6
2018-05-02 19:23:55,634 - memory_profile6_log - INFO - processing batch-7
2018-05-02 19:24:00,220 - memory_profile6_log - INFO - processing batch-8
2018-05-02 19:24:05,903 - memory_profile6_log - INFO - processing batch-9
2018-05-02 19:24:10,128 - memory_profile6_log - INFO - processing batch-10
2018-05-02 19:24:17,905 - memory_profile6_log - INFO - processing batch-11
2018-05-02 19:24:23,898 - memory_profile6_log - INFO - processing batch-12
2018-05-02 19:24:28,638 - memory_profile6_log - INFO - processing batch-13
2018-05-02 19:24:33,710 - memory_profile6_log - INFO - processing batch-14
2018-05-02 19:24:38,526 - memory_profile6_log - INFO - Saving fitted_models as history to Elasticsearch...
2018-05-02 19:24:38,582 - memory_profile6_log - INFO - Saving total data: 20000
2018-05-02 19:24:38,584 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 25
2018-05-02 19:24:38,585 - memory_profile6_log - INFO - processing batch-0
2018-05-02 19:24:45,280 - memory_profile6_log - INFO - processing batch-1
2018-05-02 19:24:50,980 - memory_profile6_log - INFO - processing batch-2
2018-05-02 19:25:00,052 - memory_profile6_log - INFO - processing batch-3
2018-05-02 19:25:08,857 - memory_profile6_log - INFO - processing batch-4
2018-05-02 19:25:13,743 - memory_profile6_log - INFO - processing batch-5
2018-05-02 19:25:20,898 - memory_profile6_log - INFO - processing batch-6
2018-05-02 19:25:27,201 - memory_profile6_log - INFO - processing batch-7
2018-05-02 19:25:34,236 - memory_profile6_log - INFO - processing batch-8
2018-05-02 19:25:39,058 - memory_profile6_log - INFO - processing batch-9
2018-05-02 19:25:47,250 - memory_profile6_log - INFO - processing batch-10
2018-05-02 19:25:55,578 - memory_profile6_log - INFO - processing batch-11
2018-05-02 19:26:04,552 - memory_profile6_log - INFO - processing batch-12
2018-05-02 19:26:12,880 - memory_profile6_log - INFO - processing batch-13
2018-05-02 19:26:17,555 - memory_profile6_log - INFO - processing batch-14
2018-05-02 19:26:24,086 - memory_profile6_log - INFO - processing batch-15
2018-05-02 19:26:33,180 - memory_profile6_log - INFO - processing batch-16
2018-05-02 19:26:43,575 - memory_profile6_log - INFO - processing batch-17
2018-05-02 19:26:55,390 - memory_profile6_log - INFO - processing batch-18
2018-05-02 19:27:02,552 - memory_profile6_log - INFO - processing batch-19
2018-05-02 19:27:08,763 - memory_profile6_log - INFO - processing batch-20
2018-05-02 19:27:13,992 - memory_profile6_log - INFO - processing batch-21
2018-05-02 19:27:21,940 - memory_profile6_log - INFO - processing batch-22
2018-05-02 19:27:29,309 - memory_profile6_log - INFO - processing batch-23
2018-05-02 19:27:36,292 - memory_profile6_log - INFO - processing batch-24
2018-05-02 19:27:46,671 - memory_profile6_log - INFO - deleting BR...
2018-05-02 19:27:46,673 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-05-02 19:27:46,674 - memory_profile6_log - INFO - deleting fitted_models_sigmant...
2018-05-02 19:27:46,674 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 19:27:46,674 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 19:27:46,676 - memory_profile6_log - INFO - ================================================

2018-05-02 19:27:46,677 - memory_profile6_log - INFO -    125    147.9 MiB    147.9 MiB   @profile

2018-05-02 19:27:46,677 - memory_profile6_log - INFO -    126                             def main(df_input, df_current, df_hist,

2018-05-02 19:27:46,677 - memory_profile6_log - INFO -    127                                      current_date, G, project_id,

2018-05-02 19:27:46,680 - memory_profile6_log - INFO -    128                                      savetrain=False, multproc=True,

2018-05-02 19:27:46,680 - memory_profile6_log - INFO -    129                                      threshold=0, start_date=None, end_date=None,

2018-05-02 19:27:46,683 - memory_profile6_log - INFO -    130                                      saveto="datastore", fitted_models_hist=None):

2018-05-02 19:27:46,683 - memory_profile6_log - INFO -    131                                 """

2018-05-02 19:27:46,684 - memory_profile6_log - INFO -    132                                     Main Process

2018-05-02 19:27:46,684 - memory_profile6_log - INFO -    133                                 """

2018-05-02 19:27:46,684 - memory_profile6_log - INFO -    134                                 # ~ Data Preprocessing ~

2018-05-02 19:27:46,686 - memory_profile6_log - INFO -    135                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-05-02 19:27:46,687 - memory_profile6_log - INFO -    136                                 # D(u, t)

2018-05-02 19:27:46,687 - memory_profile6_log - INFO -    137    147.9 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-05-02 19:27:46,688 - memory_profile6_log - INFO -    138    147.9 MiB      0.0 MiB       df_dut = df_input.copy(deep=True)

2018-05-02 19:27:46,691 - memory_profile6_log - INFO -    139    147.9 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 19:27:46,693 - memory_profile6_log - INFO -    140                             

2018-05-02 19:27:46,694 - memory_profile6_log - INFO -    141                                 # D(t)

2018-05-02 19:27:46,694 - memory_profile6_log - INFO -    142    147.9 MiB      0.0 MiB       df_dt = df_current.copy(deep=True)

2018-05-02 19:27:46,696 - memory_profile6_log - INFO -    143    147.9 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 19:27:46,697 - memory_profile6_log - INFO -    144                             

2018-05-02 19:27:46,697 - memory_profile6_log - INFO -    145                                 # ~~~~~~ Begin train ~~~~~~

2018-05-02 19:27:46,697 - memory_profile6_log - INFO -    146    147.9 MiB      0.0 MiB       t0 = time.time()

2018-05-02 19:27:46,698 - memory_profile6_log - INFO -    147    147.9 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-05-02 19:27:46,698 - memory_profile6_log - INFO -    148    147.9 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-05-02 19:27:46,700 - memory_profile6_log - INFO -    149    147.9 MiB      0.0 MiB       logger.info("apply on: %d total history...", len(df_hist))

2018-05-02 19:27:46,700 - memory_profile6_log - INFO -    150                             

2018-05-02 19:27:46,700 - memory_profile6_log - INFO -    151                                 # instantiace class

2018-05-02 19:27:46,703 - memory_profile6_log - INFO -    152    147.9 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-05-02 19:27:46,703 - memory_profile6_log - INFO -    153                             

2018-05-02 19:27:46,706 - memory_profile6_log - INFO -    154                                 # ~~ Fit ~~

2018-05-02 19:27:46,707 - memory_profile6_log - INFO -    155                                 #   handling genuine news interest < current date

2018-05-02 19:27:46,709 - memory_profile6_log - INFO -    156    148.1 MiB      0.2 MiB       NB = BR.processX(df_dut)

2018-05-02 19:27:46,709 - memory_profile6_log - INFO -    157                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-05-02 19:27:46,710 - memory_profile6_log - INFO -    158                                 #   nanti dipindah ke class train utama

2018-05-02 19:27:46,710 - memory_profile6_log - INFO -    159    148.2 MiB      0.1 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-05-02 19:27:46,710 - memory_profile6_log - INFO -    160                                 """

2018-05-02 19:27:46,710 - memory_profile6_log - INFO -    161                                     num_y = total global click for category=ci on periode t

2018-05-02 19:27:46,713 - memory_profile6_log - INFO -    162                                     num_x = total click from user_U for category=ci on periode t

2018-05-02 19:27:46,716 - memory_profile6_log - INFO -    163                                 """

2018-05-02 19:27:46,716 - memory_profile6_log - INFO -    164    148.2 MiB      0.0 MiB       fitby_sigmant = False

2018-05-02 19:27:46,717 - memory_profile6_log - INFO -    165    148.2 MiB      0.0 MiB       uniques_fit_hist = None

2018-05-02 19:27:46,717 - memory_profile6_log - INFO -    166    148.2 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-05-02 19:27:46,717 - memory_profile6_log - INFO -    167    148.2 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-05-02 19:27:46,719 - memory_profile6_log - INFO -    168    148.2 MiB      0.0 MiB                            'is_general']]

2018-05-02 19:27:46,719 - memory_profile6_log - INFO -    169                             

2018-05-02 19:27:46,720 - memory_profile6_log - INFO -    170                                 # get sigma_Nt from fitted_models_hist

2018-05-02 19:27:46,720 - memory_profile6_log - INFO -    171    148.2 MiB      0.0 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-05-02 19:27:46,720 - memory_profile6_log - INFO -    172    148.2 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-05-02 19:27:46,721 - memory_profile6_log - INFO -    173    148.3 MiB      0.0 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-05-02 19:27:46,724 - memory_profile6_log - INFO -    174    148.3 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-05-02 19:27:46,726 - memory_profile6_log - INFO -    175                             

2018-05-02 19:27:46,726 - memory_profile6_log - INFO -    176                                 # begin fit

2018-05-02 19:27:46,727 - memory_profile6_log - INFO -    177    148.3 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-05-02 19:27:46,729 - memory_profile6_log - INFO -    178    148.3 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-05-02 19:27:46,730 - memory_profile6_log - INFO -    179    150.2 MiB      1.9 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-05-02 19:27:46,730 - memory_profile6_log - INFO -    180    150.2 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-05-02 19:27:46,730 - memory_profile6_log - INFO -    181    150.2 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-05-02 19:27:46,730 - memory_profile6_log - INFO -    182                                 # print model_fit[['num_x', 'num_x', 'sigma_Nt', 'date_all_click']].loc[model_fit['user_id']=="1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157"].head(5)

2018-05-02 19:27:46,732 - memory_profile6_log - INFO -    183                             

2018-05-02 19:27:46,732 - memory_profile6_log - INFO -    184                                 # ~~ and Transform ~~

2018-05-02 19:27:46,733 - memory_profile6_log - INFO -    185                                 #   handling current news interest == current date

2018-05-02 19:27:46,733 - memory_profile6_log - INFO -    186    150.2 MiB      0.0 MiB       if df_dt.empty:

2018-05-02 19:27:46,736 - memory_profile6_log - INFO -    187                                     print "Cek your df_dt, cannot be emtpy!!"

2018-05-02 19:27:46,737 - memory_profile6_log - INFO -    188                                     return None

2018-05-02 19:27:46,739 - memory_profile6_log - INFO -    189    150.9 MiB      0.7 MiB       NB = BR.processX(df_dt)

2018-05-02 19:27:46,740 - memory_profile6_log - INFO -    190    150.9 MiB      0.0 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-05-02 19:27:46,740 - memory_profile6_log - INFO -    191                             

2018-05-02 19:27:46,740 - memory_profile6_log - INFO -    192    150.9 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-05-02 19:27:46,742 - memory_profile6_log - INFO -    193    151.1 MiB      0.2 MiB                            'num_x', 'num_y', 'is_general']]

2018-05-02 19:27:46,743 - memory_profile6_log - INFO -    194                             

2018-05-02 19:27:46,743 - memory_profile6_log - INFO -    195                                 # print "model_fit dtypes:\n", model_fit.dtypes

2018-05-02 19:27:46,743 - memory_profile6_log - INFO -    196                                 # print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-05-02 19:27:46,743 - memory_profile6_log - INFO -    197    151.1 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-05-02 19:27:46,746 - memory_profile6_log - INFO -    198    151.1 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-05-02 19:27:46,750 - memory_profile6_log - INFO -    199    151.1 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-05-02 19:27:46,750 - memory_profile6_log - INFO -    200    151.1 MiB      0.0 MiB                                                                                           "topic_id", "user_id"]],

2018-05-02 19:27:46,752 - memory_profile6_log - INFO -    201    154.9 MiB      3.7 MiB                                                     verbose=False)

2018-05-02 19:27:46,752 - memory_profile6_log - INFO -    202                             

2018-05-02 19:27:46,753 - memory_profile6_log - INFO -    203                                 # ~~~ filter is general and specific topic ~~~

2018-05-02 19:27:46,753 - memory_profile6_log - INFO -    204                                 # the idea is just we need to rerank every topic according

2018-05-02 19:27:46,755 - memory_profile6_log - INFO -    205                                 #    to user_id and and is_general by p0_posterior

2018-05-02 19:27:46,755 - memory_profile6_log - INFO -    206    154.9 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-05-02 19:27:46,756 - memory_profile6_log - INFO -    207    154.9 MiB      0.0 MiB                                     'is_general']].groupby(['topic_id',

2018-05-02 19:27:46,756 - memory_profile6_log - INFO -    208    154.9 MiB      0.0 MiB                                                             'is_general']

2018-05-02 19:27:46,759 - memory_profile6_log - INFO -    209                                                                                      ).size().to_frame().reset_index()

2018-05-02 19:27:46,760 - memory_profile6_log - INFO -    210                             

2018-05-02 19:27:46,760 - memory_profile6_log - INFO -    211                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-05-02 19:27:46,762 - memory_profile6_log - INFO -    212                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-05-02 19:27:46,763 - memory_profile6_log - INFO -    213    154.9 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-05-02 19:27:46,763 - memory_profile6_log - INFO -    214                             

2018-05-02 19:27:46,763 - memory_profile6_log - INFO -    215                                 # ~ start by provide rank for each topic type ~

2018-05-02 19:27:46,763 - memory_profile6_log - INFO -    216    160.4 MiB      5.5 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-05-02 19:27:46,765 - memory_profile6_log - INFO -    217    158.6 MiB     -1.7 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-05-02 19:27:46,765 - memory_profile6_log - INFO -    218                             

2018-05-02 19:27:46,766 - memory_profile6_log - INFO -    219                                 # ~ set threshold to filter output

2018-05-02 19:27:46,766 - memory_profile6_log - INFO -    220    158.6 MiB      0.0 MiB       if threshold > 0:

2018-05-02 19:27:46,766 - memory_profile6_log - INFO -    221    158.6 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-05-02 19:27:46,766 - memory_profile6_log - INFO -    222    158.6 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-05-02 19:27:46,766 - memory_profile6_log - INFO -    223    158.6 MiB      0.0 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-05-02 19:27:46,772 - memory_profile6_log - INFO -    224                             

2018-05-02 19:27:46,772 - memory_profile6_log - INFO -    225    158.6 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 19:27:46,775 - memory_profile6_log - INFO -    226    158.6 MiB      0.0 MiB       print model_transform[model_transform['user_id']=='1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157']

2018-05-02 19:27:46,776 - memory_profile6_log - INFO -    227    158.6 MiB      0.0 MiB       logger.info("Len of model_transform: %d", len(model_transform))

2018-05-02 19:27:46,778 - memory_profile6_log - INFO -    228    158.6 MiB      0.0 MiB       logger.info("Len of df_dt: %d", len(df_dt))

2018-05-02 19:27:46,779 - memory_profile6_log - INFO -    229    158.6 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-05-02 19:27:46,779 - memory_profile6_log - INFO -    230                             

2018-05-02 19:27:46,782 - memory_profile6_log - INFO -    231    158.6 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-05-02 19:27:46,782 - memory_profile6_log - INFO -    232                             

2018-05-02 19:27:46,783 - memory_profile6_log - INFO -    233    158.6 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-05-02 19:27:46,785 - memory_profile6_log - INFO -    234    158.6 MiB      0.0 MiB       del df_dut

2018-05-02 19:27:46,786 - memory_profile6_log - INFO -    235    158.6 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-05-02 19:27:46,786 - memory_profile6_log - INFO -    236    158.6 MiB      0.0 MiB       del df_dt

2018-05-02 19:27:46,786 - memory_profile6_log - INFO -    237    158.6 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-05-02 19:27:46,788 - memory_profile6_log - INFO -    238    158.6 MiB      0.0 MiB       del df_input

2018-05-02 19:27:46,788 - memory_profile6_log - INFO -    239    158.6 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-05-02 19:27:46,789 - memory_profile6_log - INFO -    240    158.6 MiB      0.0 MiB       del df_input_X

2018-05-02 19:27:46,789 - memory_profile6_log - INFO -    241    158.6 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-05-02 19:27:46,792 - memory_profile6_log - INFO -    242    158.6 MiB      0.0 MiB       del df_current

2018-05-02 19:27:46,793 - memory_profile6_log - INFO -    243    158.6 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-05-02 19:27:46,793 - memory_profile6_log - INFO -    244    158.6 MiB      0.0 MiB       del map_topic_isgeneral

2018-05-02 19:27:46,796 - memory_profile6_log - INFO -    245    158.6 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-05-02 19:27:46,796 - memory_profile6_log - INFO -    246    150.3 MiB     -8.4 MiB       del model_fit

2018-05-02 19:27:46,796 - memory_profile6_log - INFO -    247    150.3 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-05-02 19:27:46,798 - memory_profile6_log - INFO -    248    150.3 MiB      0.0 MiB       del result

2018-05-02 19:27:46,798 - memory_profile6_log - INFO -    249    150.3 MiB      0.0 MiB       logger.info("deleting result...")

2018-05-02 19:27:46,798 - memory_profile6_log - INFO -    250                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 19:27:46,799 - memory_profile6_log - INFO -    251                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 19:27:46,799 - memory_profile6_log - INFO -    252                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 19:27:46,799 - memory_profile6_log - INFO -    253    150.3 MiB      0.0 MiB       if savetrain:

2018-05-02 19:27:46,799 - memory_profile6_log - INFO -    254    150.8 MiB      0.5 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'p0_posterior', 'rank']].copy(deep=True)

2018-05-02 19:27:46,799 - memory_profile6_log - INFO -    255    150.8 MiB      0.1 MiB           model_transformsv['date'] = current_date  # we need manually adding date, because table not support

2018-05-02 19:27:46,803 - memory_profile6_log - INFO -    256    150.8 MiB      0.0 MiB           model_transformsv['date'] = pd.to_datetime(model_transformsv['date'],

2018-05-02 19:27:46,803 - memory_profile6_log - INFO -    257    151.0 MiB      0.2 MiB                                                      format='%Y-%m-%d', errors='coerce')

2018-05-02 19:27:46,805 - memory_profile6_log - INFO -    258    151.0 MiB      0.0 MiB           print "list(model_transformsv) before rename: ", list(model_transformsv)       

2018-05-02 19:27:46,805 - memory_profile6_log - INFO -    259    151.0 MiB      0.0 MiB           print len(model_transformsv)                

2018-05-02 19:27:46,806 - memory_profile6_log - INFO -    260    151.0 MiB      0.0 MiB           model_transformsv = model_transformsv.rename(columns={'is_general': 'topic_is_general', 'p0_posterior': 'interest_score',

2018-05-02 19:27:46,806 - memory_profile6_log - INFO -    261    151.3 MiB      0.3 MiB                                                                 'rank':'interest_rank' , 'date':'interest_score_created_at'})

2018-05-02 19:27:46,808 - memory_profile6_log - INFO -    262    151.3 MiB      0.0 MiB           print "list(model_transformsv) after rename: ", list(model_transformsv)    

2018-05-02 19:27:46,808 - memory_profile6_log - INFO -    263    151.3 MiB      0.0 MiB           print len(model_transformsv) 

2018-05-02 19:27:46,809 - memory_profile6_log - INFO -    264    151.3 MiB      0.0 MiB           del model_transform

2018-05-02 19:27:46,809 - memory_profile6_log - INFO -    265    151.3 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-05-02 19:27:46,809 - memory_profile6_log - INFO -    266    151.3 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-05-02 19:27:46,809 - memory_profile6_log - INFO -    267                             

2018-05-02 19:27:46,809 - memory_profile6_log - INFO -    268    151.3 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-05-02 19:27:46,811 - memory_profile6_log - INFO -    269                                     # ~ Place your code to save the training model here ~

2018-05-02 19:27:46,815 - memory_profile6_log - INFO -    270    151.3 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-05-02 19:27:46,816 - memory_profile6_log - INFO -    271                                         logger.info("Using google datastore as storage...")

2018-05-02 19:27:46,816 - memory_profile6_log - INFO -    272                                         if multproc:

2018-05-02 19:27:46,818 - memory_profile6_log - INFO -    273                                             # ~ save transform models ~

2018-05-02 19:27:46,819 - memory_profile6_log - INFO -    274                                             logger.info("Saving main Transform model to Google DataStore...")

2018-05-02 19:27:46,819 - memory_profile6_log - INFO -    275                                             logger.info("Saving total data: %d", len(model_transformsv))

2018-05-02 19:27:46,821 - memory_profile6_log - INFO -    276                                             mh.saveDataStorePutMulti(model_transformsv)

2018-05-02 19:27:46,821 - memory_profile6_log - INFO -    277                             

2018-05-02 19:27:46,822 - memory_profile6_log - INFO -    278                                             # ~ save fitted models ~

2018-05-02 19:27:46,822 - memory_profile6_log - INFO -    279                                             logger.info("Saving fitted_models as history to Google DataStore...")

2018-05-02 19:27:46,822 - memory_profile6_log - INFO -    280                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 19:27:46,823 - memory_profile6_log - INFO -    281                                             fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-05-02 19:27:46,826 - memory_profile6_log - INFO -    282                                             logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-05-02 19:27:46,831 - memory_profile6_log - INFO -    283                                             X_split = np.array_split(fitted_models_sigmant, 10)

2018-05-02 19:27:46,832 - memory_profile6_log - INFO -    284                                             logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-05-02 19:27:46,832 - memory_profile6_log - INFO -    285                                             for ix in range(len(X_split)):

2018-05-02 19:27:46,834 - memory_profile6_log - INFO -    286                                                 logger.info("processing batch-%d", ix)

2018-05-02 19:27:46,839 - memory_profile6_log - INFO -    287                                                 mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-05-02 19:27:46,841 - memory_profile6_log - INFO -    288                             

2018-05-02 19:27:46,842 - memory_profile6_log - INFO -    289                                             del X_split

2018-05-02 19:27:46,844 - memory_profile6_log - INFO -    290                                             logger.info("deleting X_split...")

2018-05-02 19:27:46,845 - memory_profile6_log - INFO -    291                                             del save_sigma_nt

2018-05-02 19:27:46,845 - memory_profile6_log - INFO -    292                                             logger.info("deleting save_sigma_nt...")

2018-05-02 19:27:46,848 - memory_profile6_log - INFO -    293                             

2018-05-02 19:27:46,851 - memory_profile6_log - INFO -    294                                             del BR

2018-05-02 19:27:46,855 - memory_profile6_log - INFO -    295                                             logger.info("deleting BR...")

2018-05-02 19:27:46,855 - memory_profile6_log - INFO -    296                             

2018-05-02 19:27:46,857 - memory_profile6_log - INFO -    297    151.3 MiB      0.0 MiB           elif str(saveto).lower() == "elastic":

2018-05-02 19:27:46,858 - memory_profile6_log - INFO -    298    151.3 MiB      0.0 MiB               logger.info("Using ElasticSearch as storage...")

2018-05-02 19:27:46,858 - memory_profile6_log - INFO -    299    151.3 MiB      0.0 MiB               logging.info("Saving main Transform model to Elasticsearch...")

2018-05-02 19:27:46,858 - memory_profile6_log - INFO -    300                                         # print model_transformsv.head(5)

2018-05-02 19:27:46,861 - memory_profile6_log - INFO -    301                                         

2018-05-02 19:27:46,861 - memory_profile6_log - INFO -    302    151.5 MiB      0.2 MiB               X_split = np.array_split(model_transformsv, 15)

2018-05-02 19:27:46,864 - memory_profile6_log - INFO -    303    151.5 MiB      0.0 MiB               logger.info("Saving total data: %d", len(model_transformsv))

2018-05-02 19:27:46,865 - memory_profile6_log - INFO -    304    151.5 MiB      0.0 MiB               logger.info("Len of X_split for batch save model_transformsv: %d", len(X_split))

2018-05-02 19:27:46,865 - memory_profile6_log - INFO -    305    153.7 MiB     -0.0 MiB               for ix in range(len(X_split)):

2018-05-02 19:27:46,867 - memory_profile6_log - INFO -    306    153.5 MiB     -0.8 MiB                   logger.info("processing batch-%d", ix)

2018-05-02 19:27:46,867 - memory_profile6_log - INFO -    307    153.7 MiB      1.5 MiB                   mh.saveElasticS(X_split[ix])

2018-05-02 19:27:46,868 - memory_profile6_log - INFO -    308    153.7 MiB      0.0 MiB               del X_split

2018-05-02 19:27:46,868 - memory_profile6_log - INFO -    309                                         

2018-05-02 19:27:46,868 - memory_profile6_log - INFO -    310                             

2018-05-02 19:27:46,872 - memory_profile6_log - INFO -    311    153.7 MiB      0.0 MiB               logger.info("Saving fitted_models as history to Elasticsearch...")

2018-05-02 19:27:46,874 - memory_profile6_log - INFO -    312    153.7 MiB      0.0 MiB               save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 19:27:46,875 - memory_profile6_log - INFO -    313    154.4 MiB      0.6 MiB               fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-05-02 19:27:46,875 - memory_profile6_log - INFO -    314                             

2018-05-02 19:27:46,877 - memory_profile6_log - INFO -    315    154.4 MiB      0.0 MiB               fitted_models_sigmant['uid_topid'] = fitted_models_sigmant["user_id"].map(str) + "_" + fitted_models_sigmant["topic_id"].map(str)

2018-05-02 19:27:46,877 - memory_profile6_log - INFO -    316    155.4 MiB      1.0 MiB               fitted_models_sigmant = fitted_models_sigmant[["uid_topid", "pt_posterior_x_Nt", "smoothed_pt_posterior", "p0_cat_ci", "sigma_Nt"]]

2018-05-02 19:27:46,878 - memory_profile6_log - INFO -    317                               

2018-05-02 19:27:46,878 - memory_profile6_log - INFO -    318    155.9 MiB      0.5 MiB               X_split = np.array_split(fitted_models_sigmant, 25)

2018-05-02 19:27:46,878 - memory_profile6_log - INFO -    319    155.9 MiB      0.0 MiB               logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-05-02 19:27:46,880 - memory_profile6_log - INFO -    320    155.9 MiB      0.0 MiB               logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-05-02 19:27:46,880 - memory_profile6_log - INFO -    321    161.2 MiB     -0.1 MiB               for ix in range(len(X_split)):

2018-05-02 19:27:46,882 - memory_profile6_log - INFO -    322    161.2 MiB     -0.1 MiB                   logger.info("processing batch-%d", ix)

2018-05-02 19:27:46,884 - memory_profile6_log - INFO -    323    161.2 MiB      5.2 MiB                   mh.saveElasticS(X_split[ix], esindex_name="fitted_hist_index",  estype_name='fitted_hist_type')

2018-05-02 19:27:46,885 - memory_profile6_log - INFO -    324    161.2 MiB     -0.0 MiB               del X_split

2018-05-02 19:27:46,885 - memory_profile6_log - INFO -    325                                         

2018-05-02 19:27:46,887 - memory_profile6_log - INFO -    326    161.2 MiB      0.0 MiB               del BR

2018-05-02 19:27:46,888 - memory_profile6_log - INFO -    327    161.2 MiB      0.0 MiB               logger.info("deleting BR...")

2018-05-02 19:27:46,888 - memory_profile6_log - INFO -    328    161.2 MiB      0.0 MiB               del save_sigma_nt

2018-05-02 19:27:46,888 - memory_profile6_log - INFO -    329    161.2 MiB      0.0 MiB               logger.info("deleting save_sigma_nt...")

2018-05-02 19:27:46,888 - memory_profile6_log - INFO -    330    161.2 MiB      0.0 MiB               del fitted_models_sigmant

2018-05-02 19:27:46,890 - memory_profile6_log - INFO -    331    161.2 MiB      0.0 MiB               logger.info("deleting fitted_models_sigmant...")

2018-05-02 19:27:46,890 - memory_profile6_log - INFO -    332                             

2018-05-02 19:27:46,891 - memory_profile6_log - INFO -    333                                     # need save sigma_nt for daily train

2018-05-02 19:27:46,891 - memory_profile6_log - INFO -    334                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-05-02 19:27:46,894 - memory_profile6_log - INFO -    335                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-05-02 19:27:46,894 - memory_profile6_log - INFO -    336    161.2 MiB      0.0 MiB           if start_date and end_date:

2018-05-02 19:27:46,897 - memory_profile6_log - INFO -    337                                         if not fitby_sigmant:

2018-05-02 19:27:46,898 - memory_profile6_log - INFO -    338                                             logging.info("Saving sigma Nt...")

2018-05-02 19:27:46,898 - memory_profile6_log - INFO -    339                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 19:27:46,898 - memory_profile6_log - INFO -    340                                             save_sigma_nt['start_date'] = start_date

2018-05-02 19:27:46,898 - memory_profile6_log - INFO -    341                                             save_sigma_nt['end_date'] = end_date

2018-05-02 19:27:46,900 - memory_profile6_log - INFO -    342                                             print save_sigma_nt.head(5)

2018-05-02 19:27:46,900 - memory_profile6_log - INFO -    343                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-05-02 19:27:46,901 - memory_profile6_log - INFO -    344                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-05-02 19:27:46,901 - memory_profile6_log - INFO -    345    161.2 MiB      0.0 MiB       return

2018-05-02 19:27:46,901 - memory_profile6_log - INFO - 


2018-05-02 19:27:46,904 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-05-02 19:28:20,592 - memory_profile6_log - INFO - ElasticSearch host: https://9db53c7bb4f5be2d856033a9aeb6e5a5.us-central1.gcp.cloud.es.io
2018-05-02 19:28:20,595 - memory_profile6_log - INFO - ElasticSearch port: 9243
2018-05-02 19:28:20,598 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 19:28:20,601 - memory_profile6_log - INFO - date_generated: 
2018-05-02 19:28:20,601 - memory_profile6_log - INFO -  
2018-05-02 19:28:20,601 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 19, 28, 20, 598000)]
2018-05-02 19:28:20,602 - memory_profile6_log - INFO - 

2018-05-02 19:28:20,602 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 19:28:20,602 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 19:28:20,605 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 19:28:20,806 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 19:28:20,812 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 19:28:31,210 - memory_profile6_log - INFO - size of df: 5.05 MB
2018-05-02 19:28:31,211 - memory_profile6_log - INFO - getting total: 20000 training data(genuine interest) for date: 2018-05-01
2018-05-02 19:28:31,282 - memory_profile6_log - INFO - loading history data from elastic...
2018-05-02 19:28:31,283 - memory_profile6_log - INFO - Len of X_split for batch load: 25
2018-05-02 19:28:31,285 - memory_profile6_log - INFO - Appending history data...
2018-05-02 19:28:31,322 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:28:34,661 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:28:35,641 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:28:36,415 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:28:37,510 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:28:38,346 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:28:39,279 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:28:41,470 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:28:43,844 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:28:45,423 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:28:46,746 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:28:48,066 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:28:49,000 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:28:49,927 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:28:50,865 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:28:52,585 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:28:55,480 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:28:58,592 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:29:00,776 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:29:02,055 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:29:03,746 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:29:05,262 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:29:06,730 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:29:07,822 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:29:09,032 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:29:10,058 - memory_profile6_log - INFO - Appending training data...
2018-05-02 19:29:10,059 - memory_profile6_log - INFO - len datalist: 1
2018-05-02 19:29:10,061 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-05-02 19:29:10,062 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 19:29:10,063 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 19:29:10,065 - memory_profile6_log - INFO - ================================================

2018-05-02 19:29:10,065 - memory_profile6_log - INFO -    378     87.0 MiB     87.0 MiB   @profile

2018-05-02 19:29:10,068 - memory_profile6_log - INFO -    379                             def BQPreprocess(cpu, date_generated, client, query_fit, loadfrom="elastic"):

2018-05-02 19:29:10,072 - memory_profile6_log - INFO -    380     87.0 MiB      0.0 MiB       bq_client = client

2018-05-02 19:29:10,072 - memory_profile6_log - INFO -    381     87.0 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 19:29:10,075 - memory_profile6_log - INFO -    382                             

2018-05-02 19:29:10,076 - memory_profile6_log - INFO -    383     87.0 MiB      0.0 MiB       datalist = []

2018-05-02 19:29:10,078 - memory_profile6_log - INFO -    384     87.0 MiB      0.0 MiB       datalist_hist = []

2018-05-02 19:29:10,078 - memory_profile6_log - INFO -    385                             

2018-05-02 19:29:10,081 - memory_profile6_log - INFO -    386     87.0 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-05-02 19:29:10,082 - memory_profile6_log - INFO -    387    142.6 MiB      0.0 MiB       for ndate in date_generated:

2018-05-02 19:29:10,084 - memory_profile6_log - INFO -    388    139.4 MiB     52.3 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-05-02 19:29:10,085 - memory_profile6_log - INFO -    389    139.4 MiB      0.0 MiB           if tframe is not None:

2018-05-02 19:29:10,085 - memory_profile6_log - INFO -    390    139.4 MiB      0.0 MiB               if not tframe.empty:

2018-05-02 19:29:10,086 - memory_profile6_log - INFO -    391    139.4 MiB      0.0 MiB                   if loadfrom.strip().lower() == 'datastore':

2018-05-02 19:29:10,088 - memory_profile6_log - INFO -    392                                                 X_split = np.array_split(tframe, 5)

2018-05-02 19:29:10,089 - memory_profile6_log - INFO -    393                                                 logger.info("loading history data from datastore...")

2018-05-02 19:29:10,091 - memory_profile6_log - INFO -    394                                                 logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 19:29:10,092 - memory_profile6_log - INFO -    395                                                 logger.info("Appending history data...")

2018-05-02 19:29:10,094 - memory_profile6_log - INFO -    396                                                 for ix in range(len(X_split)):

2018-05-02 19:29:10,095 - memory_profile6_log - INFO -    397                                                     # ~ loading history

2018-05-02 19:29:10,096 - memory_profile6_log - INFO -    398                                                     """

2018-05-02 19:29:10,098 - memory_profile6_log - INFO -    399                                                         disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-05-02 19:29:10,099 - memory_profile6_log - INFO -    400                                                     """

2018-05-02 19:29:10,101 - memory_profile6_log - INFO -    401                                                     logger.info("processing batch-%d", ix)

2018-05-02 19:29:10,101 - memory_profile6_log - INFO -    402                                                     # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-05-02 19:29:10,101 - memory_profile6_log - INFO -    403                                                     logger.info("creating list history data...")

2018-05-02 19:29:10,105 - memory_profile6_log - INFO -    404                                                     #lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-05-02 19:29:10,105 - memory_profile6_log - INFO -    405                                                     lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 19:29:10,107 - memory_profile6_log - INFO -    406                             

2018-05-02 19:29:10,108 - memory_profile6_log - INFO -    407                                                     logger.info("call history data...")

2018-05-02 19:29:10,108 - memory_profile6_log - INFO -    408                                                     h_frame = mh.loadDSHistory(lhistory)

2018-05-02 19:29:10,109 - memory_profile6_log - INFO -    409                             

2018-05-02 19:29:10,111 - memory_profile6_log - INFO -    410                                                     # me = os.getpid()

2018-05-02 19:29:10,111 - memory_profile6_log - INFO -    411                                                     # kill_proc_tree(me)

2018-05-02 19:29:10,112 - memory_profile6_log - INFO -    412                             

2018-05-02 19:29:10,117 - memory_profile6_log - INFO -    413                                                     logger.info("done collecting history data, appending now...")

2018-05-02 19:29:10,118 - memory_profile6_log - INFO -    414                                                     for m in h_frame:

2018-05-02 19:29:10,118 - memory_profile6_log - INFO -    415                                                         if m is not None:

2018-05-02 19:29:10,119 - memory_profile6_log - INFO -    416                                                             if len(m) > 0:

2018-05-02 19:29:10,121 - memory_profile6_log - INFO -    417                                                                 datalist_hist.append(pd.DataFrame(m))

2018-05-02 19:29:10,122 - memory_profile6_log - INFO -    418                                                     del h_frame

2018-05-02 19:29:10,124 - memory_profile6_log - INFO -    419                                                     del lhistory

2018-05-02 19:29:10,124 - memory_profile6_log - INFO -    420                             

2018-05-02 19:29:10,127 - memory_profile6_log - INFO -    421                                                 logger.info("Appending training data...")

2018-05-02 19:29:10,128 - memory_profile6_log - INFO -    422                                                 datalist.append(tframe)

2018-05-02 19:29:10,130 - memory_profile6_log - INFO -    423    139.4 MiB      0.0 MiB                   elif loadfrom.strip().lower() == 'elastic':

2018-05-02 19:29:10,131 - memory_profile6_log - INFO -    424    139.7 MiB      0.3 MiB                       X_split = np.array_split(tframe, 25)

2018-05-02 19:29:10,131 - memory_profile6_log - INFO -    425    139.7 MiB      0.0 MiB                       logger.info("loading history data from elastic...")

2018-05-02 19:29:10,134 - memory_profile6_log - INFO -    426    139.7 MiB      0.0 MiB                       logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 19:29:10,137 - memory_profile6_log - INFO -    427    139.7 MiB      0.0 MiB                       logger.info("Appending history data...")

2018-05-02 19:29:10,138 - memory_profile6_log - INFO -    428                                                 

2018-05-02 19:29:10,138 - memory_profile6_log - INFO -    429    142.6 MiB      0.0 MiB                       for ix in range(len(X_split)):

2018-05-02 19:29:10,140 - memory_profile6_log - INFO -    430    142.3 MiB      0.5 MiB                           lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 19:29:10,141 - memory_profile6_log - INFO -    431    142.3 MiB      0.0 MiB                           logger.info("call %d history data...", len(lhistory))

2018-05-02 19:29:10,141 - memory_profile6_log - INFO -    432    142.3 MiB      0.0 MiB                           inside_data = mh.loadESHistory(lhistory, es,

2018-05-02 19:29:10,142 - memory_profile6_log - INFO -    433    142.3 MiB      0.0 MiB                                                          esindex_name='fitted_hist_index',

2018-05-02 19:29:10,144 - memory_profile6_log - INFO -    434    142.6 MiB      2.2 MiB                                                          estype_name='fitted_hist_type')

2018-05-02 19:29:10,144 - memory_profile6_log - INFO -    435                                                     # split back the user_id and topic_id

2018-05-02 19:29:10,150 - memory_profile6_log - INFO -    436    142.6 MiB      0.1 MiB                           inside_data[['user_id','topic_id']] = inside_data.uid_topid.str.split('_', expand=True)

2018-05-02 19:29:10,151 - memory_profile6_log - INFO -    437    142.6 MiB      0.2 MiB                           inside_data = inside_data[["user_id","topic_id", "pt_posterior_x_Nt", "smoothed_pt_posterior", "p0_cat_ci", "sigma_Nt"]]

2018-05-02 19:29:10,151 - memory_profile6_log - INFO -    438    142.6 MiB      0.0 MiB                           if not inside_data.empty:

2018-05-02 19:29:10,153 - memory_profile6_log - INFO -    439    142.6 MiB      0.0 MiB                               datalist_hist.append(inside_data)

2018-05-02 19:29:10,154 - memory_profile6_log - INFO -    440    142.6 MiB      0.0 MiB                               del inside_data

2018-05-02 19:29:10,154 - memory_profile6_log - INFO -    441                                                 

2018-05-02 19:29:10,155 - memory_profile6_log - INFO -    442    142.6 MiB      0.0 MiB                       logger.info("Appending training data...")

2018-05-02 19:29:10,157 - memory_profile6_log - INFO -    443    142.6 MiB      0.0 MiB                       datalist.append(tframe)

2018-05-02 19:29:10,157 - memory_profile6_log - INFO -    444                                             else:

2018-05-02 19:29:10,161 - memory_profile6_log - INFO -    445                                                 logger.info("Unknows source is selected !")

2018-05-02 19:29:10,163 - memory_profile6_log - INFO -    446                                                 break

2018-05-02 19:29:10,164 - memory_profile6_log - INFO -    447                                     else: 

2018-05-02 19:29:10,164 - memory_profile6_log - INFO -    448                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-05-02 19:29:10,165 - memory_profile6_log - INFO -    449    142.6 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-05-02 19:29:10,167 - memory_profile6_log - INFO -    450    142.6 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-05-02 19:29:10,167 - memory_profile6_log - INFO -    451                             

2018-05-02 19:29:10,171 - memory_profile6_log - INFO -    452    142.6 MiB      0.0 MiB       return datalist, datalist_hist

2018-05-02 19:29:10,171 - memory_profile6_log - INFO - 


2018-05-02 19:29:10,194 - memory_profile6_log - INFO - len of big_frame_hist: 20000
2018-05-02 19:29:10,206 - memory_profile6_log - INFO - size of big_frame_hist: 5.49 MB
2018-05-02 19:29:10,220 - memory_profile6_log - INFO - size of big_frame: 5.05 MB
2018-05-02 19:29:10,220 - memory_profile6_log - INFO - len of big_frame: 20000
2018-05-02 19:29:10,227 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-05-02 19:29:18,786 - memory_profile6_log - INFO - size of df: 4.93 MB
2018-05-02 19:29:18,786 - memory_profile6_log - INFO - getting total: 20000 training data(current date interest)
2018-05-02 19:29:18,802 - memory_profile6_log - INFO - size of current_frame: 5.08 MB
2018-05-02 19:29:18,802 - memory_profile6_log - INFO - loading time of: 40000 total genuine-current interest data ~ take 58.035s
2018-05-02 19:29:18,805 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 19:29:18,806 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 19:29:18,809 - memory_profile6_log - INFO - ================================================

2018-05-02 19:29:18,809 - memory_profile6_log - INFO -    454     86.9 MiB     86.9 MiB   @profile

2018-05-02 19:29:18,809 - memory_profile6_log - INFO -    455                             def preprocess(cpu, cd, query_fit, date_generated):

2018-05-02 19:29:18,809 - memory_profile6_log - INFO -    456     87.0 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-05-02 19:29:18,809 - memory_profile6_log - INFO -    457     87.0 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 19:29:18,811 - memory_profile6_log - INFO -    458                             

2018-05-02 19:29:18,811 - memory_profile6_log - INFO -    459                                 # ~~~ Begin collecting data ~~~

2018-05-02 19:29:18,812 - memory_profile6_log - INFO -    460     87.0 MiB      0.0 MiB       t0 = time.time()

2018-05-02 19:29:18,812 - memory_profile6_log - INFO -    461    142.6 MiB     55.6 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-05-02 19:29:18,812 - memory_profile6_log - INFO -    462    142.6 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-05-02 19:29:18,812 - memory_profile6_log - INFO -    463                                     logger.info("Training cannot be empty..")

2018-05-02 19:29:18,815 - memory_profile6_log - INFO -    464                                     return False

2018-05-02 19:29:18,818 - memory_profile6_log - INFO -    465    142.9 MiB      0.3 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-05-02 19:29:18,818 - memory_profile6_log - INFO -    466    142.9 MiB      0.0 MiB       logger.info("len of big_frame_hist: %s", len(big_frame_hist))

2018-05-02 19:29:18,819 - memory_profile6_log - INFO -    467    142.9 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-05-02 19:29:18,819 - memory_profile6_log - INFO -    468                             

2018-05-02 19:29:18,819 - memory_profile6_log - INFO -    469    142.9 MiB      0.0 MiB       big_frame = pd.concat(datalist)

2018-05-02 19:29:18,819 - memory_profile6_log - INFO -    470    142.9 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-05-02 19:29:18,819 - memory_profile6_log - INFO -    471    142.9 MiB      0.0 MiB       logger.info("len of big_frame: %s", len(big_frame))

2018-05-02 19:29:18,819 - memory_profile6_log - INFO -    472    142.9 MiB      0.0 MiB       del datalist

2018-05-02 19:29:18,821 - memory_profile6_log - INFO -    473                             

2018-05-02 19:29:18,821 - memory_profile6_log - INFO -    474    142.9 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 19:29:18,822 - memory_profile6_log - INFO -    475                             

2018-05-02 19:29:18,822 - memory_profile6_log - INFO -    476                                 # ~ get current news interest ~

2018-05-02 19:29:18,822 - memory_profile6_log - INFO -    477    142.9 MiB      0.0 MiB       if not cd:

2018-05-02 19:29:18,822 - memory_profile6_log - INFO -    478    142.9 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-05-02 19:29:18,822 - memory_profile6_log - INFO -    479    147.6 MiB      4.7 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-05-02 19:29:18,823 - memory_profile6_log - INFO -    480    147.6 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-05-02 19:29:18,823 - memory_profile6_log - INFO -    481                                 else:

2018-05-02 19:29:18,828 - memory_profile6_log - INFO -    482                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-05-02 19:29:18,829 - memory_profile6_log - INFO -    483                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date) LIMIT 20000"

2018-05-02 19:29:18,829 - memory_profile6_log - INFO -    484                             

2018-05-02 19:29:18,831 - memory_profile6_log - INFO -    485                                     # safe handling of query parameter

2018-05-02 19:29:18,831 - memory_profile6_log - INFO -    486                                     query_params = [

2018-05-02 19:29:18,832 - memory_profile6_log - INFO -    487                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-05-02 19:29:18,832 - memory_profile6_log - INFO -    488                                     ]

2018-05-02 19:29:18,832 - memory_profile6_log - INFO -    489                             

2018-05-02 19:29:18,834 - memory_profile6_log - INFO -    490                                     job_config.query_parameters = query_params

2018-05-02 19:29:18,834 - memory_profile6_log - INFO -    491                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-05-02 19:29:18,834 - memory_profile6_log - INFO -    492                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-05-02 19:29:18,835 - memory_profile6_log - INFO -    493                             

2018-05-02 19:29:18,835 - memory_profile6_log - INFO -    494    147.6 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-05-02 19:29:18,835 - memory_profile6_log - INFO -    495    147.6 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-05-02 19:29:18,835 - memory_profile6_log - INFO -    496    147.6 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-05-02 19:29:18,835 - memory_profile6_log - INFO -    497    147.6 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-05-02 19:29:18,836 - memory_profile6_log - INFO -    498    147.6 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 19:29:18,841 - memory_profile6_log - INFO -    499    147.6 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-05-02 19:29:18,842 - memory_profile6_log - INFO -    500                             

2018-05-02 19:29:18,844 - memory_profile6_log - INFO -    501    147.6 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-05-02 19:29:18,844 - memory_profile6_log - INFO - 


2018-05-02 19:29:18,848 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-05-02 19:29:18,857 - memory_profile6_log - INFO - train on: 20000 total genuine interest data(D(u, t))
2018-05-02 19:29:18,858 - memory_profile6_log - INFO - transform on: 20000 total current data(D(t))
2018-05-02 19:29:18,859 - memory_profile6_log - INFO - apply on: 20000 total history...
2018-05-02 19:29:18,911 - memory_profile6_log - INFO - len of uniques_fit_hist:20000
2018-05-02 19:29:18,923 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:5480
2018-05-02 19:29:19,003 - memory_profile6_log - INFO - self.sum_all_nt before concate:

2018-05-02 19:29:19,023 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 19:29:19,025 - memory_profile6_log - INFO - 

2018-05-02 19:29:19,045 - memory_profile6_log - INFO - self.sum_all_nt after concate:

2018-05-02 19:29:19,055 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 19:29:19,056 - memory_profile6_log - INFO - 

2018-05-02 19:29:19,114 - memory_profile6_log - INFO - Len of model_fit: 20000
2018-05-02 19:29:19,115 - memory_profile6_log - INFO - Len of df_dut: 20000
2018-05-02 19:29:19,270 - memory_profile6_log - INFO - Len of fitted_models on main class: 20000
2018-05-02 19:29:19,272 - memory_profile6_log - INFO - 

2018-05-02 19:29:19,273 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-05-02 19:29:19,273 - memory_profile6_log - INFO - 

2018-05-02 19:29:19,275 - memory_profile6_log - INFO - fitted_model_hist:

2018-05-02 19:29:19,276 - memory_profile6_log - INFO - 

2018-05-02 19:29:19,289 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 19:29:19,290 - memory_profile6_log - INFO - 

2018-05-02 19:29:19,299 - memory_profile6_log - INFO - len of fitted models after concat: 40000
2018-05-02 19:29:19,301 - memory_profile6_log - INFO - 

2018-05-02 19:29:19,302 - memory_profile6_log - INFO - Recalculating fitted models...
2018-05-02 19:29:19,302 - memory_profile6_log - INFO - 

2018-05-02 19:29:19,342 - memory_profile6_log - INFO - fitted_models after cobine:

2018-05-02 19:29:19,342 - memory_profile6_log - INFO - 

2018-05-02 19:29:19,358 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 19:29:19,359 - memory_profile6_log - INFO - 

2018-05-02 19:29:19,359 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 20000
2018-05-02 19:29:19,361 - memory_profile6_log - INFO - 

2018-05-02 19:29:24,756 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-05-02 19:29:24,780 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, topic_id, pt_posterior_x_Nt, smoothed_pt_posterior, p0_cat_ci, sigma_Nt, p0_posterior, is_general, rank]
Index: []
2018-05-02 19:29:24,782 - memory_profile6_log - INFO - 

2018-05-02 19:29:24,782 - memory_profile6_log - INFO - Len of model_transform: 7846
2018-05-02 19:29:24,783 - memory_profile6_log - INFO - Len of df_dt: 20000
2018-05-02 19:29:24,786 - memory_profile6_log - INFO - Total train time: 5.911s
2018-05-02 19:29:24,786 - memory_profile6_log - INFO - memory left before cleaning: 81.200 percent memory...
2018-05-02 19:29:24,788 - memory_profile6_log - INFO - cleaning up some objects...
2018-05-02 19:29:24,788 - memory_profile6_log - INFO - deleting df_dut...
2018-05-02 19:29:24,789 - memory_profile6_log - INFO - deleting df_dt...
2018-05-02 19:29:24,789 - memory_profile6_log - INFO - deleting df_input...
2018-05-02 19:29:24,795 - memory_profile6_log - INFO - deleting df_input_X...
2018-05-02 19:29:24,796 - memory_profile6_log - INFO - deleting df_current...
2018-05-02 19:29:24,796 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-05-02 19:29:24,799 - memory_profile6_log - INFO - deleting model_fit...
2018-05-02 19:29:24,799 - memory_profile6_log - INFO - deleting result...
2018-05-02 19:29:24,809 - memory_profile6_log - INFO - list(model_transformsv) before rename: 
2018-05-02 19:29:24,809 - memory_profile6_log - INFO -  
2018-05-02 19:29:24,811 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'is_general', 'p0_posterior', 'rank', 'date']
2018-05-02 19:29:24,813 - memory_profile6_log - INFO - 

2018-05-02 19:29:24,815 - memory_profile6_log - INFO - 7846
2018-05-02 19:29:24,815 - memory_profile6_log - INFO - 

2018-05-02 19:29:24,819 - memory_profile6_log - INFO - list(model_transformsv) after rename: 
2018-05-02 19:29:24,821 - memory_profile6_log - INFO -  
2018-05-02 19:29:24,821 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'topic_is_general', 'interest_score', 'interest_rank', 'interest_score_created_at']
2018-05-02 19:29:24,822 - memory_profile6_log - INFO - 

2018-05-02 19:29:24,823 - memory_profile6_log - INFO - 7846
2018-05-02 19:29:24,825 - memory_profile6_log - INFO - 

2018-05-02 19:29:24,825 - memory_profile6_log - INFO - deleting model_transform...
2018-05-02 19:29:24,826 - memory_profile6_log - INFO - memory left after cleaning: 81.200 percent memory...
2018-05-02 19:29:24,828 - memory_profile6_log - INFO - Begin saving trained data...
2018-05-02 19:29:24,828 - memory_profile6_log - INFO - Using ElasticSearch as storage...
2018-05-02 19:29:24,849 - memory_profile6_log - INFO - Saving total data: 7846
2018-05-02 19:29:24,851 - memory_profile6_log - INFO - Len of X_split for batch save model_transformsv: 15
2018-05-02 19:29:24,852 - memory_profile6_log - INFO - processing batch-0
2018-05-02 19:30:29,914 - memory_profile6_log - INFO - ElasticSearch host: https://9db53c7bb4f5be2d856033a9aeb6e5a5.us-central1.gcp.cloud.es.io
2018-05-02 19:30:29,914 - memory_profile6_log - INFO - ElasticSearch port: 9243
2018-05-02 19:30:29,917 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 19:30:29,918 - memory_profile6_log - INFO - date_generated: 
2018-05-02 19:30:29,920 - memory_profile6_log - INFO -  
2018-05-02 19:30:29,920 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 19, 30, 29, 917000)]
2018-05-02 19:30:29,921 - memory_profile6_log - INFO - 

2018-05-02 19:30:29,921 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 19:30:29,921 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 19:30:29,921 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 19:30:30,061 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 19:30:30,065 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 19:30:40,134 - memory_profile6_log - INFO - size of df: 5.05 MB
2018-05-02 19:30:40,135 - memory_profile6_log - INFO - getting total: 20000 training data(genuine interest) for date: 2018-05-01
2018-05-02 19:30:40,209 - memory_profile6_log - INFO - loading history data from elastic...
2018-05-02 19:30:40,210 - memory_profile6_log - INFO - Len of X_split for batch load: 25
2018-05-02 19:30:40,211 - memory_profile6_log - INFO - Appending history data...
2018-05-02 19:30:40,250 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:30:43,384 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:30:46,441 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:30:48,181 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:30:50,073 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:30:51,835 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:30:53,596 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:30:55,167 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:30:56,382 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:30:57,664 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:30:59,217 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:31:00,532 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:31:01,865 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:31:03,421 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:31:04,644 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:31:06,137 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:31:07,483 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:31:08,630 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:31:09,582 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:31:10,642 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:31:11,858 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:31:12,869 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:31:14,043 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:31:15,121 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:31:17,753 - memory_profile6_log - INFO - call 800 history data...
2018-05-02 19:31:21,819 - memory_profile6_log - INFO - Appending training data...
2018-05-02 19:31:21,821 - memory_profile6_log - INFO - len datalist: 1
2018-05-02 19:31:21,822 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-05-02 19:31:21,823 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 19:31:21,825 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 19:31:21,825 - memory_profile6_log - INFO - ================================================

2018-05-02 19:31:21,826 - memory_profile6_log - INFO -    378     86.8 MiB     86.8 MiB   @profile

2018-05-02 19:31:21,828 - memory_profile6_log - INFO -    379                             def BQPreprocess(cpu, date_generated, client, query_fit, loadfrom="elastic"):

2018-05-02 19:31:21,831 - memory_profile6_log - INFO -    380     86.8 MiB      0.0 MiB       bq_client = client

2018-05-02 19:31:21,832 - memory_profile6_log - INFO -    381     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 19:31:21,835 - memory_profile6_log - INFO -    382                             

2018-05-02 19:31:21,841 - memory_profile6_log - INFO -    383     86.8 MiB      0.0 MiB       datalist = []

2018-05-02 19:31:21,845 - memory_profile6_log - INFO -    384     86.8 MiB      0.0 MiB       datalist_hist = []

2018-05-02 19:31:21,846 - memory_profile6_log - INFO -    385                             

2018-05-02 19:31:21,848 - memory_profile6_log - INFO -    386     86.8 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-05-02 19:31:21,851 - memory_profile6_log - INFO -    387    142.1 MiB      0.0 MiB       for ndate in date_generated:

2018-05-02 19:31:21,854 - memory_profile6_log - INFO -    388    140.3 MiB     53.5 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-05-02 19:31:21,855 - memory_profile6_log - INFO -    389    140.3 MiB      0.0 MiB           if tframe is not None:

2018-05-02 19:31:21,858 - memory_profile6_log - INFO -    390    140.3 MiB      0.0 MiB               if not tframe.empty:

2018-05-02 19:31:21,859 - memory_profile6_log - INFO -    391    140.3 MiB      0.0 MiB                   if loadfrom.strip().lower() == 'datastore':

2018-05-02 19:31:21,864 - memory_profile6_log - INFO -    392                                                 X_split = np.array_split(tframe, 5)

2018-05-02 19:31:21,865 - memory_profile6_log - INFO -    393                                                 logger.info("loading history data from datastore...")

2018-05-02 19:31:21,868 - memory_profile6_log - INFO -    394                                                 logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 19:31:21,868 - memory_profile6_log - INFO -    395                                                 logger.info("Appending history data...")

2018-05-02 19:31:21,874 - memory_profile6_log - INFO -    396                                                 for ix in range(len(X_split)):

2018-05-02 19:31:21,875 - memory_profile6_log - INFO -    397                                                     # ~ loading history

2018-05-02 19:31:21,878 - memory_profile6_log - INFO -    398                                                     """

2018-05-02 19:31:21,880 - memory_profile6_log - INFO -    399                                                         disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-05-02 19:31:21,881 - memory_profile6_log - INFO -    400                                                     """

2018-05-02 19:31:21,884 - memory_profile6_log - INFO -    401                                                     logger.info("processing batch-%d", ix)

2018-05-02 19:31:21,887 - memory_profile6_log - INFO -    402                                                     # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-05-02 19:31:21,891 - memory_profile6_log - INFO -    403                                                     logger.info("creating list history data...")

2018-05-02 19:31:21,891 - memory_profile6_log - INFO -    404                                                     #lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-05-02 19:31:21,894 - memory_profile6_log - INFO -    405                                                     lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 19:31:21,895 - memory_profile6_log - INFO -    406                             

2018-05-02 19:31:21,897 - memory_profile6_log - INFO -    407                                                     logger.info("call history data...")

2018-05-02 19:31:21,898 - memory_profile6_log - INFO -    408                                                     h_frame = mh.loadDSHistory(lhistory)

2018-05-02 19:31:21,898 - memory_profile6_log - INFO -    409                             

2018-05-02 19:31:21,900 - memory_profile6_log - INFO -    410                                                     # me = os.getpid()

2018-05-02 19:31:21,901 - memory_profile6_log - INFO -    411                                                     # kill_proc_tree(me)

2018-05-02 19:31:21,903 - memory_profile6_log - INFO -    412                             

2018-05-02 19:31:21,904 - memory_profile6_log - INFO -    413                                                     logger.info("done collecting history data, appending now...")

2018-05-02 19:31:21,905 - memory_profile6_log - INFO -    414                                                     for m in h_frame:

2018-05-02 19:31:21,907 - memory_profile6_log - INFO -    415                                                         if m is not None:

2018-05-02 19:31:21,907 - memory_profile6_log - INFO -    416                                                             if len(m) > 0:

2018-05-02 19:31:21,910 - memory_profile6_log - INFO -    417                                                                 datalist_hist.append(pd.DataFrame(m))

2018-05-02 19:31:21,911 - memory_profile6_log - INFO -    418                                                     del h_frame

2018-05-02 19:31:21,911 - memory_profile6_log - INFO -    419                                                     del lhistory

2018-05-02 19:31:21,914 - memory_profile6_log - INFO -    420                             

2018-05-02 19:31:21,915 - memory_profile6_log - INFO -    421                                                 logger.info("Appending training data...")

2018-05-02 19:31:21,920 - memory_profile6_log - INFO -    422                                                 datalist.append(tframe)

2018-05-02 19:31:21,921 - memory_profile6_log - INFO -    423    140.3 MiB      0.0 MiB                   elif loadfrom.strip().lower() == 'elastic':

2018-05-02 19:31:21,921 - memory_profile6_log - INFO -    424    140.7 MiB      0.3 MiB                       X_split = np.array_split(tframe, 25)

2018-05-02 19:31:21,924 - memory_profile6_log - INFO -    425    140.7 MiB      0.0 MiB                       logger.info("loading history data from elastic...")

2018-05-02 19:31:21,924 - memory_profile6_log - INFO -    426    140.7 MiB      0.0 MiB                       logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 19:31:21,927 - memory_profile6_log - INFO -    427    140.7 MiB      0.0 MiB                       logger.info("Appending history data...")

2018-05-02 19:31:21,928 - memory_profile6_log - INFO -    428                                                 

2018-05-02 19:31:21,930 - memory_profile6_log - INFO -    429    142.1 MiB      0.0 MiB                       for ix in range(len(X_split)):

2018-05-02 19:31:21,930 - memory_profile6_log - INFO -    430    142.1 MiB      0.5 MiB                           lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 19:31:21,931 - memory_profile6_log - INFO -    431    142.1 MiB      0.0 MiB                           logger.info("call %d history data...", len(lhistory))

2018-05-02 19:31:21,934 - memory_profile6_log - INFO -    432    142.1 MiB      0.0 MiB                           inside_data = mh.loadESHistory(lhistory, es,

2018-05-02 19:31:21,936 - memory_profile6_log - INFO -    433    142.1 MiB      0.0 MiB                                                          esindex_name='fitted_hist_index',

2018-05-02 19:31:21,938 - memory_profile6_log - INFO -    434    142.1 MiB      0.7 MiB                                                          estype_name='fitted_hist_type')

2018-05-02 19:31:21,940 - memory_profile6_log - INFO -    435                                                     # split back the user_id and topic_id

2018-05-02 19:31:21,940 - memory_profile6_log - INFO -    436    142.1 MiB      0.1 MiB                           inside_data[['user_id','topic_id']] = inside_data.uid_topid.str.split('_', expand=True)

2018-05-02 19:31:21,941 - memory_profile6_log - INFO -    437    142.1 MiB      0.2 MiB                           inside_data = inside_data[["user_id","topic_id", "pt_posterior_x_Nt", "smoothed_pt_posterior", "p0_cat_ci", "sigma_Nt"]]

2018-05-02 19:31:21,944 - memory_profile6_log - INFO -    438    142.1 MiB      0.0 MiB                           if not inside_data.empty:

2018-05-02 19:31:21,946 - memory_profile6_log - INFO -    439    142.1 MiB      0.0 MiB                               datalist_hist.append(inside_data)

2018-05-02 19:31:21,947 - memory_profile6_log - INFO -    440    142.1 MiB      0.0 MiB                               del inside_data

2018-05-02 19:31:21,948 - memory_profile6_log - INFO -    441                                                 

2018-05-02 19:31:21,950 - memory_profile6_log - INFO -    442    142.1 MiB      0.0 MiB                       logger.info("Appending training data...")

2018-05-02 19:31:21,951 - memory_profile6_log - INFO -    443    142.1 MiB      0.0 MiB                       datalist.append(tframe)

2018-05-02 19:31:21,953 - memory_profile6_log - INFO -    444                                             else:

2018-05-02 19:31:21,954 - memory_profile6_log - INFO -    445                                                 logger.info("Unknows source is selected !")

2018-05-02 19:31:21,957 - memory_profile6_log - INFO -    446                                                 break

2018-05-02 19:31:21,957 - memory_profile6_log - INFO -    447                                     else: 

2018-05-02 19:31:21,959 - memory_profile6_log - INFO -    448                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-05-02 19:31:21,960 - memory_profile6_log - INFO -    449    142.1 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-05-02 19:31:21,960 - memory_profile6_log - INFO -    450    142.1 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-05-02 19:31:21,961 - memory_profile6_log - INFO -    451                             

2018-05-02 19:31:21,963 - memory_profile6_log - INFO -    452    142.1 MiB      0.0 MiB       return datalist, datalist_hist

2018-05-02 19:31:21,964 - memory_profile6_log - INFO - 


2018-05-02 19:31:21,993 - memory_profile6_log - INFO - len of big_frame_hist: 20000
2018-05-02 19:31:22,005 - memory_profile6_log - INFO - size of big_frame_hist: 5.49 MB
2018-05-02 19:31:22,019 - memory_profile6_log - INFO - size of big_frame: 5.05 MB
2018-05-02 19:31:22,020 - memory_profile6_log - INFO - len of big_frame: 20000
2018-05-02 19:31:22,026 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-05-02 19:31:30,986 - memory_profile6_log - INFO - size of df: 4.93 MB
2018-05-02 19:31:30,987 - memory_profile6_log - INFO - getting total: 20000 training data(current date interest)
2018-05-02 19:31:31,003 - memory_profile6_log - INFO - size of current_frame: 5.08 MB
2018-05-02 19:31:31,005 - memory_profile6_log - INFO - loading time of: 40000 total genuine-current interest data ~ take 60.980s
2018-05-02 19:31:31,006 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 19:31:31,007 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 19:31:31,007 - memory_profile6_log - INFO - ================================================

2018-05-02 19:31:31,010 - memory_profile6_log - INFO -    454     86.7 MiB     86.7 MiB   @profile

2018-05-02 19:31:31,012 - memory_profile6_log - INFO -    455                             def preprocess(cpu, cd, query_fit, date_generated):

2018-05-02 19:31:31,012 - memory_profile6_log - INFO -    456     86.8 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-05-02 19:31:31,013 - memory_profile6_log - INFO -    457     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 19:31:31,013 - memory_profile6_log - INFO -    458                             

2018-05-02 19:31:31,013 - memory_profile6_log - INFO -    459                                 # ~~~ Begin collecting data ~~~

2018-05-02 19:31:31,015 - memory_profile6_log - INFO -    460     86.8 MiB      0.0 MiB       t0 = time.time()

2018-05-02 19:31:31,015 - memory_profile6_log - INFO -    461    142.1 MiB     55.2 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-05-02 19:31:31,019 - memory_profile6_log - INFO -    462    142.1 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-05-02 19:31:31,022 - memory_profile6_log - INFO -    463                                     logger.info("Training cannot be empty..")

2018-05-02 19:31:31,023 - memory_profile6_log - INFO -    464                                     return False

2018-05-02 19:31:31,023 - memory_profile6_log - INFO -    465    142.1 MiB      0.0 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-05-02 19:31:31,026 - memory_profile6_log - INFO -    466    142.1 MiB      0.0 MiB       logger.info("len of big_frame_hist: %s", len(big_frame_hist))

2018-05-02 19:31:31,026 - memory_profile6_log - INFO -    467    142.1 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-05-02 19:31:31,026 - memory_profile6_log - INFO -    468                             

2018-05-02 19:31:31,026 - memory_profile6_log - INFO -    469    142.1 MiB      0.0 MiB       big_frame = pd.concat(datalist)

2018-05-02 19:31:31,026 - memory_profile6_log - INFO -    470    142.1 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-05-02 19:31:31,028 - memory_profile6_log - INFO -    471    142.1 MiB      0.0 MiB       logger.info("len of big_frame: %s", len(big_frame))

2018-05-02 19:31:31,028 - memory_profile6_log - INFO -    472    142.1 MiB      0.0 MiB       del datalist

2018-05-02 19:31:31,028 - memory_profile6_log - INFO -    473                             

2018-05-02 19:31:31,029 - memory_profile6_log - INFO -    474    142.1 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 19:31:31,032 - memory_profile6_log - INFO -    475                             

2018-05-02 19:31:31,035 - memory_profile6_log - INFO -    476                                 # ~ get current news interest ~

2018-05-02 19:31:31,035 - memory_profile6_log - INFO -    477    142.1 MiB      0.0 MiB       if not cd:

2018-05-02 19:31:31,036 - memory_profile6_log - INFO -    478    142.1 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-05-02 19:31:31,036 - memory_profile6_log - INFO -    479    147.6 MiB      5.6 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-05-02 19:31:31,036 - memory_profile6_log - INFO -    480    147.6 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-05-02 19:31:31,038 - memory_profile6_log - INFO -    481                                 else:

2018-05-02 19:31:31,038 - memory_profile6_log - INFO -    482                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-05-02 19:31:31,039 - memory_profile6_log - INFO -    483                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date) LIMIT 20000"

2018-05-02 19:31:31,039 - memory_profile6_log - INFO -    484                             

2018-05-02 19:31:31,039 - memory_profile6_log - INFO -    485                                     # safe handling of query parameter

2018-05-02 19:31:31,039 - memory_profile6_log - INFO -    486                                     query_params = [

2018-05-02 19:31:31,046 - memory_profile6_log - INFO -    487                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-05-02 19:31:31,046 - memory_profile6_log - INFO -    488                                     ]

2018-05-02 19:31:31,048 - memory_profile6_log - INFO -    489                             

2018-05-02 19:31:31,048 - memory_profile6_log - INFO -    490                                     job_config.query_parameters = query_params

2018-05-02 19:31:31,049 - memory_profile6_log - INFO -    491                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-05-02 19:31:31,051 - memory_profile6_log - INFO -    492                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-05-02 19:31:31,052 - memory_profile6_log - INFO -    493                             

2018-05-02 19:31:31,052 - memory_profile6_log - INFO -    494    147.7 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-05-02 19:31:31,052 - memory_profile6_log - INFO -    495    147.7 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-05-02 19:31:31,058 - memory_profile6_log - INFO -    496    147.7 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-05-02 19:31:31,059 - memory_profile6_log - INFO -    497    147.7 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-05-02 19:31:31,059 - memory_profile6_log - INFO -    498    147.7 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 19:31:31,059 - memory_profile6_log - INFO -    499    147.7 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-05-02 19:31:31,061 - memory_profile6_log - INFO -    500                             

2018-05-02 19:31:31,061 - memory_profile6_log - INFO -    501    147.7 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-05-02 19:31:31,061 - memory_profile6_log - INFO - 


2018-05-02 19:31:31,066 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-05-02 19:31:31,075 - memory_profile6_log - INFO - train on: 20000 total genuine interest data(D(u, t))
2018-05-02 19:31:31,076 - memory_profile6_log - INFO - transform on: 20000 total current data(D(t))
2018-05-02 19:31:31,079 - memory_profile6_log - INFO - apply on: 20000 total history...
2018-05-02 19:31:31,131 - memory_profile6_log - INFO - len of uniques_fit_hist:20000
2018-05-02 19:31:31,141 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:5485
2018-05-02 19:31:31,217 - memory_profile6_log - INFO - self.sum_all_nt before concate:

2018-05-02 19:31:31,236 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 19:31:31,237 - memory_profile6_log - INFO - 

2018-05-02 19:31:31,253 - memory_profile6_log - INFO - self.sum_all_nt after concate:

2018-05-02 19:31:31,265 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 19:31:31,266 - memory_profile6_log - INFO - 

2018-05-02 19:31:31,312 - memory_profile6_log - INFO - Len of model_fit: 20000
2018-05-02 19:31:31,313 - memory_profile6_log - INFO - Len of df_dut: 20000
2018-05-02 19:31:31,461 - memory_profile6_log - INFO - Len of fitted_models on main class: 20000
2018-05-02 19:31:31,463 - memory_profile6_log - INFO - 

2018-05-02 19:31:31,463 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-05-02 19:31:31,466 - memory_profile6_log - INFO - 

2018-05-02 19:31:31,466 - memory_profile6_log - INFO - fitted_model_hist:

2018-05-02 19:31:31,467 - memory_profile6_log - INFO - 

2018-05-02 19:31:31,483 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 19:31:31,483 - memory_profile6_log - INFO - 

2018-05-02 19:31:31,493 - memory_profile6_log - INFO - len of fitted models after concat: 40000
2018-05-02 19:31:31,494 - memory_profile6_log - INFO - 

2018-05-02 19:31:31,496 - memory_profile6_log - INFO - Recalculating fitted models...
2018-05-02 19:31:31,497 - memory_profile6_log - INFO - 

2018-05-02 19:31:31,536 - memory_profile6_log - INFO - fitted_models after cobine:

2018-05-02 19:31:31,539 - memory_profile6_log - INFO - 

2018-05-02 19:31:31,556 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 19:31:31,556 - memory_profile6_log - INFO - 

2018-05-02 19:31:31,559 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 20000
2018-05-02 19:31:31,559 - memory_profile6_log - INFO - 

2018-05-02 19:31:36,713 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-05-02 19:31:36,736 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, topic_id, pt_posterior_x_Nt, smoothed_pt_posterior, p0_cat_ci, sigma_Nt, p0_posterior, is_general, rank]
Index: []
2018-05-02 19:31:36,736 - memory_profile6_log - INFO - 

2018-05-02 19:31:36,737 - memory_profile6_log - INFO - Len of model_transform: 7846
2018-05-02 19:31:36,740 - memory_profile6_log - INFO - Len of df_dt: 20000
2018-05-02 19:31:36,740 - memory_profile6_log - INFO - Total train time: 5.648s
2018-05-02 19:31:36,742 - memory_profile6_log - INFO - memory left before cleaning: 80.300 percent memory...
2018-05-02 19:31:36,743 - memory_profile6_log - INFO - cleaning up some objects...
2018-05-02 19:31:36,743 - memory_profile6_log - INFO - deleting df_dut...
2018-05-02 19:31:36,744 - memory_profile6_log - INFO - deleting df_dt...
2018-05-02 19:31:36,747 - memory_profile6_log - INFO - deleting df_input...
2018-05-02 19:31:36,750 - memory_profile6_log - INFO - deleting df_input_X...
2018-05-02 19:31:36,750 - memory_profile6_log - INFO - deleting df_current...
2018-05-02 19:31:36,752 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-05-02 19:31:36,753 - memory_profile6_log - INFO - deleting model_fit...
2018-05-02 19:31:36,755 - memory_profile6_log - INFO - deleting result...
2018-05-02 19:31:36,763 - memory_profile6_log - INFO - list(model_transformsv) before rename: 
2018-05-02 19:31:36,765 - memory_profile6_log - INFO -  
2018-05-02 19:31:36,766 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'is_general', 'p0_posterior', 'rank', 'date']
2018-05-02 19:31:36,769 - memory_profile6_log - INFO - 

2018-05-02 19:31:36,770 - memory_profile6_log - INFO - 7846
2018-05-02 19:31:36,772 - memory_profile6_log - INFO - 

2018-05-02 19:31:36,776 - memory_profile6_log - INFO - list(model_transformsv) after rename: 
2018-05-02 19:31:36,778 - memory_profile6_log - INFO -  
2018-05-02 19:31:36,779 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'topic_is_general', 'interest_score', 'interest_rank', 'interest_score_created_at']
2018-05-02 19:31:36,779 - memory_profile6_log - INFO - 

2018-05-02 19:31:36,780 - memory_profile6_log - INFO - 7846
2018-05-02 19:31:36,782 - memory_profile6_log - INFO - 

2018-05-02 19:31:36,782 - memory_profile6_log - INFO - deleting model_transform...
2018-05-02 19:31:36,783 - memory_profile6_log - INFO - memory left after cleaning: 80.300 percent memory...
2018-05-02 19:31:36,785 - memory_profile6_log - INFO - Begin saving trained data...
2018-05-02 19:31:36,788 - memory_profile6_log - INFO - Using ElasticSearch as storage...
2018-05-02 19:31:36,812 - memory_profile6_log - INFO - Saving total data: 7846
2018-05-02 19:31:36,813 - memory_profile6_log - INFO - Len of X_split for batch save model_transformsv: 15
2018-05-02 19:31:36,815 - memory_profile6_log - INFO - processing batch-0
2018-05-02 19:31:40,246 - memory_profile6_log - INFO - processing batch-1
2018-05-02 19:31:45,569 - memory_profile6_log - INFO - processing batch-2
2018-05-02 19:31:48,914 - memory_profile6_log - INFO - processing batch-3
2018-05-02 19:31:51,730 - memory_profile6_log - INFO - processing batch-4
2018-05-02 19:31:54,576 - memory_profile6_log - INFO - processing batch-5
2018-05-02 19:31:57,198 - memory_profile6_log - INFO - processing batch-6
2018-05-02 19:32:03,167 - memory_profile6_log - INFO - processing batch-7
2018-05-02 19:32:07,759 - memory_profile6_log - INFO - processing batch-8
2018-05-02 19:32:11,372 - memory_profile6_log - INFO - processing batch-9
2018-05-02 19:32:14,168 - memory_profile6_log - INFO - processing batch-10
2018-05-02 19:32:17,421 - memory_profile6_log - INFO - processing batch-11
2018-05-02 19:32:20,338 - memory_profile6_log - INFO - processing batch-12
2018-05-02 19:32:24,861 - memory_profile6_log - INFO - processing batch-13
2018-05-02 19:32:29,431 - memory_profile6_log - INFO - processing batch-14
2018-05-02 19:32:34,042 - memory_profile6_log - INFO - Saving fitted_models as history to Elasticsearch...
2018-05-02 19:32:34,108 - memory_profile6_log - INFO - Saving total data: 20000
2018-05-02 19:32:34,109 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 25
2018-05-02 19:32:34,111 - memory_profile6_log - INFO - processing batch-0
2018-05-02 19:32:39,299 - memory_profile6_log - INFO - processing batch-1
2018-05-02 19:32:43,971 - memory_profile6_log - INFO - processing batch-2
2018-05-02 19:32:50,874 - memory_profile6_log - INFO - processing batch-3
2018-05-02 19:32:54,575 - memory_profile6_log - INFO - processing batch-4
2018-05-02 19:32:58,400 - memory_profile6_log - INFO - processing batch-5
2018-05-02 19:33:05,484 - memory_profile6_log - INFO - processing batch-6
2018-05-02 19:33:11,720 - memory_profile6_log - INFO - processing batch-7
2018-05-02 19:33:17,532 - memory_profile6_log - INFO - processing batch-8
2018-05-02 19:33:23,201 - memory_profile6_log - INFO - processing batch-9
2018-05-02 19:33:27,940 - memory_profile6_log - INFO - processing batch-10
2018-05-02 19:33:33,260 - memory_profile6_log - INFO - processing batch-11
2018-05-02 19:33:39,187 - memory_profile6_log - INFO - processing batch-12
2018-05-02 19:33:44,267 - memory_profile6_log - INFO - processing batch-13
2018-05-02 19:33:47,828 - memory_profile6_log - INFO - processing batch-14
2018-05-02 19:33:53,832 - memory_profile6_log - INFO - processing batch-15
2018-05-02 19:33:59,687 - memory_profile6_log - INFO - processing batch-16
2018-05-02 19:34:04,384 - memory_profile6_log - INFO - processing batch-17
2018-05-02 19:34:07,913 - memory_profile6_log - INFO - processing batch-18
2018-05-02 19:34:13,309 - memory_profile6_log - INFO - processing batch-19
2018-05-02 19:34:19,426 - memory_profile6_log - INFO - processing batch-20
2018-05-02 19:34:25,186 - memory_profile6_log - INFO - processing batch-21
2018-05-02 19:34:29,655 - memory_profile6_log - INFO - processing batch-22
2018-05-02 19:34:34,747 - memory_profile6_log - INFO - processing batch-23
2018-05-02 19:34:41,308 - memory_profile6_log - INFO - processing batch-24
2018-05-02 19:34:45,867 - memory_profile6_log - INFO - deleting BR...
2018-05-02 19:34:45,868 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-05-02 19:34:45,869 - memory_profile6_log - INFO - deleting fitted_models_sigmant...
2018-05-02 19:34:45,871 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 19:34:45,872 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 19:34:45,875 - memory_profile6_log - INFO - ================================================

2018-05-02 19:34:45,875 - memory_profile6_log - INFO -    130    147.7 MiB    147.7 MiB   @profile

2018-05-02 19:34:45,877 - memory_profile6_log - INFO -    131                             def main(df_input, df_current, df_hist,

2018-05-02 19:34:45,878 - memory_profile6_log - INFO -    132                                      current_date, G, project_id,

2018-05-02 19:34:45,878 - memory_profile6_log - INFO -    133                                      savetrain=False, multproc=True,

2018-05-02 19:34:45,878 - memory_profile6_log - INFO -    134                                      threshold=0, start_date=None, end_date=None,

2018-05-02 19:34:45,880 - memory_profile6_log - INFO -    135                                      saveto="datastore", fitted_models_hist=None):

2018-05-02 19:34:45,881 - memory_profile6_log - INFO -    136                                 """

2018-05-02 19:34:45,881 - memory_profile6_log - INFO -    137                                     Main Process

2018-05-02 19:34:45,881 - memory_profile6_log - INFO -    138                                 """

2018-05-02 19:34:45,882 - memory_profile6_log - INFO -    139                                 # ~ Data Preprocessing ~

2018-05-02 19:34:45,882 - memory_profile6_log - INFO -    140                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-05-02 19:34:45,884 - memory_profile6_log - INFO -    141                                 # D(u, t)

2018-05-02 19:34:45,887 - memory_profile6_log - INFO -    142    147.7 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-05-02 19:34:45,888 - memory_profile6_log - INFO -    143    147.7 MiB      0.0 MiB       df_dut = df_input.copy(deep=True)

2018-05-02 19:34:45,888 - memory_profile6_log - INFO -    144    147.7 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 19:34:45,888 - memory_profile6_log - INFO -    145                             

2018-05-02 19:34:45,890 - memory_profile6_log - INFO -    146                                 # D(t)

2018-05-02 19:34:45,890 - memory_profile6_log - INFO -    147    147.7 MiB      0.0 MiB       df_dt = df_current.copy(deep=True)

2018-05-02 19:34:45,891 - memory_profile6_log - INFO -    148    147.7 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 19:34:45,891 - memory_profile6_log - INFO -    149                             

2018-05-02 19:34:45,891 - memory_profile6_log - INFO -    150                                 # ~~~~~~ Begin train ~~~~~~

2018-05-02 19:34:45,892 - memory_profile6_log - INFO -    151    147.7 MiB      0.0 MiB       t0 = time.time()

2018-05-02 19:34:45,892 - memory_profile6_log - INFO -    152    147.7 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-05-02 19:34:45,894 - memory_profile6_log - INFO -    153    147.7 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-05-02 19:34:45,894 - memory_profile6_log - INFO -    154    147.7 MiB      0.0 MiB       logger.info("apply on: %d total history...", len(df_hist))

2018-05-02 19:34:45,897 - memory_profile6_log - INFO -    155                             

2018-05-02 19:34:45,898 - memory_profile6_log - INFO -    156                                 # instantiace class

2018-05-02 19:34:45,898 - memory_profile6_log - INFO -    157    147.7 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-05-02 19:34:45,900 - memory_profile6_log - INFO -    158                             

2018-05-02 19:34:45,900 - memory_profile6_log - INFO -    159                                 # ~~ Fit ~~

2018-05-02 19:34:45,901 - memory_profile6_log - INFO -    160                                 #   handling genuine news interest < current date

2018-05-02 19:34:45,901 - memory_profile6_log - INFO -    161    147.9 MiB      0.2 MiB       NB = BR.processX(df_dut)

2018-05-02 19:34:45,901 - memory_profile6_log - INFO -    162                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-05-02 19:34:45,901 - memory_profile6_log - INFO -    163                                 #   nanti dipindah ke class train utama

2018-05-02 19:34:45,901 - memory_profile6_log - INFO -    164    148.0 MiB      0.1 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-05-02 19:34:45,903 - memory_profile6_log - INFO -    165                                 """

2018-05-02 19:34:45,903 - memory_profile6_log - INFO -    166                                     num_y = total global click for category=ci on periode t

2018-05-02 19:34:45,904 - memory_profile6_log - INFO -    167                                     num_x = total click from user_U for category=ci on periode t

2018-05-02 19:34:45,904 - memory_profile6_log - INFO -    168                                 """

2018-05-02 19:34:45,904 - memory_profile6_log - INFO -    169    148.0 MiB      0.0 MiB       fitby_sigmant = False

2018-05-02 19:34:45,904 - memory_profile6_log - INFO -    170    148.0 MiB      0.0 MiB       uniques_fit_hist = None

2018-05-02 19:34:45,904 - memory_profile6_log - INFO -    171    148.0 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-05-02 19:34:45,908 - memory_profile6_log - INFO -    172    148.0 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-05-02 19:34:45,910 - memory_profile6_log - INFO -    173    148.0 MiB      0.0 MiB                            'is_general']]

2018-05-02 19:34:45,910 - memory_profile6_log - INFO -    174                             

2018-05-02 19:34:45,911 - memory_profile6_log - INFO -    175                                 # get sigma_Nt from fitted_models_hist

2018-05-02 19:34:45,911 - memory_profile6_log - INFO -    176    148.0 MiB      0.0 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-05-02 19:34:45,911 - memory_profile6_log - INFO -    177    148.0 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-05-02 19:34:45,911 - memory_profile6_log - INFO -    178    148.0 MiB      0.0 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-05-02 19:34:45,913 - memory_profile6_log - INFO -    179    148.0 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-05-02 19:34:45,914 - memory_profile6_log - INFO -    180                             

2018-05-02 19:34:45,914 - memory_profile6_log - INFO -    181                                 # begin fit

2018-05-02 19:34:45,914 - memory_profile6_log - INFO -    182    148.0 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-05-02 19:34:45,914 - memory_profile6_log - INFO -    183    148.0 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-05-02 19:34:45,914 - memory_profile6_log - INFO -    184    150.3 MiB      2.2 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-05-02 19:34:45,915 - memory_profile6_log - INFO -    185    150.3 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-05-02 19:34:45,915 - memory_profile6_log - INFO -    186    150.3 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-05-02 19:34:45,915 - memory_profile6_log - INFO -    187                                 # print model_fit[['num_x', 'num_x', 'sigma_Nt', 'date_all_click']].loc[model_fit['user_id']=="1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157"].head(5)

2018-05-02 19:34:45,917 - memory_profile6_log - INFO -    188                             

2018-05-02 19:34:45,917 - memory_profile6_log - INFO -    189                                 # ~~ and Transform ~~

2018-05-02 19:34:45,921 - memory_profile6_log - INFO -    190                                 #   handling current news interest == current date

2018-05-02 19:34:45,921 - memory_profile6_log - INFO -    191    150.3 MiB      0.0 MiB       if df_dt.empty:

2018-05-02 19:34:45,923 - memory_profile6_log - INFO -    192                                     print "Cek your df_dt, cannot be emtpy!!"

2018-05-02 19:34:45,923 - memory_profile6_log - INFO -    193                                     return None

2018-05-02 19:34:45,924 - memory_profile6_log - INFO -    194    150.7 MiB      0.5 MiB       NB = BR.processX(df_dt)

2018-05-02 19:34:45,924 - memory_profile6_log - INFO -    195    150.7 MiB      0.0 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-05-02 19:34:45,924 - memory_profile6_log - INFO -    196                             

2018-05-02 19:34:45,926 - memory_profile6_log - INFO -    197    150.7 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-05-02 19:34:45,926 - memory_profile6_log - INFO -    198    151.0 MiB      0.3 MiB                            'num_x', 'num_y', 'is_general']]

2018-05-02 19:34:45,927 - memory_profile6_log - INFO -    199                             

2018-05-02 19:34:45,927 - memory_profile6_log - INFO -    200                                 # print "model_fit dtypes:\n", model_fit.dtypes

2018-05-02 19:34:45,927 - memory_profile6_log - INFO -    201                                 # print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-05-02 19:34:45,927 - memory_profile6_log - INFO -    202    151.0 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-05-02 19:34:45,927 - memory_profile6_log - INFO -    203    151.0 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-05-02 19:34:45,928 - memory_profile6_log - INFO -    204    151.0 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-05-02 19:34:45,928 - memory_profile6_log - INFO -    205    151.0 MiB      0.0 MiB                                                                                           "topic_id", "user_id"]],

2018-05-02 19:34:45,933 - memory_profile6_log - INFO -    206    155.2 MiB      4.2 MiB                                                     verbose=False)

2018-05-02 19:34:45,934 - memory_profile6_log - INFO -    207                             

2018-05-02 19:34:45,934 - memory_profile6_log - INFO -    208                                 # ~~~ filter is general and specific topic ~~~

2018-05-02 19:34:45,936 - memory_profile6_log - INFO -    209                                 # the idea is just we need to rerank every topic according

2018-05-02 19:34:45,936 - memory_profile6_log - INFO -    210                                 #    to user_id and and is_general by p0_posterior

2018-05-02 19:34:45,937 - memory_profile6_log - INFO -    211    155.2 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-05-02 19:34:45,937 - memory_profile6_log - INFO -    212    155.2 MiB      0.0 MiB                                     'is_general']].groupby(['topic_id',

2018-05-02 19:34:45,937 - memory_profile6_log - INFO -    213    155.2 MiB      0.0 MiB                                                             'is_general']

2018-05-02 19:34:45,937 - memory_profile6_log - INFO -    214                                                                                      ).size().to_frame().reset_index()

2018-05-02 19:34:45,937 - memory_profile6_log - INFO -    215                             

2018-05-02 19:34:45,938 - memory_profile6_log - INFO -    216                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-05-02 19:34:45,938 - memory_profile6_log - INFO -    217                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-05-02 19:34:45,938 - memory_profile6_log - INFO -    218    155.3 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-05-02 19:34:45,940 - memory_profile6_log - INFO -    219                             

2018-05-02 19:34:45,940 - memory_profile6_log - INFO -    220                                 # ~ start by provide rank for each topic type ~

2018-05-02 19:34:45,940 - memory_profile6_log - INFO -    221    160.3 MiB      5.1 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-05-02 19:34:45,940 - memory_profile6_log - INFO -    222    159.2 MiB     -1.1 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-05-02 19:34:45,940 - memory_profile6_log - INFO -    223                             

2018-05-02 19:34:45,944 - memory_profile6_log - INFO -    224                                 # ~ set threshold to filter output

2018-05-02 19:34:45,946 - memory_profile6_log - INFO -    225    159.2 MiB      0.0 MiB       if threshold > 0:

2018-05-02 19:34:45,947 - memory_profile6_log - INFO -    226    159.2 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-05-02 19:34:45,947 - memory_profile6_log - INFO -    227    159.2 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-05-02 19:34:45,947 - memory_profile6_log - INFO -    228    157.7 MiB     -1.6 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-05-02 19:34:45,948 - memory_profile6_log - INFO -    229                             

2018-05-02 19:34:45,950 - memory_profile6_log - INFO -    230    157.7 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 19:34:45,950 - memory_profile6_log - INFO -    231    157.7 MiB      0.0 MiB       print model_transform[model_transform['user_id']=='1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157']

2018-05-02 19:34:45,950 - memory_profile6_log - INFO -    232    157.7 MiB      0.0 MiB       logger.info("Len of model_transform: %d", len(model_transform))

2018-05-02 19:34:45,950 - memory_profile6_log - INFO -    233    157.7 MiB      0.0 MiB       logger.info("Len of df_dt: %d", len(df_dt))

2018-05-02 19:34:45,950 - memory_profile6_log - INFO -    234    157.7 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-05-02 19:34:45,951 - memory_profile6_log - INFO -    235                             

2018-05-02 19:34:45,951 - memory_profile6_log - INFO -    236    157.7 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-05-02 19:34:45,953 - memory_profile6_log - INFO -    237                             

2018-05-02 19:34:45,953 - memory_profile6_log - INFO -    238    157.7 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-05-02 19:34:45,953 - memory_profile6_log - INFO -    239    157.7 MiB      0.0 MiB       del df_dut

2018-05-02 19:34:45,957 - memory_profile6_log - INFO -    240    157.7 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-05-02 19:34:45,957 - memory_profile6_log - INFO -    241    157.7 MiB      0.0 MiB       del df_dt

2018-05-02 19:34:45,957 - memory_profile6_log - INFO -    242    157.7 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-05-02 19:34:45,959 - memory_profile6_log - INFO -    243    157.7 MiB      0.0 MiB       del df_input

2018-05-02 19:34:45,960 - memory_profile6_log - INFO -    244    157.7 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-05-02 19:34:45,960 - memory_profile6_log - INFO -    245    157.7 MiB      0.0 MiB       del df_input_X

2018-05-02 19:34:45,960 - memory_profile6_log - INFO -    246    157.7 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-05-02 19:34:45,960 - memory_profile6_log - INFO -    247    157.7 MiB      0.0 MiB       del df_current

2018-05-02 19:34:45,960 - memory_profile6_log - INFO -    248    157.7 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-05-02 19:34:45,961 - memory_profile6_log - INFO -    249    157.7 MiB      0.0 MiB       del map_topic_isgeneral

2018-05-02 19:34:45,961 - memory_profile6_log - INFO -    250    157.7 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-05-02 19:34:45,963 - memory_profile6_log - INFO -    251    155.9 MiB     -1.7 MiB       del model_fit

2018-05-02 19:34:45,963 - memory_profile6_log - INFO -    252    155.9 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-05-02 19:34:45,963 - memory_profile6_log - INFO -    253    155.9 MiB      0.0 MiB       del result

2018-05-02 19:34:45,963 - memory_profile6_log - INFO -    254    155.9 MiB      0.0 MiB       logger.info("deleting result...")

2018-05-02 19:34:45,963 - memory_profile6_log - INFO -    255                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 19:34:45,963 - memory_profile6_log - INFO -    256                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 19:34:45,964 - memory_profile6_log - INFO -    257                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 19:34:45,964 - memory_profile6_log - INFO -    258    155.9 MiB      0.0 MiB       if savetrain:

2018-05-02 19:34:45,964 - memory_profile6_log - INFO -    259    155.9 MiB      0.0 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'p0_posterior', 'rank']].copy(deep=True)

2018-05-02 19:34:45,970 - memory_profile6_log - INFO -    260    155.9 MiB      0.0 MiB           model_transformsv['date'] = current_date  # we need manually adding date, because table not support

2018-05-02 19:34:45,970 - memory_profile6_log - INFO -    261    155.9 MiB      0.0 MiB           model_transformsv['date'] = pd.to_datetime(model_transformsv['date'],

2018-05-02 19:34:45,970 - memory_profile6_log - INFO -    262    156.0 MiB      0.0 MiB                                                      format='%Y-%m-%d', errors='coerce')

2018-05-02 19:34:45,971 - memory_profile6_log - INFO -    263    156.0 MiB      0.0 MiB           print "list(model_transformsv) before rename: ", list(model_transformsv)       

2018-05-02 19:34:45,971 - memory_profile6_log - INFO -    264    156.0 MiB      0.0 MiB           print len(model_transformsv)                

2018-05-02 19:34:45,973 - memory_profile6_log - INFO -    265    156.0 MiB      0.0 MiB           model_transformsv = model_transformsv.rename(columns={'is_general': 'topic_is_general', 'p0_posterior': 'interest_score',

2018-05-02 19:34:45,973 - memory_profile6_log - INFO -    266    156.0 MiB      0.0 MiB                                                                 'rank':'interest_rank' , 'date':'interest_score_created_at'})

2018-05-02 19:34:45,973 - memory_profile6_log - INFO -    267    156.0 MiB      0.0 MiB           print "list(model_transformsv) after rename: ", list(model_transformsv)    

2018-05-02 19:34:45,973 - memory_profile6_log - INFO -    268    156.0 MiB      0.0 MiB           print len(model_transformsv) 

2018-05-02 19:34:45,973 - memory_profile6_log - INFO -    269    156.0 MiB      0.0 MiB           del model_transform

2018-05-02 19:34:45,974 - memory_profile6_log - INFO -    270    156.0 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-05-02 19:34:45,974 - memory_profile6_log - INFO -    271    156.0 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-05-02 19:34:45,974 - memory_profile6_log - INFO -    272                             

2018-05-02 19:34:45,976 - memory_profile6_log - INFO -    273    156.0 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-05-02 19:34:45,976 - memory_profile6_log - INFO -    274                                     # ~ Place your code to save the training model here ~

2018-05-02 19:34:45,976 - memory_profile6_log - INFO -    275    156.0 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-05-02 19:34:45,976 - memory_profile6_log - INFO -    276                                         logger.info("Using google datastore as storage...")

2018-05-02 19:34:45,976 - memory_profile6_log - INFO -    277                                         if multproc:

2018-05-02 19:34:45,976 - memory_profile6_log - INFO -    278                                             # ~ save transform models ~

2018-05-02 19:34:45,982 - memory_profile6_log - INFO -    279                                             logger.info("Saving main Transform model to Google DataStore...")

2018-05-02 19:34:45,982 - memory_profile6_log - INFO -    280                                             logger.info("Saving total data: %d", len(model_transformsv))

2018-05-02 19:34:45,983 - memory_profile6_log - INFO -    281                                             mh.saveDataStorePutMulti(model_transformsv)

2018-05-02 19:34:45,983 - memory_profile6_log - INFO -    282                             

2018-05-02 19:34:45,983 - memory_profile6_log - INFO -    283                                             # ~ save fitted models ~

2018-05-02 19:34:45,984 - memory_profile6_log - INFO -    284                                             logger.info("Saving fitted_models as history to Google DataStore...")

2018-05-02 19:34:45,986 - memory_profile6_log - INFO -    285                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 19:34:45,986 - memory_profile6_log - INFO -    286                                             fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-05-02 19:34:45,986 - memory_profile6_log - INFO -    287                                             logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-05-02 19:34:45,986 - memory_profile6_log - INFO -    288                                             X_split = np.array_split(fitted_models_sigmant, 10)

2018-05-02 19:34:45,987 - memory_profile6_log - INFO -    289                                             logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-05-02 19:34:45,989 - memory_profile6_log - INFO -    290                                             for ix in range(len(X_split)):

2018-05-02 19:34:45,989 - memory_profile6_log - INFO -    291                                                 logger.info("processing batch-%d", ix)

2018-05-02 19:34:45,990 - memory_profile6_log - INFO -    292                                                 mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-05-02 19:34:45,993 - memory_profile6_log - INFO -    293                             

2018-05-02 19:34:45,993 - memory_profile6_log - INFO -    294                                             del X_split

2018-05-02 19:34:45,994 - memory_profile6_log - INFO -    295                                             logger.info("deleting X_split...")

2018-05-02 19:34:45,996 - memory_profile6_log - INFO -    296                                             del save_sigma_nt

2018-05-02 19:34:45,996 - memory_profile6_log - INFO -    297                                             logger.info("deleting save_sigma_nt...")

2018-05-02 19:34:45,997 - memory_profile6_log - INFO -    298                             

2018-05-02 19:34:45,997 - memory_profile6_log - INFO -    299                                             del BR

2018-05-02 19:34:45,999 - memory_profile6_log - INFO -    300                                             logger.info("deleting BR...")

2018-05-02 19:34:45,999 - memory_profile6_log - INFO -    301                             

2018-05-02 19:34:46,000 - memory_profile6_log - INFO -    302    156.0 MiB      0.0 MiB           elif str(saveto).lower() == "elastic":

2018-05-02 19:34:46,000 - memory_profile6_log - INFO -    303    156.0 MiB      0.0 MiB               logger.info("Using ElasticSearch as storage...")

2018-05-02 19:34:46,000 - memory_profile6_log - INFO -    304    156.0 MiB      0.0 MiB               logging.info("Saving main Transform model to Elasticsearch...")

2018-05-02 19:34:46,003 - memory_profile6_log - INFO -    305                                         # print model_transformsv.head(5)

2018-05-02 19:34:46,005 - memory_profile6_log - INFO -    306                                         

2018-05-02 19:34:46,005 - memory_profile6_log - INFO -    307    156.0 MiB      0.0 MiB               X_split = np.array_split(model_transformsv, 15)

2018-05-02 19:34:46,006 - memory_profile6_log - INFO -    308    156.0 MiB      0.0 MiB               logger.info("Saving total data: %d", len(model_transformsv))

2018-05-02 19:34:46,006 - memory_profile6_log - INFO -    309    156.0 MiB      0.0 MiB               logger.info("Len of X_split for batch save model_transformsv: %d", len(X_split))

2018-05-02 19:34:46,007 - memory_profile6_log - INFO -    310    156.0 MiB      0.0 MiB               for ix in range(len(X_split)):

2018-05-02 19:34:46,009 - memory_profile6_log - INFO -    311    156.0 MiB    -81.8 MiB                   logger.info("processing batch-%d", ix)

2018-05-02 19:34:46,009 - memory_profile6_log - INFO -    312    150.5 MiB    -87.4 MiB                   mh.saveElasticS(X_split[ix], esp)

2018-05-02 19:34:46,010 - memory_profile6_log - INFO -    313    150.5 MiB     -5.5 MiB               del X_split

2018-05-02 19:34:46,012 - memory_profile6_log - INFO -    314                                         

2018-05-02 19:34:46,013 - memory_profile6_log - INFO -    315                             

2018-05-02 19:34:46,016 - memory_profile6_log - INFO -    316    150.5 MiB      0.0 MiB               logger.info("Saving fitted_models as history to Elasticsearch...")

2018-05-02 19:34:46,016 - memory_profile6_log - INFO -    317    150.5 MiB      0.0 MiB               save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 19:34:46,017 - memory_profile6_log - INFO -    318    150.9 MiB      0.4 MiB               fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-05-02 19:34:46,019 - memory_profile6_log - INFO -    319                             

2018-05-02 19:34:46,019 - memory_profile6_log - INFO -    320    150.9 MiB      0.0 MiB               fitted_models_sigmant['uid_topid'] = fitted_models_sigmant["user_id"].map(str) + "_" + fitted_models_sigmant["topic_id"].map(str)

2018-05-02 19:34:46,019 - memory_profile6_log - INFO -    321    151.8 MiB      0.9 MiB               fitted_models_sigmant = fitted_models_sigmant[["uid_topid", "pt_posterior_x_Nt", "smoothed_pt_posterior", "p0_cat_ci", "sigma_Nt"]]

2018-05-02 19:34:46,020 - memory_profile6_log - INFO -    322                               

2018-05-02 19:34:46,022 - memory_profile6_log - INFO -    323    152.4 MiB      0.6 MiB               X_split = np.array_split(fitted_models_sigmant, 25)

2018-05-02 19:34:46,023 - memory_profile6_log - INFO -    324    152.4 MiB      0.0 MiB               logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-05-02 19:34:46,029 - memory_profile6_log - INFO -    325    152.4 MiB      0.0 MiB               logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-05-02 19:34:46,030 - memory_profile6_log - INFO -    326    154.4 MiB     -0.0 MiB               for ix in range(len(X_split)):

2018-05-02 19:34:46,032 - memory_profile6_log - INFO -    327    154.4 MiB     -0.0 MiB                   logger.info("processing batch-%d", ix)

2018-05-02 19:34:46,033 - memory_profile6_log - INFO -    328    154.4 MiB     -0.0 MiB                   mh.saveElasticS(X_split[ix], esp,

2018-05-02 19:34:46,036 - memory_profile6_log - INFO -    329    154.4 MiB     -0.0 MiB                                   esindex_name="fitted_hist_index",

2018-05-02 19:34:46,038 - memory_profile6_log - INFO -    330    154.4 MiB      1.9 MiB                                   estype_name='fitted_hist_type')

2018-05-02 19:34:46,040 - memory_profile6_log - INFO -    331    154.4 MiB      0.0 MiB               del X_split

2018-05-02 19:34:46,042 - memory_profile6_log - INFO -    332                                         

2018-05-02 19:34:46,043 - memory_profile6_log - INFO -    333    154.4 MiB      0.0 MiB               del BR

2018-05-02 19:34:46,045 - memory_profile6_log - INFO -    334    154.4 MiB      0.0 MiB               logger.info("deleting BR...")

2018-05-02 19:34:46,046 - memory_profile6_log - INFO -    335    154.4 MiB      0.0 MiB               del save_sigma_nt

2018-05-02 19:34:46,046 - memory_profile6_log - INFO -    336    154.4 MiB      0.0 MiB               logger.info("deleting save_sigma_nt...")

2018-05-02 19:34:46,052 - memory_profile6_log - INFO -    337    154.4 MiB      0.0 MiB               del fitted_models_sigmant

2018-05-02 19:34:46,053 - memory_profile6_log - INFO -    338    154.4 MiB      0.0 MiB               logger.info("deleting fitted_models_sigmant...")

2018-05-02 19:34:46,055 - memory_profile6_log - INFO -    339                             

2018-05-02 19:34:46,056 - memory_profile6_log - INFO -    340                                     # need save sigma_nt for daily train

2018-05-02 19:34:46,059 - memory_profile6_log - INFO -    341                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-05-02 19:34:46,061 - memory_profile6_log - INFO -    342                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-05-02 19:34:46,065 - memory_profile6_log - INFO -    343    154.4 MiB      0.0 MiB           if start_date and end_date:

2018-05-02 19:34:46,066 - memory_profile6_log - INFO -    344                                         if not fitby_sigmant:

2018-05-02 19:34:46,068 - memory_profile6_log - INFO -    345                                             logging.info("Saving sigma Nt...")

2018-05-02 19:34:46,069 - memory_profile6_log - INFO -    346                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 19:34:46,069 - memory_profile6_log - INFO -    347                                             save_sigma_nt['start_date'] = start_date

2018-05-02 19:34:46,071 - memory_profile6_log - INFO -    348                                             save_sigma_nt['end_date'] = end_date

2018-05-02 19:34:46,072 - memory_profile6_log - INFO -    349                                             print save_sigma_nt.head(5)

2018-05-02 19:34:46,075 - memory_profile6_log - INFO -    350                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-05-02 19:34:46,075 - memory_profile6_log - INFO -    351                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-05-02 19:34:46,076 - memory_profile6_log - INFO -    352    154.4 MiB      0.0 MiB       return

2018-05-02 19:34:46,078 - memory_profile6_log - INFO - 


2018-05-02 19:34:46,078 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-05-02 21:37:25,828 - memory_profile6_log - INFO - ElasticSearch host: https://9db53c7bb4f5be2d856033a9aeb6e5a5.us-central1.gcp.cloud.es.io
2018-05-02 21:37:25,885 - memory_profile6_log - INFO - ElasticSearch port: 9243
2018-05-02 21:37:25,888 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 21:37:25,890 - memory_profile6_log - INFO - date_generated: 
2018-05-02 21:37:25,891 - memory_profile6_log - INFO -  
2018-05-02 21:37:25,891 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 21, 37, 25, 888000)]
2018-05-02 21:37:25,891 - memory_profile6_log - INFO - 

2018-05-02 21:37:25,891 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 21:37:25,892 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 21:37:25,892 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 21:37:26,039 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 21:37:26,042 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 21:37:34,007 - memory_profile6_log - INFO - size of df: 2.53 MB
2018-05-02 21:37:34,009 - memory_profile6_log - INFO - getting total: 10000 training data(genuine interest) for date: 2018-05-01
2018-05-02 21:37:34,101 - memory_profile6_log - INFO - loading history data from elastic...
2018-05-02 21:37:34,104 - memory_profile6_log - INFO - Len of X_split for batch load: 25
2018-05-02 21:37:34,105 - memory_profile6_log - INFO - Appending history data...
2018-05-02 21:37:34,150 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:37:37,204 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:37:37,742 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:37:38,290 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:37:38,904 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:37:39,905 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:37:40,642 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:37:41,289 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:37:42,217 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:37:43,316 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:37:44,385 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:37:45,200 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:37:45,815 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:37:46,280 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:37:46,859 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:37:47,377 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:37:47,969 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:37:48,447 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:37:48,980 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:37:49,490 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:37:50,134 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:37:50,691 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:37:51,451 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:37:52,220 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:37:52,835 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:37:53,410 - memory_profile6_log - INFO - Appending training data...
2018-05-02 21:37:53,411 - memory_profile6_log - INFO - len datalist: 1
2018-05-02 21:37:53,413 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-05-02 21:37:53,414 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 21:37:53,415 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 21:37:53,417 - memory_profile6_log - INFO - ================================================

2018-05-02 21:37:53,417 - memory_profile6_log - INFO -    379     86.7 MiB     86.7 MiB   @profile

2018-05-02 21:37:53,418 - memory_profile6_log - INFO -    380                             def BQPreprocess(cpu, date_generated, client, query_fit, loadfrom="elastic"):

2018-05-02 21:37:53,421 - memory_profile6_log - INFO -    381     86.7 MiB      0.0 MiB       bq_client = client

2018-05-02 21:37:53,423 - memory_profile6_log - INFO -    382     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 21:37:53,424 - memory_profile6_log - INFO -    383                             

2018-05-02 21:37:53,426 - memory_profile6_log - INFO -    384     86.7 MiB      0.0 MiB       datalist = []

2018-05-02 21:37:53,427 - memory_profile6_log - INFO -    385     86.7 MiB      0.0 MiB       datalist_hist = []

2018-05-02 21:37:53,428 - memory_profile6_log - INFO -    386                             

2018-05-02 21:37:53,430 - memory_profile6_log - INFO -    387     86.7 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-05-02 21:37:53,434 - memory_profile6_log - INFO -    388    116.9 MiB      0.0 MiB       for ndate in date_generated:

2018-05-02 21:37:53,434 - memory_profile6_log - INFO -    389    116.0 MiB     29.3 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-05-02 21:37:53,436 - memory_profile6_log - INFO -    390    116.0 MiB      0.0 MiB           if tframe is not None:

2018-05-02 21:37:53,437 - memory_profile6_log - INFO -    391    116.0 MiB      0.0 MiB               if not tframe.empty:

2018-05-02 21:37:53,437 - memory_profile6_log - INFO -    392    116.0 MiB      0.0 MiB                   if loadfrom.strip().lower() == 'datastore':

2018-05-02 21:37:53,438 - memory_profile6_log - INFO -    393                                                 X_split = np.array_split(tframe, 5)

2018-05-02 21:37:53,440 - memory_profile6_log - INFO -    394                                                 logger.info("loading history data from datastore...")

2018-05-02 21:37:53,440 - memory_profile6_log - INFO -    395                                                 logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 21:37:53,444 - memory_profile6_log - INFO -    396                                                 logger.info("Appending history data...")

2018-05-02 21:37:53,444 - memory_profile6_log - INFO -    397                                                 for ix in range(len(X_split)):

2018-05-02 21:37:53,446 - memory_profile6_log - INFO -    398                                                     # ~ loading history

2018-05-02 21:37:53,447 - memory_profile6_log - INFO -    399                                                     """

2018-05-02 21:37:53,448 - memory_profile6_log - INFO -    400                                                         disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-05-02 21:37:53,450 - memory_profile6_log - INFO -    401                                                     """

2018-05-02 21:37:53,450 - memory_profile6_log - INFO -    402                                                     logger.info("processing batch-%d", ix)

2018-05-02 21:37:53,450 - memory_profile6_log - INFO -    403                                                     # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-05-02 21:37:53,451 - memory_profile6_log - INFO -    404                                                     logger.info("creating list history data...")

2018-05-02 21:37:53,454 - memory_profile6_log - INFO -    405                                                     #lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-05-02 21:37:53,456 - memory_profile6_log - INFO -    406                                                     lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 21:37:53,457 - memory_profile6_log - INFO -    407                             

2018-05-02 21:37:53,459 - memory_profile6_log - INFO -    408                                                     logger.info("call history data...")

2018-05-02 21:37:53,460 - memory_profile6_log - INFO -    409                                                     h_frame = mh.loadDSHistory(lhistory)

2018-05-02 21:37:53,460 - memory_profile6_log - INFO -    410                             

2018-05-02 21:37:53,461 - memory_profile6_log - INFO -    411                                                     # me = os.getpid()

2018-05-02 21:37:53,461 - memory_profile6_log - INFO -    412                                                     # kill_proc_tree(me)

2018-05-02 21:37:53,463 - memory_profile6_log - INFO -    413                             

2018-05-02 21:37:53,466 - memory_profile6_log - INFO -    414                                                     logger.info("done collecting history data, appending now...")

2018-05-02 21:37:53,467 - memory_profile6_log - INFO -    415                                                     for m in h_frame:

2018-05-02 21:37:53,467 - memory_profile6_log - INFO -    416                                                         if m is not None:

2018-05-02 21:37:53,469 - memory_profile6_log - INFO -    417                                                             if len(m) > 0:

2018-05-02 21:37:53,469 - memory_profile6_log - INFO -    418                                                                 datalist_hist.append(pd.DataFrame(m))

2018-05-02 21:37:53,470 - memory_profile6_log - INFO -    419                                                     del h_frame

2018-05-02 21:37:53,470 - memory_profile6_log - INFO -    420                                                     del lhistory

2018-05-02 21:37:53,471 - memory_profile6_log - INFO -    421                             

2018-05-02 21:37:53,471 - memory_profile6_log - INFO -    422                                                 logger.info("Appending training data...")

2018-05-02 21:37:53,473 - memory_profile6_log - INFO -    423                                                 datalist.append(tframe)

2018-05-02 21:37:53,476 - memory_profile6_log - INFO -    424    116.0 MiB      0.0 MiB                   elif loadfrom.strip().lower() == 'elastic':

2018-05-02 21:37:53,477 - memory_profile6_log - INFO -    425    116.1 MiB      0.1 MiB                       X_split = np.array_split(tframe, 25)

2018-05-02 21:37:53,479 - memory_profile6_log - INFO -    426    116.1 MiB      0.0 MiB                       logger.info("loading history data from elastic...")

2018-05-02 21:37:53,479 - memory_profile6_log - INFO -    427    116.1 MiB      0.0 MiB                       logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 21:37:53,480 - memory_profile6_log - INFO -    428    116.1 MiB      0.0 MiB                       logger.info("Appending history data...")

2018-05-02 21:37:53,482 - memory_profile6_log - INFO -    429                                                 

2018-05-02 21:37:53,483 - memory_profile6_log - INFO -    430    116.9 MiB     -0.0 MiB                       for ix in range(len(X_split)):

2018-05-02 21:37:53,483 - memory_profile6_log - INFO -    431    116.9 MiB      0.3 MiB                           lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 21:37:53,483 - memory_profile6_log - INFO -    432    116.9 MiB     -0.0 MiB                           logger.info("call %d history data...", len(lhistory))

2018-05-02 21:37:53,486 - memory_profile6_log - INFO -    433    116.9 MiB     -0.0 MiB                           inside_data = mh.loadESHistory(lhistory, es,

2018-05-02 21:37:53,490 - memory_profile6_log - INFO -    434    116.9 MiB     -0.0 MiB                                                          esindex_name='fitted_hist_index',

2018-05-02 21:37:53,493 - memory_profile6_log - INFO -    435    116.9 MiB      0.1 MiB                                                          estype_name='fitted_hist_type')

2018-05-02 21:37:53,493 - memory_profile6_log - INFO -    436                                                     # split back the user_id and topic_id

2018-05-02 21:37:53,494 - memory_profile6_log - INFO -    437    116.9 MiB      0.0 MiB                           inside_data[['user_id','topic_id']] = inside_data.uid_topid.str.split('_', expand=True)

2018-05-02 21:37:53,496 - memory_profile6_log - INFO -    438    116.9 MiB      0.2 MiB                           inside_data = inside_data[["user_id","topic_id", "pt_posterior_x_Nt", "smoothed_pt_posterior", "p0_cat_ci", "sigma_Nt"]]

2018-05-02 21:37:53,497 - memory_profile6_log - INFO -    439    116.9 MiB     -0.0 MiB                           if not inside_data.empty:

2018-05-02 21:37:53,499 - memory_profile6_log - INFO -    440    116.9 MiB     -0.0 MiB                               datalist_hist.append(inside_data)

2018-05-02 21:37:53,500 - memory_profile6_log - INFO -    441    116.9 MiB     -0.0 MiB                               del inside_data

2018-05-02 21:37:53,502 - memory_profile6_log - INFO -    442                                                 

2018-05-02 21:37:53,502 - memory_profile6_log - INFO -    443    116.9 MiB      0.0 MiB                       logger.info("Appending training data...")

2018-05-02 21:37:53,503 - memory_profile6_log - INFO -    444    116.9 MiB      0.0 MiB                       datalist.append(tframe)

2018-05-02 21:37:53,503 - memory_profile6_log - INFO -    445                                             else:

2018-05-02 21:37:53,505 - memory_profile6_log - INFO -    446                                                 logger.info("Unknows source is selected !")

2018-05-02 21:37:53,505 - memory_profile6_log - INFO -    447                                                 break

2018-05-02 21:37:53,509 - memory_profile6_log - INFO -    448                                     else: 

2018-05-02 21:37:53,509 - memory_profile6_log - INFO -    449                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-05-02 21:37:53,510 - memory_profile6_log - INFO -    450    116.9 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-05-02 21:37:53,512 - memory_profile6_log - INFO -    451    116.9 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-05-02 21:37:53,513 - memory_profile6_log - INFO -    452                             

2018-05-02 21:37:53,513 - memory_profile6_log - INFO -    453    116.9 MiB      0.0 MiB       return datalist, datalist_hist

2018-05-02 21:37:53,516 - memory_profile6_log - INFO - 


2018-05-02 21:37:53,538 - memory_profile6_log - INFO - len of big_frame_hist: 10000
2018-05-02 21:37:53,548 - memory_profile6_log - INFO - size of big_frame_hist: 2.75 MB
2018-05-02 21:37:53,559 - memory_profile6_log - INFO - size of big_frame: 2.53 MB
2018-05-02 21:37:53,561 - memory_profile6_log - INFO - len of big_frame: 10000
2018-05-02 21:37:53,563 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-05-02 21:38:01,072 - memory_profile6_log - INFO - size of df: 2.46 MB
2018-05-02 21:38:01,073 - memory_profile6_log - INFO - getting total: 10000 training data(current date interest)
2018-05-02 21:38:01,085 - memory_profile6_log - INFO - size of current_frame: 2.54 MB
2018-05-02 21:38:01,086 - memory_profile6_log - INFO - loading time of: 20000 total genuine-current interest data ~ take 35.085s
2018-05-02 21:38:01,088 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 21:38:01,089 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 21:38:01,091 - memory_profile6_log - INFO - ================================================

2018-05-02 21:38:01,092 - memory_profile6_log - INFO -    455     86.6 MiB     86.6 MiB   @profile

2018-05-02 21:38:01,094 - memory_profile6_log - INFO -    456                             def preprocess(cpu, cd, query_fit, date_generated):

2018-05-02 21:38:01,095 - memory_profile6_log - INFO -    457     86.7 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-05-02 21:38:01,095 - memory_profile6_log - INFO -    458     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 21:38:01,096 - memory_profile6_log - INFO -    459                             

2018-05-02 21:38:01,096 - memory_profile6_log - INFO -    460                                 # ~~~ Begin collecting data ~~~

2018-05-02 21:38:01,096 - memory_profile6_log - INFO -    461     86.7 MiB      0.0 MiB       t0 = time.time()

2018-05-02 21:38:01,098 - memory_profile6_log - INFO -    462    116.9 MiB     30.2 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-05-02 21:38:01,099 - memory_profile6_log - INFO -    463    116.9 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-05-02 21:38:01,099 - memory_profile6_log - INFO -    464                                     logger.info("Training cannot be empty..")

2018-05-02 21:38:01,101 - memory_profile6_log - INFO -    465                                     return False

2018-05-02 21:38:01,101 - memory_profile6_log - INFO -    466    116.9 MiB      0.0 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-05-02 21:38:01,101 - memory_profile6_log - INFO -    467    116.9 MiB      0.0 MiB       logger.info("len of big_frame_hist: %s", len(big_frame_hist))

2018-05-02 21:38:01,101 - memory_profile6_log - INFO -    468    116.9 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-05-02 21:38:01,107 - memory_profile6_log - INFO -    469                             

2018-05-02 21:38:01,107 - memory_profile6_log - INFO -    470    116.9 MiB      0.0 MiB       big_frame = pd.concat(datalist)

2018-05-02 21:38:01,109 - memory_profile6_log - INFO -    471    116.9 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-05-02 21:38:01,109 - memory_profile6_log - INFO -    472    116.9 MiB      0.0 MiB       logger.info("len of big_frame: %s", len(big_frame))

2018-05-02 21:38:01,111 - memory_profile6_log - INFO -    473    116.9 MiB      0.0 MiB       del datalist

2018-05-02 21:38:01,111 - memory_profile6_log - INFO -    474                             

2018-05-02 21:38:01,111 - memory_profile6_log - INFO -    475    116.9 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 21:38:01,111 - memory_profile6_log - INFO -    476                             

2018-05-02 21:38:01,111 - memory_profile6_log - INFO -    477                                 # ~ get current news interest ~

2018-05-02 21:38:01,112 - memory_profile6_log - INFO -    478    116.9 MiB      0.0 MiB       if not cd:

2018-05-02 21:38:01,112 - memory_profile6_log - INFO -    479    116.9 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-05-02 21:38:01,114 - memory_profile6_log - INFO -    480    120.2 MiB      3.3 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-05-02 21:38:01,114 - memory_profile6_log - INFO -    481    120.2 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-05-02 21:38:01,114 - memory_profile6_log - INFO -    482                                 else:

2018-05-02 21:38:01,119 - memory_profile6_log - INFO -    483                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-05-02 21:38:01,121 - memory_profile6_log - INFO -    484                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date) LIMIT 10000"

2018-05-02 21:38:01,121 - memory_profile6_log - INFO -    485                             

2018-05-02 21:38:01,121 - memory_profile6_log - INFO -    486                                     # safe handling of query parameter

2018-05-02 21:38:01,122 - memory_profile6_log - INFO -    487                                     query_params = [

2018-05-02 21:38:01,122 - memory_profile6_log - INFO -    488                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-05-02 21:38:01,125 - memory_profile6_log - INFO -    489                                     ]

2018-05-02 21:38:01,125 - memory_profile6_log - INFO -    490                             

2018-05-02 21:38:01,125 - memory_profile6_log - INFO -    491                                     job_config.query_parameters = query_params

2018-05-02 21:38:01,127 - memory_profile6_log - INFO -    492                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-05-02 21:38:01,127 - memory_profile6_log - INFO -    493                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-05-02 21:38:01,130 - memory_profile6_log - INFO -    494                             

2018-05-02 21:38:01,131 - memory_profile6_log - INFO -    495    120.2 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-05-02 21:38:01,131 - memory_profile6_log - INFO -    496    120.2 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-05-02 21:38:01,134 - memory_profile6_log - INFO -    497    120.2 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-05-02 21:38:01,134 - memory_profile6_log - INFO -    498    120.2 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-05-02 21:38:01,134 - memory_profile6_log - INFO -    499    120.2 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 21:38:01,134 - memory_profile6_log - INFO -    500    120.2 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-05-02 21:38:01,135 - memory_profile6_log - INFO -    501                             

2018-05-02 21:38:01,135 - memory_profile6_log - INFO -    502    120.2 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-05-02 21:38:01,137 - memory_profile6_log - INFO - 


2018-05-02 21:38:01,141 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-05-02 21:38:01,148 - memory_profile6_log - INFO - train on: 10000 total genuine interest data(D(u, t))
2018-05-02 21:38:01,148 - memory_profile6_log - INFO - transform on: 10000 total current data(D(t))
2018-05-02 21:38:01,151 - memory_profile6_log - INFO - apply on: 10000 total history...
2018-05-02 21:38:01,203 - memory_profile6_log - INFO - len of uniques_fit_hist:10000
2018-05-02 21:38:01,211 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:1413
2018-05-02 21:38:01,272 - memory_profile6_log - INFO - self.sum_all_nt before concate:

2018-05-02 21:38:01,289 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 21:38:01,289 - memory_profile6_log - INFO - 

2018-05-02 21:38:01,302 - memory_profile6_log - INFO - self.sum_all_nt after concate:

2018-05-02 21:38:01,312 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 21:38:01,312 - memory_profile6_log - INFO - 

2018-05-02 21:38:01,358 - memory_profile6_log - INFO - Len of model_fit: 10000
2018-05-02 21:38:01,358 - memory_profile6_log - INFO - Len of df_dut: 10000
2018-05-02 21:38:01,496 - memory_profile6_log - INFO - Len of fitted_models on main class: 10000
2018-05-02 21:38:01,496 - memory_profile6_log - INFO - 

2018-05-02 21:38:01,499 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-05-02 21:38:01,500 - memory_profile6_log - INFO - 

2018-05-02 21:38:01,502 - memory_profile6_log - INFO - fitted_model_hist:

2018-05-02 21:38:01,503 - memory_profile6_log - INFO - 

2018-05-02 21:38:01,515 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 21:38:01,516 - memory_profile6_log - INFO - 

2018-05-02 21:38:01,526 - memory_profile6_log - INFO - len of fitted models after concat: 20000
2018-05-02 21:38:01,529 - memory_profile6_log - INFO - 

2018-05-02 21:38:01,529 - memory_profile6_log - INFO - Recalculating fitted models...
2018-05-02 21:38:01,533 - memory_profile6_log - INFO - 

2018-05-02 21:38:01,562 - memory_profile6_log - INFO - fitted_models after cobine:

2018-05-02 21:38:01,562 - memory_profile6_log - INFO - 

2018-05-02 21:38:01,572 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 21:38:01,573 - memory_profile6_log - INFO - 

2018-05-02 21:38:01,575 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 10000
2018-05-02 21:38:01,575 - memory_profile6_log - INFO - 

2018-05-02 21:38:03,125 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-05-02 21:38:03,141 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, topic_id, pt_posterior_x_Nt, smoothed_pt_posterior, p0_cat_ci, sigma_Nt, p0_posterior, is_general, rank]
Index: []
2018-05-02 21:38:03,142 - memory_profile6_log - INFO - 

2018-05-02 21:38:03,144 - memory_profile6_log - INFO - Len of model_transform: 4955
2018-05-02 21:38:03,145 - memory_profile6_log - INFO - Len of df_dt: 10000
2018-05-02 21:38:03,147 - memory_profile6_log - INFO - Total train time: 1.985s
2018-05-02 21:38:03,148 - memory_profile6_log - INFO - memory left before cleaning: 73.200 percent memory...
2018-05-02 21:38:03,148 - memory_profile6_log - INFO - cleaning up some objects...
2018-05-02 21:38:03,150 - memory_profile6_log - INFO - deleting df_dut...
2018-05-02 21:38:03,151 - memory_profile6_log - INFO - deleting df_dt...
2018-05-02 21:38:03,151 - memory_profile6_log - INFO - deleting df_input...
2018-05-02 21:38:03,154 - memory_profile6_log - INFO - deleting df_input_X...
2018-05-02 21:38:03,155 - memory_profile6_log - INFO - deleting df_current...
2018-05-02 21:38:03,157 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-05-02 21:38:03,158 - memory_profile6_log - INFO - deleting model_fit...
2018-05-02 21:38:03,160 - memory_profile6_log - INFO - deleting result...
2018-05-02 21:38:03,167 - memory_profile6_log - INFO - list(model_transformsv) before rename: 
2018-05-02 21:38:03,167 - memory_profile6_log - INFO -  
2018-05-02 21:38:03,168 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'is_general', 'p0_posterior', 'rank', 'date']
2018-05-02 21:38:03,171 - memory_profile6_log - INFO - 

2018-05-02 21:38:03,191 - memory_profile6_log - INFO -                       date
5  2018-05-02 21:37:25.888
27 2018-05-02 21:37:25.888
59 2018-05-02 21:37:25.888
70 2018-05-02 21:37:25.888
84 2018-05-02 21:37:25.888
2018-05-02 21:38:03,193 - memory_profile6_log - INFO - 

2018-05-02 21:38:03,194 - memory_profile6_log - INFO - 4955
2018-05-02 21:38:03,194 - memory_profile6_log - INFO - 

2018-05-02 21:38:03,197 - memory_profile6_log - INFO - list(model_transformsv) after rename: 
2018-05-02 21:38:03,197 - memory_profile6_log - INFO -  
2018-05-02 21:38:03,198 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'topic_is_general', 'interest_score', 'interest_rank', 'interest_score_created_at']
2018-05-02 21:38:03,200 - memory_profile6_log - INFO - 

2018-05-02 21:38:03,203 - memory_profile6_log - INFO - 4955
2018-05-02 21:38:03,203 - memory_profile6_log - INFO - 

2018-05-02 21:38:03,204 - memory_profile6_log - INFO - deleting model_transform...
2018-05-02 21:38:03,206 - memory_profile6_log - INFO - memory left after cleaning: 73.200 percent memory...
2018-05-02 21:38:03,207 - memory_profile6_log - INFO - Begin saving trained data...
2018-05-02 21:38:03,207 - memory_profile6_log - INFO - Using ElasticSearch as storage...
2018-05-02 21:38:03,226 - memory_profile6_log - INFO - Saving total data: 4955
2018-05-02 21:38:03,227 - memory_profile6_log - INFO - Len of X_split for batch save model_transformsv: 15
2018-05-02 21:38:03,230 - memory_profile6_log - INFO - processing batch-0
2018-05-02 21:38:08,674 - memory_profile6_log - INFO - processing batch-1
2018-05-02 21:38:11,424 - memory_profile6_log - INFO - processing batch-2
2018-05-02 21:38:13,750 - memory_profile6_log - INFO - processing batch-3
2018-05-02 21:38:15,826 - memory_profile6_log - INFO - processing batch-4
2018-05-02 21:38:26,017 - memory_profile6_log - INFO - processing batch-5
2018-05-02 21:38:29,467 - memory_profile6_log - INFO - processing batch-6
2018-05-02 21:38:31,746 - memory_profile6_log - INFO - processing batch-7
2018-05-02 21:38:33,865 - memory_profile6_log - INFO - processing batch-8
2018-05-02 21:38:35,763 - memory_profile6_log - INFO - processing batch-9
2018-05-02 21:38:37,335 - memory_profile6_log - INFO - processing batch-10
2018-05-02 21:38:38,959 - memory_profile6_log - INFO - processing batch-11
2018-05-02 21:38:42,555 - memory_profile6_log - INFO - processing batch-12
2018-05-02 21:38:45,553 - memory_profile6_log - INFO - processing batch-13
2018-05-02 21:38:48,055 - memory_profile6_log - INFO - processing batch-14
2018-05-02 21:38:50,263 - memory_profile6_log - INFO - Saving fitted_models as history to Elasticsearch...
2018-05-02 21:38:50,316 - memory_profile6_log - INFO - Saving total data: 10000
2018-05-02 21:38:50,318 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 25
2018-05-02 21:38:50,319 - memory_profile6_log - INFO - processing batch-0
2018-05-02 21:38:52,140 - memory_profile6_log - INFO - processing batch-1
2018-05-02 21:38:54,220 - memory_profile6_log - INFO - processing batch-2
2018-05-02 21:38:56,319 - memory_profile6_log - INFO - processing batch-3
2018-05-02 21:38:58,499 - memory_profile6_log - INFO - processing batch-4
2018-05-02 21:39:00,165 - memory_profile6_log - INFO - processing batch-5
2018-05-02 21:39:01,907 - memory_profile6_log - INFO - processing batch-6
2018-05-02 21:39:03,778 - memory_profile6_log - INFO - processing batch-7
2018-05-02 21:39:05,259 - memory_profile6_log - INFO - processing batch-8
2018-05-02 21:39:07,118 - memory_profile6_log - INFO - processing batch-9
2018-05-02 21:39:08,450 - memory_profile6_log - INFO - processing batch-10
2018-05-02 21:39:10,194 - memory_profile6_log - INFO - processing batch-11
2018-05-02 21:39:11,390 - memory_profile6_log - INFO - processing batch-12
2018-05-02 21:39:12,539 - memory_profile6_log - INFO - processing batch-13
2018-05-02 21:39:13,671 - memory_profile6_log - INFO - processing batch-14
2018-05-02 21:39:14,890 - memory_profile6_log - INFO - processing batch-15
2018-05-02 21:39:16,022 - memory_profile6_log - INFO - processing batch-16
2018-05-02 21:39:17,226 - memory_profile6_log - INFO - processing batch-17
2018-05-02 21:39:18,358 - memory_profile6_log - INFO - processing batch-18
2018-05-02 21:39:19,901 - memory_profile6_log - INFO - processing batch-19
2018-05-02 21:39:21,144 - memory_profile6_log - INFO - processing batch-20
2018-05-02 21:39:22,380 - memory_profile6_log - INFO - processing batch-21
2018-05-02 21:39:25,627 - memory_profile6_log - INFO - processing batch-22
2018-05-02 21:39:27,914 - memory_profile6_log - INFO - processing batch-23
2018-05-02 21:39:29,756 - memory_profile6_log - INFO - processing batch-24
2018-05-02 21:39:31,482 - memory_profile6_log - INFO - deleting BR...
2018-05-02 21:39:31,483 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-05-02 21:39:31,484 - memory_profile6_log - INFO - deleting fitted_models_sigmant...
2018-05-02 21:39:31,486 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 21:39:31,486 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 21:39:31,486 - memory_profile6_log - INFO - ================================================

2018-05-02 21:39:31,486 - memory_profile6_log - INFO -    130    120.2 MiB    120.2 MiB   @profile

2018-05-02 21:39:31,487 - memory_profile6_log - INFO -    131                             def main(df_input, df_current, df_hist,

2018-05-02 21:39:31,487 - memory_profile6_log - INFO -    132                                      current_date, G, project_id,

2018-05-02 21:39:31,490 - memory_profile6_log - INFO -    133                                      savetrain=False, multproc=True,

2018-05-02 21:39:31,490 - memory_profile6_log - INFO -    134                                      threshold=0, start_date=None, end_date=None,

2018-05-02 21:39:31,492 - memory_profile6_log - INFO -    135                                      saveto="datastore", fitted_models_hist=None):

2018-05-02 21:39:31,492 - memory_profile6_log - INFO -    136                                 """

2018-05-02 21:39:31,492 - memory_profile6_log - INFO -    137                                     Main Process

2018-05-02 21:39:31,493 - memory_profile6_log - INFO -    138                                 """

2018-05-02 21:39:31,493 - memory_profile6_log - INFO -    139                                 # ~ Data Preprocessing ~

2018-05-02 21:39:31,493 - memory_profile6_log - INFO -    140                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-05-02 21:39:31,493 - memory_profile6_log - INFO -    141                                 # D(u, t)

2018-05-02 21:39:31,493 - memory_profile6_log - INFO -    142    120.2 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-05-02 21:39:31,493 - memory_profile6_log - INFO -    143    120.2 MiB      0.0 MiB       df_dut = df_input.copy(deep=True)

2018-05-02 21:39:31,494 - memory_profile6_log - INFO -    144    120.2 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 21:39:31,494 - memory_profile6_log - INFO -    145                             

2018-05-02 21:39:31,496 - memory_profile6_log - INFO -    146                                 # D(t)

2018-05-02 21:39:31,496 - memory_profile6_log - INFO -    147    120.2 MiB      0.0 MiB       df_dt = df_current.copy(deep=True)

2018-05-02 21:39:31,496 - memory_profile6_log - INFO -    148    120.2 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 21:39:31,497 - memory_profile6_log - INFO -    149                             

2018-05-02 21:39:31,497 - memory_profile6_log - INFO -    150                                 # ~~~~~~ Begin train ~~~~~~

2018-05-02 21:39:31,499 - memory_profile6_log - INFO -    151    120.2 MiB      0.0 MiB       t0 = time.time()

2018-05-02 21:39:31,499 - memory_profile6_log - INFO -    152    120.2 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-05-02 21:39:31,505 - memory_profile6_log - INFO -    153    120.2 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-05-02 21:39:31,505 - memory_profile6_log - INFO -    154    120.2 MiB      0.0 MiB       logger.info("apply on: %d total history...", len(df_hist))

2018-05-02 21:39:31,506 - memory_profile6_log - INFO -    155                             

2018-05-02 21:39:31,506 - memory_profile6_log - INFO -    156                                 # instantiace class

2018-05-02 21:39:31,506 - memory_profile6_log - INFO -    157    120.2 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-05-02 21:39:31,507 - memory_profile6_log - INFO -    158                             

2018-05-02 21:39:31,507 - memory_profile6_log - INFO -    159                                 # ~~ Fit ~~

2018-05-02 21:39:31,509 - memory_profile6_log - INFO -    160                                 #   handling genuine news interest < current date

2018-05-02 21:39:31,509 - memory_profile6_log - INFO -    161    120.6 MiB      0.4 MiB       NB = BR.processX(df_dut)

2018-05-02 21:39:31,509 - memory_profile6_log - INFO -    162                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-05-02 21:39:31,509 - memory_profile6_log - INFO -    163                                 #   nanti dipindah ke class train utama

2018-05-02 21:39:31,510 - memory_profile6_log - INFO -    164    120.7 MiB      0.1 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-05-02 21:39:31,512 - memory_profile6_log - INFO -    165                                 """

2018-05-02 21:39:31,512 - memory_profile6_log - INFO -    166                                     num_y = total global click for category=ci on periode t

2018-05-02 21:39:31,516 - memory_profile6_log - INFO -    167                                     num_x = total click from user_U for category=ci on periode t

2018-05-02 21:39:31,517 - memory_profile6_log - INFO -    168                                 """

2018-05-02 21:39:31,517 - memory_profile6_log - INFO -    169    120.7 MiB      0.0 MiB       fitby_sigmant = False

2018-05-02 21:39:31,519 - memory_profile6_log - INFO -    170    120.7 MiB      0.0 MiB       uniques_fit_hist = None

2018-05-02 21:39:31,519 - memory_profile6_log - INFO -    171    120.7 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-05-02 21:39:31,519 - memory_profile6_log - INFO -    172    120.7 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-05-02 21:39:31,520 - memory_profile6_log - INFO -    173    120.7 MiB      0.0 MiB                            'is_general']]

2018-05-02 21:39:31,520 - memory_profile6_log - INFO -    174                             

2018-05-02 21:39:31,523 - memory_profile6_log - INFO -    175                                 # get sigma_Nt from fitted_models_hist

2018-05-02 21:39:31,523 - memory_profile6_log - INFO -    176    120.7 MiB      0.0 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-05-02 21:39:31,523 - memory_profile6_log - INFO -    177    120.7 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-05-02 21:39:31,525 - memory_profile6_log - INFO -    178    120.9 MiB      0.1 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-05-02 21:39:31,525 - memory_profile6_log - INFO -    179    120.9 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-05-02 21:39:31,529 - memory_profile6_log - INFO -    180                             

2018-05-02 21:39:31,530 - memory_profile6_log - INFO -    181                                 # begin fit

2018-05-02 21:39:31,530 - memory_profile6_log - INFO -    182    120.9 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-05-02 21:39:31,532 - memory_profile6_log - INFO -    183    120.9 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-05-02 21:39:31,532 - memory_profile6_log - INFO -    184    122.2 MiB      1.4 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-05-02 21:39:31,532 - memory_profile6_log - INFO -    185    122.2 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-05-02 21:39:31,533 - memory_profile6_log - INFO -    186    122.2 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-05-02 21:39:31,533 - memory_profile6_log - INFO -    187                                 # print model_fit[['num_x', 'num_x', 'sigma_Nt', 'date_all_click']].loc[model_fit['user_id']=="1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157"].head(5)

2018-05-02 21:39:31,535 - memory_profile6_log - INFO -    188                             

2018-05-02 21:39:31,536 - memory_profile6_log - INFO -    189                                 # ~~ and Transform ~~

2018-05-02 21:39:31,536 - memory_profile6_log - INFO -    190                                 #   handling current news interest == current date

2018-05-02 21:39:31,536 - memory_profile6_log - INFO -    191    122.2 MiB      0.0 MiB       if df_dt.empty:

2018-05-02 21:39:31,538 - memory_profile6_log - INFO -    192                                     print "Cek your df_dt, cannot be emtpy!!"

2018-05-02 21:39:31,542 - memory_profile6_log - INFO -    193                                     return None

2018-05-02 21:39:31,542 - memory_profile6_log - INFO -    194    122.5 MiB      0.3 MiB       NB = BR.processX(df_dt)

2018-05-02 21:39:31,543 - memory_profile6_log - INFO -    195    122.5 MiB      0.0 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-05-02 21:39:31,543 - memory_profile6_log - INFO -    196                             

2018-05-02 21:39:31,545 - memory_profile6_log - INFO -    197    122.5 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-05-02 21:39:31,546 - memory_profile6_log - INFO -    198    122.7 MiB      0.2 MiB                            'num_x', 'num_y', 'is_general']]

2018-05-02 21:39:31,546 - memory_profile6_log - INFO -    199                             

2018-05-02 21:39:31,548 - memory_profile6_log - INFO -    200                                 # print "model_fit dtypes:\n", model_fit.dtypes

2018-05-02 21:39:31,548 - memory_profile6_log - INFO -    201                                 # print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-05-02 21:39:31,549 - memory_profile6_log - INFO -    202    122.7 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-05-02 21:39:31,549 - memory_profile6_log - INFO -    203    122.7 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-05-02 21:39:31,549 - memory_profile6_log - INFO -    204    122.7 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-05-02 21:39:31,553 - memory_profile6_log - INFO -    205    122.7 MiB      0.0 MiB                                                                                           "topic_id", "user_id"]],

2018-05-02 21:39:31,555 - memory_profile6_log - INFO -    206    124.2 MiB      1.5 MiB                                                     verbose=False)

2018-05-02 21:39:31,555 - memory_profile6_log - INFO -    207                             

2018-05-02 21:39:31,556 - memory_profile6_log - INFO -    208                                 # ~~~ filter is general and specific topic ~~~

2018-05-02 21:39:31,556 - memory_profile6_log - INFO -    209                                 # the idea is just we need to rerank every topic according

2018-05-02 21:39:31,558 - memory_profile6_log - INFO -    210                                 #    to user_id and and is_general by p0_posterior

2018-05-02 21:39:31,559 - memory_profile6_log - INFO -    211    124.2 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-05-02 21:39:31,559 - memory_profile6_log - INFO -    212    124.2 MiB      0.0 MiB                                     'is_general']].groupby(['topic_id',

2018-05-02 21:39:31,559 - memory_profile6_log - INFO -    213    124.3 MiB      0.1 MiB                                                             'is_general']

2018-05-02 21:39:31,559 - memory_profile6_log - INFO -    214                                                                                      ).size().to_frame().reset_index()

2018-05-02 21:39:31,559 - memory_profile6_log - INFO -    215                             

2018-05-02 21:39:31,561 - memory_profile6_log - INFO -    216                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-05-02 21:39:31,561 - memory_profile6_log - INFO -    217                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-05-02 21:39:31,562 - memory_profile6_log - INFO -    218    124.3 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-05-02 21:39:31,562 - memory_profile6_log - INFO -    219                             

2018-05-02 21:39:31,562 - memory_profile6_log - INFO -    220                                 # ~ start by provide rank for each topic type ~

2018-05-02 21:39:31,568 - memory_profile6_log - INFO -    221    125.9 MiB      1.6 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-05-02 21:39:31,568 - memory_profile6_log - INFO -    222    126.0 MiB      0.0 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-05-02 21:39:31,569 - memory_profile6_log - INFO -    223                             

2018-05-02 21:39:31,569 - memory_profile6_log - INFO -    224                                 # ~ set threshold to filter output

2018-05-02 21:39:31,569 - memory_profile6_log - INFO -    225    126.0 MiB      0.0 MiB       if threshold > 0:

2018-05-02 21:39:31,571 - memory_profile6_log - INFO -    226    126.0 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-05-02 21:39:31,571 - memory_profile6_log - INFO -    227    126.0 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-05-02 21:39:31,572 - memory_profile6_log - INFO -    228    126.0 MiB      0.0 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-05-02 21:39:31,572 - memory_profile6_log - INFO -    229                             

2018-05-02 21:39:31,573 - memory_profile6_log - INFO -    230    126.0 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 21:39:31,573 - memory_profile6_log - INFO -    231    126.0 MiB      0.0 MiB       print model_transform[model_transform['user_id']=='1616f009d96b1-0285d8288a5bce-70217860-38400-1616f009d98157']

2018-05-02 21:39:31,575 - memory_profile6_log - INFO -    232    126.0 MiB      0.0 MiB       logger.info("Len of model_transform: %d", len(model_transform))

2018-05-02 21:39:31,575 - memory_profile6_log - INFO -    233    126.0 MiB      0.0 MiB       logger.info("Len of df_dt: %d", len(df_dt))

2018-05-02 21:39:31,578 - memory_profile6_log - INFO -    234    126.0 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-05-02 21:39:31,579 - memory_profile6_log - INFO -    235                             

2018-05-02 21:39:31,579 - memory_profile6_log - INFO -    236    126.0 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-05-02 21:39:31,579 - memory_profile6_log - INFO -    237                             

2018-05-02 21:39:31,581 - memory_profile6_log - INFO -    238    126.0 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-05-02 21:39:31,581 - memory_profile6_log - INFO -    239    126.0 MiB      0.0 MiB       del df_dut

2018-05-02 21:39:31,582 - memory_profile6_log - INFO -    240    126.0 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-05-02 21:39:31,582 - memory_profile6_log - INFO -    241    126.0 MiB      0.0 MiB       del df_dt

2018-05-02 21:39:31,582 - memory_profile6_log - INFO -    242    126.0 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-05-02 21:39:31,582 - memory_profile6_log - INFO -    243    126.0 MiB      0.0 MiB       del df_input

2018-05-02 21:39:31,582 - memory_profile6_log - INFO -    244    126.0 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-05-02 21:39:31,584 - memory_profile6_log - INFO -    245    126.0 MiB      0.0 MiB       del df_input_X

2018-05-02 21:39:31,585 - memory_profile6_log - INFO -    246    126.0 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-05-02 21:39:31,585 - memory_profile6_log - INFO -    247    126.0 MiB      0.0 MiB       del df_current

2018-05-02 21:39:31,585 - memory_profile6_log - INFO -    248    126.0 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-05-02 21:39:31,586 - memory_profile6_log - INFO -    249    126.0 MiB      0.0 MiB       del map_topic_isgeneral

2018-05-02 21:39:31,592 - memory_profile6_log - INFO -    250    126.0 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-05-02 21:39:31,592 - memory_profile6_log - INFO -    251    126.0 MiB      0.0 MiB       del model_fit

2018-05-02 21:39:31,592 - memory_profile6_log - INFO -    252    126.0 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-05-02 21:39:31,592 - memory_profile6_log - INFO -    253    126.0 MiB      0.0 MiB       del result

2018-05-02 21:39:31,594 - memory_profile6_log - INFO -    254    126.0 MiB      0.0 MiB       logger.info("deleting result...")

2018-05-02 21:39:31,594 - memory_profile6_log - INFO -    255                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 21:39:31,594 - memory_profile6_log - INFO -    256                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 21:39:31,595 - memory_profile6_log - INFO -    257                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-05-02 21:39:31,595 - memory_profile6_log - INFO -    258    126.0 MiB      0.0 MiB       if savetrain:

2018-05-02 21:39:31,595 - memory_profile6_log - INFO -    259    126.0 MiB      0.0 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'p0_posterior', 'rank']].copy(deep=True)

2018-05-02 21:39:31,595 - memory_profile6_log - INFO -    260    126.0 MiB      0.0 MiB           model_transformsv['date'] = current_date  # we need manually adding date, because table not support

2018-05-02 21:39:31,596 - memory_profile6_log - INFO -    261    126.0 MiB      0.0 MiB           model_transformsv['date'] = pd.to_datetime(model_transformsv['date'],

2018-05-02 21:39:31,596 - memory_profile6_log - INFO -    262    126.0 MiB      0.0 MiB                                                      format='%Y-%m-%d', errors='coerce')

2018-05-02 21:39:31,598 - memory_profile6_log - INFO -    263    126.0 MiB      0.0 MiB           print "list(model_transformsv) before rename: ", list(model_transformsv) 

2018-05-02 21:39:31,598 - memory_profile6_log - INFO -    264    126.0 MiB      0.0 MiB           print model_transformsv[['date']].head(5)      

2018-05-02 21:39:31,598 - memory_profile6_log - INFO -    265    126.0 MiB      0.0 MiB           print len(model_transformsv)                

2018-05-02 21:39:31,599 - memory_profile6_log - INFO -    266    126.0 MiB      0.0 MiB           model_transformsv = model_transformsv.rename(columns={'is_general': 'topic_is_general', 'p0_posterior': 'interest_score',

2018-05-02 21:39:31,599 - memory_profile6_log - INFO -    267    126.0 MiB      0.0 MiB                                                                 'rank':'interest_rank' , 'date':'interest_score_created_at'})

2018-05-02 21:39:31,605 - memory_profile6_log - INFO -    268    126.0 MiB      0.0 MiB           print "list(model_transformsv) after rename: ", list(model_transformsv)    

2018-05-02 21:39:31,605 - memory_profile6_log - INFO -    269    126.0 MiB      0.0 MiB           print len(model_transformsv) 

2018-05-02 21:39:31,605 - memory_profile6_log - INFO -    270    126.0 MiB      0.0 MiB           del model_transform

2018-05-02 21:39:31,607 - memory_profile6_log - INFO -    271    126.0 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-05-02 21:39:31,607 - memory_profile6_log - INFO -    272    126.0 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-05-02 21:39:31,607 - memory_profile6_log - INFO -    273                             

2018-05-02 21:39:31,608 - memory_profile6_log - INFO -    274    126.0 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-05-02 21:39:31,608 - memory_profile6_log - INFO -    275                                     # ~ Place your code to save the training model here ~

2018-05-02 21:39:31,608 - memory_profile6_log - INFO -    276    126.0 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-05-02 21:39:31,608 - memory_profile6_log - INFO -    277                                         logger.info("Using google datastore as storage...")

2018-05-02 21:39:31,608 - memory_profile6_log - INFO -    278                                         if multproc:

2018-05-02 21:39:31,609 - memory_profile6_log - INFO -    279                                             # ~ save transform models ~

2018-05-02 21:39:31,609 - memory_profile6_log - INFO -    280                                             logger.info("Saving main Transform model to Google DataStore...")

2018-05-02 21:39:31,609 - memory_profile6_log - INFO -    281                                             logger.info("Saving total data: %d", len(model_transformsv))

2018-05-02 21:39:31,611 - memory_profile6_log - INFO -    282                                             mh.saveDataStorePutMulti(model_transformsv)

2018-05-02 21:39:31,611 - memory_profile6_log - INFO -    283                             

2018-05-02 21:39:31,611 - memory_profile6_log - INFO -    284                                             # ~ save fitted models ~

2018-05-02 21:39:31,611 - memory_profile6_log - INFO -    285                                             logger.info("Saving fitted_models as history to Google DataStore...")

2018-05-02 21:39:31,611 - memory_profile6_log - INFO -    286                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 21:39:31,612 - memory_profile6_log - INFO -    287                                             fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-05-02 21:39:31,612 - memory_profile6_log - INFO -    288                                             logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-05-02 21:39:31,618 - memory_profile6_log - INFO -    289                                             X_split = np.array_split(fitted_models_sigmant, 10)

2018-05-02 21:39:31,618 - memory_profile6_log - INFO -    290                                             logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-05-02 21:39:31,618 - memory_profile6_log - INFO -    291                                             for ix in range(len(X_split)):

2018-05-02 21:39:31,618 - memory_profile6_log - INFO -    292                                                 logger.info("processing batch-%d", ix)

2018-05-02 21:39:31,619 - memory_profile6_log - INFO -    293                                                 mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-05-02 21:39:31,619 - memory_profile6_log - INFO -    294                             

2018-05-02 21:39:31,619 - memory_profile6_log - INFO -    295                                             del X_split

2018-05-02 21:39:31,621 - memory_profile6_log - INFO -    296                                             logger.info("deleting X_split...")

2018-05-02 21:39:31,621 - memory_profile6_log - INFO -    297                                             del save_sigma_nt

2018-05-02 21:39:31,621 - memory_profile6_log - INFO -    298                                             logger.info("deleting save_sigma_nt...")

2018-05-02 21:39:31,621 - memory_profile6_log - INFO -    299                             

2018-05-02 21:39:31,621 - memory_profile6_log - INFO -    300                                             del BR

2018-05-02 21:39:31,622 - memory_profile6_log - INFO -    301                                             logger.info("deleting BR...")

2018-05-02 21:39:31,622 - memory_profile6_log - INFO -    302                             

2018-05-02 21:39:31,622 - memory_profile6_log - INFO -    303    126.0 MiB      0.0 MiB           elif str(saveto).lower() == "elastic":

2018-05-02 21:39:31,624 - memory_profile6_log - INFO -    304    126.0 MiB      0.0 MiB               logger.info("Using ElasticSearch as storage...")

2018-05-02 21:39:31,624 - memory_profile6_log - INFO -    305    126.0 MiB      0.0 MiB               logging.info("Saving main Transform model to Elasticsearch...")

2018-05-02 21:39:31,624 - memory_profile6_log - INFO -    306                                         # print model_transformsv.head(5)

2018-05-02 21:39:31,625 - memory_profile6_log - INFO -    307                                         

2018-05-02 21:39:31,625 - memory_profile6_log - INFO -    308    126.0 MiB      0.0 MiB               X_split = np.array_split(model_transformsv, 15)

2018-05-02 21:39:31,625 - memory_profile6_log - INFO -    309    126.0 MiB      0.0 MiB               logger.info("Saving total data: %d", len(model_transformsv))

2018-05-02 21:39:31,630 - memory_profile6_log - INFO -    310    126.0 MiB      0.0 MiB               logger.info("Len of X_split for batch save model_transformsv: %d", len(X_split))

2018-05-02 21:39:31,630 - memory_profile6_log - INFO -    311    126.0 MiB     -0.6 MiB               for ix in range(len(X_split)):

2018-05-02 21:39:31,631 - memory_profile6_log - INFO -    312    126.0 MiB     -0.6 MiB                   logger.info("processing batch-%d", ix)

2018-05-02 21:39:31,631 - memory_profile6_log - INFO -    313    126.0 MiB     -0.6 MiB                   mh.saveElasticS(X_split[ix], esp)

2018-05-02 21:39:31,631 - memory_profile6_log - INFO -    314    126.0 MiB     -0.1 MiB               del X_split

2018-05-02 21:39:31,631 - memory_profile6_log - INFO -    315                                         

2018-05-02 21:39:31,631 - memory_profile6_log - INFO -    316                             

2018-05-02 21:39:31,631 - memory_profile6_log - INFO -    317    126.0 MiB      0.0 MiB               logger.info("Saving fitted_models as history to Elasticsearch...")

2018-05-02 21:39:31,632 - memory_profile6_log - INFO -    318    126.0 MiB      0.0 MiB               save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 21:39:31,632 - memory_profile6_log - INFO -    319    126.0 MiB      0.0 MiB               fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-05-02 21:39:31,632 - memory_profile6_log - INFO -    320                             

2018-05-02 21:39:31,634 - memory_profile6_log - INFO -    321    126.0 MiB      0.0 MiB               fitted_models_sigmant['uid_topid'] = fitted_models_sigmant["user_id"].map(str) + "_" + fitted_models_sigmant["topic_id"].map(str)

2018-05-02 21:39:31,634 - memory_profile6_log - INFO -    322    126.0 MiB      0.0 MiB               fitted_models_sigmant = fitted_models_sigmant[["uid_topid", "pt_posterior_x_Nt", "smoothed_pt_posterior", "p0_cat_ci", "sigma_Nt"]]

2018-05-02 21:39:31,634 - memory_profile6_log - INFO -    323                               

2018-05-02 21:39:31,634 - memory_profile6_log - INFO -    324    126.0 MiB      0.0 MiB               X_split = np.array_split(fitted_models_sigmant, 25)

2018-05-02 21:39:31,634 - memory_profile6_log - INFO -    325    126.0 MiB      0.0 MiB               logger.info("Saving total data: %d", len(fitted_models_sigmant))

2018-05-02 21:39:31,635 - memory_profile6_log - INFO -    326    126.0 MiB      0.0 MiB               logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-05-02 21:39:31,635 - memory_profile6_log - INFO -    327    126.0 MiB     -9.4 MiB               for ix in range(len(X_split)):

2018-05-02 21:39:31,635 - memory_profile6_log - INFO -    328    126.0 MiB     -8.3 MiB                   logger.info("processing batch-%d", ix)

2018-05-02 21:39:31,637 - memory_profile6_log - INFO -    329    126.0 MiB     -8.3 MiB                   mh.saveElasticS(X_split[ix], esp,

2018-05-02 21:39:31,637 - memory_profile6_log - INFO -    330    126.0 MiB     -8.3 MiB                                   esindex_name="fitted_hist_index",

2018-05-02 21:39:31,637 - memory_profile6_log - INFO -    331    126.0 MiB     -9.3 MiB                                   estype_name='fitted_hist_type')

2018-05-02 21:39:31,638 - memory_profile6_log - INFO -    332    124.9 MiB     -1.0 MiB               del X_split

2018-05-02 21:39:31,638 - memory_profile6_log - INFO -    333                                         

2018-05-02 21:39:31,638 - memory_profile6_log - INFO -    334    124.9 MiB      0.0 MiB               del BR

2018-05-02 21:39:31,642 - memory_profile6_log - INFO -    335    124.9 MiB      0.0 MiB               logger.info("deleting BR...")

2018-05-02 21:39:31,644 - memory_profile6_log - INFO -    336    124.9 MiB      0.0 MiB               del save_sigma_nt

2018-05-02 21:39:31,644 - memory_profile6_log - INFO -    337    124.9 MiB      0.0 MiB               logger.info("deleting save_sigma_nt...")

2018-05-02 21:39:31,644 - memory_profile6_log - INFO -    338    124.9 MiB      0.0 MiB               del fitted_models_sigmant

2018-05-02 21:39:31,644 - memory_profile6_log - INFO -    339    124.9 MiB      0.0 MiB               logger.info("deleting fitted_models_sigmant...")

2018-05-02 21:39:31,644 - memory_profile6_log - INFO -    340                             

2018-05-02 21:39:31,645 - memory_profile6_log - INFO -    341                                     # need save sigma_nt for daily train

2018-05-02 21:39:31,645 - memory_profile6_log - INFO -    342                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-05-02 21:39:31,645 - memory_profile6_log - INFO -    343                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-05-02 21:39:31,647 - memory_profile6_log - INFO -    344    124.9 MiB      0.0 MiB           if start_date and end_date:

2018-05-02 21:39:31,647 - memory_profile6_log - INFO -    345                                         if not fitby_sigmant:

2018-05-02 21:39:31,648 - memory_profile6_log - INFO -    346                                             logging.info("Saving sigma Nt...")

2018-05-02 21:39:31,648 - memory_profile6_log - INFO -    347                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-05-02 21:39:31,648 - memory_profile6_log - INFO -    348                                             save_sigma_nt['start_date'] = start_date

2018-05-02 21:39:31,648 - memory_profile6_log - INFO -    349                                             save_sigma_nt['end_date'] = end_date

2018-05-02 21:39:31,650 - memory_profile6_log - INFO -    350                                             print save_sigma_nt.head(5)

2018-05-02 21:39:31,651 - memory_profile6_log - INFO -    351                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-05-02 21:39:31,651 - memory_profile6_log - INFO -    352                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-05-02 21:39:31,651 - memory_profile6_log - INFO -    353    124.9 MiB      0.0 MiB       return

2018-05-02 21:39:31,655 - memory_profile6_log - INFO - 


2018-05-02 21:39:31,655 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-05-02 21:39:52,719 - memory_profile6_log - INFO - ElasticSearch host: https://9db53c7bb4f5be2d856033a9aeb6e5a5.us-central1.gcp.cloud.es.io
2018-05-02 21:39:52,720 - memory_profile6_log - INFO - ElasticSearch port: 9243
2018-05-02 21:39:52,721 - memory_profile6_log - INFO - Generating date range with N: 1
2018-05-02 21:39:52,723 - memory_profile6_log - INFO - date_generated: 
2018-05-02 21:39:52,723 - memory_profile6_log - INFO -  
2018-05-02 21:39:52,723 - memory_profile6_log - INFO - [datetime.datetime(2018, 5, 1, 21, 39, 52, 722000)]
2018-05-02 21:39:52,724 - memory_profile6_log - INFO - 

2018-05-02 21:39:52,724 - memory_profile6_log - INFO - using current date: 2018-05-02
2018-05-02 21:39:52,724 - memory_profile6_log - INFO - using start date: 2018-05-01 00:00:00
2018-05-02 21:39:52,726 - memory_profile6_log - INFO - using end date: 2018-05-01
2018-05-02 21:39:52,868 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-05-02 21:39:52,872 - memory_profile6_log - INFO - Collecting training data for date: 2018-05-01
2018-05-02 21:39:58,559 - memory_profile6_log - INFO - size of df: 2.53 MB
2018-05-02 21:39:58,561 - memory_profile6_log - INFO - getting total: 10000 training data(genuine interest) for date: 2018-05-01
2018-05-02 21:39:58,660 - memory_profile6_log - INFO - loading history data from elastic...
2018-05-02 21:39:58,661 - memory_profile6_log - INFO - Len of X_split for batch load: 25
2018-05-02 21:39:58,663 - memory_profile6_log - INFO - Appending history data...
2018-05-02 21:39:58,700 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:40:01,187 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:40:02,010 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:40:02,532 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:40:03,069 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:40:03,568 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:40:04,137 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:40:04,701 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:40:05,256 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:40:05,811 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:40:06,336 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:40:06,937 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:40:07,417 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:40:07,960 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:40:08,513 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:40:09,053 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:40:09,676 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:40:10,267 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:40:10,859 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:40:11,338 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:40:11,967 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:40:12,825 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:40:13,849 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:40:14,494 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:40:15,145 - memory_profile6_log - INFO - call 400 history data...
2018-05-02 21:40:15,963 - memory_profile6_log - INFO - Appending training data...
2018-05-02 21:40:15,964 - memory_profile6_log - INFO - len datalist: 1
2018-05-02 21:40:15,967 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-05-02 21:40:15,967 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 21:40:15,969 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 21:40:15,970 - memory_profile6_log - INFO - ================================================

2018-05-02 21:40:15,970 - memory_profile6_log - INFO -    379     87.0 MiB     87.0 MiB   @profile

2018-05-02 21:40:15,971 - memory_profile6_log - INFO -    380                             def BQPreprocess(cpu, date_generated, client, query_fit, loadfrom="elastic"):

2018-05-02 21:40:15,973 - memory_profile6_log - INFO -    381     87.0 MiB      0.0 MiB       bq_client = client

2018-05-02 21:40:15,976 - memory_profile6_log - INFO -    382     87.0 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 21:40:15,976 - memory_profile6_log - INFO -    383                             

2018-05-02 21:40:15,977 - memory_profile6_log - INFO -    384     87.0 MiB      0.0 MiB       datalist = []

2018-05-02 21:40:15,979 - memory_profile6_log - INFO -    385     87.0 MiB      0.0 MiB       datalist_hist = []

2018-05-02 21:40:15,980 - memory_profile6_log - INFO -    386                             

2018-05-02 21:40:15,980 - memory_profile6_log - INFO -    387     87.0 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-05-02 21:40:15,982 - memory_profile6_log - INFO -    388    116.0 MiB      0.0 MiB       for ndate in date_generated:

2018-05-02 21:40:15,983 - memory_profile6_log - INFO -    389    115.2 MiB     28.2 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-05-02 21:40:15,983 - memory_profile6_log - INFO -    390    115.2 MiB      0.0 MiB           if tframe is not None:

2018-05-02 21:40:15,986 - memory_profile6_log - INFO -    391    115.2 MiB      0.0 MiB               if not tframe.empty:

2018-05-02 21:40:15,987 - memory_profile6_log - INFO -    392    115.2 MiB      0.0 MiB                   if loadfrom.strip().lower() == 'datastore':

2018-05-02 21:40:15,989 - memory_profile6_log - INFO -    393                                                 X_split = np.array_split(tframe, 5)

2018-05-02 21:40:15,989 - memory_profile6_log - INFO -    394                                                 logger.info("loading history data from datastore...")

2018-05-02 21:40:15,990 - memory_profile6_log - INFO -    395                                                 logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 21:40:15,990 - memory_profile6_log - INFO -    396                                                 logger.info("Appending history data...")

2018-05-02 21:40:15,990 - memory_profile6_log - INFO -    397                                                 for ix in range(len(X_split)):

2018-05-02 21:40:15,992 - memory_profile6_log - INFO -    398                                                     # ~ loading history

2018-05-02 21:40:15,992 - memory_profile6_log - INFO -    399                                                     """

2018-05-02 21:40:15,993 - memory_profile6_log - INFO -    400                                                         disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-05-02 21:40:15,993 - memory_profile6_log - INFO -    401                                                     """

2018-05-02 21:40:15,993 - memory_profile6_log - INFO -    402                                                     logger.info("processing batch-%d", ix)

2018-05-02 21:40:15,997 - memory_profile6_log - INFO -    403                                                     # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-05-02 21:40:15,999 - memory_profile6_log - INFO -    404                                                     logger.info("creating list history data...")

2018-05-02 21:40:15,999 - memory_profile6_log - INFO -    405                                                     #lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-05-02 21:40:16,000 - memory_profile6_log - INFO -    406                                                     lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 21:40:16,000 - memory_profile6_log - INFO -    407                             

2018-05-02 21:40:16,000 - memory_profile6_log - INFO -    408                                                     logger.info("call history data...")

2018-05-02 21:40:16,002 - memory_profile6_log - INFO -    409                                                     h_frame = mh.loadDSHistory(lhistory)

2018-05-02 21:40:16,003 - memory_profile6_log - INFO -    410                             

2018-05-02 21:40:16,003 - memory_profile6_log - INFO -    411                                                     # me = os.getpid()

2018-05-02 21:40:16,003 - memory_profile6_log - INFO -    412                                                     # kill_proc_tree(me)

2018-05-02 21:40:16,005 - memory_profile6_log - INFO -    413                             

2018-05-02 21:40:16,005 - memory_profile6_log - INFO -    414                                                     logger.info("done collecting history data, appending now...")

2018-05-02 21:40:16,007 - memory_profile6_log - INFO -    415                                                     for m in h_frame:

2018-05-02 21:40:16,009 - memory_profile6_log - INFO -    416                                                         if m is not None:

2018-05-02 21:40:16,009 - memory_profile6_log - INFO -    417                                                             if len(m) > 0:

2018-05-02 21:40:16,010 - memory_profile6_log - INFO -    418                                                                 datalist_hist.append(pd.DataFrame(m))

2018-05-02 21:40:16,012 - memory_profile6_log - INFO -    419                                                     del h_frame

2018-05-02 21:40:16,012 - memory_profile6_log - INFO -    420                                                     del lhistory

2018-05-02 21:40:16,013 - memory_profile6_log - INFO -    421                             

2018-05-02 21:40:16,013 - memory_profile6_log - INFO -    422                                                 logger.info("Appending training data...")

2018-05-02 21:40:16,013 - memory_profile6_log - INFO -    423                                                 datalist.append(tframe)

2018-05-02 21:40:16,015 - memory_profile6_log - INFO -    424    115.2 MiB      0.0 MiB                   elif loadfrom.strip().lower() == 'elastic':

2018-05-02 21:40:16,016 - memory_profile6_log - INFO -    425    115.2 MiB      0.1 MiB                       X_split = np.array_split(tframe, 25)

2018-05-02 21:40:16,019 - memory_profile6_log - INFO -    426    115.2 MiB      0.0 MiB                       logger.info("loading history data from elastic...")

2018-05-02 21:40:16,019 - memory_profile6_log - INFO -    427    115.2 MiB      0.0 MiB                       logger.info("Len of X_split for batch load: %d", len(X_split))

2018-05-02 21:40:16,022 - memory_profile6_log - INFO -    428    115.2 MiB      0.0 MiB                       logger.info("Appending history data...")

2018-05-02 21:40:16,023 - memory_profile6_log - INFO -    429                                                 

2018-05-02 21:40:16,023 - memory_profile6_log - INFO -    430    116.0 MiB     -0.0 MiB                       for ix in range(len(X_split)):

2018-05-02 21:40:16,025 - memory_profile6_log - INFO -    431    116.0 MiB      0.3 MiB                           lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-05-02 21:40:16,028 - memory_profile6_log - INFO -    432    116.0 MiB     -0.0 MiB                           logger.info("call %d history data...", len(lhistory))

2018-05-02 21:40:16,030 - memory_profile6_log - INFO -    433    116.0 MiB     -0.0 MiB                           inside_data = mh.loadESHistory(lhistory, es,

2018-05-02 21:40:16,032 - memory_profile6_log - INFO -    434    116.0 MiB     -0.0 MiB                                                          esindex_name='fitted_hist_index',

2018-05-02 21:40:16,032 - memory_profile6_log - INFO -    435    116.0 MiB      0.2 MiB                                                          estype_name='fitted_hist_type')

2018-05-02 21:40:16,035 - memory_profile6_log - INFO -    436                                                     # split back the user_id and topic_id

2018-05-02 21:40:16,036 - memory_profile6_log - INFO -    437    116.0 MiB      0.0 MiB                           inside_data[['user_id','topic_id']] = inside_data.uid_topid.str.split('_', expand=True)

2018-05-02 21:40:16,036 - memory_profile6_log - INFO -    438    116.0 MiB      0.2 MiB                           inside_data = inside_data[["user_id","topic_id", "pt_posterior_x_Nt", "smoothed_pt_posterior", "p0_cat_ci", "sigma_Nt"]]

2018-05-02 21:40:16,038 - memory_profile6_log - INFO -    439    116.0 MiB     -0.0 MiB                           if not inside_data.empty:

2018-05-02 21:40:16,040 - memory_profile6_log - INFO -    440    116.0 MiB     -0.0 MiB                               datalist_hist.append(inside_data)

2018-05-02 21:40:16,042 - memory_profile6_log - INFO -    441    116.0 MiB     -0.0 MiB                               del inside_data

2018-05-02 21:40:16,042 - memory_profile6_log - INFO -    442                                                 

2018-05-02 21:40:16,043 - memory_profile6_log - INFO -    443    116.0 MiB     -0.0 MiB                       logger.info("Appending training data...")

2018-05-02 21:40:16,046 - memory_profile6_log - INFO -    444    116.0 MiB      0.0 MiB                       datalist.append(tframe)

2018-05-02 21:40:16,046 - memory_profile6_log - INFO -    445                                             else:

2018-05-02 21:40:16,048 - memory_profile6_log - INFO -    446                                                 logger.info("Unknows source is selected !")

2018-05-02 21:40:16,052 - memory_profile6_log - INFO -    447                                                 break

2018-05-02 21:40:16,052 - memory_profile6_log - INFO -    448                                     else: 

2018-05-02 21:40:16,053 - memory_profile6_log - INFO -    449                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-05-02 21:40:16,055 - memory_profile6_log - INFO -    450    116.0 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-05-02 21:40:16,055 - memory_profile6_log - INFO -    451    116.0 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-05-02 21:40:16,056 - memory_profile6_log - INFO -    452                             

2018-05-02 21:40:16,058 - memory_profile6_log - INFO -    453    116.0 MiB      0.0 MiB       return datalist, datalist_hist

2018-05-02 21:40:16,059 - memory_profile6_log - INFO - 


2018-05-02 21:40:16,088 - memory_profile6_log - INFO - len of big_frame_hist: 10000
2018-05-02 21:40:16,099 - memory_profile6_log - INFO - size of big_frame_hist: 2.75 MB
2018-05-02 21:40:16,115 - memory_profile6_log - INFO - size of big_frame: 2.53 MB
2018-05-02 21:40:16,115 - memory_profile6_log - INFO - len of big_frame: 10000
2018-05-02 21:40:16,121 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-05-02 21:40:21,125 - memory_profile6_log - INFO - size of df: 2.46 MB
2018-05-02 21:40:21,128 - memory_profile6_log - INFO - getting total: 10000 training data(current date interest)
2018-05-02 21:40:21,140 - memory_profile6_log - INFO - size of current_frame: 2.54 MB
2018-05-02 21:40:21,141 - memory_profile6_log - INFO - loading time of: 20000 total genuine-current interest data ~ take 28.310s
2018-05-02 21:40:21,141 - memory_profile6_log - INFO - Filename: .\daily.py


2018-05-02 21:40:21,144 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-05-02 21:40:21,144 - memory_profile6_log - INFO - ================================================

2018-05-02 21:40:21,144 - memory_profile6_log - INFO -    455     86.9 MiB     86.9 MiB   @profile

2018-05-02 21:40:21,147 - memory_profile6_log - INFO -    456                             def preprocess(cpu, cd, query_fit, date_generated):

2018-05-02 21:40:21,148 - memory_profile6_log - INFO -    457     87.0 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-05-02 21:40:21,151 - memory_profile6_log - INFO -    458     87.0 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-05-02 21:40:21,151 - memory_profile6_log - INFO -    459                             

2018-05-02 21:40:21,151 - memory_profile6_log - INFO -    460                                 # ~~~ Begin collecting data ~~~

2018-05-02 21:40:21,153 - memory_profile6_log - INFO -    461     87.0 MiB      0.0 MiB       t0 = time.time()

2018-05-02 21:40:21,153 - memory_profile6_log - INFO -    462    116.0 MiB     29.0 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-05-02 21:40:21,154 - memory_profile6_log - INFO -    463    116.0 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-05-02 21:40:21,154 - memory_profile6_log - INFO -    464                                     logger.info("Training cannot be empty..")

2018-05-02 21:40:21,154 - memory_profile6_log - INFO -    465                                     return False

2018-05-02 21:40:21,154 - memory_profile6_log - INFO -    466    116.0 MiB      0.0 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-05-02 21:40:21,154 - memory_profile6_log - INFO -    467    116.0 MiB      0.0 MiB       logger.info("len of big_frame_hist: %s", len(big_frame_hist))

2018-05-02 21:40:21,154 - memory_profile6_log - INFO -    468    116.0 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-05-02 21:40:21,160 - memory_profile6_log - INFO -    469                             

2018-05-02 21:40:21,160 - memory_profile6_log - INFO -    470    116.0 MiB      0.0 MiB       big_frame = pd.concat(datalist)

2018-05-02 21:40:21,164 - memory_profile6_log - INFO -    471    116.0 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-05-02 21:40:21,164 - memory_profile6_log - INFO -    472    116.0 MiB      0.0 MiB       logger.info("len of big_frame: %s", len(big_frame))

2018-05-02 21:40:21,165 - memory_profile6_log - INFO -    473    116.0 MiB      0.0 MiB       del datalist

2018-05-02 21:40:21,167 - memory_profile6_log - INFO -    474                             

2018-05-02 21:40:21,170 - memory_profile6_log - INFO -    475    116.0 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-05-02 21:40:21,171 - memory_profile6_log - INFO -    476                             

2018-05-02 21:40:21,171 - memory_profile6_log - INFO -    477                                 # ~ get current news interest ~

2018-05-02 21:40:21,173 - memory_profile6_log - INFO -    478    116.0 MiB      0.0 MiB       if not cd:

2018-05-02 21:40:21,174 - memory_profile6_log - INFO -    479    116.0 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-05-02 21:40:21,176 - memory_profile6_log - INFO -    480    120.4 MiB      4.4 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-05-02 21:40:21,177 - memory_profile6_log - INFO -    481    120.4 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-05-02 21:40:21,177 - memory_profile6_log - INFO -    482                                 else:

2018-05-02 21:40:21,180 - memory_profile6_log - INFO -    483                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-05-02 21:40:21,183 - memory_profile6_log - INFO -    484                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date) LIMIT 10000"

2018-05-02 21:40:21,184 - memory_profile6_log - INFO -    485                             

2018-05-02 21:40:21,184 - memory_profile6_log - INFO -    486                                     # safe handling of query parameter

2018-05-02 21:40:21,186 - memory_profile6_log - INFO -    487                                     query_params = [

2018-05-02 21:40:21,187 - memory_profile6_log - INFO -    488                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-05-02 21:40:21,187 - memory_profile6_log - INFO -    489                                     ]

2018-05-02 21:40:21,187 - memory_profile6_log - INFO -    490                             

2018-05-02 21:40:21,187 - memory_profile6_log - INFO -    491                                     job_config.query_parameters = query_params

2018-05-02 21:40:21,187 - memory_profile6_log - INFO -    492                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-05-02 21:40:21,188 - memory_profile6_log - INFO -    493                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-05-02 21:40:21,188 - memory_profile6_log - INFO -    494                             

2018-05-02 21:40:21,193 - memory_profile6_log - INFO -    495    120.4 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-05-02 21:40:21,194 - memory_profile6_log - INFO -    496    120.4 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-05-02 21:40:21,197 - memory_profile6_log - INFO -    497    120.4 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-05-02 21:40:21,198 - memory_profile6_log - INFO -    498    120.5 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-05-02 21:40:21,198 - memory_profile6_log - INFO -    499    120.5 MiB      0.0 MiB       train_time = time.time() - t0

2018-05-02 21:40:21,200 - memory_profile6_log - INFO -    500    120.5 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-05-02 21:40:21,203 - memory_profile6_log - INFO -    501                             

2018-05-02 21:40:21,203 - memory_profile6_log - INFO -    502    120.5 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-05-02 21:40:21,204 - memory_profile6_log - INFO - 


2018-05-02 21:40:21,211 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-05-02 21:40:21,221 - memory_profile6_log - INFO - train on: 10000 total genuine interest data(D(u, t))
2018-05-02 21:40:21,223 - memory_profile6_log - INFO - transform on: 10000 total current data(D(t))
2018-05-02 21:40:21,226 - memory_profile6_log - INFO - apply on: 10000 total history...
2018-05-02 21:40:21,273 - memory_profile6_log - INFO - len of uniques_fit_hist:10000
2018-05-02 21:40:21,279 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:1387
2018-05-02 21:40:21,342 - memory_profile6_log - INFO - self.sum_all_nt before concate:

2018-05-02 21:40:21,361 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 21:40:21,364 - memory_profile6_log - INFO - 

2018-05-02 21:40:21,375 - memory_profile6_log - INFO - self.sum_all_nt after concate:

2018-05-02 21:40:21,384 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, sigma_Nt]
Index: []
2018-05-02 21:40:21,385 - memory_profile6_log - INFO - 

2018-05-02 21:40:21,428 - memory_profile6_log - INFO - Len of model_fit: 10000
2018-05-02 21:40:21,430 - memory_profile6_log - INFO - Len of df_dut: 10000
2018-05-02 21:40:21,559 - memory_profile6_log - INFO - Len of fitted_models on main class: 10000
2018-05-02 21:40:21,559 - memory_profile6_log - INFO - 

2018-05-02 21:40:21,562 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-05-02 21:40:21,562 - memory_profile6_log - INFO - 

2018-05-02 21:40:21,562 - memory_profile6_log - INFO - fitted_model_hist:

2018-05-02 21:40:21,563 - memory_profile6_log - INFO - 

2018-05-02 21:40:21,575 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 21:40:21,576 - memory_profile6_log - INFO - 

2018-05-02 21:40:21,586 - memory_profile6_log - INFO - len of fitted models after concat: 20000
2018-05-02 21:40:21,588 - memory_profile6_log - INFO - 

2018-05-02 21:40:21,588 - memory_profile6_log - INFO - Recalculating fitted models...
2018-05-02 21:40:21,589 - memory_profile6_log - INFO - 

2018-05-02 21:40:21,621 - memory_profile6_log - INFO - fitted_models after cobine:

2018-05-02 21:40:21,622 - memory_profile6_log - INFO - 

2018-05-02 21:40:21,635 - memory_profile6_log - INFO - Empty DataFrame
Columns: [pt_posterior_x_Nt, topic_id, user_id]
Index: []
2018-05-02 21:40:21,637 - memory_profile6_log - INFO - 

2018-05-02 21:40:21,637 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 10000
2018-05-02 21:40:21,638 - memory_profile6_log - INFO - 

2018-05-02 21:40:23,260 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-05-02 21:40:23,282 - memory_profile6_log - INFO - Empty DataFrame
Columns: [user_id, topic_id, pt_posterior_x_Nt, smoothed_pt_posterior, p0_cat_ci, sigma_Nt, p0_posterior, is_general, rank]
Index: []
2018-05-02 21:40:23,282 - memory_profile6_log - INFO - 

2018-05-02 21:40:23,283 - memory_profile6_log - INFO - Len of model_transform: 4955
2018-05-02 21:40:23,285 - memory_profile6_log - INFO - Len of df_dt: 10000
2018-05-02 21:40:23,286 - memory_profile6_log - INFO - Total train time: 2.048s
2018-05-02 21:40:23,286 - memory_profile6_log - INFO - memory left before cleaning: 72.800 percent memory...
2018-05-02 21:40:23,288 - memory_profile6_log - INFO - cleaning up some objects...
2018-05-02 21:40:23,289 - memory_profile6_log - INFO - deleting df_dut...
2018-05-02 21:40:23,290 - memory_profile6_log - INFO - deleting df_dt...
2018-05-02 21:40:23,292 - memory_profile6_log - INFO - deleting df_input...
2018-05-02 21:40:23,293 - memory_profile6_log - INFO - deleting df_input_X...
2018-05-02 21:40:23,296 - memory_profile6_log - INFO - deleting df_current...
2018-05-02 21:40:23,296 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-05-02 21:40:23,298 - memory_profile6_log - INFO - deleting model_fit...
2018-05-02 21:40:23,298 - memory_profile6_log - INFO - deleting result...
2018-05-02 21:40:23,315 - memory_profile6_log - INFO - list(model_transformsv) before rename: 
2018-05-02 21:40:23,315 - memory_profile6_log - INFO -  
2018-05-02 21:40:23,316 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'is_general', 'p0_posterior', 'rank', 'date']
2018-05-02 21:40:23,318 - memory_profile6_log - INFO - 

2018-05-02 21:40:23,328 - memory_profile6_log - INFO -          date
5  2018-05-02
27 2018-05-02
59 2018-05-02
70 2018-05-02
84 2018-05-02
2018-05-02 21:40:23,329 - memory_profile6_log - INFO - 

2018-05-02 21:40:23,332 - memory_profile6_log - INFO - 4955
2018-05-02 21:40:23,332 - memory_profile6_log - INFO - 

2018-05-02 21:40:23,335 - memory_profile6_log - INFO - list(model_transformsv) after rename: 
2018-05-02 21:40:23,338 - memory_profile6_log - INFO -  
2018-05-02 21:40:23,339 - memory_profile6_log - INFO - ['user_id', 'topic_id', 'topic_is_general', 'interest_score', 'interest_rank', 'interest_score_created_at']
2018-05-02 21:40:23,341 - memory_profile6_log - INFO - 

2018-05-02 21:40:23,342 - memory_profile6_log - INFO - 4955
2018-05-02 21:40:23,345 - memory_profile6_log - INFO - 

2018-05-02 21:40:23,348 - memory_profile6_log - INFO - deleting model_transform...
2018-05-02 21:40:23,348 - memory_profile6_log - INFO - memory left after cleaning: 72.800 percent memory...
2018-05-02 21:40:23,349 - memory_profile6_log - INFO - Begin saving trained data...
2018-05-02 21:40:23,351 - memory_profile6_log - INFO - Using ElasticSearch as storage...
2018-05-02 21:40:23,375 - memory_profile6_log - INFO - Saving total data: 4955
2018-05-02 21:40:23,377 - memory_profile6_log - INFO - Len of X_split for batch save model_transformsv: 15
2018-05-02 21:40:23,378 - memory_profile6_log - INFO - processing batch-0
2018-05-02 21:40:27,624 - memory_profile6_log - INFO - processing batch-1
2018-05-02 21:40:29,901 - memory_profile6_log - INFO - processing batch-2
2018-05-02 21:40:33,763 - memory_profile6_log - INFO - processing batch-3
2018-05-02 21:40:36,463 - memory_profile6_log - INFO - processing batch-4
2018-05-02 21:40:38,822 - memory_profile6_log - INFO - processing batch-5
2018-05-02 21:40:40,539 - memory_profile6_log - INFO - processing batch-6
2018-05-02 21:40:42,180 - memory_profile6_log - INFO - processing batch-7
2018-05-02 21:40:43,982 - memory_profile6_log - INFO - processing batch-8
2018-05-02 21:40:45,927 - memory_profile6_log - INFO - processing batch-9
2018-05-02 21:40:49,056 - memory_profile6_log - INFO - processing batch-10
2018-05-02 21:40:51,267 - memory_profile6_log - INFO - processing batch-11
2018-05-02 21:40:53,407 - memory_profile6_log - INFO - processing batch-12
2018-05-02 21:40:55,154 - memory_profile6_log - INFO - processing batch-13
2018-05-02 21:40:56,779 - memory_profile6_log - INFO - processing batch-14
2018-05-02 21:40:58,101 - memory_profile6_log - INFO - Saving fitted_models as history to Elasticsearch...
2018-05-02 21:40:58,151 - memory_profile6_log - INFO - Saving total data: 10000
2018-05-02 21:40:58,153 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 25
2018-05-02 21:40:58,154 - memory_profile6_log - INFO - processing batch-0
2018-05-02 21:40:59,947 - memory_profile6_log - INFO - processing batch-1
2018-05-02 21:41:01,913 - memory_profile6_log - INFO - processing batch-2
2018-05-02 21:41:04,785 - memory_profile6_log - INFO - processing batch-3
2018-05-02 21:41:09,427 - memory_profile6_log - INFO - processing batch-4
2018-05-02 21:41:13,313 - memory_profile6_log - INFO - processing batch-5
2018-05-02 21:41:15,903 - memory_profile6_log - INFO - processing batch-6
2018-05-02 21:41:18,066 - memory_profile6_log - INFO - processing batch-7
2018-05-02 21:41:20,138 - memory_profile6_log - INFO - processing batch-8
2018-05-02 21:41:22,108 - memory_profile6_log - INFO - processing batch-9
2018-05-02 21:41:23,862 - memory_profile6_log - INFO - processing batch-10
2018-05-02 21:41:25,487 - memory_profile6_log - INFO - processing batch-11
2018-05-02 21:41:27,331 - memory_profile6_log - INFO - processing batch-12
2018-05-02 21:41:32,233 - memory_profile6_log - INFO - processing batch-13
2018-05-02 21:41:35,773 - memory_profile6_log - INFO - processing batch-14
2018-05-02 21:41:38,990 - memory_profile6_log - INFO - processing batch-15
2018-05-02 21:41:41,622 - memory_profile6_log - INFO - processing batch-16
2018-05-02 21:41:45,786 - memory_profile6_log - INFO - processing batch-17
2018-05-02 21:41:48,237 - memory_profile6_log - INFO - processing batch-18
2018-05-02 21:41:50,552 - memory_profile6_log - INFO - processing batch-19
2018-05-02 21:41:52,628 - memory_profile6_log - INFO - processing batch-20
2018-05-02 21:41:55,869 - memory_profile6_log - INFO - processing batch-21
2018-05-02 21:41:58,867 - memory_profile6_log - INFO - processing batch-22
2018-05-02 21:42:01,194 - memory_profile6_log - INFO - processing batch-23

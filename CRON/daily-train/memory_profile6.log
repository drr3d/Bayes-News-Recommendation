2018-04-29 11:06:32,338 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 11:06:32,342 - memory_profile6_log - INFO - date_generated: 
2018-04-29 11:06:32,342 - memory_profile6_log - INFO -  
2018-04-29 11:06:32,342 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 28, 11, 6, 32, 339000)]
2018-04-29 11:06:32,342 - memory_profile6_log - INFO - 

2018-04-29 11:06:32,342 - memory_profile6_log - INFO - using current date: 2018-04-29
2018-04-29 11:06:32,344 - memory_profile6_log - INFO - using start date: 2018-04-28 00:00:00
2018-04-29 11:06:32,344 - memory_profile6_log - INFO - using end date: 2018-04-28
2018-04-29 11:06:32,476 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 11:06:32,480 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-28
2018-04-29 11:06:36,414 - memory_profile6_log - INFO - size of df: 24.0 Byte
2018-04-29 11:06:36,415 - memory_profile6_log - INFO - 2018-04-28 data is empty!
2018-04-29 11:06:36,417 - memory_profile6_log - INFO - tframe for date: 2018-04-28 is empty
2018-04-29 11:06:36,418 - memory_profile6_log - INFO - len datalist: 0
2018-04-29 11:06:36,420 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 11:06:36,421 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:06:36,421 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:06:36,421 - memory_profile6_log - INFO - ================================================

2018-04-29 11:06:36,423 - memory_profile6_log - INFO -    289     87.0 MiB     87.0 MiB   @profile

2018-04-29 11:06:36,424 - memory_profile6_log - INFO -    290                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 11:06:36,424 - memory_profile6_log - INFO -    291     87.0 MiB      0.0 MiB       bq_client = client

2018-04-29 11:06:36,426 - memory_profile6_log - INFO -    292     87.0 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:06:36,427 - memory_profile6_log - INFO -    293                             

2018-04-29 11:06:36,427 - memory_profile6_log - INFO -    294     87.0 MiB      0.0 MiB       datalist = []

2018-04-29 11:06:36,428 - memory_profile6_log - INFO -    295     87.0 MiB      0.0 MiB       datalist_hist = []

2018-04-29 11:06:36,430 - memory_profile6_log - INFO -    296                             

2018-04-29 11:06:36,431 - memory_profile6_log - INFO -    297     87.0 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 11:06:36,433 - memory_profile6_log - INFO -    298     90.6 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 11:06:36,434 - memory_profile6_log - INFO -    299     90.6 MiB      3.6 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 11:06:36,436 - memory_profile6_log - INFO -    300     90.6 MiB      0.0 MiB           if tframe is not None:

2018-04-29 11:06:36,437 - memory_profile6_log - INFO -    301                                         if not tframe.empty:

2018-04-29 11:06:36,437 - memory_profile6_log - INFO -    302                                             X_split = np.array_split(tframe, 5)

2018-04-29 11:06:36,438 - memory_profile6_log - INFO -    303                                             logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 11:06:36,440 - memory_profile6_log - INFO -    304                                             logger.info("Appending history data...")

2018-04-29 11:06:36,446 - memory_profile6_log - INFO -    305                                             for ix in range(len(X_split)):

2018-04-29 11:06:36,448 - memory_profile6_log - INFO -    306                                                 # ~ loading history

2018-04-29 11:06:36,450 - memory_profile6_log - INFO -    307                                                 """

2018-04-29 11:06:36,451 - memory_profile6_log - INFO -    308                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 11:06:36,453 - memory_profile6_log - INFO -    309                                                 """

2018-04-29 11:06:36,459 - memory_profile6_log - INFO -    310                                                 logger.info("processing batch-%d", ix)

2018-04-29 11:06:36,460 - memory_profile6_log - INFO -    311                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 11:06:36,463 - memory_profile6_log - INFO -    312                                                 logger.info("creating list history data...")

2018-04-29 11:06:36,464 - memory_profile6_log - INFO -    313                                                 lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 11:06:36,464 - memory_profile6_log - INFO -    314                             

2018-04-29 11:06:36,467 - memory_profile6_log - INFO -    315                                                 logger.info("call history data...")

2018-04-29 11:06:36,469 - memory_profile6_log - INFO -    316                                                 h_frame = mh.loadDSHistory(lhistory)

2018-04-29 11:06:36,470 - memory_profile6_log - INFO -    317                             

2018-04-29 11:06:36,471 - memory_profile6_log - INFO -    318                                                 # me = os.getpid()

2018-04-29 11:06:36,473 - memory_profile6_log - INFO -    319                                                 # kill_proc_tree(me)

2018-04-29 11:06:36,474 - memory_profile6_log - INFO -    320                             

2018-04-29 11:06:36,480 - memory_profile6_log - INFO -    321                                                 logger.info("done collecting history data, appending now...")

2018-04-29 11:06:36,480 - memory_profile6_log - INFO -    322                                                 for m in h_frame:

2018-04-29 11:06:36,483 - memory_profile6_log - INFO -    323                                                     if m is not None:

2018-04-29 11:06:36,483 - memory_profile6_log - INFO -    324                                                         if len(m) > 0:

2018-04-29 11:06:36,484 - memory_profile6_log - INFO -    325                                                             datalist_hist.append(pd.DataFrame(m))

2018-04-29 11:06:36,486 - memory_profile6_log - INFO -    326                                                 del h_frame

2018-04-29 11:06:36,486 - memory_profile6_log - INFO -    327                                                 del lhistory

2018-04-29 11:06:36,490 - memory_profile6_log - INFO -    328                             

2018-04-29 11:06:36,490 - memory_profile6_log - INFO -    329                                             logger.info("Appending training data...")

2018-04-29 11:06:36,492 - memory_profile6_log - INFO -    330                                             datalist.append(tframe)

2018-04-29 11:06:36,493 - memory_profile6_log - INFO -    331                                     else: 

2018-04-29 11:06:36,493 - memory_profile6_log - INFO -    332     90.6 MiB      0.0 MiB               logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 11:06:36,493 - memory_profile6_log - INFO -    333     90.6 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 11:06:36,494 - memory_profile6_log - INFO -    334     90.6 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 11:06:36,496 - memory_profile6_log - INFO -    335                             

2018-04-29 11:06:36,497 - memory_profile6_log - INFO -    336     90.6 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 11:06:36,500 - memory_profile6_log - INFO - 


2018-04-29 11:10:03,549 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 11:10:03,552 - memory_profile6_log - INFO - date_generated: 
2018-04-29 11:10:03,552 - memory_profile6_log - INFO -  
2018-04-29 11:10:03,552 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 28, 11, 10, 3, 549000)]
2018-04-29 11:10:03,552 - memory_profile6_log - INFO - 

2018-04-29 11:10:03,552 - memory_profile6_log - INFO - using current date: 2018-04-29
2018-04-29 11:10:03,552 - memory_profile6_log - INFO - using start date: 2018-04-28 00:00:00
2018-04-29 11:10:03,553 - memory_profile6_log - INFO - using end date: 2018-04-28
2018-04-29 11:10:03,683 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 11:10:03,687 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-28
2018-04-29 11:10:07,240 - memory_profile6_log - INFO - size of df: 24.0 Byte
2018-04-29 11:10:07,240 - memory_profile6_log - INFO - 2018-04-28 data is empty!
2018-04-29 11:10:07,243 - memory_profile6_log - INFO - tframe for date: 2018-04-28 is empty
2018-04-29 11:10:07,244 - memory_profile6_log - INFO - len datalist: 0
2018-04-29 11:10:07,246 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 11:10:07,247 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:10:07,249 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:10:07,249 - memory_profile6_log - INFO - ================================================

2018-04-29 11:10:07,252 - memory_profile6_log - INFO -    289     87.1 MiB     87.1 MiB   @profile

2018-04-29 11:10:07,253 - memory_profile6_log - INFO -    290                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 11:10:07,253 - memory_profile6_log - INFO -    291     87.1 MiB      0.0 MiB       bq_client = client

2018-04-29 11:10:07,255 - memory_profile6_log - INFO -    292     87.1 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:10:07,256 - memory_profile6_log - INFO -    293                             

2018-04-29 11:10:07,257 - memory_profile6_log - INFO -    294     87.1 MiB      0.0 MiB       datalist = []

2018-04-29 11:10:07,259 - memory_profile6_log - INFO -    295     87.1 MiB      0.0 MiB       datalist_hist = []

2018-04-29 11:10:07,259 - memory_profile6_log - INFO -    296                             

2018-04-29 11:10:07,262 - memory_profile6_log - INFO -    297     87.1 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 11:10:07,263 - memory_profile6_log - INFO -    298     90.6 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 11:10:07,266 - memory_profile6_log - INFO -    299     90.6 MiB      3.5 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 11:10:07,266 - memory_profile6_log - INFO -    300     90.6 MiB      0.0 MiB           if tframe is not None:

2018-04-29 11:10:07,266 - memory_profile6_log - INFO -    301                                         if not tframe.empty:

2018-04-29 11:10:07,267 - memory_profile6_log - INFO -    302                                             X_split = np.array_split(tframe, 5)

2018-04-29 11:10:07,269 - memory_profile6_log - INFO -    303                                             logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 11:10:07,269 - memory_profile6_log - INFO -    304                                             logger.info("Appending history data...")

2018-04-29 11:10:07,273 - memory_profile6_log - INFO -    305                                             for ix in range(len(X_split)):

2018-04-29 11:10:07,273 - memory_profile6_log - INFO -    306                                                 # ~ loading history

2018-04-29 11:10:07,275 - memory_profile6_log - INFO -    307                                                 """

2018-04-29 11:10:07,276 - memory_profile6_log - INFO -    308                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 11:10:07,278 - memory_profile6_log - INFO -    309                                                 """

2018-04-29 11:10:07,279 - memory_profile6_log - INFO -    310                                                 logger.info("processing batch-%d", ix)

2018-04-29 11:10:07,282 - memory_profile6_log - INFO -    311                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 11:10:07,286 - memory_profile6_log - INFO -    312                                                 logger.info("creating list history data...")

2018-04-29 11:10:07,286 - memory_profile6_log - INFO -    313                                                 lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 11:10:07,288 - memory_profile6_log - INFO -    314                             

2018-04-29 11:10:07,289 - memory_profile6_log - INFO -    315                                                 logger.info("call history data...")

2018-04-29 11:10:07,289 - memory_profile6_log - INFO -    316                                                 h_frame = mh.loadDSHistory(lhistory)

2018-04-29 11:10:07,290 - memory_profile6_log - INFO -    317                             

2018-04-29 11:10:07,290 - memory_profile6_log - INFO -    318                                                 # me = os.getpid()

2018-04-29 11:10:07,292 - memory_profile6_log - INFO -    319                                                 # kill_proc_tree(me)

2018-04-29 11:10:07,292 - memory_profile6_log - INFO -    320                             

2018-04-29 11:10:07,298 - memory_profile6_log - INFO -    321                                                 logger.info("done collecting history data, appending now...")

2018-04-29 11:10:07,299 - memory_profile6_log - INFO -    322                                                 for m in h_frame:

2018-04-29 11:10:07,299 - memory_profile6_log - INFO -    323                                                     if m is not None:

2018-04-29 11:10:07,301 - memory_profile6_log - INFO -    324                                                         if len(m) > 0:

2018-04-29 11:10:07,302 - memory_profile6_log - INFO -    325                                                             datalist_hist.append(pd.DataFrame(m))

2018-04-29 11:10:07,302 - memory_profile6_log - INFO -    326                                                 del h_frame

2018-04-29 11:10:07,303 - memory_profile6_log - INFO -    327                                                 del lhistory

2018-04-29 11:10:07,305 - memory_profile6_log - INFO -    328                             

2018-04-29 11:10:07,306 - memory_profile6_log - INFO -    329                                             logger.info("Appending training data...")

2018-04-29 11:10:07,308 - memory_profile6_log - INFO -    330                                             datalist.append(tframe)

2018-04-29 11:10:07,309 - memory_profile6_log - INFO -    331                                     else: 

2018-04-29 11:10:07,309 - memory_profile6_log - INFO -    332     90.6 MiB      0.0 MiB               logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 11:10:07,311 - memory_profile6_log - INFO -    333     90.6 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 11:10:07,311 - memory_profile6_log - INFO -    334     90.6 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 11:10:07,312 - memory_profile6_log - INFO -    335                             

2018-04-29 11:10:07,312 - memory_profile6_log - INFO -    336     90.6 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 11:10:07,313 - memory_profile6_log - INFO - 


2018-04-29 11:11:47,276 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 11:11:47,279 - memory_profile6_log - INFO - date_generated: 
2018-04-29 11:11:47,279 - memory_profile6_log - INFO -  
2018-04-29 11:11:47,279 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 28, 11, 11, 47, 277000)]
2018-04-29 11:11:47,279 - memory_profile6_log - INFO - 

2018-04-29 11:11:47,279 - memory_profile6_log - INFO - using current date: 2018-04-29
2018-04-29 11:11:47,280 - memory_profile6_log - INFO - using start date: 2018-04-28 00:00:00
2018-04-29 11:11:47,280 - memory_profile6_log - INFO - using end date: 2018-04-28
2018-04-29 11:11:47,424 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 11:11:47,427 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-28
2018-04-29 11:11:51,010 - memory_profile6_log - INFO - size of df: 24.0 Byte
2018-04-29 11:11:51,012 - memory_profile6_log - INFO - 2018-04-28 data is empty!
2018-04-29 11:11:51,013 - memory_profile6_log - INFO - tframe for date: 2018-04-28 is empty
2018-04-29 11:11:51,015 - memory_profile6_log - INFO - len datalist: 0
2018-04-29 11:11:51,016 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 11:11:51,017 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:11:51,019 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:11:51,019 - memory_profile6_log - INFO - ================================================

2018-04-29 11:11:51,023 - memory_profile6_log - INFO -    289     86.8 MiB     86.8 MiB   @profile

2018-04-29 11:11:51,025 - memory_profile6_log - INFO -    290                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 11:11:51,026 - memory_profile6_log - INFO -    291     86.8 MiB      0.0 MiB       bq_client = client

2018-04-29 11:11:51,026 - memory_profile6_log - INFO -    292     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:11:51,029 - memory_profile6_log - INFO -    293                             

2018-04-29 11:11:51,029 - memory_profile6_log - INFO -    294     86.8 MiB      0.0 MiB       datalist = []

2018-04-29 11:11:51,032 - memory_profile6_log - INFO -    295     86.8 MiB      0.0 MiB       datalist_hist = []

2018-04-29 11:11:51,033 - memory_profile6_log - INFO -    296                             

2018-04-29 11:11:51,035 - memory_profile6_log - INFO -    297     86.8 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 11:11:51,036 - memory_profile6_log - INFO -    298     90.5 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 11:11:51,036 - memory_profile6_log - INFO -    299     90.5 MiB      3.6 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 11:11:51,038 - memory_profile6_log - INFO -    300     90.5 MiB      0.0 MiB           if tframe is not None:

2018-04-29 11:11:51,038 - memory_profile6_log - INFO -    301                                         if not tframe.empty:

2018-04-29 11:11:51,039 - memory_profile6_log - INFO -    302                                             X_split = np.array_split(tframe, 5)

2018-04-29 11:11:51,039 - memory_profile6_log - INFO -    303                                             logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 11:11:51,042 - memory_profile6_log - INFO -    304                                             logger.info("Appending history data...")

2018-04-29 11:11:51,043 - memory_profile6_log - INFO -    305                                             for ix in range(len(X_split)):

2018-04-29 11:11:51,045 - memory_profile6_log - INFO -    306                                                 # ~ loading history

2018-04-29 11:11:51,046 - memory_profile6_log - INFO -    307                                                 """

2018-04-29 11:11:51,046 - memory_profile6_log - INFO -    308                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 11:11:51,048 - memory_profile6_log - INFO -    309                                                 """

2018-04-29 11:11:51,049 - memory_profile6_log - INFO -    310                                                 logger.info("processing batch-%d", ix)

2018-04-29 11:11:51,049 - memory_profile6_log - INFO -    311                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 11:11:51,051 - memory_profile6_log - INFO -    312                                                 logger.info("creating list history data...")

2018-04-29 11:11:51,052 - memory_profile6_log - INFO -    313                                                 lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 11:11:51,055 - memory_profile6_log - INFO -    314                             

2018-04-29 11:11:51,055 - memory_profile6_log - INFO -    315                                                 logger.info("call history data...")

2018-04-29 11:11:51,059 - memory_profile6_log - INFO -    316                                                 h_frame = mh.loadDSHistory(lhistory)

2018-04-29 11:11:51,059 - memory_profile6_log - INFO -    317                             

2018-04-29 11:11:51,061 - memory_profile6_log - INFO -    318                                                 # me = os.getpid()

2018-04-29 11:11:51,061 - memory_profile6_log - INFO -    319                                                 # kill_proc_tree(me)

2018-04-29 11:11:51,062 - memory_profile6_log - INFO -    320                             

2018-04-29 11:11:51,065 - memory_profile6_log - INFO -    321                                                 logger.info("done collecting history data, appending now...")

2018-04-29 11:11:51,065 - memory_profile6_log - INFO -    322                                                 for m in h_frame:

2018-04-29 11:11:51,068 - memory_profile6_log - INFO -    323                                                     if m is not None:

2018-04-29 11:11:51,069 - memory_profile6_log - INFO -    324                                                         if len(m) > 0:

2018-04-29 11:11:51,071 - memory_profile6_log - INFO -    325                                                             datalist_hist.append(pd.DataFrame(m))

2018-04-29 11:11:51,072 - memory_profile6_log - INFO -    326                                                 del h_frame

2018-04-29 11:11:51,075 - memory_profile6_log - INFO -    327                                                 del lhistory

2018-04-29 11:11:51,076 - memory_profile6_log - INFO -    328                             

2018-04-29 11:11:51,078 - memory_profile6_log - INFO -    329                                             logger.info("Appending training data...")

2018-04-29 11:11:51,078 - memory_profile6_log - INFO -    330                                             datalist.append(tframe)

2018-04-29 11:11:51,079 - memory_profile6_log - INFO -    331                                     else: 

2018-04-29 11:11:51,081 - memory_profile6_log - INFO -    332     90.5 MiB      0.0 MiB               logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 11:11:51,082 - memory_profile6_log - INFO -    333     90.5 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 11:11:51,082 - memory_profile6_log - INFO -    334     90.5 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 11:11:51,084 - memory_profile6_log - INFO -    335                             

2018-04-29 11:11:51,085 - memory_profile6_log - INFO -    336     90.5 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 11:11:51,086 - memory_profile6_log - INFO - 


2018-04-29 11:12:06,967 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 11:12:06,970 - memory_profile6_log - INFO - date_generated: 
2018-04-29 11:12:06,971 - memory_profile6_log - INFO -  
2018-04-29 11:12:06,971 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 28, 11, 12, 6, 968000)]
2018-04-29 11:12:06,973 - memory_profile6_log - INFO - 

2018-04-29 11:12:06,973 - memory_profile6_log - INFO - using current date: 2018-04-29
2018-04-29 11:12:06,973 - memory_profile6_log - INFO - using start date: 2018-04-28 00:00:00
2018-04-29 11:12:06,974 - memory_profile6_log - INFO - using end date: 2018-04-28
2018-04-29 11:12:07,118 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 11:12:07,124 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-28
2018-04-29 11:12:09,585 - memory_profile6_log - INFO - size of df: 24.0 Byte
2018-04-29 11:12:09,586 - memory_profile6_log - INFO - 2018-04-28 data is empty!
2018-04-29 11:12:09,588 - memory_profile6_log - INFO - tframe for date: 2018-04-28 is empty
2018-04-29 11:12:09,589 - memory_profile6_log - INFO - len datalist: 0
2018-04-29 11:12:09,592 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 11:12:09,592 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:12:09,594 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:12:09,594 - memory_profile6_log - INFO - ================================================

2018-04-29 11:12:09,596 - memory_profile6_log - INFO -    289     86.8 MiB     86.8 MiB   @profile

2018-04-29 11:12:09,598 - memory_profile6_log - INFO -    290                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 11:12:09,598 - memory_profile6_log - INFO -    291     86.8 MiB      0.0 MiB       bq_client = client

2018-04-29 11:12:09,599 - memory_profile6_log - INFO -    292     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:12:09,601 - memory_profile6_log - INFO -    293                             

2018-04-29 11:12:09,601 - memory_profile6_log - INFO -    294     86.8 MiB      0.0 MiB       datalist = []

2018-04-29 11:12:09,602 - memory_profile6_log - INFO -    295     86.8 MiB      0.0 MiB       datalist_hist = []

2018-04-29 11:12:09,604 - memory_profile6_log - INFO -    296                             

2018-04-29 11:12:09,605 - memory_profile6_log - INFO -    297     86.8 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 11:12:09,608 - memory_profile6_log - INFO -    298     90.5 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 11:12:09,608 - memory_profile6_log - INFO -    299     90.5 MiB      3.6 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 11:12:09,609 - memory_profile6_log - INFO -    300     90.5 MiB      0.0 MiB           if tframe is not None:

2018-04-29 11:12:09,611 - memory_profile6_log - INFO -    301                                         if not tframe.empty:

2018-04-29 11:12:09,611 - memory_profile6_log - INFO -    302                                             X_split = np.array_split(tframe, 5)

2018-04-29 11:12:09,612 - memory_profile6_log - INFO -    303                                             logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 11:12:09,614 - memory_profile6_log - INFO -    304                                             logger.info("Appending history data...")

2018-04-29 11:12:09,615 - memory_profile6_log - INFO -    305                                             for ix in range(len(X_split)):

2018-04-29 11:12:09,615 - memory_profile6_log - INFO -    306                                                 # ~ loading history

2018-04-29 11:12:09,618 - memory_profile6_log - INFO -    307                                                 """

2018-04-29 11:12:09,618 - memory_profile6_log - INFO -    308                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 11:12:09,619 - memory_profile6_log - INFO -    309                                                 """

2018-04-29 11:12:09,621 - memory_profile6_log - INFO -    310                                                 logger.info("processing batch-%d", ix)

2018-04-29 11:12:09,621 - memory_profile6_log - INFO -    311                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 11:12:09,622 - memory_profile6_log - INFO -    312                                                 logger.info("creating list history data...")

2018-04-29 11:12:09,624 - memory_profile6_log - INFO -    313                                                 lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 11:12:09,625 - memory_profile6_log - INFO -    314                             

2018-04-29 11:12:09,625 - memory_profile6_log - INFO -    315                                                 logger.info("call history data...")

2018-04-29 11:12:09,627 - memory_profile6_log - INFO -    316                                                 h_frame = mh.loadDSHistory(lhistory)

2018-04-29 11:12:09,630 - memory_profile6_log - INFO -    317                             

2018-04-29 11:12:09,631 - memory_profile6_log - INFO -    318                                                 # me = os.getpid()

2018-04-29 11:12:09,631 - memory_profile6_log - INFO -    319                                                 # kill_proc_tree(me)

2018-04-29 11:12:09,632 - memory_profile6_log - INFO -    320                             

2018-04-29 11:12:09,632 - memory_profile6_log - INFO -    321                                                 logger.info("done collecting history data, appending now...")

2018-04-29 11:12:09,634 - memory_profile6_log - INFO -    322                                                 for m in h_frame:

2018-04-29 11:12:09,634 - memory_profile6_log - INFO -    323                                                     if m is not None:

2018-04-29 11:12:09,635 - memory_profile6_log - INFO -    324                                                         if len(m) > 0:

2018-04-29 11:12:09,637 - memory_profile6_log - INFO -    325                                                             datalist_hist.append(pd.DataFrame(m))

2018-04-29 11:12:09,640 - memory_profile6_log - INFO -    326                                                 del h_frame

2018-04-29 11:12:09,641 - memory_profile6_log - INFO -    327                                                 del lhistory

2018-04-29 11:12:09,641 - memory_profile6_log - INFO -    328                             

2018-04-29 11:12:09,642 - memory_profile6_log - INFO -    329                                             logger.info("Appending training data...")

2018-04-29 11:12:09,644 - memory_profile6_log - INFO -    330                                             datalist.append(tframe)

2018-04-29 11:12:09,644 - memory_profile6_log - INFO -    331                                     else: 

2018-04-29 11:12:09,645 - memory_profile6_log - INFO -    332     90.5 MiB      0.0 MiB               logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 11:12:09,648 - memory_profile6_log - INFO -    333     90.5 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 11:12:09,648 - memory_profile6_log - INFO -    334     90.5 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 11:12:09,651 - memory_profile6_log - INFO -    335                             

2018-04-29 11:12:09,653 - memory_profile6_log - INFO -    336     90.5 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 11:12:09,654 - memory_profile6_log - INFO - 


2018-04-29 11:12:09,654 - memory_profile6_log - INFO - Training cannot be empty..
2018-04-29 11:12:09,655 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:12:09,655 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:12:09,657 - memory_profile6_log - INFO - ================================================

2018-04-29 11:12:09,657 - memory_profile6_log - INFO -    338     86.7 MiB     86.7 MiB   @profile

2018-04-29 11:12:09,657 - memory_profile6_log - INFO -    339                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 11:12:09,658 - memory_profile6_log - INFO -    340     86.8 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 11:12:09,658 - memory_profile6_log - INFO -    341     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:12:09,663 - memory_profile6_log - INFO -    342                             

2018-04-29 11:12:09,663 - memory_profile6_log - INFO -    343                                 # ~~~ Begin collecting data ~~~

2018-04-29 11:12:09,664 - memory_profile6_log - INFO -    344     86.8 MiB      0.0 MiB       t0 = time.time()

2018-04-29 11:12:09,664 - memory_profile6_log - INFO -    345     90.5 MiB      3.6 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 11:12:09,664 - memory_profile6_log - INFO -    346     90.5 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 11:12:09,664 - memory_profile6_log - INFO -    347     90.5 MiB      0.0 MiB           logger.info("Training cannot be empty..")

2018-04-29 11:12:09,665 - memory_profile6_log - INFO -    348     90.5 MiB      0.0 MiB           return

2018-04-29 11:12:09,665 - memory_profile6_log - INFO -    349                                 big_frame_hist = pd.concat(datalist_hist)

2018-04-29 11:12:09,667 - memory_profile6_log - INFO -    350                                 print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 11:12:09,667 - memory_profile6_log - INFO -    351                                 logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 11:12:09,667 - memory_profile6_log - INFO -    352                             

2018-04-29 11:12:09,667 - memory_profile6_log - INFO -    353                                 big_frame = pd.concat(datalist)

2018-04-29 11:12:09,668 - memory_profile6_log - INFO -    354                                 logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 11:12:09,668 - memory_profile6_log - INFO -    355                                 del datalist

2018-04-29 11:12:09,668 - memory_profile6_log - INFO -    356                             

2018-04-29 11:12:09,670 - memory_profile6_log - INFO -    357                                 big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 11:12:09,670 - memory_profile6_log - INFO -    358                             

2018-04-29 11:12:09,674 - memory_profile6_log - INFO -    359                                 # ~ get current news interest ~

2018-04-29 11:12:09,674 - memory_profile6_log - INFO -    360                                 if not cd:

2018-04-29 11:12:09,676 - memory_profile6_log - INFO -    361                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 11:12:09,676 - memory_profile6_log - INFO -    362                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 11:12:09,677 - memory_profile6_log - INFO -    363                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 11:12:09,677 - memory_profile6_log - INFO -    364                                 else:

2018-04-29 11:12:09,677 - memory_profile6_log - INFO -    365                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 11:12:09,677 - memory_profile6_log - INFO -    366                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 11:12:09,678 - memory_profile6_log - INFO -    367                             

2018-04-29 11:12:09,680 - memory_profile6_log - INFO -    368                                     # safe handling of query parameter

2018-04-29 11:12:09,680 - memory_profile6_log - INFO -    369                                     query_params = [

2018-04-29 11:12:09,680 - memory_profile6_log - INFO -    370                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 11:12:09,681 - memory_profile6_log - INFO -    371                                     ]

2018-04-29 11:12:09,681 - memory_profile6_log - INFO -    372                             

2018-04-29 11:12:09,686 - memory_profile6_log - INFO -    373                                     job_config.query_parameters = query_params

2018-04-29 11:12:09,686 - memory_profile6_log - INFO -    374                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 11:12:09,687 - memory_profile6_log - INFO -    375                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 11:12:09,687 - memory_profile6_log - INFO -    376                             

2018-04-29 11:12:09,687 - memory_profile6_log - INFO -    377                                 current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 11:12:09,688 - memory_profile6_log - INFO -    378                                 current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 11:12:09,688 - memory_profile6_log - INFO -    379                                                                        format='%Y-%m-%d', errors='coerce')

2018-04-29 11:12:09,690 - memory_profile6_log - INFO -    380                                 logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 11:12:09,690 - memory_profile6_log - INFO -    381                                 train_time = time.time() - t0

2018-04-29 11:12:09,690 - memory_profile6_log - INFO -    382                                 logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 11:12:09,691 - memory_profile6_log - INFO -    383                             

2018-04-29 11:12:09,694 - memory_profile6_log - INFO -    384                                 return big_frame, current_frame, big_frame_hist

2018-04-29 11:12:09,694 - memory_profile6_log - INFO - 


2018-04-29 11:12:52,010 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 11:12:52,013 - memory_profile6_log - INFO - date_generated: 
2018-04-29 11:12:52,013 - memory_profile6_log - INFO -  
2018-04-29 11:12:52,015 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 28, 11, 12, 52, 12000)]
2018-04-29 11:12:52,016 - memory_profile6_log - INFO - 

2018-04-29 11:12:52,016 - memory_profile6_log - INFO - using current date: 2018-04-29
2018-04-29 11:12:52,016 - memory_profile6_log - INFO - using start date: 2018-04-28 00:00:00
2018-04-29 11:12:52,017 - memory_profile6_log - INFO - using end date: 2018-04-28
2018-04-29 11:12:52,150 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 11:12:52,154 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-28
2018-04-29 11:12:54,575 - memory_profile6_log - INFO - size of df: 24.0 Byte
2018-04-29 11:12:54,576 - memory_profile6_log - INFO - 2018-04-28 data is empty!
2018-04-29 11:12:54,578 - memory_profile6_log - INFO - tframe for date: 2018-04-28 is empty
2018-04-29 11:12:54,579 - memory_profile6_log - INFO - len datalist: 0
2018-04-29 11:12:54,581 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 11:12:54,582 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:12:54,582 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:12:54,584 - memory_profile6_log - INFO - ================================================

2018-04-29 11:12:54,585 - memory_profile6_log - INFO -    289     86.6 MiB     86.6 MiB   @profile

2018-04-29 11:12:54,586 - memory_profile6_log - INFO -    290                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 11:12:54,588 - memory_profile6_log - INFO -    291     86.6 MiB      0.0 MiB       bq_client = client

2018-04-29 11:12:54,588 - memory_profile6_log - INFO -    292     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:12:54,589 - memory_profile6_log - INFO -    293                             

2018-04-29 11:12:54,591 - memory_profile6_log - INFO -    294     86.6 MiB      0.0 MiB       datalist = []

2018-04-29 11:12:54,592 - memory_profile6_log - INFO -    295     86.6 MiB      0.0 MiB       datalist_hist = []

2018-04-29 11:12:54,592 - memory_profile6_log - INFO -    296                             

2018-04-29 11:12:54,592 - memory_profile6_log - INFO -    297     86.6 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 11:12:54,594 - memory_profile6_log - INFO -    298     90.2 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 11:12:54,594 - memory_profile6_log - INFO -    299     90.2 MiB      3.6 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 11:12:54,598 - memory_profile6_log - INFO -    300     90.2 MiB      0.0 MiB           if tframe is not None:

2018-04-29 11:12:54,598 - memory_profile6_log - INFO -    301                                         if not tframe.empty:

2018-04-29 11:12:54,598 - memory_profile6_log - INFO -    302                                             X_split = np.array_split(tframe, 5)

2018-04-29 11:12:54,599 - memory_profile6_log - INFO -    303                                             logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 11:12:54,601 - memory_profile6_log - INFO -    304                                             logger.info("Appending history data...")

2018-04-29 11:12:54,601 - memory_profile6_log - INFO -    305                                             for ix in range(len(X_split)):

2018-04-29 11:12:54,601 - memory_profile6_log - INFO -    306                                                 # ~ loading history

2018-04-29 11:12:54,602 - memory_profile6_log - INFO -    307                                                 """

2018-04-29 11:12:54,602 - memory_profile6_log - INFO -    308                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 11:12:54,604 - memory_profile6_log - INFO -    309                                                 """

2018-04-29 11:12:54,605 - memory_profile6_log - INFO -    310                                                 logger.info("processing batch-%d", ix)

2018-04-29 11:12:54,608 - memory_profile6_log - INFO -    311                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 11:12:54,608 - memory_profile6_log - INFO -    312                                                 logger.info("creating list history data...")

2018-04-29 11:12:54,609 - memory_profile6_log - INFO -    313                                                 lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 11:12:54,611 - memory_profile6_log - INFO -    314                             

2018-04-29 11:12:54,611 - memory_profile6_log - INFO -    315                                                 logger.info("call history data...")

2018-04-29 11:12:54,612 - memory_profile6_log - INFO -    316                                                 h_frame = mh.loadDSHistory(lhistory)

2018-04-29 11:12:54,614 - memory_profile6_log - INFO -    317                             

2018-04-29 11:12:54,615 - memory_profile6_log - INFO -    318                                                 # me = os.getpid()

2018-04-29 11:12:54,615 - memory_profile6_log - INFO -    319                                                 # kill_proc_tree(me)

2018-04-29 11:12:54,615 - memory_profile6_log - INFO -    320                             

2018-04-29 11:12:54,618 - memory_profile6_log - INFO -    321                                                 logger.info("done collecting history data, appending now...")

2018-04-29 11:12:54,619 - memory_profile6_log - INFO -    322                                                 for m in h_frame:

2018-04-29 11:12:54,619 - memory_profile6_log - INFO -    323                                                     if m is not None:

2018-04-29 11:12:54,621 - memory_profile6_log - INFO -    324                                                         if len(m) > 0:

2018-04-29 11:12:54,621 - memory_profile6_log - INFO -    325                                                             datalist_hist.append(pd.DataFrame(m))

2018-04-29 11:12:54,622 - memory_profile6_log - INFO -    326                                                 del h_frame

2018-04-29 11:12:54,624 - memory_profile6_log - INFO -    327                                                 del lhistory

2018-04-29 11:12:54,624 - memory_profile6_log - INFO -    328                             

2018-04-29 11:12:54,625 - memory_profile6_log - INFO -    329                                             logger.info("Appending training data...")

2018-04-29 11:12:54,625 - memory_profile6_log - INFO -    330                                             datalist.append(tframe)

2018-04-29 11:12:54,627 - memory_profile6_log - INFO -    331                                     else: 

2018-04-29 11:12:54,630 - memory_profile6_log - INFO -    332     90.2 MiB      0.0 MiB               logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 11:12:54,631 - memory_profile6_log - INFO -    333     90.2 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 11:12:54,631 - memory_profile6_log - INFO -    334     90.2 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 11:12:54,632 - memory_profile6_log - INFO -    335                             

2018-04-29 11:12:54,634 - memory_profile6_log - INFO -    336     90.2 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 11:12:54,634 - memory_profile6_log - INFO - 


2018-04-29 11:14:36,135 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 11:14:36,138 - memory_profile6_log - INFO - date_generated: 
2018-04-29 11:14:36,138 - memory_profile6_log - INFO -  
2018-04-29 11:14:36,138 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 28, 11, 14, 36, 136000)]
2018-04-29 11:14:36,138 - memory_profile6_log - INFO - 

2018-04-29 11:14:36,140 - memory_profile6_log - INFO - using current date: 2018-04-29
2018-04-29 11:14:36,140 - memory_profile6_log - INFO - using start date: 2018-04-28 00:00:00
2018-04-29 11:14:36,140 - memory_profile6_log - INFO - using end date: 2018-04-28
2018-04-29 11:14:36,273 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 11:14:36,276 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-28
2018-04-29 11:14:38,703 - memory_profile6_log - INFO - size of df: 24.0 Byte
2018-04-29 11:14:38,704 - memory_profile6_log - INFO - 2018-04-28 data is empty!
2018-04-29 11:14:38,707 - memory_profile6_log - INFO - tframe for date: 2018-04-28 is empty
2018-04-29 11:14:38,707 - memory_profile6_log - INFO - len datalist: 0
2018-04-29 11:14:38,709 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 11:14:38,710 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:14:38,710 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:14:38,710 - memory_profile6_log - INFO - ================================================

2018-04-29 11:14:38,711 - memory_profile6_log - INFO -    289     87.1 MiB     87.1 MiB   @profile

2018-04-29 11:14:38,713 - memory_profile6_log - INFO -    290                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 11:14:38,714 - memory_profile6_log - INFO -    291     87.1 MiB      0.0 MiB       bq_client = client

2018-04-29 11:14:38,716 - memory_profile6_log - INFO -    292     87.1 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:14:38,717 - memory_profile6_log - INFO -    293                             

2018-04-29 11:14:38,717 - memory_profile6_log - INFO -    294     87.1 MiB      0.0 MiB       datalist = []

2018-04-29 11:14:38,717 - memory_profile6_log - INFO -    295     87.1 MiB      0.0 MiB       datalist_hist = []

2018-04-29 11:14:38,719 - memory_profile6_log - INFO -    296                             

2018-04-29 11:14:38,720 - memory_profile6_log - INFO -    297     87.1 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 11:14:38,720 - memory_profile6_log - INFO -    298     90.6 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 11:14:38,720 - memory_profile6_log - INFO -    299     90.6 MiB      3.6 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 11:14:38,721 - memory_profile6_log - INFO -    300     90.6 MiB      0.0 MiB           if tframe is not None:

2018-04-29 11:14:38,723 - memory_profile6_log - INFO -    301                                         if not tframe.empty:

2018-04-29 11:14:38,723 - memory_profile6_log - INFO -    302                                             X_split = np.array_split(tframe, 5)

2018-04-29 11:14:38,724 - memory_profile6_log - INFO -    303                                             logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 11:14:38,726 - memory_profile6_log - INFO -    304                                             logger.info("Appending history data...")

2018-04-29 11:14:38,726 - memory_profile6_log - INFO -    305                                             for ix in range(len(X_split)):

2018-04-29 11:14:38,727 - memory_profile6_log - INFO -    306                                                 # ~ loading history

2018-04-29 11:14:38,729 - memory_profile6_log - INFO -    307                                                 """

2018-04-29 11:14:38,730 - memory_profile6_log - INFO -    308                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 11:14:38,730 - memory_profile6_log - INFO -    309                                                 """

2018-04-29 11:14:38,732 - memory_profile6_log - INFO -    310                                                 logger.info("processing batch-%d", ix)

2018-04-29 11:14:38,734 - memory_profile6_log - INFO -    311                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 11:14:38,736 - memory_profile6_log - INFO -    312                                                 logger.info("creating list history data...")

2018-04-29 11:14:38,736 - memory_profile6_log - INFO -    313                                                 lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 11:14:38,737 - memory_profile6_log - INFO -    314                             

2018-04-29 11:14:38,740 - memory_profile6_log - INFO -    315                                                 logger.info("call history data...")

2018-04-29 11:14:38,740 - memory_profile6_log - INFO -    316                                                 h_frame = mh.loadDSHistory(lhistory)

2018-04-29 11:14:38,742 - memory_profile6_log - INFO -    317                             

2018-04-29 11:14:38,743 - memory_profile6_log - INFO -    318                                                 # me = os.getpid()

2018-04-29 11:14:38,744 - memory_profile6_log - INFO -    319                                                 # kill_proc_tree(me)

2018-04-29 11:14:38,746 - memory_profile6_log - INFO -    320                             

2018-04-29 11:14:38,746 - memory_profile6_log - INFO -    321                                                 logger.info("done collecting history data, appending now...")

2018-04-29 11:14:38,749 - memory_profile6_log - INFO -    322                                                 for m in h_frame:

2018-04-29 11:14:38,750 - memory_profile6_log - INFO -    323                                                     if m is not None:

2018-04-29 11:14:38,752 - memory_profile6_log - INFO -    324                                                         if len(m) > 0:

2018-04-29 11:14:38,753 - memory_profile6_log - INFO -    325                                                             datalist_hist.append(pd.DataFrame(m))

2018-04-29 11:14:38,753 - memory_profile6_log - INFO -    326                                                 del h_frame

2018-04-29 11:14:38,756 - memory_profile6_log - INFO -    327                                                 del lhistory

2018-04-29 11:14:38,756 - memory_profile6_log - INFO -    328                             

2018-04-29 11:14:38,759 - memory_profile6_log - INFO -    329                                             logger.info("Appending training data...")

2018-04-29 11:14:38,759 - memory_profile6_log - INFO -    330                                             datalist.append(tframe)

2018-04-29 11:14:38,760 - memory_profile6_log - INFO -    331                                     else: 

2018-04-29 11:14:38,762 - memory_profile6_log - INFO -    332     90.6 MiB      0.0 MiB               logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 11:14:38,763 - memory_profile6_log - INFO -    333     90.6 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 11:14:38,765 - memory_profile6_log - INFO -    334     90.6 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 11:14:38,766 - memory_profile6_log - INFO -    335                             

2018-04-29 11:14:38,767 - memory_profile6_log - INFO -    336     90.6 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 11:14:38,769 - memory_profile6_log - INFO - 


2018-04-29 11:14:38,769 - memory_profile6_log - INFO - []
2018-04-29 11:14:38,770 - memory_profile6_log - INFO - 

2018-04-29 11:14:53,122 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 11:14:53,125 - memory_profile6_log - INFO - date_generated: 
2018-04-29 11:14:53,127 - memory_profile6_log - INFO -  
2018-04-29 11:14:53,127 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 28, 11, 14, 53, 124000)]
2018-04-29 11:14:53,128 - memory_profile6_log - INFO - 

2018-04-29 11:14:53,128 - memory_profile6_log - INFO - using current date: 2018-04-29
2018-04-29 11:14:53,130 - memory_profile6_log - INFO - using start date: 2018-04-28 00:00:00
2018-04-29 11:14:53,130 - memory_profile6_log - INFO - using end date: 2018-04-28
2018-04-29 11:14:53,259 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 11:14:53,263 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-28
2018-04-29 11:14:56,737 - memory_profile6_log - INFO - size of df: 24.0 Byte
2018-04-29 11:14:56,740 - memory_profile6_log - INFO - 2018-04-28 data is empty!
2018-04-29 11:14:56,742 - memory_profile6_log - INFO - tframe for date: 2018-04-28 is empty
2018-04-29 11:14:56,743 - memory_profile6_log - INFO - len datalist: 0
2018-04-29 11:14:56,743 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 11:14:56,746 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:14:56,746 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:14:56,749 - memory_profile6_log - INFO - ================================================

2018-04-29 11:14:56,750 - memory_profile6_log - INFO -    289     87.1 MiB     87.1 MiB   @profile

2018-04-29 11:14:56,752 - memory_profile6_log - INFO -    290                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 11:14:56,753 - memory_profile6_log - INFO -    291     87.1 MiB      0.0 MiB       bq_client = client

2018-04-29 11:14:56,753 - memory_profile6_log - INFO -    292     87.1 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:14:56,755 - memory_profile6_log - INFO -    293                             

2018-04-29 11:14:56,756 - memory_profile6_log - INFO -    294     87.1 MiB      0.0 MiB       datalist = []

2018-04-29 11:14:56,756 - memory_profile6_log - INFO -    295     87.1 MiB      0.0 MiB       datalist_hist = []

2018-04-29 11:14:56,756 - memory_profile6_log - INFO -    296                             

2018-04-29 11:14:56,759 - memory_profile6_log - INFO -    297     87.1 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 11:14:56,760 - memory_profile6_log - INFO -    298     90.8 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 11:14:56,762 - memory_profile6_log - INFO -    299     90.8 MiB      3.7 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 11:14:56,763 - memory_profile6_log - INFO -    300     90.8 MiB      0.0 MiB           if tframe is not None:

2018-04-29 11:14:56,763 - memory_profile6_log - INFO -    301                                         if not tframe.empty:

2018-04-29 11:14:56,763 - memory_profile6_log - INFO -    302                                             X_split = np.array_split(tframe, 5)

2018-04-29 11:14:56,765 - memory_profile6_log - INFO -    303                                             logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 11:14:56,766 - memory_profile6_log - INFO -    304                                             logger.info("Appending history data...")

2018-04-29 11:14:56,766 - memory_profile6_log - INFO -    305                                             for ix in range(len(X_split)):

2018-04-29 11:14:56,766 - memory_profile6_log - INFO -    306                                                 # ~ loading history

2018-04-29 11:14:56,767 - memory_profile6_log - INFO -    307                                                 """

2018-04-29 11:14:56,770 - memory_profile6_log - INFO -    308                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 11:14:56,772 - memory_profile6_log - INFO -    309                                                 """

2018-04-29 11:14:56,776 - memory_profile6_log - INFO -    310                                                 logger.info("processing batch-%d", ix)

2018-04-29 11:14:56,776 - memory_profile6_log - INFO -    311                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 11:14:56,778 - memory_profile6_log - INFO -    312                                                 logger.info("creating list history data...")

2018-04-29 11:14:56,779 - memory_profile6_log - INFO -    313                                                 lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 11:14:56,779 - memory_profile6_log - INFO -    314                             

2018-04-29 11:14:56,782 - memory_profile6_log - INFO -    315                                                 logger.info("call history data...")

2018-04-29 11:14:56,783 - memory_profile6_log - INFO -    316                                                 h_frame = mh.loadDSHistory(lhistory)

2018-04-29 11:14:56,785 - memory_profile6_log - INFO -    317                             

2018-04-29 11:14:56,785 - memory_profile6_log - INFO -    318                                                 # me = os.getpid()

2018-04-29 11:14:56,786 - memory_profile6_log - INFO -    319                                                 # kill_proc_tree(me)

2018-04-29 11:14:56,786 - memory_profile6_log - INFO -    320                             

2018-04-29 11:14:56,788 - memory_profile6_log - INFO -    321                                                 logger.info("done collecting history data, appending now...")

2018-04-29 11:14:56,788 - memory_profile6_log - INFO -    322                                                 for m in h_frame:

2018-04-29 11:14:56,789 - memory_profile6_log - INFO -    323                                                     if m is not None:

2018-04-29 11:14:56,789 - memory_profile6_log - INFO -    324                                                         if len(m) > 0:

2018-04-29 11:14:56,793 - memory_profile6_log - INFO -    325                                                             datalist_hist.append(pd.DataFrame(m))

2018-04-29 11:14:56,793 - memory_profile6_log - INFO -    326                                                 del h_frame

2018-04-29 11:14:56,795 - memory_profile6_log - INFO -    327                                                 del lhistory

2018-04-29 11:14:56,796 - memory_profile6_log - INFO -    328                             

2018-04-29 11:14:56,796 - memory_profile6_log - INFO -    329                                             logger.info("Appending training data...")

2018-04-29 11:14:56,798 - memory_profile6_log - INFO -    330                                             datalist.append(tframe)

2018-04-29 11:14:56,799 - memory_profile6_log - INFO -    331                                     else: 

2018-04-29 11:14:56,799 - memory_profile6_log - INFO -    332     90.8 MiB      0.0 MiB               logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 11:14:56,801 - memory_profile6_log - INFO -    333     90.8 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 11:14:56,802 - memory_profile6_log - INFO -    334     90.8 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 11:14:56,803 - memory_profile6_log - INFO -    335                             

2018-04-29 11:14:56,805 - memory_profile6_log - INFO -    336     90.8 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 11:14:56,805 - memory_profile6_log - INFO - 


2018-04-29 11:14:56,806 - memory_profile6_log - INFO - 0
2018-04-29 11:14:56,808 - memory_profile6_log - INFO - 

2018-04-29 11:15:23,556 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 11:15:23,559 - memory_profile6_log - INFO - date_generated: 
2018-04-29 11:15:23,561 - memory_profile6_log - INFO -  
2018-04-29 11:15:23,562 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 28, 11, 15, 23, 558000)]
2018-04-29 11:15:23,562 - memory_profile6_log - INFO - 

2018-04-29 11:15:23,562 - memory_profile6_log - INFO - using current date: 2018-04-29
2018-04-29 11:15:23,563 - memory_profile6_log - INFO - using start date: 2018-04-28 00:00:00
2018-04-29 11:15:23,565 - memory_profile6_log - INFO - using end date: 2018-04-28
2018-04-29 11:15:23,697 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 11:15:23,701 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-28
2018-04-29 11:15:27,134 - memory_profile6_log - INFO - size of df: 24.0 Byte
2018-04-29 11:15:27,135 - memory_profile6_log - INFO - 2018-04-28 data is empty!
2018-04-29 11:15:27,137 - memory_profile6_log - INFO - tframe for date: 2018-04-28 is empty
2018-04-29 11:15:27,138 - memory_profile6_log - INFO - len datalist: 0
2018-04-29 11:15:27,140 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 11:15:27,141 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:15:27,141 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:15:27,144 - memory_profile6_log - INFO - ================================================

2018-04-29 11:15:27,145 - memory_profile6_log - INFO -    289     87.0 MiB     87.0 MiB   @profile

2018-04-29 11:15:27,147 - memory_profile6_log - INFO -    290                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 11:15:27,148 - memory_profile6_log - INFO -    291     87.0 MiB      0.0 MiB       bq_client = client

2018-04-29 11:15:27,148 - memory_profile6_log - INFO -    292     87.0 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:15:27,150 - memory_profile6_log - INFO -    293                             

2018-04-29 11:15:27,151 - memory_profile6_log - INFO -    294     87.0 MiB      0.0 MiB       datalist = []

2018-04-29 11:15:27,151 - memory_profile6_log - INFO -    295     87.0 MiB      0.0 MiB       datalist_hist = []

2018-04-29 11:15:27,153 - memory_profile6_log - INFO -    296                             

2018-04-29 11:15:27,154 - memory_profile6_log - INFO -    297     87.0 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 11:15:27,155 - memory_profile6_log - INFO -    298     90.6 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 11:15:27,157 - memory_profile6_log - INFO -    299     90.6 MiB      3.6 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 11:15:27,157 - memory_profile6_log - INFO -    300     90.6 MiB      0.0 MiB           if tframe is not None:

2018-04-29 11:15:27,160 - memory_profile6_log - INFO -    301                                         if not tframe.empty:

2018-04-29 11:15:27,161 - memory_profile6_log - INFO -    302                                             X_split = np.array_split(tframe, 5)

2018-04-29 11:15:27,161 - memory_profile6_log - INFO -    303                                             logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 11:15:27,163 - memory_profile6_log - INFO -    304                                             logger.info("Appending history data...")

2018-04-29 11:15:27,164 - memory_profile6_log - INFO -    305                                             for ix in range(len(X_split)):

2018-04-29 11:15:27,167 - memory_profile6_log - INFO -    306                                                 # ~ loading history

2018-04-29 11:15:27,167 - memory_profile6_log - INFO -    307                                                 """

2018-04-29 11:15:27,168 - memory_profile6_log - INFO -    308                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 11:15:27,170 - memory_profile6_log - INFO -    309                                                 """

2018-04-29 11:15:27,171 - memory_profile6_log - INFO -    310                                                 logger.info("processing batch-%d", ix)

2018-04-29 11:15:27,171 - memory_profile6_log - INFO -    311                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 11:15:27,173 - memory_profile6_log - INFO -    312                                                 logger.info("creating list history data...")

2018-04-29 11:15:27,174 - memory_profile6_log - INFO -    313                                                 lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 11:15:27,177 - memory_profile6_log - INFO -    314                             

2018-04-29 11:15:27,177 - memory_profile6_log - INFO -    315                                                 logger.info("call history data...")

2018-04-29 11:15:27,178 - memory_profile6_log - INFO -    316                                                 h_frame = mh.loadDSHistory(lhistory)

2018-04-29 11:15:27,180 - memory_profile6_log - INFO -    317                             

2018-04-29 11:15:27,181 - memory_profile6_log - INFO -    318                                                 # me = os.getpid()

2018-04-29 11:15:27,183 - memory_profile6_log - INFO -    319                                                 # kill_proc_tree(me)

2018-04-29 11:15:27,183 - memory_profile6_log - INFO -    320                             

2018-04-29 11:15:27,184 - memory_profile6_log - INFO -    321                                                 logger.info("done collecting history data, appending now...")

2018-04-29 11:15:27,187 - memory_profile6_log - INFO -    322                                                 for m in h_frame:

2018-04-29 11:15:27,187 - memory_profile6_log - INFO -    323                                                     if m is not None:

2018-04-29 11:15:27,190 - memory_profile6_log - INFO -    324                                                         if len(m) > 0:

2018-04-29 11:15:27,190 - memory_profile6_log - INFO -    325                                                             datalist_hist.append(pd.DataFrame(m))

2018-04-29 11:15:27,190 - memory_profile6_log - INFO -    326                                                 del h_frame

2018-04-29 11:15:27,191 - memory_profile6_log - INFO -    327                                                 del lhistory

2018-04-29 11:15:27,193 - memory_profile6_log - INFO -    328                             

2018-04-29 11:15:27,194 - memory_profile6_log - INFO -    329                                             logger.info("Appending training data...")

2018-04-29 11:15:27,194 - memory_profile6_log - INFO -    330                                             datalist.append(tframe)

2018-04-29 11:15:27,197 - memory_profile6_log - INFO -    331                                     else: 

2018-04-29 11:15:27,197 - memory_profile6_log - INFO -    332     90.6 MiB      0.0 MiB               logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 11:15:27,200 - memory_profile6_log - INFO -    333     90.6 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 11:15:27,200 - memory_profile6_log - INFO -    334     90.6 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 11:15:27,201 - memory_profile6_log - INFO -    335                             

2018-04-29 11:15:27,203 - memory_profile6_log - INFO -    336     90.6 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 11:15:27,203 - memory_profile6_log - INFO - 


2018-04-29 11:15:27,204 - memory_profile6_log - INFO - Training cannot be empty..
2018-04-29 11:15:27,206 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:15:27,207 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:15:27,207 - memory_profile6_log - INFO - ================================================

2018-04-29 11:15:27,210 - memory_profile6_log - INFO -    338     86.8 MiB     86.8 MiB   @profile

2018-04-29 11:15:27,210 - memory_profile6_log - INFO -    339                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 11:15:27,211 - memory_profile6_log - INFO -    340     87.0 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 11:15:27,213 - memory_profile6_log - INFO -    341     87.0 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:15:27,213 - memory_profile6_log - INFO -    342                             

2018-04-29 11:15:27,213 - memory_profile6_log - INFO -    343                                 # ~~~ Begin collecting data ~~~

2018-04-29 11:15:27,213 - memory_profile6_log - INFO -    344     87.0 MiB      0.0 MiB       t0 = time.time()

2018-04-29 11:15:27,214 - memory_profile6_log - INFO -    345     90.6 MiB      3.7 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 11:15:27,214 - memory_profile6_log - INFO -    346     90.6 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 11:15:27,216 - memory_profile6_log - INFO -    347     90.6 MiB      0.0 MiB           logger.info("Training cannot be empty..")

2018-04-29 11:15:27,216 - memory_profile6_log - INFO -    348     90.6 MiB      0.0 MiB           return False

2018-04-29 11:15:27,220 - memory_profile6_log - INFO -    349                                 big_frame_hist = pd.concat(datalist_hist)

2018-04-29 11:15:27,220 - memory_profile6_log - INFO -    350                                 print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 11:15:27,221 - memory_profile6_log - INFO -    351                                 logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 11:15:27,223 - memory_profile6_log - INFO -    352                             

2018-04-29 11:15:27,223 - memory_profile6_log - INFO -    353                                 big_frame = pd.concat(datalist)

2018-04-29 11:15:27,223 - memory_profile6_log - INFO -    354                                 logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 11:15:27,223 - memory_profile6_log - INFO -    355                                 del datalist

2018-04-29 11:15:27,224 - memory_profile6_log - INFO -    356                             

2018-04-29 11:15:27,224 - memory_profile6_log - INFO -    357                                 big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 11:15:27,224 - memory_profile6_log - INFO -    358                             

2018-04-29 11:15:27,226 - memory_profile6_log - INFO -    359                                 # ~ get current news interest ~

2018-04-29 11:15:27,226 - memory_profile6_log - INFO -    360                                 if not cd:

2018-04-29 11:15:27,226 - memory_profile6_log - INFO -    361                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 11:15:27,226 - memory_profile6_log - INFO -    362                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 11:15:27,226 - memory_profile6_log - INFO -    363                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 11:15:27,232 - memory_profile6_log - INFO -    364                                 else:

2018-04-29 11:15:27,232 - memory_profile6_log - INFO -    365                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 11:15:27,233 - memory_profile6_log - INFO -    366                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 11:15:27,233 - memory_profile6_log - INFO -    367                             

2018-04-29 11:15:27,234 - memory_profile6_log - INFO -    368                                     # safe handling of query parameter

2018-04-29 11:15:27,234 - memory_profile6_log - INFO -    369                                     query_params = [

2018-04-29 11:15:27,236 - memory_profile6_log - INFO -    370                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 11:15:27,237 - memory_profile6_log - INFO -    371                                     ]

2018-04-29 11:15:27,237 - memory_profile6_log - INFO -    372                             

2018-04-29 11:15:27,239 - memory_profile6_log - INFO -    373                                     job_config.query_parameters = query_params

2018-04-29 11:15:27,239 - memory_profile6_log - INFO -    374                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 11:15:27,239 - memory_profile6_log - INFO -    375                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 11:15:27,240 - memory_profile6_log - INFO -    376                             

2018-04-29 11:15:27,240 - memory_profile6_log - INFO -    377                                 current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 11:15:27,240 - memory_profile6_log - INFO -    378                                 current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 11:15:27,243 - memory_profile6_log - INFO -    379                                                                        format='%Y-%m-%d', errors='coerce')

2018-04-29 11:15:27,244 - memory_profile6_log - INFO -    380                                 logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 11:15:27,246 - memory_profile6_log - INFO -    381                                 train_time = time.time() - t0

2018-04-29 11:15:27,246 - memory_profile6_log - INFO -    382                                 logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 11:15:27,247 - memory_profile6_log - INFO -    383                             

2018-04-29 11:15:27,247 - memory_profile6_log - INFO -    384                                 return big_frame, current_frame, big_frame_hist

2018-04-29 11:15:27,249 - memory_profile6_log - INFO - 


2018-04-29 11:17:06,348 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 11:17:06,351 - memory_profile6_log - INFO - date_generated: 
2018-04-29 11:17:06,351 - memory_profile6_log - INFO -  
2018-04-29 11:17:06,352 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 28, 11, 17, 6, 350000)]
2018-04-29 11:17:06,352 - memory_profile6_log - INFO - 

2018-04-29 11:17:06,354 - memory_profile6_log - INFO - using current date: 2018-04-29
2018-04-29 11:17:06,354 - memory_profile6_log - INFO - using start date: 2018-04-28 00:00:00
2018-04-29 11:17:06,355 - memory_profile6_log - INFO - using end date: 2018-04-28
2018-04-29 11:17:06,516 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 11:17:06,519 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-28
2018-04-29 11:17:08,996 - memory_profile6_log - INFO - size of df: 24.0 Byte
2018-04-29 11:17:08,999 - memory_profile6_log - INFO - 2018-04-28 data is empty!
2018-04-29 11:17:09,000 - memory_profile6_log - INFO - tframe for date: 2018-04-28 is empty
2018-04-29 11:17:09,000 - memory_profile6_log - INFO - len datalist: 0
2018-04-29 11:17:09,002 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 11:17:09,003 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:17:09,003 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:17:09,005 - memory_profile6_log - INFO - ================================================

2018-04-29 11:17:09,006 - memory_profile6_log - INFO -    289     87.0 MiB     87.0 MiB   @profile

2018-04-29 11:17:09,007 - memory_profile6_log - INFO -    290                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 11:17:09,009 - memory_profile6_log - INFO -    291     87.0 MiB      0.0 MiB       bq_client = client

2018-04-29 11:17:09,009 - memory_profile6_log - INFO -    292     87.0 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:17:09,009 - memory_profile6_log - INFO -    293                             

2018-04-29 11:17:09,010 - memory_profile6_log - INFO -    294     87.0 MiB      0.0 MiB       datalist = []

2018-04-29 11:17:09,010 - memory_profile6_log - INFO -    295     87.0 MiB      0.0 MiB       datalist_hist = []

2018-04-29 11:17:09,012 - memory_profile6_log - INFO -    296                             

2018-04-29 11:17:09,013 - memory_profile6_log - INFO -    297     87.0 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 11:17:09,013 - memory_profile6_log - INFO -    298     90.7 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 11:17:09,015 - memory_profile6_log - INFO -    299     90.7 MiB      3.7 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 11:17:09,016 - memory_profile6_log - INFO -    300     90.7 MiB      0.0 MiB           if tframe is not None:

2018-04-29 11:17:09,017 - memory_profile6_log - INFO -    301                                         if not tframe.empty:

2018-04-29 11:17:09,017 - memory_profile6_log - INFO -    302                                             X_split = np.array_split(tframe, 5)

2018-04-29 11:17:09,019 - memory_profile6_log - INFO -    303                                             logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 11:17:09,019 - memory_profile6_log - INFO -    304                                             logger.info("Appending history data...")

2018-04-29 11:17:09,020 - memory_profile6_log - INFO -    305                                             for ix in range(len(X_split)):

2018-04-29 11:17:09,023 - memory_profile6_log - INFO -    306                                                 # ~ loading history

2018-04-29 11:17:09,025 - memory_profile6_log - INFO -    307                                                 """

2018-04-29 11:17:09,025 - memory_profile6_log - INFO -    308                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 11:17:09,028 - memory_profile6_log - INFO -    309                                                 """

2018-04-29 11:17:09,029 - memory_profile6_log - INFO -    310                                                 logger.info("processing batch-%d", ix)

2018-04-29 11:17:09,029 - memory_profile6_log - INFO -    311                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 11:17:09,032 - memory_profile6_log - INFO -    312                                                 logger.info("creating list history data...")

2018-04-29 11:17:09,032 - memory_profile6_log - INFO -    313                                                 lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 11:17:09,033 - memory_profile6_log - INFO -    314                             

2018-04-29 11:17:09,033 - memory_profile6_log - INFO -    315                                                 logger.info("call history data...")

2018-04-29 11:17:09,035 - memory_profile6_log - INFO -    316                                                 h_frame = mh.loadDSHistory(lhistory)

2018-04-29 11:17:09,036 - memory_profile6_log - INFO -    317                             

2018-04-29 11:17:09,039 - memory_profile6_log - INFO -    318                                                 # me = os.getpid()

2018-04-29 11:17:09,039 - memory_profile6_log - INFO -    319                                                 # kill_proc_tree(me)

2018-04-29 11:17:09,042 - memory_profile6_log - INFO -    320                             

2018-04-29 11:17:09,042 - memory_profile6_log - INFO -    321                                                 logger.info("done collecting history data, appending now...")

2018-04-29 11:17:09,042 - memory_profile6_log - INFO -    322                                                 for m in h_frame:

2018-04-29 11:17:09,043 - memory_profile6_log - INFO -    323                                                     if m is not None:

2018-04-29 11:17:09,045 - memory_profile6_log - INFO -    324                                                         if len(m) > 0:

2018-04-29 11:17:09,046 - memory_profile6_log - INFO -    325                                                             datalist_hist.append(pd.DataFrame(m))

2018-04-29 11:17:09,046 - memory_profile6_log - INFO -    326                                                 del h_frame

2018-04-29 11:17:09,049 - memory_profile6_log - INFO -    327                                                 del lhistory

2018-04-29 11:17:09,049 - memory_profile6_log - INFO -    328                             

2018-04-29 11:17:09,052 - memory_profile6_log - INFO -    329                                             logger.info("Appending training data...")

2018-04-29 11:17:09,052 - memory_profile6_log - INFO -    330                                             datalist.append(tframe)

2018-04-29 11:17:09,053 - memory_profile6_log - INFO -    331                                     else: 

2018-04-29 11:17:09,055 - memory_profile6_log - INFO -    332     90.7 MiB      0.0 MiB               logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 11:17:09,055 - memory_profile6_log - INFO -    333     90.7 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 11:17:09,056 - memory_profile6_log - INFO -    334     90.7 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 11:17:09,058 - memory_profile6_log - INFO -    335                             

2018-04-29 11:17:09,059 - memory_profile6_log - INFO -    336     90.7 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 11:17:09,061 - memory_profile6_log - INFO - 


2018-04-29 11:17:09,062 - memory_profile6_log - INFO - Training cannot be empty..
2018-04-29 11:17:09,062 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:17:09,062 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:17:09,062 - memory_profile6_log - INFO - ================================================

2018-04-29 11:17:09,063 - memory_profile6_log - INFO -    338     86.8 MiB     86.8 MiB   @profile

2018-04-29 11:17:09,063 - memory_profile6_log - INFO -    339                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 11:17:09,065 - memory_profile6_log - INFO -    340     87.0 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 11:17:09,065 - memory_profile6_log - INFO -    341     87.0 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:17:09,065 - memory_profile6_log - INFO -    342                             

2018-04-29 11:17:09,065 - memory_profile6_log - INFO -    343                                 # ~~~ Begin collecting data ~~~

2018-04-29 11:17:09,066 - memory_profile6_log - INFO -    344     87.0 MiB      0.0 MiB       t0 = time.time()

2018-04-29 11:17:09,066 - memory_profile6_log - INFO -    345     90.7 MiB      3.7 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 11:17:09,066 - memory_profile6_log - INFO -    346     90.7 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 11:17:09,071 - memory_profile6_log - INFO -    347     90.7 MiB      0.0 MiB           logger.info("Training cannot be empty..")

2018-04-29 11:17:09,072 - memory_profile6_log - INFO -    348     90.7 MiB      0.0 MiB           return False

2018-04-29 11:17:09,073 - memory_profile6_log - INFO -    349                                 big_frame_hist = pd.concat(datalist_hist)

2018-04-29 11:17:09,075 - memory_profile6_log - INFO -    350                                 print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 11:17:09,075 - memory_profile6_log - INFO -    351                                 logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 11:17:09,075 - memory_profile6_log - INFO -    352                             

2018-04-29 11:17:09,075 - memory_profile6_log - INFO -    353                                 big_frame = pd.concat(datalist)

2018-04-29 11:17:09,076 - memory_profile6_log - INFO -    354                                 logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 11:17:09,076 - memory_profile6_log - INFO -    355                                 del datalist

2018-04-29 11:17:09,078 - memory_profile6_log - INFO -    356                             

2018-04-29 11:17:09,084 - memory_profile6_log - INFO -    357                                 big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 11:17:09,085 - memory_profile6_log - INFO -    358                             

2018-04-29 11:17:09,085 - memory_profile6_log - INFO -    359                                 # ~ get current news interest ~

2018-04-29 11:17:09,085 - memory_profile6_log - INFO -    360                                 if not cd:

2018-04-29 11:17:09,086 - memory_profile6_log - INFO -    361                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 11:17:09,088 - memory_profile6_log - INFO -    362                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 11:17:09,088 - memory_profile6_log - INFO -    363                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 11:17:09,089 - memory_profile6_log - INFO -    364                                 else:

2018-04-29 11:17:09,092 - memory_profile6_log - INFO -    365                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 11:17:09,092 - memory_profile6_log - INFO -    366                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 11:17:09,095 - memory_profile6_log - INFO -    367                             

2018-04-29 11:17:09,095 - memory_profile6_log - INFO -    368                                     # safe handling of query parameter

2018-04-29 11:17:09,095 - memory_profile6_log - INFO -    369                                     query_params = [

2018-04-29 11:17:09,096 - memory_profile6_log - INFO -    370                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 11:17:09,096 - memory_profile6_log - INFO -    371                                     ]

2018-04-29 11:17:09,101 - memory_profile6_log - INFO -    372                             

2018-04-29 11:17:09,104 - memory_profile6_log - INFO -    373                                     job_config.query_parameters = query_params

2018-04-29 11:17:09,105 - memory_profile6_log - INFO -    374                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 11:17:09,107 - memory_profile6_log - INFO -    375                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 11:17:09,108 - memory_profile6_log - INFO -    376                             

2018-04-29 11:17:09,108 - memory_profile6_log - INFO -    377                                 current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 11:17:09,109 - memory_profile6_log - INFO -    378                                 current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 11:17:09,109 - memory_profile6_log - INFO -    379                                                                        format='%Y-%m-%d', errors='coerce')

2018-04-29 11:17:09,111 - memory_profile6_log - INFO -    380                                 logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 11:17:09,111 - memory_profile6_log - INFO -    381                                 train_time = time.time() - t0

2018-04-29 11:17:09,115 - memory_profile6_log - INFO -    382                                 logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 11:17:09,115 - memory_profile6_log - INFO -    383                             

2018-04-29 11:17:09,117 - memory_profile6_log - INFO -    384                                 return big_frame, current_frame, big_frame_hist

2018-04-29 11:17:09,118 - memory_profile6_log - INFO - 


2018-04-29 11:17:09,118 - memory_profile6_log - INFO - Train return False, please cek your data availability
2018-04-29 11:17:48,953 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 11:17:48,956 - memory_profile6_log - INFO - date_generated: 
2018-04-29 11:17:48,956 - memory_profile6_log - INFO -  
2018-04-29 11:17:48,957 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 28, 11, 17, 48, 954000)]
2018-04-29 11:17:48,957 - memory_profile6_log - INFO - 

2018-04-29 11:17:48,957 - memory_profile6_log - INFO - using current date: 2018-04-29
2018-04-29 11:17:48,957 - memory_profile6_log - INFO - using start date: 2018-04-28 00:00:00
2018-04-29 11:17:48,957 - memory_profile6_log - INFO - using end date: 2018-04-28
2018-04-29 11:17:49,085 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 11:17:49,088 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-28
2018-04-29 11:17:52,569 - memory_profile6_log - INFO - size of df: 24.0 Byte
2018-04-29 11:17:52,569 - memory_profile6_log - INFO - 2018-04-28 data is empty!
2018-04-29 11:17:52,572 - memory_profile6_log - INFO - tframe for date: 2018-04-28 is empty
2018-04-29 11:17:52,572 - memory_profile6_log - INFO - len datalist: 0
2018-04-29 11:17:52,573 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 11:17:52,575 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:17:52,575 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:17:52,576 - memory_profile6_log - INFO - ================================================

2018-04-29 11:17:52,578 - memory_profile6_log - INFO -    289     87.2 MiB     87.2 MiB   @profile

2018-04-29 11:17:52,578 - memory_profile6_log - INFO -    290                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 11:17:52,579 - memory_profile6_log - INFO -    291     87.2 MiB      0.0 MiB       bq_client = client

2018-04-29 11:17:52,581 - memory_profile6_log - INFO -    292     87.2 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:17:52,582 - memory_profile6_log - INFO -    293                             

2018-04-29 11:17:52,582 - memory_profile6_log - INFO -    294     87.2 MiB      0.0 MiB       datalist = []

2018-04-29 11:17:52,582 - memory_profile6_log - INFO -    295     87.2 MiB      0.0 MiB       datalist_hist = []

2018-04-29 11:17:52,584 - memory_profile6_log - INFO -    296                             

2018-04-29 11:17:52,585 - memory_profile6_log - INFO -    297     87.2 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 11:17:52,585 - memory_profile6_log - INFO -    298     90.9 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 11:17:52,585 - memory_profile6_log - INFO -    299     90.9 MiB      3.7 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 11:17:52,586 - memory_profile6_log - INFO -    300     90.9 MiB      0.0 MiB           if tframe is not None:

2018-04-29 11:17:52,588 - memory_profile6_log - INFO -    301                                         if not tframe.empty:

2018-04-29 11:17:52,589 - memory_profile6_log - INFO -    302                                             X_split = np.array_split(tframe, 5)

2018-04-29 11:17:52,591 - memory_profile6_log - INFO -    303                                             logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 11:17:52,592 - memory_profile6_log - INFO -    304                                             logger.info("Appending history data...")

2018-04-29 11:17:52,592 - memory_profile6_log - INFO -    305                                             for ix in range(len(X_split)):

2018-04-29 11:17:52,592 - memory_profile6_log - INFO -    306                                                 # ~ loading history

2018-04-29 11:17:52,594 - memory_profile6_log - INFO -    307                                                 """

2018-04-29 11:17:52,595 - memory_profile6_log - INFO -    308                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 11:17:52,595 - memory_profile6_log - INFO -    309                                                 """

2018-04-29 11:17:52,596 - memory_profile6_log - INFO -    310                                                 logger.info("processing batch-%d", ix)

2018-04-29 11:17:52,596 - memory_profile6_log - INFO -    311                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 11:17:52,601 - memory_profile6_log - INFO -    312                                                 logger.info("creating list history data...")

2018-04-29 11:17:52,601 - memory_profile6_log - INFO -    313                                                 lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 11:17:52,602 - memory_profile6_log - INFO -    314                             

2018-04-29 11:17:52,604 - memory_profile6_log - INFO -    315                                                 logger.info("call history data...")

2018-04-29 11:17:52,605 - memory_profile6_log - INFO -    316                                                 h_frame = mh.loadDSHistory(lhistory)

2018-04-29 11:17:52,605 - memory_profile6_log - INFO -    317                             

2018-04-29 11:17:52,607 - memory_profile6_log - INFO -    318                                                 # me = os.getpid()

2018-04-29 11:17:52,607 - memory_profile6_log - INFO -    319                                                 # kill_proc_tree(me)

2018-04-29 11:17:52,608 - memory_profile6_log - INFO -    320                             

2018-04-29 11:17:52,611 - memory_profile6_log - INFO -    321                                                 logger.info("done collecting history data, appending now...")

2018-04-29 11:17:52,611 - memory_profile6_log - INFO -    322                                                 for m in h_frame:

2018-04-29 11:17:52,612 - memory_profile6_log - INFO -    323                                                     if m is not None:

2018-04-29 11:17:52,614 - memory_profile6_log - INFO -    324                                                         if len(m) > 0:

2018-04-29 11:17:52,615 - memory_profile6_log - INFO -    325                                                             datalist_hist.append(pd.DataFrame(m))

2018-04-29 11:17:52,617 - memory_profile6_log - INFO -    326                                                 del h_frame

2018-04-29 11:17:52,617 - memory_profile6_log - INFO -    327                                                 del lhistory

2018-04-29 11:17:52,618 - memory_profile6_log - INFO -    328                             

2018-04-29 11:17:52,618 - memory_profile6_log - INFO -    329                                             logger.info("Appending training data...")

2018-04-29 11:17:52,621 - memory_profile6_log - INFO -    330                                             datalist.append(tframe)

2018-04-29 11:17:52,622 - memory_profile6_log - INFO -    331                                     else: 

2018-04-29 11:17:52,625 - memory_profile6_log - INFO -    332     90.9 MiB      0.0 MiB               logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 11:17:52,628 - memory_profile6_log - INFO -    333     90.9 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 11:17:52,628 - memory_profile6_log - INFO -    334     90.9 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 11:17:52,628 - memory_profile6_log - INFO -    335                             

2018-04-29 11:17:52,631 - memory_profile6_log - INFO -    336     90.9 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 11:17:52,632 - memory_profile6_log - INFO - 


2018-04-29 11:17:52,634 - memory_profile6_log - INFO - Training cannot be empty..
2018-04-29 11:17:52,635 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:17:52,635 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:17:52,637 - memory_profile6_log - INFO - ================================================

2018-04-29 11:17:52,637 - memory_profile6_log - INFO -    338     87.1 MiB     87.1 MiB   @profile

2018-04-29 11:17:52,637 - memory_profile6_log - INFO -    339                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 11:17:52,638 - memory_profile6_log - INFO -    340     87.2 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 11:17:52,638 - memory_profile6_log - INFO -    341     87.2 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:17:52,638 - memory_profile6_log - INFO -    342                             

2018-04-29 11:17:52,638 - memory_profile6_log - INFO -    343                                 # ~~~ Begin collecting data ~~~

2018-04-29 11:17:52,642 - memory_profile6_log - INFO -    344     87.2 MiB      0.0 MiB       t0 = time.time()

2018-04-29 11:17:52,642 - memory_profile6_log - INFO -    345     90.9 MiB      3.7 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 11:17:52,644 - memory_profile6_log - INFO -    346     90.9 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 11:17:52,644 - memory_profile6_log - INFO -    347     90.9 MiB      0.0 MiB           logger.info("Training cannot be empty..")

2018-04-29 11:17:52,645 - memory_profile6_log - INFO -    348     90.9 MiB      0.0 MiB           return False

2018-04-29 11:17:52,645 - memory_profile6_log - INFO -    349                                 big_frame_hist = pd.concat(datalist_hist)

2018-04-29 11:17:52,647 - memory_profile6_log - INFO -    350                                 print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 11:17:52,648 - memory_profile6_log - INFO -    351                                 logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 11:17:52,648 - memory_profile6_log - INFO -    352                             

2018-04-29 11:17:52,650 - memory_profile6_log - INFO -    353                                 big_frame = pd.concat(datalist)

2018-04-29 11:17:52,651 - memory_profile6_log - INFO -    354                                 logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 11:17:52,653 - memory_profile6_log - INFO -    355                                 del datalist

2018-04-29 11:17:52,654 - memory_profile6_log - INFO -    356                             

2018-04-29 11:17:52,655 - memory_profile6_log - INFO -    357                                 big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 11:17:52,655 - memory_profile6_log - INFO -    358                             

2018-04-29 11:17:52,657 - memory_profile6_log - INFO -    359                                 # ~ get current news interest ~

2018-04-29 11:17:52,658 - memory_profile6_log - INFO -    360                                 if not cd:

2018-04-29 11:17:52,660 - memory_profile6_log - INFO -    361                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 11:17:52,660 - memory_profile6_log - INFO -    362                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 11:17:52,661 - memory_profile6_log - INFO -    363                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 11:17:52,661 - memory_profile6_log - INFO -    364                                 else:

2018-04-29 11:17:52,661 - memory_profile6_log - INFO -    365                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 11:17:52,664 - memory_profile6_log - INFO -    366                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 11:17:52,667 - memory_profile6_log - INFO -    367                             

2018-04-29 11:17:52,668 - memory_profile6_log - INFO -    368                                     # safe handling of query parameter

2018-04-29 11:17:52,668 - memory_profile6_log - INFO -    369                                     query_params = [

2018-04-29 11:17:52,670 - memory_profile6_log - INFO -    370                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 11:17:52,670 - memory_profile6_log - INFO -    371                                     ]

2018-04-29 11:17:52,671 - memory_profile6_log - INFO -    372                             

2018-04-29 11:17:52,671 - memory_profile6_log - INFO -    373                                     job_config.query_parameters = query_params

2018-04-29 11:17:52,671 - memory_profile6_log - INFO -    374                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 11:17:52,671 - memory_profile6_log - INFO -    375                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 11:17:52,674 - memory_profile6_log - INFO -    376                             

2018-04-29 11:17:52,676 - memory_profile6_log - INFO -    377                                 current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 11:17:52,677 - memory_profile6_log - INFO -    378                                 current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 11:17:52,677 - memory_profile6_log - INFO -    379                                                                        format='%Y-%m-%d', errors='coerce')

2018-04-29 11:17:52,678 - memory_profile6_log - INFO -    380                                 logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 11:17:52,680 - memory_profile6_log - INFO -    381                                 train_time = time.time() - t0

2018-04-29 11:17:52,680 - memory_profile6_log - INFO -    382                                 logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 11:17:52,681 - memory_profile6_log - INFO -    383                             

2018-04-29 11:17:52,681 - memory_profile6_log - INFO -    384                                 return big_frame, current_frame, big_frame_hist

2018-04-29 11:17:52,681 - memory_profile6_log - INFO - 


2018-04-29 11:17:52,683 - memory_profile6_log - INFO - Train return False, please cek your data availability
2018-04-29 11:18:11,003 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 11:18:11,006 - memory_profile6_log - INFO - date_generated: 
2018-04-29 11:18:11,006 - memory_profile6_log - INFO -  
2018-04-29 11:18:11,007 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 11:18:11,009 - memory_profile6_log - INFO - 

2018-04-29 11:18:11,009 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 11:18:11,009 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 11:18:11,009 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 11:18:11,138 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 11:18:11,142 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 11:19:24,322 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 11:19:24,323 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 11:19:24,368 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 11:19:24,369 - memory_profile6_log - INFO - Appending history data...
2018-04-29 11:19:24,371 - memory_profile6_log - INFO - processing batch-0
2018-04-29 11:19:24,372 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:19:24,431 - memory_profile6_log - INFO - call history data...
2018-04-29 11:20:17,217 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:20:17,864 - memory_profile6_log - INFO - processing batch-1
2018-04-29 11:20:17,865 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:20:17,872 - memory_profile6_log - INFO - call history data...
2018-04-29 11:21:02,270 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:21:03,061 - memory_profile6_log - INFO - processing batch-2
2018-04-29 11:21:03,062 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:21:03,069 - memory_profile6_log - INFO - call history data...
2018-04-29 11:21:43,440 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:21:44,213 - memory_profile6_log - INFO - processing batch-3
2018-04-29 11:21:44,214 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:21:44,226 - memory_profile6_log - INFO - call history data...
2018-04-29 11:22:24,788 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:22:25,665 - memory_profile6_log - INFO - processing batch-4
2018-04-29 11:22:25,667 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:22:25,674 - memory_profile6_log - INFO - call history data...
2018-04-29 11:23:05,878 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:23:06,538 - memory_profile6_log - INFO - Appending training data...
2018-04-29 11:23:06,539 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 11:23:06,540 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 11:23:06,542 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:23:06,543 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:23:06,545 - memory_profile6_log - INFO - ================================================

2018-04-29 11:23:06,546 - memory_profile6_log - INFO -    289     86.9 MiB     86.9 MiB   @profile

2018-04-29 11:23:06,546 - memory_profile6_log - INFO -    290                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 11:23:06,548 - memory_profile6_log - INFO -    291     86.9 MiB      0.0 MiB       bq_client = client

2018-04-29 11:23:06,549 - memory_profile6_log - INFO -    292     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:23:06,551 - memory_profile6_log - INFO -    293                             

2018-04-29 11:23:06,555 - memory_profile6_log - INFO -    294     86.9 MiB      0.0 MiB       datalist = []

2018-04-29 11:23:06,555 - memory_profile6_log - INFO -    295     86.9 MiB      0.0 MiB       datalist_hist = []

2018-04-29 11:23:06,556 - memory_profile6_log - INFO -    296                             

2018-04-29 11:23:06,558 - memory_profile6_log - INFO -    297     86.9 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 11:23:06,559 - memory_profile6_log - INFO -    298    350.3 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 11:23:06,561 - memory_profile6_log - INFO -    299    339.0 MiB    252.1 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 11:23:06,562 - memory_profile6_log - INFO -    300    339.0 MiB      0.0 MiB           if tframe is not None:

2018-04-29 11:23:06,562 - memory_profile6_log - INFO -    301    339.0 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 11:23:06,563 - memory_profile6_log - INFO -    302    346.9 MiB      7.9 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 11:23:06,565 - memory_profile6_log - INFO -    303    346.9 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 11:23:06,565 - memory_profile6_log - INFO -    304    346.9 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 11:23:06,565 - memory_profile6_log - INFO -    305    350.3 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 11:23:06,566 - memory_profile6_log - INFO -    306                                                 # ~ loading history

2018-04-29 11:23:06,568 - memory_profile6_log - INFO -    307                                                 """

2018-04-29 11:23:06,571 - memory_profile6_log - INFO -    308                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 11:23:06,572 - memory_profile6_log - INFO -    309                                                 """

2018-04-29 11:23:06,572 - memory_profile6_log - INFO -    310    350.1 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 11:23:06,573 - memory_profile6_log - INFO -    311                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 11:23:06,575 - memory_profile6_log - INFO -    312    350.1 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 11:23:06,575 - memory_profile6_log - INFO -    313    350.1 MiB      0.5 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 11:23:06,576 - memory_profile6_log - INFO -    314                             

2018-04-29 11:23:06,578 - memory_profile6_log - INFO -    315    350.1 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 11:23:06,581 - memory_profile6_log - INFO -    316    350.3 MiB      2.2 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 11:23:06,581 - memory_profile6_log - INFO -    317                             

2018-04-29 11:23:06,582 - memory_profile6_log - INFO -    318                                                 # me = os.getpid()

2018-04-29 11:23:06,582 - memory_profile6_log - INFO -    319                                                 # kill_proc_tree(me)

2018-04-29 11:23:06,585 - memory_profile6_log - INFO -    320                             

2018-04-29 11:23:06,585 - memory_profile6_log - INFO -    321    350.3 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 11:23:06,586 - memory_profile6_log - INFO -    322    350.3 MiB     -0.1 MiB                       for m in h_frame:

2018-04-29 11:23:06,588 - memory_profile6_log - INFO -    323    350.3 MiB     -0.1 MiB                           if m is not None:

2018-04-29 11:23:06,591 - memory_profile6_log - INFO -    324    350.3 MiB     -0.1 MiB                               if len(m) > 0:

2018-04-29 11:23:06,592 - memory_profile6_log - INFO -    325    350.3 MiB      0.6 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 11:23:06,592 - memory_profile6_log - INFO -    326    350.3 MiB     -0.0 MiB                       del h_frame

2018-04-29 11:23:06,592 - memory_profile6_log - INFO -    327    350.3 MiB      0.0 MiB                       del lhistory

2018-04-29 11:23:06,595 - memory_profile6_log - INFO -    328                             

2018-04-29 11:23:06,595 - memory_profile6_log - INFO -    329    350.3 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 11:23:06,596 - memory_profile6_log - INFO -    330    350.3 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 11:23:06,596 - memory_profile6_log - INFO -    331                                     else: 

2018-04-29 11:23:06,598 - memory_profile6_log - INFO -    332                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 11:23:06,601 - memory_profile6_log - INFO -    333    350.3 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 11:23:06,601 - memory_profile6_log - INFO -    334    350.3 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 11:23:06,605 - memory_profile6_log - INFO -    335                             

2018-04-29 11:23:06,605 - memory_profile6_log - INFO -    336    350.3 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 11:23:06,607 - memory_profile6_log - INFO - 


2018-04-29 11:23:07,664 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 11:23:07,749 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0   0.001543         655.760344        45             665.760344  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
1   0.001543        1595.092728        37            1605.092728  10960288  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
2   0.001543         100.714046       293             110.714046  10960288  1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...
3   0.000023      109703.065469       779          109713.065469  10959239  161129f951435-0d660a18dbcd87-173a7640-100200-1...
4   0.001543        1241.341593       356            1251.341593  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
5   0.001543        1455.707850       478            1465.707850  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
6   0.001543         151.693334       366             161.693334  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
0   0.001543         259.277950       205             269.277950  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
1   0.001543         133.525862       221             143.525862  10960288  161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...
2   0.001543          87.305371       169              97.305371  10960288  161e219cd891c8-0e882943705df4-15290f5f-38400-1...
3   0.001543         156.963912        94             166.963912  10960288  16169731ce618-00742c5e307396-5b4d6a36-38400-16...
4   0.001543         136.906453       441             146.906453  10960288  1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...
5   0.001543         125.571130       235             135.571130  10960288  1626fd160d9114-055d801ae6b39d-66265f05-38400-1...
0   0.001543          51.589538       143              61.589538  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
1   0.001543          52.695028       280              62.695028  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
2   0.001543         175.650092        42             185.650092  10960288  16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...
3   0.001543         150.557222        49             160.557222  10960288  1612d88e55ab8-09af9658f30a46-65117b23-38400-16...
4   0.001543          63.281471       193              73.281471  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
5   0.001543         139.194413       106             149.194413  10960288  16149f0a1611-00a034775-63446032-38400-16149f0a...
6   0.001543          88.645278       187              98.645278  10960288  16132f1045f30b-0613b72b33445e-5d640103-38400-1...
2018-04-29 11:23:07,750 - memory_profile6_log - INFO - 

2018-04-29 11:23:07,759 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 11:23:07,828 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 11:23:07,842 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 11:23:58,000 - memory_profile6_log - INFO - size of df: 58.30 MB
2018-04-29 11:23:58,002 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-29 11:23:58,071 - memory_profile6_log - INFO - size of current_frame: 58.30 MB
2018-04-29 11:23:58,072 - memory_profile6_log - INFO - loading time of: 484420 total genuine-current interest data ~ take 346.956s
2018-04-29 11:23:58,078 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:23:58,078 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:23:58,079 - memory_profile6_log - INFO - ================================================

2018-04-29 11:23:58,082 - memory_profile6_log - INFO -    338     86.8 MiB     86.8 MiB   @profile

2018-04-29 11:23:58,082 - memory_profile6_log - INFO -    339                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 11:23:58,085 - memory_profile6_log - INFO -    340     86.9 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 11:23:58,085 - memory_profile6_log - INFO -    341     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:23:58,088 - memory_profile6_log - INFO -    342                             

2018-04-29 11:23:58,088 - memory_profile6_log - INFO -    343                                 # ~~~ Begin collecting data ~~~

2018-04-29 11:23:58,088 - memory_profile6_log - INFO -    344     86.9 MiB      0.0 MiB       t0 = time.time()

2018-04-29 11:23:58,088 - memory_profile6_log - INFO -    345    350.3 MiB    263.4 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 11:23:58,089 - memory_profile6_log - INFO -    346    350.3 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 11:23:58,089 - memory_profile6_log - INFO -    347                                     logger.info("Training cannot be empty..")

2018-04-29 11:23:58,091 - memory_profile6_log - INFO -    348                                     return False

2018-04-29 11:23:58,092 - memory_profile6_log - INFO -    349    350.9 MiB      0.7 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 11:23:58,096 - memory_profile6_log - INFO -    350    351.0 MiB      0.1 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 11:23:58,096 - memory_profile6_log - INFO -    351    351.0 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 11:23:58,098 - memory_profile6_log - INFO -    352                             

2018-04-29 11:23:58,098 - memory_profile6_log - INFO -    353    356.8 MiB      5.8 MiB       big_frame = pd.concat(datalist)

2018-04-29 11:23:58,098 - memory_profile6_log - INFO -    354    356.8 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 11:23:58,099 - memory_profile6_log - INFO -    355    351.0 MiB     -5.8 MiB       del datalist

2018-04-29 11:23:58,099 - memory_profile6_log - INFO -    356                             

2018-04-29 11:23:58,099 - memory_profile6_log - INFO -    357    351.0 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 11:23:58,101 - memory_profile6_log - INFO -    358                             

2018-04-29 11:23:58,101 - memory_profile6_log - INFO -    359                                 # ~ get current news interest ~

2018-04-29 11:23:58,105 - memory_profile6_log - INFO -    360    351.0 MiB      0.0 MiB       if not cd:

2018-04-29 11:23:58,105 - memory_profile6_log - INFO -    361                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 11:23:58,108 - memory_profile6_log - INFO -    362                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 11:23:58,108 - memory_profile6_log - INFO -    363                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 11:23:58,108 - memory_profile6_log - INFO -    364                                 else:

2018-04-29 11:23:58,108 - memory_profile6_log - INFO -    365    351.0 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 11:23:58,109 - memory_profile6_log - INFO -    366    351.0 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 11:23:58,111 - memory_profile6_log - INFO -    367                             

2018-04-29 11:23:58,115 - memory_profile6_log - INFO -    368                                     # safe handling of query parameter

2018-04-29 11:23:58,115 - memory_profile6_log - INFO -    369                                     query_params = [

2018-04-29 11:23:58,117 - memory_profile6_log - INFO -    370    351.0 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 11:23:58,118 - memory_profile6_log - INFO -    371                                     ]

2018-04-29 11:23:58,118 - memory_profile6_log - INFO -    372                             

2018-04-29 11:23:58,121 - memory_profile6_log - INFO -    373    351.0 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-29 11:23:58,121 - memory_profile6_log - INFO -    374    420.6 MiB     69.6 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 11:23:58,121 - memory_profile6_log - INFO -    375    420.6 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 11:23:58,124 - memory_profile6_log - INFO -    376                             

2018-04-29 11:23:58,125 - memory_profile6_log - INFO -    377    420.7 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 11:23:58,125 - memory_profile6_log - INFO -    378    420.7 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 11:23:58,127 - memory_profile6_log - INFO -    379    420.7 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 11:23:58,127 - memory_profile6_log - INFO -    380    420.7 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 11:23:58,130 - memory_profile6_log - INFO -    381    420.7 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 11:23:58,130 - memory_profile6_log - INFO -    382    420.7 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 11:23:58,131 - memory_profile6_log - INFO -    383                             

2018-04-29 11:23:58,131 - memory_profile6_log - INFO -    384    420.7 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 11:23:58,131 - memory_profile6_log - INFO - 


2018-04-29 11:23:58,134 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 11:23:58,165 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 11:23:58,167 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-29 11:23:58,168 - memory_profile6_log - INFO - apply on: 5000 total history data(D(t))
2018-04-29 11:23:59,267 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 11:23:59,269 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 11:24:40,630 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-29 11:24:40,657 - memory_profile6_log - INFO - Total train time: 42.492s
2018-04-29 11:24:40,658 - memory_profile6_log - INFO - memory left before cleaning: 70.600 percent memory...
2018-04-29 11:24:40,660 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-29 11:24:40,661 - memory_profile6_log - INFO - deleting df_dut...
2018-04-29 11:24:40,663 - memory_profile6_log - INFO - deleting df_dt...
2018-04-29 11:24:40,664 - memory_profile6_log - INFO - deleting df_input...
2018-04-29 11:24:40,671 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-29 11:24:40,673 - memory_profile6_log - INFO - deleting df_current...
2018-04-29 11:24:40,674 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-29 11:24:40,684 - memory_profile6_log - INFO - deleting model_fit...
2018-04-29 11:24:40,687 - memory_profile6_log - INFO - deleting result...
2018-04-29 11:24:40,706 - memory_profile6_log - INFO - deleting model_transform...
2018-04-29 11:24:40,707 - memory_profile6_log - INFO - memory left after cleaning: 70.400 percent memory...
2018-04-29 11:24:40,709 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-29 11:24:40,710 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-29 11:25:09,720 - memory_profile6_log - INFO - Saving fitted_models as history...
2018-04-29 11:37:04,145 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 11:37:04,148 - memory_profile6_log - INFO - date_generated: 
2018-04-29 11:37:04,148 - memory_profile6_log - INFO -  
2018-04-29 11:37:04,148 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 11:37:04,148 - memory_profile6_log - INFO - 

2018-04-29 11:37:04,148 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 11:37:04,150 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 11:37:04,150 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 11:37:04,272 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 11:37:04,275 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 11:38:14,252 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 11:38:14,253 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 11:38:14,295 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 11:38:14,296 - memory_profile6_log - INFO - Appending history data...
2018-04-29 11:38:14,298 - memory_profile6_log - INFO - processing batch-0
2018-04-29 11:38:14,299 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:38:14,341 - memory_profile6_log - INFO - call history data...
2018-04-29 11:38:59,778 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:39:00,421 - memory_profile6_log - INFO - processing batch-1
2018-04-29 11:39:00,423 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:39:00,430 - memory_profile6_log - INFO - call history data...
2018-04-29 11:39:46,427 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:39:47,104 - memory_profile6_log - INFO - processing batch-2
2018-04-29 11:39:47,105 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:39:47,115 - memory_profile6_log - INFO - call history data...
2018-04-29 11:40:35,519 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:40:36,200 - memory_profile6_log - INFO - processing batch-3
2018-04-29 11:40:36,200 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:40:36,209 - memory_profile6_log - INFO - call history data...
2018-04-29 11:41:18,819 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:41:19,486 - memory_profile6_log - INFO - processing batch-4
2018-04-29 11:41:19,486 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:41:19,496 - memory_profile6_log - INFO - call history data...
2018-04-29 11:42:01,890 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:42:02,546 - memory_profile6_log - INFO - Appending training data...
2018-04-29 11:42:02,546 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 11:42:02,548 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 11:42:02,551 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:42:02,552 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:42:02,552 - memory_profile6_log - INFO - ================================================

2018-04-29 11:42:02,553 - memory_profile6_log - INFO -    289     86.8 MiB     86.8 MiB   @profile

2018-04-29 11:42:02,555 - memory_profile6_log - INFO -    290                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 11:42:02,556 - memory_profile6_log - INFO -    291     86.8 MiB      0.0 MiB       bq_client = client

2018-04-29 11:42:02,559 - memory_profile6_log - INFO -    292     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:42:02,559 - memory_profile6_log - INFO -    293                             

2018-04-29 11:42:02,561 - memory_profile6_log - INFO -    294     86.8 MiB      0.0 MiB       datalist = []

2018-04-29 11:42:02,562 - memory_profile6_log - INFO -    295     86.8 MiB      0.0 MiB       datalist_hist = []

2018-04-29 11:42:02,562 - memory_profile6_log - INFO -    296                             

2018-04-29 11:42:02,563 - memory_profile6_log - INFO -    297     86.8 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 11:42:02,565 - memory_profile6_log - INFO -    298    350.5 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 11:42:02,565 - memory_profile6_log - INFO -    299    339.4 MiB    252.6 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 11:42:02,569 - memory_profile6_log - INFO -    300    339.4 MiB      0.0 MiB           if tframe is not None:

2018-04-29 11:42:02,571 - memory_profile6_log - INFO -    301    339.4 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 11:42:02,572 - memory_profile6_log - INFO -    302    347.3 MiB      7.9 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 11:42:02,572 - memory_profile6_log - INFO -    303    347.3 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 11:42:02,573 - memory_profile6_log - INFO -    304    347.3 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 11:42:02,575 - memory_profile6_log - INFO -    305    350.5 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 11:42:02,575 - memory_profile6_log - INFO -    306                                                 # ~ loading history

2018-04-29 11:42:02,578 - memory_profile6_log - INFO -    307                                                 """

2018-04-29 11:42:02,578 - memory_profile6_log - INFO -    308                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 11:42:02,579 - memory_profile6_log - INFO -    309                                                 """

2018-04-29 11:42:02,581 - memory_profile6_log - INFO -    310    350.4 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 11:42:02,582 - memory_profile6_log - INFO -    311                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 11:42:02,584 - memory_profile6_log - INFO -    312    350.4 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 11:42:02,585 - memory_profile6_log - INFO -    313    350.4 MiB      0.7 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 11:42:02,588 - memory_profile6_log - INFO -    314                             

2018-04-29 11:42:02,588 - memory_profile6_log - INFO -    315    350.4 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 11:42:02,589 - memory_profile6_log - INFO -    316    350.5 MiB      1.9 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 11:42:02,591 - memory_profile6_log - INFO -    317                             

2018-04-29 11:42:02,592 - memory_profile6_log - INFO -    318                                                 # me = os.getpid()

2018-04-29 11:42:02,594 - memory_profile6_log - INFO -    319                                                 # kill_proc_tree(me)

2018-04-29 11:42:02,595 - memory_profile6_log - INFO -    320                             

2018-04-29 11:42:02,595 - memory_profile6_log - INFO -    321    350.5 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 11:42:02,598 - memory_profile6_log - INFO -    322    350.5 MiB      0.0 MiB                       for m in h_frame:

2018-04-29 11:42:02,598 - memory_profile6_log - INFO -    323    350.5 MiB      0.0 MiB                           if m is not None:

2018-04-29 11:42:02,599 - memory_profile6_log - INFO -    324    350.5 MiB      0.0 MiB                               if len(m) > 0:

2018-04-29 11:42:02,601 - memory_profile6_log - INFO -    325    350.5 MiB      0.5 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 11:42:02,601 - memory_profile6_log - INFO -    326    350.5 MiB      0.0 MiB                       del h_frame

2018-04-29 11:42:02,601 - memory_profile6_log - INFO -    327    350.5 MiB      0.0 MiB                       del lhistory

2018-04-29 11:42:02,602 - memory_profile6_log - INFO -    328                             

2018-04-29 11:42:02,604 - memory_profile6_log - INFO -    329    350.5 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 11:42:02,605 - memory_profile6_log - INFO -    330    350.5 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 11:42:02,608 - memory_profile6_log - INFO -    331                                     else: 

2018-04-29 11:42:02,608 - memory_profile6_log - INFO -    332                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 11:42:02,611 - memory_profile6_log - INFO -    333    350.5 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 11:42:02,611 - memory_profile6_log - INFO -    334    350.5 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 11:42:02,612 - memory_profile6_log - INFO -    335                             

2018-04-29 11:42:02,614 - memory_profile6_log - INFO -    336    350.5 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 11:42:02,615 - memory_profile6_log - INFO - 


2018-04-29 11:42:03,648 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 11:42:03,721 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0   0.001543        1455.707850       478            1465.707850  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
1   0.001543         655.760344        45             665.760344  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
2   0.001543         100.714046       293             110.714046  10960288  1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...
3   0.001543        1241.341593       356            1251.341593  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
4   0.001543        1595.092728        37            1605.092728  10960288  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
5   0.000023      109703.065469       779          109713.065469  10959239  161129f951435-0d660a18dbcd87-173a7640-100200-1...
6   0.001543         151.693334       366             161.693334  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
0   0.001543         133.525862       221             143.525862  10960288  161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...
1   0.001543         259.277950       205             269.277950  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
2   0.001543         156.963912        94             166.963912  10960288  16169731ce618-00742c5e307396-5b4d6a36-38400-16...
3   0.001543         125.571130       235             135.571130  10960288  1626fd160d9114-055d801ae6b39d-66265f05-38400-1...
4   0.001543         136.906453       441             146.906453  10960288  1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...
5   0.001543          87.305371       169              97.305371  10960288  161e219cd891c8-0e882943705df4-15290f5f-38400-1...
0   0.001543         175.650092        42             185.650092  10960288  16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...
1   0.001543         139.194413       106             149.194413  10960288  16149f0a1611-00a034775-63446032-38400-16149f0a...
2   0.001543          88.645278       187              98.645278  10960288  16132f1045f30b-0613b72b33445e-5d640103-38400-1...
3   0.001543          52.695028       280              62.695028  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
4   0.001543          63.281471       193              73.281471  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
5   0.001543         150.557222        49             160.557222  10960288  1612d88e55ab8-09af9658f30a46-65117b23-38400-16...
6   0.001543          51.589538       143              61.589538  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
2018-04-29 11:42:03,723 - memory_profile6_log - INFO - 

2018-04-29 11:42:03,733 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 11:42:03,812 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 11:42:03,825 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 11:42:55,512 - memory_profile6_log - INFO - size of df: 58.30 MB
2018-04-29 11:42:55,513 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-29 11:42:55,573 - memory_profile6_log - INFO - size of current_frame: 58.30 MB
2018-04-29 11:42:55,575 - memory_profile6_log - INFO - loading time of: 484420 total genuine-current interest data ~ take 351.325s
2018-04-29 11:42:55,581 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:42:55,582 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:42:55,582 - memory_profile6_log - INFO - ================================================

2018-04-29 11:42:55,585 - memory_profile6_log - INFO -    338     86.7 MiB     86.7 MiB   @profile

2018-04-29 11:42:55,585 - memory_profile6_log - INFO -    339                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 11:42:55,588 - memory_profile6_log - INFO -    340     86.8 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 11:42:55,588 - memory_profile6_log - INFO -    341     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 11:42:55,588 - memory_profile6_log - INFO -    342                             

2018-04-29 11:42:55,589 - memory_profile6_log - INFO -    343                                 # ~~~ Begin collecting data ~~~

2018-04-29 11:42:55,589 - memory_profile6_log - INFO -    344     86.8 MiB      0.0 MiB       t0 = time.time()

2018-04-29 11:42:55,591 - memory_profile6_log - INFO -    345    350.5 MiB    263.7 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 11:42:55,591 - memory_profile6_log - INFO -    346    350.5 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 11:42:55,592 - memory_profile6_log - INFO -    347                                     logger.info("Training cannot be empty..")

2018-04-29 11:42:55,595 - memory_profile6_log - INFO -    348                                     return False

2018-04-29 11:42:55,595 - memory_profile6_log - INFO -    349    351.1 MiB      0.6 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 11:42:55,598 - memory_profile6_log - INFO -    350    351.3 MiB      0.1 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 11:42:55,598 - memory_profile6_log - INFO -    351    351.3 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 11:42:55,598 - memory_profile6_log - INFO -    352                             

2018-04-29 11:42:55,599 - memory_profile6_log - INFO -    353    357.1 MiB      5.8 MiB       big_frame = pd.concat(datalist)

2018-04-29 11:42:55,601 - memory_profile6_log - INFO -    354    357.1 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 11:42:55,601 - memory_profile6_log - INFO -    355    351.3 MiB     -5.8 MiB       del datalist

2018-04-29 11:42:55,602 - memory_profile6_log - INFO -    356                             

2018-04-29 11:42:55,602 - memory_profile6_log - INFO -    357    351.3 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 11:42:55,602 - memory_profile6_log - INFO -    358                             

2018-04-29 11:42:55,605 - memory_profile6_log - INFO -    359                                 # ~ get current news interest ~

2018-04-29 11:42:55,605 - memory_profile6_log - INFO -    360    351.3 MiB      0.0 MiB       if not cd:

2018-04-29 11:42:55,608 - memory_profile6_log - INFO -    361                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 11:42:55,608 - memory_profile6_log - INFO -    362                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 11:42:55,609 - memory_profile6_log - INFO -    363                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 11:42:55,611 - memory_profile6_log - INFO -    364                                 else:

2018-04-29 11:42:55,611 - memory_profile6_log - INFO -    365    351.3 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 11:42:55,612 - memory_profile6_log - INFO -    366    351.3 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 11:42:55,612 - memory_profile6_log - INFO -    367                             

2018-04-29 11:42:55,614 - memory_profile6_log - INFO -    368                                     # safe handling of query parameter

2018-04-29 11:42:55,614 - memory_profile6_log - INFO -    369                                     query_params = [

2018-04-29 11:42:55,617 - memory_profile6_log - INFO -    370    351.3 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 11:42:55,618 - memory_profile6_log - INFO -    371                                     ]

2018-04-29 11:42:55,619 - memory_profile6_log - INFO -    372                             

2018-04-29 11:42:55,621 - memory_profile6_log - INFO -    373    351.3 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-29 11:42:55,621 - memory_profile6_log - INFO -    374    419.0 MiB     67.8 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 11:42:55,622 - memory_profile6_log - INFO -    375    419.0 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 11:42:55,622 - memory_profile6_log - INFO -    376                             

2018-04-29 11:42:55,624 - memory_profile6_log - INFO -    377    419.1 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 11:42:55,625 - memory_profile6_log - INFO -    378    419.1 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 11:42:55,625 - memory_profile6_log - INFO -    379    419.1 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 11:42:55,625 - memory_profile6_log - INFO -    380    419.1 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 11:42:55,630 - memory_profile6_log - INFO -    381    419.1 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 11:42:55,631 - memory_profile6_log - INFO -    382    419.1 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 11:42:55,631 - memory_profile6_log - INFO -    383                             

2018-04-29 11:42:55,632 - memory_profile6_log - INFO -    384    419.1 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 11:42:55,634 - memory_profile6_log - INFO - 


2018-04-29 11:42:55,637 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 11:42:55,667 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 11:42:55,670 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-29 11:42:55,671 - memory_profile6_log - INFO - apply on: 5000 total history data(D(t))
2018-04-29 11:42:56,755 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 11:42:56,756 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 11:43:38,575 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-29 11:43:38,604 - memory_profile6_log - INFO - Total train time: 42.936s
2018-04-29 11:43:38,605 - memory_profile6_log - INFO - memory left before cleaning: 69.400 percent memory...
2018-04-29 11:43:38,605 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-29 11:43:38,607 - memory_profile6_log - INFO - deleting df_dut...
2018-04-29 11:43:38,608 - memory_profile6_log - INFO - deleting df_dt...
2018-04-29 11:43:38,609 - memory_profile6_log - INFO - deleting df_input...
2018-04-29 11:43:38,618 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-29 11:43:38,618 - memory_profile6_log - INFO - deleting df_current...
2018-04-29 11:43:38,619 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-29 11:43:38,631 - memory_profile6_log - INFO - deleting model_fit...
2018-04-29 11:43:38,632 - memory_profile6_log - INFO - deleting result...
2018-04-29 11:43:38,654 - memory_profile6_log - INFO - deleting model_transform...
2018-04-29 11:43:38,655 - memory_profile6_log - INFO - memory left after cleaning: 69.200 percent memory...
2018-04-29 11:43:38,657 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-29 11:43:38,657 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-29 11:44:08,559 - memory_profile6_log - INFO - Saving fitted_models as history...
2018-04-29 11:44:39,819 - memory_profile6_log - INFO - deleting BR...
2018-04-29 11:44:39,822 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-04-29 11:44:39,832 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 11:44:39,834 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 11:44:39,834 - memory_profile6_log - INFO - ================================================

2018-04-29 11:44:39,838 - memory_profile6_log - INFO -    113    419.1 MiB    419.1 MiB   @profile

2018-04-29 11:44:39,839 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-29 11:44:39,841 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-29 11:44:39,841 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-29 11:44:39,842 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-29 11:44:39,842 - memory_profile6_log - INFO -    118                                      saveto="datastore"):

2018-04-29 11:44:39,842 - memory_profile6_log - INFO -    119                                 """

2018-04-29 11:44:39,844 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-29 11:44:39,845 - memory_profile6_log - INFO -    121                                 """

2018-04-29 11:44:39,845 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-29 11:44:39,845 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-29 11:44:39,846 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-29 11:44:39,848 - memory_profile6_log - INFO -    125    419.1 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-29 11:44:39,851 - memory_profile6_log - INFO -    126    427.1 MiB      8.0 MiB       df_dut = df_input.copy(deep=True)

2018-04-29 11:44:39,854 - memory_profile6_log - INFO -    127    427.1 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 11:44:39,855 - memory_profile6_log - INFO -    128                             

2018-04-29 11:44:39,855 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-29 11:44:39,855 - memory_profile6_log - INFO -    130    434.3 MiB      7.3 MiB       df_dt = df_current.copy(deep=True)

2018-04-29 11:44:39,857 - memory_profile6_log - INFO -    131    434.3 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 11:44:39,858 - memory_profile6_log - INFO -    132                             

2018-04-29 11:44:39,858 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-29 11:44:39,861 - memory_profile6_log - INFO -    134    434.3 MiB      0.0 MiB       t0 = time.time()

2018-04-29 11:44:39,862 - memory_profile6_log - INFO -    135    434.3 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-29 11:44:39,865 - memory_profile6_log - INFO -    136    434.3 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-29 11:44:39,865 - memory_profile6_log - INFO -    137    434.3 MiB      0.0 MiB       logger.info("apply on: %d total history data(D(t))", len(df_hist))

2018-04-29 11:44:39,865 - memory_profile6_log - INFO -    138                             

2018-04-29 11:44:39,865 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-29 11:44:39,867 - memory_profile6_log - INFO -    140    434.3 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-29 11:44:39,867 - memory_profile6_log - INFO -    141                             

2018-04-29 11:44:39,868 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-29 11:44:39,868 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-29 11:44:39,871 - memory_profile6_log - INFO -    144    437.1 MiB      2.8 MiB       NB = BR.processX(df_dut)

2018-04-29 11:44:39,872 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-29 11:44:39,875 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-29 11:44:39,875 - memory_profile6_log - INFO -    147    447.0 MiB      9.9 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-29 11:44:39,875 - memory_profile6_log - INFO -    148                                 """

2018-04-29 11:44:39,877 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-29 11:44:39,878 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-29 11:44:39,878 - memory_profile6_log - INFO -    151                                 """

2018-04-29 11:44:39,880 - memory_profile6_log - INFO -    152    447.0 MiB      0.0 MiB       fitby_sigmant = True

2018-04-29 11:44:39,880 - memory_profile6_log - INFO -    153    447.0 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-29 11:44:39,884 - memory_profile6_log - INFO -    154    447.0 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-29 11:44:39,884 - memory_profile6_log - INFO -    155    456.7 MiB      9.7 MiB                            'is_general']]

2018-04-29 11:44:39,888 - memory_profile6_log - INFO -    156    456.7 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-29 11:44:39,888 - memory_profile6_log - INFO -    157    456.7 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-29 11:44:39,888 - memory_profile6_log - INFO -    158    483.0 MiB     26.3 MiB                          verbose=False)

2018-04-29 11:44:39,890 - memory_profile6_log - INFO -    159    483.0 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-29 11:44:39,891 - memory_profile6_log - INFO -    160    483.0 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-29 11:44:39,891 - memory_profile6_log - INFO -    161                             

2018-04-29 11:44:39,894 - memory_profile6_log - INFO -    162                                 # ~~ and Transform ~~

2018-04-29 11:44:39,894 - memory_profile6_log - INFO -    163                                 #   handling current news interest == current date

2018-04-29 11:44:39,897 - memory_profile6_log - INFO -    164    483.0 MiB      0.0 MiB       if df_dt.empty:

2018-04-29 11:44:39,897 - memory_profile6_log - INFO -    165                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-29 11:44:39,898 - memory_profile6_log - INFO -    166                                     return None

2018-04-29 11:44:39,898 - memory_profile6_log - INFO -    167    483.9 MiB      0.9 MiB       NB = BR.processX(df_dt)

2018-04-29 11:44:39,898 - memory_profile6_log - INFO -    168    494.5 MiB     10.6 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-29 11:44:39,900 - memory_profile6_log - INFO -    169                             

2018-04-29 11:44:39,900 - memory_profile6_log - INFO -    170    494.5 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-29 11:44:39,901 - memory_profile6_log - INFO -    171    493.6 MiB     -0.9 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-29 11:44:39,901 - memory_profile6_log - INFO -    172    493.6 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-29 11:44:39,901 - memory_profile6_log - INFO -    173    493.6 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-29 11:44:39,904 - memory_profile6_log - INFO -    174    512.5 MiB     18.9 MiB                                                     verbose=False)

2018-04-29 11:44:39,905 - memory_profile6_log - INFO -    175                                 # ~~~ filter is general and specific topic ~~~

2018-04-29 11:44:39,907 - memory_profile6_log - INFO -    176                                 # the idea is just we need to rerank every topic according

2018-04-29 11:44:39,907 - memory_profile6_log - INFO -    177                                 # user_id and and is_general by p0_posterior

2018-04-29 11:44:39,908 - memory_profile6_log - INFO -    178    512.5 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-29 11:44:39,910 - memory_profile6_log - INFO -    179    514.4 MiB      1.9 MiB                                     'is_general']].groupby(['topic_id',

2018-04-29 11:44:39,911 - memory_profile6_log - INFO -    180    512.5 MiB     -1.9 MiB                                                             'is_general']

2018-04-29 11:44:39,911 - memory_profile6_log - INFO -    181                                                                                      ).size().to_frame().reset_index()

2018-04-29 11:44:39,911 - memory_profile6_log - INFO -    182                             

2018-04-29 11:44:39,913 - memory_profile6_log - INFO -    183                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-29 11:44:39,914 - memory_profile6_log - INFO -    184                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-29 11:44:39,915 - memory_profile6_log - INFO -    185    512.5 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-29 11:44:39,917 - memory_profile6_log - INFO -    186                             

2018-04-29 11:44:39,917 - memory_profile6_log - INFO -    187                                 # ~ start by provide rank for each topic type ~

2018-04-29 11:44:39,918 - memory_profile6_log - INFO -    188    536.4 MiB     23.9 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-29 11:44:39,920 - memory_profile6_log - INFO -    189    539.9 MiB      3.5 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-29 11:44:39,921 - memory_profile6_log - INFO -    190                             

2018-04-29 11:44:39,921 - memory_profile6_log - INFO -    191                                 # ~ set threshold to filter output

2018-04-29 11:44:39,923 - memory_profile6_log - INFO -    192    539.9 MiB      0.0 MiB       if threshold > 0:

2018-04-29 11:44:39,923 - memory_profile6_log - INFO -    193    539.9 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-29 11:44:39,924 - memory_profile6_log - INFO -    194    539.9 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-29 11:44:39,927 - memory_profile6_log - INFO -    195    538.8 MiB     -1.1 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-29 11:44:39,928 - memory_profile6_log - INFO -    196                             

2018-04-29 11:44:39,930 - memory_profile6_log - INFO -    197    538.8 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 11:44:39,930 - memory_profile6_log - INFO -    198    538.8 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-29 11:44:39,930 - memory_profile6_log - INFO -    199                             

2018-04-29 11:44:39,931 - memory_profile6_log - INFO -    200    538.8 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 11:44:39,931 - memory_profile6_log - INFO -    201                             

2018-04-29 11:44:39,933 - memory_profile6_log - INFO -    202    538.8 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-29 11:44:39,933 - memory_profile6_log - INFO -    203    538.8 MiB      0.0 MiB       del df_dut

2018-04-29 11:44:39,934 - memory_profile6_log - INFO -    204    538.8 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-29 11:44:39,934 - memory_profile6_log - INFO -    205    538.8 MiB      0.0 MiB       del df_dt

2018-04-29 11:44:39,934 - memory_profile6_log - INFO -    206    538.8 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-29 11:44:39,934 - memory_profile6_log - INFO -    207    538.8 MiB      0.0 MiB       del df_input

2018-04-29 11:44:39,936 - memory_profile6_log - INFO -    208    538.8 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-29 11:44:39,940 - memory_profile6_log - INFO -    209    530.0 MiB     -8.8 MiB       del df_input_X

2018-04-29 11:44:39,940 - memory_profile6_log - INFO -    210    530.0 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-29 11:44:39,941 - memory_profile6_log - INFO -    211    530.0 MiB      0.0 MiB       del df_current

2018-04-29 11:44:39,943 - memory_profile6_log - INFO -    212    530.0 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-29 11:44:39,944 - memory_profile6_log - INFO -    213    530.0 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-29 11:44:39,944 - memory_profile6_log - INFO -    214    530.0 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-29 11:44:39,944 - memory_profile6_log - INFO -    215    506.8 MiB    -23.3 MiB       del model_fit

2018-04-29 11:44:39,946 - memory_profile6_log - INFO -    216    506.8 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-29 11:44:39,946 - memory_profile6_log - INFO -    217    506.8 MiB      0.0 MiB       del result

2018-04-29 11:44:39,947 - memory_profile6_log - INFO -    218    506.8 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-29 11:44:39,947 - memory_profile6_log - INFO -    219                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 11:44:39,947 - memory_profile6_log - INFO -    220                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 11:44:39,947 - memory_profile6_log - INFO -    221    506.8 MiB      0.0 MiB       if savetrain:

2018-04-29 11:44:39,951 - memory_profile6_log - INFO -    222    512.2 MiB      5.4 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-29 11:44:39,953 - memory_profile6_log - INFO -    223    512.2 MiB      0.0 MiB           del model_transform

2018-04-29 11:44:39,953 - memory_profile6_log - INFO -    224    512.2 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-29 11:44:39,954 - memory_profile6_log - INFO -    225    512.2 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 11:44:39,954 - memory_profile6_log - INFO -    226                             

2018-04-29 11:44:39,956 - memory_profile6_log - INFO -    227    512.2 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-29 11:44:39,956 - memory_profile6_log - INFO -    228                                     # ~ Place your code to save the training model here ~

2018-04-29 11:44:39,957 - memory_profile6_log - INFO -    229    512.2 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-29 11:44:39,957 - memory_profile6_log - INFO -    230    512.2 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-29 11:44:39,957 - memory_profile6_log - INFO -    231    512.2 MiB      0.0 MiB               if multproc:

2018-04-29 11:44:39,957 - memory_profile6_log - INFO -    232    489.3 MiB    -22.9 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-29 11:44:39,959 - memory_profile6_log - INFO -    233                             

2018-04-29 11:44:39,960 - memory_profile6_log - INFO -    234    489.3 MiB      0.0 MiB                   logger.info("Saving fitted_models as history...")

2018-04-29 11:44:39,963 - memory_profile6_log - INFO -    235    489.3 MiB      0.0 MiB                   save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 11:44:39,963 - memory_profile6_log - INFO -    236    503.2 MiB     13.9 MiB                   fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-29 11:44:39,964 - memory_profile6_log - INFO -    237    505.1 MiB      2.0 MiB                   mh.saveDataStorePutMulti(fitted_models_sigmant, kinds='topic_recomendation_history')

2018-04-29 11:44:39,966 - memory_profile6_log - INFO -    238                             

2018-04-29 11:44:39,966 - memory_profile6_log - INFO -    239    505.1 MiB      0.0 MiB                   del BR

2018-04-29 11:44:39,967 - memory_profile6_log - INFO -    240    505.1 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-29 11:44:39,967 - memory_profile6_log - INFO -    241                             

2018-04-29 11:44:39,967 - memory_profile6_log - INFO -    242    505.1 MiB      0.0 MiB                   del save_sigma_nt

2018-04-29 11:44:39,967 - memory_profile6_log - INFO -    243    505.1 MiB      0.0 MiB                   logger.info("deleting save_sigma_nt...")

2018-04-29 11:44:39,967 - memory_profile6_log - INFO -    244                                         else:

2018-04-29 11:44:39,969 - memory_profile6_log - INFO -    245                                             mh.saveDatastore(model_transformsv)

2018-04-29 11:44:39,970 - memory_profile6_log - INFO -    246                                             

2018-04-29 11:44:39,970 - memory_profile6_log - INFO -    247                                     elif str(saveto).lower() == "elastic":

2018-04-29 11:44:39,970 - memory_profile6_log - INFO -    248                                         logger.info("Using ElasticSearch as storage...")

2018-04-29 11:44:39,970 - memory_profile6_log - INFO -    249                                         mh.saveElasticS(model_transformsv)

2018-04-29 11:44:39,971 - memory_profile6_log - INFO -    250                             

2018-04-29 11:44:39,974 - memory_profile6_log - INFO -    251                                     # need save sigma_nt for daily train

2018-04-29 11:44:39,976 - memory_profile6_log - INFO -    252                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-29 11:44:39,976 - memory_profile6_log - INFO -    253                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-29 11:44:39,977 - memory_profile6_log - INFO -    254    505.1 MiB      0.0 MiB           if start_date and end_date:

2018-04-29 11:44:39,979 - memory_profile6_log - INFO -    255                                         if not fitby_sigmant:

2018-04-29 11:44:39,979 - memory_profile6_log - INFO -    256                                             logging.info("Saving sigma Nt...")

2018-04-29 11:44:39,980 - memory_profile6_log - INFO -    257                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 11:44:39,980 - memory_profile6_log - INFO -    258                                             save_sigma_nt['start_date'] = start_date

2018-04-29 11:44:39,980 - memory_profile6_log - INFO -    259                                             save_sigma_nt['end_date'] = end_date

2018-04-29 11:44:39,982 - memory_profile6_log - INFO -    260                                             print save_sigma_nt.head(5)

2018-04-29 11:44:39,983 - memory_profile6_log - INFO -    261                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-29 11:44:39,986 - memory_profile6_log - INFO -    262                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-29 11:44:39,986 - memory_profile6_log - INFO -    263    505.1 MiB      0.0 MiB       return

2018-04-29 11:44:39,987 - memory_profile6_log - INFO - 


2018-04-29 11:44:39,989 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-29 11:52:41,150 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 11:52:41,153 - memory_profile6_log - INFO - date_generated: 
2018-04-29 11:52:41,154 - memory_profile6_log - INFO -  
2018-04-29 11:52:41,154 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 11:52:41,154 - memory_profile6_log - INFO - 

2018-04-29 11:52:41,154 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 11:52:41,155 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 11:52:41,155 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 11:52:41,278 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 11:52:41,282 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 11:53:52,644 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 11:53:52,645 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 11:53:52,686 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 11:53:52,687 - memory_profile6_log - INFO - Appending history data...
2018-04-29 11:53:52,687 - memory_profile6_log - INFO - processing batch-0
2018-04-29 11:53:52,688 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:53:52,729 - memory_profile6_log - INFO - call history data...
2018-04-29 11:54:36,417 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:54:37,142 - memory_profile6_log - INFO - processing batch-1
2018-04-29 11:54:37,144 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:54:37,154 - memory_profile6_log - INFO - call history data...
2018-04-29 11:55:20,265 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:55:20,982 - memory_profile6_log - INFO - processing batch-2
2018-04-29 11:55:20,983 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:55:20,992 - memory_profile6_log - INFO - call history data...
2018-04-29 11:56:07,220 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:56:07,885 - memory_profile6_log - INFO - processing batch-3
2018-04-29 11:56:07,887 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:56:07,894 - memory_profile6_log - INFO - call history data...
2018-04-29 11:56:53,726 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:56:54,492 - memory_profile6_log - INFO - processing batch-4
2018-04-29 11:56:54,493 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:56:54,505 - memory_profile6_log - INFO - call history data...
2018-04-29 11:57:22,335 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 11:57:22,336 - memory_profile6_log - INFO - date_generated: 
2018-04-29 11:57:22,338 - memory_profile6_log - INFO -  
2018-04-29 11:57:22,338 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 11:57:22,338 - memory_profile6_log - INFO - 

2018-04-29 11:57:22,338 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 11:57:22,338 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 11:57:22,338 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 11:57:22,463 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 11:57:22,467 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 11:58:33,766 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 11:58:33,767 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 11:58:33,809 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 11:58:33,812 - memory_profile6_log - INFO - Appending history data...
2018-04-29 11:58:33,812 - memory_profile6_log - INFO - processing batch-0
2018-04-29 11:58:33,813 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:58:33,857 - memory_profile6_log - INFO - call history data...
2018-04-29 11:59:18,463 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:59:19,131 - memory_profile6_log - INFO - processing batch-1
2018-04-29 11:59:19,131 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:59:19,140 - memory_profile6_log - INFO - call history data...
2018-04-29 11:59:59,285 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 11:59:59,960 - memory_profile6_log - INFO - processing batch-2
2018-04-29 11:59:59,960 - memory_profile6_log - INFO - creating list history data...
2018-04-29 11:59:59,969 - memory_profile6_log - INFO - call history data...
2018-04-29 12:00:39,341 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 12:00:40,026 - memory_profile6_log - INFO - processing batch-3
2018-04-29 12:00:40,026 - memory_profile6_log - INFO - creating list history data...
2018-04-29 12:00:40,033 - memory_profile6_log - INFO - call history data...
2018-04-29 12:01:21,158 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 12:01:21,967 - memory_profile6_log - INFO - processing batch-4
2018-04-29 12:01:21,969 - memory_profile6_log - INFO - creating list history data...
2018-04-29 12:01:21,976 - memory_profile6_log - INFO - call history data...
2018-04-29 12:02:03,395 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 12:02:04,095 - memory_profile6_log - INFO - Appending training data...
2018-04-29 12:02:04,095 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 12:02:04,098 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 12:02:04,099 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 12:02:04,101 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 12:02:04,101 - memory_profile6_log - INFO - ================================================

2018-04-29 12:02:04,102 - memory_profile6_log - INFO -    295     86.8 MiB     86.8 MiB   @profile

2018-04-29 12:02:04,104 - memory_profile6_log - INFO -    296                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 12:02:04,107 - memory_profile6_log - INFO -    297     86.8 MiB      0.0 MiB       bq_client = client

2018-04-29 12:02:04,108 - memory_profile6_log - INFO -    298     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 12:02:04,109 - memory_profile6_log - INFO -    299                             

2018-04-29 12:02:04,111 - memory_profile6_log - INFO -    300     86.8 MiB      0.0 MiB       datalist = []

2018-04-29 12:02:04,111 - memory_profile6_log - INFO -    301     86.8 MiB      0.0 MiB       datalist_hist = []

2018-04-29 12:02:04,112 - memory_profile6_log - INFO -    302                             

2018-04-29 12:02:04,114 - memory_profile6_log - INFO -    303     86.8 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 12:02:04,115 - memory_profile6_log - INFO -    304    350.4 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 12:02:04,118 - memory_profile6_log - INFO -    305    340.3 MiB    253.5 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 12:02:04,118 - memory_profile6_log - INFO -    306    340.3 MiB      0.0 MiB           if tframe is not None:

2018-04-29 12:02:04,119 - memory_profile6_log - INFO -    307    340.3 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 12:02:04,121 - memory_profile6_log - INFO -    308    347.1 MiB      6.8 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 12:02:04,122 - memory_profile6_log - INFO -    309    347.1 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 12:02:04,124 - memory_profile6_log - INFO -    310    347.1 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 12:02:04,125 - memory_profile6_log - INFO -    311    350.4 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 12:02:04,128 - memory_profile6_log - INFO -    312                                                 # ~ loading history

2018-04-29 12:02:04,128 - memory_profile6_log - INFO -    313                                                 """

2018-04-29 12:02:04,130 - memory_profile6_log - INFO -    314                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 12:02:04,131 - memory_profile6_log - INFO -    315                                                 """

2018-04-29 12:02:04,132 - memory_profile6_log - INFO -    316    350.3 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 12:02:04,134 - memory_profile6_log - INFO -    317                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 12:02:04,134 - memory_profile6_log - INFO -    318    350.3 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 12:02:04,135 - memory_profile6_log - INFO -    319    350.3 MiB      0.8 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 12:02:04,135 - memory_profile6_log - INFO -    320                             

2018-04-29 12:02:04,138 - memory_profile6_log - INFO -    321    350.3 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 12:02:04,140 - memory_profile6_log - INFO -    322    350.4 MiB      1.8 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 12:02:04,141 - memory_profile6_log - INFO -    323                             

2018-04-29 12:02:04,141 - memory_profile6_log - INFO -    324                                                 # me = os.getpid()

2018-04-29 12:02:04,142 - memory_profile6_log - INFO -    325                                                 # kill_proc_tree(me)

2018-04-29 12:02:04,144 - memory_profile6_log - INFO -    326                             

2018-04-29 12:02:04,144 - memory_profile6_log - INFO -    327    350.4 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 12:02:04,145 - memory_profile6_log - INFO -    328    350.4 MiB     -0.1 MiB                       for m in h_frame:

2018-04-29 12:02:04,145 - memory_profile6_log - INFO -    329    350.4 MiB     -0.1 MiB                           if m is not None:

2018-04-29 12:02:04,148 - memory_profile6_log - INFO -    330    350.4 MiB     -0.1 MiB                               if len(m) > 0:

2018-04-29 12:02:04,150 - memory_profile6_log - INFO -    331    350.4 MiB      0.6 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 12:02:04,151 - memory_profile6_log - INFO -    332    350.4 MiB     -0.0 MiB                       del h_frame

2018-04-29 12:02:04,151 - memory_profile6_log - INFO -    333    350.4 MiB      0.0 MiB                       del lhistory

2018-04-29 12:02:04,153 - memory_profile6_log - INFO -    334                             

2018-04-29 12:02:04,154 - memory_profile6_log - INFO -    335    350.4 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 12:02:04,154 - memory_profile6_log - INFO -    336    350.4 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 12:02:04,155 - memory_profile6_log - INFO -    337                                     else: 

2018-04-29 12:02:04,157 - memory_profile6_log - INFO -    338                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 12:02:04,160 - memory_profile6_log - INFO -    339    350.4 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 12:02:04,161 - memory_profile6_log - INFO -    340    350.4 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 12:02:04,161 - memory_profile6_log - INFO -    341                             

2018-04-29 12:02:04,161 - memory_profile6_log - INFO -    342    350.4 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 12:02:04,163 - memory_profile6_log - INFO - 


2018-04-29 12:02:05,316 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 12:02:05,390 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0   0.000646        1844.325967        16            1854.325967  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
1   0.000646        1844.325967        32            1854.325967  10960288  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
2        NaN      333823.000000       256          333833.000000  10959239  161129f951435-0d660a18dbcd87-173a7640-100200-1...
3   0.000646        1844.325967        32            1854.325967  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
4   0.000646        1844.325967        32            1854.325967  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
5   0.000646        1844.325967        16            1854.325967  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
6   0.000646        1844.325967        16            1854.325967  10960288  1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...
0   0.000646        1844.325967        16            1854.325967  10960288  1626fd160d9114-055d801ae6b39d-66265f05-38400-1...
1   0.000646        1844.325967         8            1854.325967  10960288  16169731ce618-00742c5e307396-5b4d6a36-38400-16...
2   0.000646        1844.325967         8            1854.325967  10960288  1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...
3   0.000646        1844.325967         8            1854.325967  10960288  161e219cd891c8-0e882943705df4-15290f5f-38400-1...
4   0.000646        1844.325967        16            1854.325967  10960288  161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...
5   0.000646        1844.325967        16            1854.325967  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
0   0.000646        1844.325967         4            1854.325967  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
1   0.000646        1844.325967         4            1854.325967  10960288  16132f1045f30b-0613b72b33445e-5d640103-38400-1...
2   0.000646        1844.325967         8            1854.325967  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
3   0.000646        1844.325967         4            1854.325967  10960288  16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...
4   0.000646        1844.325967         4            1854.325967  10960288  1612d88e55ab8-09af9658f30a46-65117b23-38400-16...
5   0.000646        1844.325967         8            1854.325967  10960288  16149f0a1611-00a034775-63446032-38400-16149f0a...
6   0.000646        1844.325967         4            1854.325967  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
2018-04-29 12:02:05,391 - memory_profile6_log - INFO - 

2018-04-29 12:02:05,400 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 12:02:05,469 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 12:02:05,480 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 12:02:58,546 - memory_profile6_log - INFO - size of df: 58.30 MB
2018-04-29 12:02:58,548 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-29 12:02:58,609 - memory_profile6_log - INFO - size of current_frame: 58.30 MB
2018-04-29 12:02:58,611 - memory_profile6_log - INFO - loading time of: 484420 total genuine-current interest data ~ take 336.168s
2018-04-29 12:02:58,617 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 12:02:58,618 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 12:02:58,618 - memory_profile6_log - INFO - ================================================

2018-04-29 12:02:58,618 - memory_profile6_log - INFO -    344     86.7 MiB     86.7 MiB   @profile

2018-04-29 12:02:58,621 - memory_profile6_log - INFO -    345                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 12:02:58,621 - memory_profile6_log - INFO -    346     86.8 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 12:02:58,624 - memory_profile6_log - INFO -    347     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 12:02:58,625 - memory_profile6_log - INFO -    348                             

2018-04-29 12:02:58,625 - memory_profile6_log - INFO -    349                                 # ~~~ Begin collecting data ~~~

2018-04-29 12:02:58,625 - memory_profile6_log - INFO -    350     86.8 MiB      0.0 MiB       t0 = time.time()

2018-04-29 12:02:58,627 - memory_profile6_log - INFO -    351    350.5 MiB    263.6 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 12:02:58,627 - memory_profile6_log - INFO -    352    350.5 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 12:02:58,628 - memory_profile6_log - INFO -    353                                     logger.info("Training cannot be empty..")

2018-04-29 12:02:58,628 - memory_profile6_log - INFO -    354                                     return False

2018-04-29 12:02:58,628 - memory_profile6_log - INFO -    355    351.3 MiB      0.9 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 12:02:58,631 - memory_profile6_log - INFO -    356    351.5 MiB      0.1 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 12:02:58,631 - memory_profile6_log - INFO -    357    351.5 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 12:02:58,634 - memory_profile6_log - INFO -    358                             

2018-04-29 12:02:58,634 - memory_profile6_log - INFO -    359    357.3 MiB      5.8 MiB       big_frame = pd.concat(datalist)

2018-04-29 12:02:58,635 - memory_profile6_log - INFO -    360    357.3 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 12:02:58,635 - memory_profile6_log - INFO -    361    351.5 MiB     -5.8 MiB       del datalist

2018-04-29 12:02:58,637 - memory_profile6_log - INFO -    362                             

2018-04-29 12:02:58,638 - memory_profile6_log - INFO -    363    351.5 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 12:02:58,638 - memory_profile6_log - INFO -    364                             

2018-04-29 12:02:58,641 - memory_profile6_log - INFO -    365                                 # ~ get current news interest ~

2018-04-29 12:02:58,642 - memory_profile6_log - INFO -    366    351.5 MiB      0.0 MiB       if not cd:

2018-04-29 12:02:58,644 - memory_profile6_log - INFO -    367                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 12:02:58,644 - memory_profile6_log - INFO -    368                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 12:02:58,645 - memory_profile6_log - INFO -    369                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 12:02:58,647 - memory_profile6_log - INFO -    370                                 else:

2018-04-29 12:02:58,648 - memory_profile6_log - INFO -    371    351.5 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 12:02:58,648 - memory_profile6_log - INFO -    372    351.5 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 12:02:58,650 - memory_profile6_log - INFO -    373                             

2018-04-29 12:02:58,653 - memory_profile6_log - INFO -    374                                     # safe handling of query parameter

2018-04-29 12:02:58,653 - memory_profile6_log - INFO -    375                                     query_params = [

2018-04-29 12:02:58,654 - memory_profile6_log - INFO -    376    351.5 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 12:02:58,655 - memory_profile6_log - INFO -    377                                     ]

2018-04-29 12:02:58,657 - memory_profile6_log - INFO -    378                             

2018-04-29 12:02:58,657 - memory_profile6_log - INFO -    379    351.5 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-29 12:02:58,657 - memory_profile6_log - INFO -    380    420.2 MiB     68.7 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 12:02:58,658 - memory_profile6_log - INFO -    381    420.2 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 12:02:58,658 - memory_profile6_log - INFO -    382                             

2018-04-29 12:02:58,660 - memory_profile6_log - INFO -    383    420.2 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 12:02:58,660 - memory_profile6_log - INFO -    384    420.2 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 12:02:58,663 - memory_profile6_log - INFO -    385    420.2 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 12:02:58,663 - memory_profile6_log - INFO -    386    420.2 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 12:02:58,665 - memory_profile6_log - INFO -    387    420.2 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 12:02:58,665 - memory_profile6_log - INFO -    388    420.2 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 12:02:58,667 - memory_profile6_log - INFO -    389                             

2018-04-29 12:02:58,667 - memory_profile6_log - INFO -    390    420.2 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 12:02:58,667 - memory_profile6_log - INFO - 


2018-04-29 12:02:58,671 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 12:02:58,703 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 12:02:58,704 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-29 12:02:58,706 - memory_profile6_log - INFO - apply on: 5000 total history data(D(t))
2018-04-29 12:02:59,792 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 12:02:59,793 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 12:03:43,881 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-29 12:03:43,913 - memory_profile6_log - INFO - Total train time: 45.209s
2018-04-29 12:03:43,914 - memory_profile6_log - INFO - memory left before cleaning: 67.000 percent memory...
2018-04-29 12:03:43,914 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-29 12:03:43,915 - memory_profile6_log - INFO - deleting df_dut...
2018-04-29 12:03:43,917 - memory_profile6_log - INFO - deleting df_dt...
2018-04-29 12:03:43,917 - memory_profile6_log - INFO - deleting df_input...
2018-04-29 12:03:43,926 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-29 12:03:43,927 - memory_profile6_log - INFO - deleting df_current...
2018-04-29 12:03:43,928 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-29 12:03:43,940 - memory_profile6_log - INFO - deleting model_fit...
2018-04-29 12:03:43,943 - memory_profile6_log - INFO - deleting result...
2018-04-29 12:03:43,963 - memory_profile6_log - INFO - deleting model_transform...
2018-04-29 12:03:43,963 - memory_profile6_log - INFO - memory left after cleaning: 66.800 percent memory...
2018-04-29 12:03:43,964 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-29 12:03:43,967 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-29 12:03:44,148 - memory_profile6_log - INFO - Saving fitted_models as history...
2018-04-29 12:03:44,253 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 10
2018-04-29 12:03:44,255 - memory_profile6_log - INFO - processing batch-0
2018-04-29 12:03:44,444 - memory_profile6_log - INFO - processing batch-1
2018-04-29 12:03:44,628 - memory_profile6_log - INFO - processing batch-2
2018-04-29 12:03:44,819 - memory_profile6_log - INFO - processing batch-3
2018-04-29 12:03:45,035 - memory_profile6_log - INFO - processing batch-4
2018-04-29 12:03:45,230 - memory_profile6_log - INFO - processing batch-5
2018-04-29 12:03:45,446 - memory_profile6_log - INFO - processing batch-6
2018-04-29 12:03:45,651 - memory_profile6_log - INFO - processing batch-7
2018-04-29 12:03:45,835 - memory_profile6_log - INFO - processing batch-8
2018-04-29 12:03:46,030 - memory_profile6_log - INFO - processing batch-9
2018-04-29 12:03:46,250 - memory_profile6_log - INFO - deleting BR...
2018-04-29 12:03:46,252 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-04-29 12:03:46,263 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 12:03:46,263 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 12:03:46,263 - memory_profile6_log - INFO - ================================================

2018-04-29 12:03:46,263 - memory_profile6_log - INFO -    113    420.2 MiB    420.2 MiB   @profile

2018-04-29 12:03:46,266 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-29 12:03:46,267 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-29 12:03:46,269 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-29 12:03:46,269 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-29 12:03:46,269 - memory_profile6_log - INFO -    118                                      saveto="datastore"):

2018-04-29 12:03:46,270 - memory_profile6_log - INFO -    119                                 """

2018-04-29 12:03:46,270 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-29 12:03:46,272 - memory_profile6_log - INFO -    121                                 """

2018-04-29 12:03:46,272 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-29 12:03:46,273 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-29 12:03:46,273 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-29 12:03:46,273 - memory_profile6_log - INFO -    125    420.2 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-29 12:03:46,279 - memory_profile6_log - INFO -    126    428.2 MiB      8.0 MiB       df_dut = df_input.copy(deep=True)

2018-04-29 12:03:46,279 - memory_profile6_log - INFO -    127    428.2 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 12:03:46,283 - memory_profile6_log - INFO -    128                             

2018-04-29 12:03:46,285 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-29 12:03:46,289 - memory_profile6_log - INFO -    130    435.5 MiB      7.3 MiB       df_dt = df_current.copy(deep=True)

2018-04-29 12:03:46,289 - memory_profile6_log - INFO -    131    435.5 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 12:03:46,293 - memory_profile6_log - INFO -    132                             

2018-04-29 12:03:46,295 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-29 12:03:46,296 - memory_profile6_log - INFO -    134    435.5 MiB      0.0 MiB       t0 = time.time()

2018-04-29 12:03:46,296 - memory_profile6_log - INFO -    135    435.5 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-29 12:03:46,296 - memory_profile6_log - INFO -    136    435.5 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-29 12:03:46,296 - memory_profile6_log - INFO -    137    435.5 MiB      0.0 MiB       logger.info("apply on: %d total history data(D(t))", len(df_hist))

2018-04-29 12:03:46,299 - memory_profile6_log - INFO -    138                             

2018-04-29 12:03:46,302 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-29 12:03:46,302 - memory_profile6_log - INFO -    140    435.5 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-29 12:03:46,303 - memory_profile6_log - INFO -    141                             

2018-04-29 12:03:46,305 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-29 12:03:46,305 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-29 12:03:46,305 - memory_profile6_log - INFO -    144    437.9 MiB      2.4 MiB       NB = BR.processX(df_dut)

2018-04-29 12:03:46,305 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-29 12:03:46,305 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-29 12:03:46,306 - memory_profile6_log - INFO -    147    447.8 MiB     10.0 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-29 12:03:46,306 - memory_profile6_log - INFO -    148                                 """

2018-04-29 12:03:46,308 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-29 12:03:46,311 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-29 12:03:46,311 - memory_profile6_log - INFO -    151                                 """

2018-04-29 12:03:46,312 - memory_profile6_log - INFO -    152    447.8 MiB      0.0 MiB       fitby_sigmant = True

2018-04-29 12:03:46,312 - memory_profile6_log - INFO -    153    447.8 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-29 12:03:46,312 - memory_profile6_log - INFO -    154    447.8 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-29 12:03:46,313 - memory_profile6_log - INFO -    155    457.5 MiB      9.7 MiB                            'is_general']]

2018-04-29 12:03:46,313 - memory_profile6_log - INFO -    156    457.5 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-29 12:03:46,313 - memory_profile6_log - INFO -    157    457.5 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-29 12:03:46,315 - memory_profile6_log - INFO -    158    483.9 MiB     26.4 MiB                          verbose=False)

2018-04-29 12:03:46,315 - memory_profile6_log - INFO -    159    483.9 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-29 12:03:46,315 - memory_profile6_log - INFO -    160    483.9 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-29 12:03:46,315 - memory_profile6_log - INFO -    161                             

2018-04-29 12:03:46,316 - memory_profile6_log - INFO -    162                                 # ~~ and Transform ~~

2018-04-29 12:03:46,316 - memory_profile6_log - INFO -    163                                 #   handling current news interest == current date

2018-04-29 12:03:46,321 - memory_profile6_log - INFO -    164    483.9 MiB      0.0 MiB       if df_dt.empty:

2018-04-29 12:03:46,322 - memory_profile6_log - INFO -    165                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-29 12:03:46,322 - memory_profile6_log - INFO -    166                                     return None

2018-04-29 12:03:46,322 - memory_profile6_log - INFO -    167    484.9 MiB      1.1 MiB       NB = BR.processX(df_dt)

2018-04-29 12:03:46,323 - memory_profile6_log - INFO -    168    495.5 MiB     10.6 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-29 12:03:46,323 - memory_profile6_log - INFO -    169                             

2018-04-29 12:03:46,323 - memory_profile6_log - INFO -    170    495.5 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-29 12:03:46,325 - memory_profile6_log - INFO -    171    494.6 MiB     -0.9 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-29 12:03:46,325 - memory_profile6_log - INFO -    172    494.6 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-29 12:03:46,326 - memory_profile6_log - INFO -    173    494.6 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-29 12:03:46,326 - memory_profile6_log - INFO -    174    514.6 MiB     19.9 MiB                                                     verbose=False)

2018-04-29 12:03:46,328 - memory_profile6_log - INFO -    175                                 # ~~~ filter is general and specific topic ~~~

2018-04-29 12:03:46,328 - memory_profile6_log - INFO -    176                                 # the idea is just we need to rerank every topic according

2018-04-29 12:03:46,328 - memory_profile6_log - INFO -    177                                 # user_id and and is_general by p0_posterior

2018-04-29 12:03:46,328 - memory_profile6_log - INFO -    178    514.6 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-29 12:03:46,328 - memory_profile6_log - INFO -    179    516.5 MiB      1.9 MiB                                     'is_general']].groupby(['topic_id',

2018-04-29 12:03:46,334 - memory_profile6_log - INFO -    180    514.6 MiB     -1.9 MiB                                                             'is_general']

2018-04-29 12:03:46,335 - memory_profile6_log - INFO -    181                                                                                      ).size().to_frame().reset_index()

2018-04-29 12:03:46,335 - memory_profile6_log - INFO -    182                             

2018-04-29 12:03:46,335 - memory_profile6_log - INFO -    183                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-29 12:03:46,336 - memory_profile6_log - INFO -    184                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-29 12:03:46,336 - memory_profile6_log - INFO -    185    514.6 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-29 12:03:46,338 - memory_profile6_log - INFO -    186                             

2018-04-29 12:03:46,338 - memory_profile6_log - INFO -    187                                 # ~ start by provide rank for each topic type ~

2018-04-29 12:03:46,339 - memory_profile6_log - INFO -    188    538.6 MiB     24.0 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-29 12:03:46,339 - memory_profile6_log - INFO -    189    542.6 MiB      4.0 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-29 12:03:46,339 - memory_profile6_log - INFO -    190                             

2018-04-29 12:03:46,342 - memory_profile6_log - INFO -    191                                 # ~ set threshold to filter output

2018-04-29 12:03:46,346 - memory_profile6_log - INFO -    192    542.6 MiB      0.0 MiB       if threshold > 0:

2018-04-29 12:03:46,346 - memory_profile6_log - INFO -    193    542.6 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-29 12:03:46,346 - memory_profile6_log - INFO -    194    542.6 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-29 12:03:46,348 - memory_profile6_log - INFO -    195    541.5 MiB     -1.1 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-29 12:03:46,348 - memory_profile6_log - INFO -    196                             

2018-04-29 12:03:46,348 - memory_profile6_log - INFO -    197    541.5 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 12:03:46,349 - memory_profile6_log - INFO -    198    541.5 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-29 12:03:46,351 - memory_profile6_log - INFO -    199                             

2018-04-29 12:03:46,351 - memory_profile6_log - INFO -    200    541.5 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 12:03:46,355 - memory_profile6_log - INFO -    201                             

2018-04-29 12:03:46,355 - memory_profile6_log - INFO -    202    541.5 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-29 12:03:46,357 - memory_profile6_log - INFO -    203    541.5 MiB      0.0 MiB       del df_dut

2018-04-29 12:03:46,357 - memory_profile6_log - INFO -    204    541.5 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-29 12:03:46,357 - memory_profile6_log - INFO -    205    541.5 MiB      0.0 MiB       del df_dt

2018-04-29 12:03:46,358 - memory_profile6_log - INFO -    206    541.5 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-29 12:03:46,358 - memory_profile6_log - INFO -    207    541.5 MiB      0.0 MiB       del df_input

2018-04-29 12:03:46,358 - memory_profile6_log - INFO -    208    541.5 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-29 12:03:46,359 - memory_profile6_log - INFO -    209    532.7 MiB     -8.8 MiB       del df_input_X

2018-04-29 12:03:46,359 - memory_profile6_log - INFO -    210    532.7 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-29 12:03:46,361 - memory_profile6_log - INFO -    211    532.7 MiB      0.0 MiB       del df_current

2018-04-29 12:03:46,361 - memory_profile6_log - INFO -    212    532.7 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-29 12:03:46,361 - memory_profile6_log - INFO -    213    532.7 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-29 12:03:46,361 - memory_profile6_log - INFO -    214    532.7 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-29 12:03:46,361 - memory_profile6_log - INFO -    215    509.4 MiB    -23.3 MiB       del model_fit

2018-04-29 12:03:46,365 - memory_profile6_log - INFO -    216    509.4 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-29 12:03:46,371 - memory_profile6_log - INFO -    217    509.4 MiB      0.0 MiB       del result

2018-04-29 12:03:46,371 - memory_profile6_log - INFO -    218    509.4 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-29 12:03:46,371 - memory_profile6_log - INFO -    219                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 12:03:46,372 - memory_profile6_log - INFO -    220                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 12:03:46,375 - memory_profile6_log - INFO -    221    509.4 MiB      0.0 MiB       if savetrain:

2018-04-29 12:03:46,377 - memory_profile6_log - INFO -    222    514.9 MiB      5.4 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-29 12:03:46,378 - memory_profile6_log - INFO -    223    514.9 MiB      0.0 MiB           del model_transform

2018-04-29 12:03:46,378 - memory_profile6_log - INFO -    224    514.9 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-29 12:03:46,378 - memory_profile6_log - INFO -    225    514.9 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 12:03:46,380 - memory_profile6_log - INFO -    226                             

2018-04-29 12:03:46,381 - memory_profile6_log - INFO -    227    514.9 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-29 12:03:46,381 - memory_profile6_log - INFO -    228                                     # ~ Place your code to save the training model here ~

2018-04-29 12:03:46,381 - memory_profile6_log - INFO -    229    514.9 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-29 12:03:46,382 - memory_profile6_log - INFO -    230    514.9 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-29 12:03:46,382 - memory_profile6_log - INFO -    231    514.9 MiB      0.0 MiB               if multproc:

2018-04-29 12:03:46,385 - memory_profile6_log - INFO -    232    492.8 MiB    -22.1 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-29 12:03:46,387 - memory_profile6_log - INFO -    233                             

2018-04-29 12:03:46,388 - memory_profile6_log - INFO -    234    492.8 MiB      0.0 MiB                   logger.info("Saving fitted_models as history...")

2018-04-29 12:03:46,390 - memory_profile6_log - INFO -    235    492.8 MiB      0.0 MiB                   save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 12:03:46,390 - memory_profile6_log - INFO -    236    506.4 MiB     13.7 MiB                   fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-29 12:03:46,391 - memory_profile6_log - INFO -    237                             

2018-04-29 12:03:46,392 - memory_profile6_log - INFO -    238    511.9 MiB      5.5 MiB                   X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-29 12:03:46,394 - memory_profile6_log - INFO -    239    511.9 MiB      0.0 MiB                   logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-29 12:03:46,394 - memory_profile6_log - INFO -    240    513.4 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 12:03:46,397 - memory_profile6_log - INFO -    241    513.4 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 12:03:46,398 - memory_profile6_log - INFO -    242    513.4 MiB      1.5 MiB                       mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-29 12:03:46,398 - memory_profile6_log - INFO -    243                             

2018-04-29 12:03:46,400 - memory_profile6_log - INFO -    244    512.3 MiB     -1.2 MiB                   del X_split

2018-04-29 12:03:46,401 - memory_profile6_log - INFO -    245    512.3 MiB      0.0 MiB                   del BR

2018-04-29 12:03:46,401 - memory_profile6_log - INFO -    246    512.3 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-29 12:03:46,403 - memory_profile6_log - INFO -    247                             

2018-04-29 12:03:46,403 - memory_profile6_log - INFO -    248    512.3 MiB      0.0 MiB                   del save_sigma_nt

2018-04-29 12:03:46,404 - memory_profile6_log - INFO -    249    512.3 MiB      0.0 MiB                   logger.info("deleting save_sigma_nt...")

2018-04-29 12:03:46,404 - memory_profile6_log - INFO -    250                                         else:

2018-04-29 12:03:46,404 - memory_profile6_log - INFO -    251                                             mh.saveDatastore(model_transformsv)

2018-04-29 12:03:46,407 - memory_profile6_log - INFO -    252                                             

2018-04-29 12:03:46,408 - memory_profile6_log - INFO -    253                                     elif str(saveto).lower() == "elastic":

2018-04-29 12:03:46,411 - memory_profile6_log - INFO -    254                                         logger.info("Using ElasticSearch as storage...")

2018-04-29 12:03:46,411 - memory_profile6_log - INFO -    255                                         mh.saveElasticS(model_transformsv)

2018-04-29 12:03:46,413 - memory_profile6_log - INFO -    256                             

2018-04-29 12:03:46,413 - memory_profile6_log - INFO -    257                                     # need save sigma_nt for daily train

2018-04-29 12:03:46,414 - memory_profile6_log - INFO -    258                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-29 12:03:46,414 - memory_profile6_log - INFO -    259                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-29 12:03:46,414 - memory_profile6_log - INFO -    260    512.3 MiB      0.0 MiB           if start_date and end_date:

2018-04-29 12:03:46,415 - memory_profile6_log - INFO -    261                                         if not fitby_sigmant:

2018-04-29 12:03:46,418 - memory_profile6_log - INFO -    262                                             logging.info("Saving sigma Nt...")

2018-04-29 12:03:46,418 - memory_profile6_log - INFO -    263                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 12:03:46,421 - memory_profile6_log - INFO -    264                                             save_sigma_nt['start_date'] = start_date

2018-04-29 12:03:46,421 - memory_profile6_log - INFO -    265                                             save_sigma_nt['end_date'] = end_date

2018-04-29 12:03:46,421 - memory_profile6_log - INFO -    266                                             print save_sigma_nt.head(5)

2018-04-29 12:03:46,421 - memory_profile6_log - INFO -    267                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-29 12:03:46,423 - memory_profile6_log - INFO -    268                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-29 12:03:46,423 - memory_profile6_log - INFO -    269    512.3 MiB      0.0 MiB       return

2018-04-29 12:03:46,423 - memory_profile6_log - INFO - 


2018-04-29 12:03:46,424 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-29 12:23:00,022 - memory_profile6_log - INFO - Generating date range with N: 6
2018-04-29 12:23:00,025 - memory_profile6_log - INFO - date_generated: 
2018-04-29 12:23:00,025 - memory_profile6_log - INFO -  
2018-04-29 12:23:00,025 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 12:23:00,025 - memory_profile6_log - INFO - 

2018-04-29 12:23:00,026 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 12:23:00,026 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 12:23:00,026 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 12:23:00,151 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 12:23:00,154 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 12:24:56,684 - memory_profile6_log - INFO - Generating date range with N: 6
2018-04-29 12:24:56,687 - memory_profile6_log - INFO - date_generated: 
2018-04-29 12:24:56,687 - memory_profile6_log - INFO -  
2018-04-29 12:24:56,688 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0), datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 12:24:56,688 - memory_profile6_log - INFO - 

2018-04-29 12:24:56,688 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 12:24:56,688 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 12:24:56,690 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 12:24:56,821 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 12:24:56,823 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 12:27:09,190 - memory_profile6_log - INFO - Generating date range with N: 6
2018-04-29 12:27:09,194 - memory_profile6_log - INFO - date_generated: 
2018-04-29 12:27:09,194 - memory_profile6_log - INFO -  
2018-04-29 12:27:09,194 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 12:27:09,194 - memory_profile6_log - INFO - 

2018-04-29 12:27:09,194 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 12:27:09,194 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 12:27:09,196 - memory_profile6_log - INFO - using end date: 2018-04-08
2018-04-29 12:27:09,319 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 12:27:09,322 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 12:27:47,746 - memory_profile6_log - INFO - Generating date range with N: 6
2018-04-29 12:27:47,749 - memory_profile6_log - INFO - date_generated: 
2018-04-29 12:27:47,750 - memory_profile6_log - INFO -  
2018-04-29 12:27:47,750 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 8, 0, 0), datetime.datetime(2018, 4, 9, 0, 0), datetime.datetime(2018, 4, 10, 0, 0), datetime.datetime(2018, 4, 11, 0, 0), datetime.datetime(2018, 4, 12, 0, 0), datetime.datetime(2018, 4, 13, 0, 0), datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 12:27:47,750 - memory_profile6_log - INFO - 

2018-04-29 12:27:47,750 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 12:27:47,750 - memory_profile6_log - INFO - using start date: 2018-04-08 00:00:00
2018-04-29 12:27:47,750 - memory_profile6_log - INFO - using end date: 2018-04-08
2018-04-29 12:27:47,878 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 12:27:47,882 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-08
2018-04-29 12:28:28,931 - memory_profile6_log - INFO - Generating date range with N: 6
2018-04-29 12:28:28,934 - memory_profile6_log - INFO - date_generated: 
2018-04-29 12:28:28,934 - memory_profile6_log - INFO -  
2018-04-29 12:28:28,936 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 8, 0, 0), datetime.datetime(2018, 4, 9, 0, 0), datetime.datetime(2018, 4, 10, 0, 0), datetime.datetime(2018, 4, 11, 0, 0), datetime.datetime(2018, 4, 12, 0, 0), datetime.datetime(2018, 4, 13, 0, 0), datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 12:28:28,937 - memory_profile6_log - INFO - 

2018-04-29 12:28:28,937 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 12:28:28,937 - memory_profile6_log - INFO - using start date: 2018-04-08 00:00:00
2018-04-29 12:28:28,937 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 12:28:29,069 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 12:28:29,072 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-08
2018-04-29 12:30:09,130 - memory_profile6_log - INFO - size of df: 87.23 MB
2018-04-29 12:30:09,134 - memory_profile6_log - INFO - getting total: 344288 training data(genuine interest) for date: 2018-04-08
2018-04-29 12:30:09,217 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 12:30:09,217 - memory_profile6_log - INFO - Appending history data...
2018-04-29 12:30:09,219 - memory_profile6_log - INFO - processing batch-0
2018-04-29 12:30:09,220 - memory_profile6_log - INFO - creating list history data...
2018-04-29 12:30:09,309 - memory_profile6_log - INFO - call history data...
2018-04-29 12:45:21,756 - memory_profile6_log - INFO - Generating date range with N: 6
2018-04-29 12:45:21,759 - memory_profile6_log - INFO - date_generated: 
2018-04-29 12:45:21,759 - memory_profile6_log - INFO -  
2018-04-29 12:45:21,759 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 8, 0, 0), datetime.datetime(2018, 4, 9, 0, 0), datetime.datetime(2018, 4, 10, 0, 0), datetime.datetime(2018, 4, 11, 0, 0), datetime.datetime(2018, 4, 12, 0, 0), datetime.datetime(2018, 4, 13, 0, 0), datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 12:45:21,760 - memory_profile6_log - INFO - 

2018-04-29 12:45:21,760 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 12:45:21,762 - memory_profile6_log - INFO - using start date: 2018-04-08 00:00:00
2018-04-29 12:45:21,762 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 12:45:21,881 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 12:45:21,884 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-08
2018-04-29 12:46:55,066 - memory_profile6_log - INFO - size of df: 87.23 MB
2018-04-29 12:46:55,069 - memory_profile6_log - INFO - getting total: 344288 training data(genuine interest) for date: 2018-04-08
2018-04-29 12:46:55,127 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 12:46:55,128 - memory_profile6_log - INFO - Appending history data...
2018-04-29 12:46:55,128 - memory_profile6_log - INFO - processing batch-0
2018-04-29 12:46:55,130 - memory_profile6_log - INFO - creating list history data...
2018-04-29 12:46:55,217 - memory_profile6_log - INFO - call history data...
2018-04-29 12:48:29,744 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 12:48:31,190 - memory_profile6_log - INFO - processing batch-1
2018-04-29 12:48:31,190 - memory_profile6_log - INFO - creating list history data...
2018-04-29 12:48:31,253 - memory_profile6_log - INFO - call history data...
2018-04-29 12:50:03,388 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 12:50:05,000 - memory_profile6_log - INFO - processing batch-2
2018-04-29 12:50:05,006 - memory_profile6_log - INFO - creating list history data...
2018-04-29 12:50:05,075 - memory_profile6_log - INFO - call history data...
2018-04-29 12:51:41,326 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 12:51:42,759 - memory_profile6_log - INFO - processing batch-3
2018-04-29 12:51:42,760 - memory_profile6_log - INFO - creating list history data...
2018-04-29 12:51:42,826 - memory_profile6_log - INFO - call history data...
2018-04-29 12:53:16,128 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 12:53:17,562 - memory_profile6_log - INFO - processing batch-4
2018-04-29 12:53:17,563 - memory_profile6_log - INFO - creating list history data...
2018-04-29 12:53:17,624 - memory_profile6_log - INFO - call history data...
2018-04-29 12:54:51,486 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 12:54:52,956 - memory_profile6_log - INFO - Appending training data...
2018-04-29 12:54:52,960 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-09
2018-04-29 12:56:39,528 - memory_profile6_log - INFO - size of df: 102.25 MB
2018-04-29 12:56:39,529 - memory_profile6_log - INFO - getting total: 403532 training data(genuine interest) for date: 2018-04-09
2018-04-29 12:56:39,601 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 12:56:39,602 - memory_profile6_log - INFO - Appending history data...
2018-04-29 12:56:39,605 - memory_profile6_log - INFO - processing batch-0
2018-04-29 12:56:39,607 - memory_profile6_log - INFO - creating list history data...
2018-04-29 12:56:39,703 - memory_profile6_log - INFO - call history data...
2018-04-29 12:58:18,256 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 12:58:19,812 - memory_profile6_log - INFO - processing batch-1
2018-04-29 12:58:19,815 - memory_profile6_log - INFO - creating list history data...
2018-04-29 12:58:19,904 - memory_profile6_log - INFO - call history data...
2018-04-29 12:59:56,663 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 12:59:58,253 - memory_profile6_log - INFO - processing batch-2
2018-04-29 12:59:58,255 - memory_profile6_log - INFO - creating list history data...
2018-04-29 12:59:58,342 - memory_profile6_log - INFO - call history data...
2018-04-29 13:01:37,622 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:01:39,217 - memory_profile6_log - INFO - processing batch-3
2018-04-29 13:01:39,219 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:01:39,299 - memory_profile6_log - INFO - call history data...
2018-04-29 13:03:17,069 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:03:18,674 - memory_profile6_log - INFO - processing batch-4
2018-04-29 13:03:18,677 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:03:18,746 - memory_profile6_log - INFO - call history data...
2018-04-29 13:04:57,709 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:04:59,322 - memory_profile6_log - INFO - Appending training data...
2018-04-29 13:04:59,326 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-10
2018-04-29 13:06:53,644 - memory_profile6_log - INFO - size of df: 108.51 MB
2018-04-29 13:06:53,645 - memory_profile6_log - INFO - getting total: 427635 training data(genuine interest) for date: 2018-04-10
2018-04-29 13:06:53,726 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 13:06:53,726 - memory_profile6_log - INFO - Appending history data...
2018-04-29 13:06:53,729 - memory_profile6_log - INFO - processing batch-0
2018-04-29 13:06:53,730 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:06:53,812 - memory_profile6_log - INFO - call history data...
2018-04-29 13:08:37,424 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:08:39,076 - memory_profile6_log - INFO - processing batch-1
2018-04-29 13:08:39,078 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:08:39,170 - memory_profile6_log - INFO - call history data...
2018-04-29 13:11:01,941 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:11:01,944 - memory_profile6_log - INFO - date_generated: 
2018-04-29 13:11:01,944 - memory_profile6_log - INFO -  
2018-04-29 13:11:01,944 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 8, 0, 0), datetime.datetime(2018, 4, 9, 0, 0)]
2018-04-29 13:11:01,944 - memory_profile6_log - INFO - 

2018-04-29 13:11:01,946 - memory_profile6_log - INFO - using current date: 2018-04-29 13:11:01.944000
2018-04-29 13:11:01,946 - memory_profile6_log - INFO - using start date: 2018-04-08 00:00:00
2018-04-29 13:11:01,946 - memory_profile6_log - INFO - using end date: 2018-04-09
2018-04-29 13:11:02,071 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 13:11:02,073 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-08
2018-04-29 13:11:42,326 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:11:42,329 - memory_profile6_log - INFO - date_generated: 
2018-04-29 13:11:42,329 - memory_profile6_log - INFO -  
2018-04-29 13:11:42,329 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 9, 0, 0)]
2018-04-29 13:11:42,329 - memory_profile6_log - INFO - 

2018-04-29 13:11:42,329 - memory_profile6_log - INFO - using current date: 2018-04-29 13:11:42.330000
2018-04-29 13:11:42,331 - memory_profile6_log - INFO - using start date: 2018-04-09 00:00:00
2018-04-29 13:11:42,331 - memory_profile6_log - INFO - using end date: 2018-04-09
2018-04-29 13:11:42,456 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 13:11:42,460 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-09
2018-04-29 13:13:29,351 - memory_profile6_log - INFO - size of df: 102.25 MB
2018-04-29 13:13:29,351 - memory_profile6_log - INFO - getting total: 403532 training data(genuine interest) for date: 2018-04-09
2018-04-29 13:13:29,407 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 13:13:29,408 - memory_profile6_log - INFO - Appending history data...
2018-04-29 13:13:29,410 - memory_profile6_log - INFO - processing batch-0
2018-04-29 13:13:29,411 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:13:29,507 - memory_profile6_log - INFO - call history data...
2018-04-29 13:15:27,878 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:15:29,519 - memory_profile6_log - INFO - processing batch-1
2018-04-29 13:15:29,520 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:15:29,591 - memory_profile6_log - INFO - call history data...
2018-04-29 13:17:11,180 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:17:12,960 - memory_profile6_log - INFO - processing batch-2
2018-04-29 13:17:12,963 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:17:13,046 - memory_profile6_log - INFO - call history data...
2018-04-29 13:18:53,845 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:18:55,714 - memory_profile6_log - INFO - processing batch-3
2018-04-29 13:18:55,717 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:18:55,799 - memory_profile6_log - INFO - call history data...
2018-04-29 13:20:58,088 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:21:00,032 - memory_profile6_log - INFO - processing batch-4
2018-04-29 13:21:00,045 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:21:00,125 - memory_profile6_log - INFO - call history data...
2018-04-29 13:22:37,661 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:22:39,673 - memory_profile6_log - INFO - Appending training data...
2018-04-29 13:22:39,674 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 13:22:39,677 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 13:22:39,694 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 13:22:39,694 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 13:22:39,696 - memory_profile6_log - INFO - ================================================

2018-04-29 13:22:39,697 - memory_profile6_log - INFO -    295     86.7 MiB     86.7 MiB   @profile

2018-04-29 13:22:39,698 - memory_profile6_log - INFO -    296                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 13:22:39,700 - memory_profile6_log - INFO -    297     86.7 MiB      0.0 MiB       bq_client = client

2018-04-29 13:22:39,701 - memory_profile6_log - INFO -    298     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 13:22:39,703 - memory_profile6_log - INFO -    299                             

2018-04-29 13:22:39,706 - memory_profile6_log - INFO -    300     86.7 MiB      0.0 MiB       datalist = []

2018-04-29 13:22:39,707 - memory_profile6_log - INFO -    301     86.7 MiB      0.0 MiB       datalist_hist = []

2018-04-29 13:22:39,709 - memory_profile6_log - INFO -    302                             

2018-04-29 13:22:39,710 - memory_profile6_log - INFO -    303     86.7 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 13:22:39,710 - memory_profile6_log - INFO -    304    654.5 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 13:22:39,713 - memory_profile6_log - INFO -    305    393.1 MiB    306.4 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 13:22:39,713 - memory_profile6_log - INFO -    306    393.1 MiB      0.0 MiB           if tframe is not None:

2018-04-29 13:22:39,716 - memory_profile6_log - INFO -    307    393.1 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 13:22:39,717 - memory_profile6_log - INFO -    308    405.7 MiB     12.5 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 13:22:39,719 - memory_profile6_log - INFO -    309    405.7 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 13:22:39,720 - memory_profile6_log - INFO -    310    405.7 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 13:22:39,720 - memory_profile6_log - INFO -    311    654.5 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 13:22:39,721 - memory_profile6_log - INFO -    312                                                 # ~ loading history

2018-04-29 13:22:39,723 - memory_profile6_log - INFO -    313                                                 """

2018-04-29 13:22:39,723 - memory_profile6_log - INFO -    314                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 13:22:39,726 - memory_profile6_log - INFO -    315                                                 """

2018-04-29 13:22:39,729 - memory_profile6_log - INFO -    316    629.5 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 13:22:39,730 - memory_profile6_log - INFO -    317                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 13:22:39,730 - memory_profile6_log - INFO -    318    629.5 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 13:22:39,732 - memory_profile6_log - INFO -    319                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 13:22:39,733 - memory_profile6_log - INFO -    320    630.2 MiB      5.4 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 13:22:39,733 - memory_profile6_log - INFO -    321                             

2018-04-29 13:22:39,736 - memory_profile6_log - INFO -    322    630.2 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 13:22:39,737 - memory_profile6_log - INFO -    323    704.4 MiB    527.6 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 13:22:39,739 - memory_profile6_log - INFO -    324                             

2018-04-29 13:22:39,740 - memory_profile6_log - INFO -    325                                                 # me = os.getpid()

2018-04-29 13:22:39,740 - memory_profile6_log - INFO -    326                                                 # kill_proc_tree(me)

2018-04-29 13:22:39,743 - memory_profile6_log - INFO -    327                             

2018-04-29 13:22:39,743 - memory_profile6_log - INFO -    328    704.4 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 13:22:39,744 - memory_profile6_log - INFO -    329    706.4 MiB     -0.7 MiB                       for m in h_frame:

2018-04-29 13:22:39,747 - memory_profile6_log - INFO -    330    706.4 MiB     -0.7 MiB                           if m is not None:

2018-04-29 13:22:39,749 - memory_profile6_log - INFO -    331    706.4 MiB     -0.7 MiB                               if len(m) > 0:

2018-04-29 13:22:39,750 - memory_profile6_log - INFO -    332    706.4 MiB      8.9 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 13:22:39,750 - memory_profile6_log - INFO -    333    654.5 MiB   -293.6 MiB                       del h_frame

2018-04-29 13:22:39,752 - memory_profile6_log - INFO -    334    654.5 MiB      0.0 MiB                       del lhistory

2018-04-29 13:22:39,753 - memory_profile6_log - INFO -    335                             

2018-04-29 13:22:39,753 - memory_profile6_log - INFO -    336    654.5 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 13:22:39,755 - memory_profile6_log - INFO -    337    654.5 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 13:22:39,756 - memory_profile6_log - INFO -    338                                     else: 

2018-04-29 13:22:39,757 - memory_profile6_log - INFO -    339                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 13:22:39,759 - memory_profile6_log - INFO -    340    654.5 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 13:22:39,759 - memory_profile6_log - INFO -    341    654.5 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 13:22:39,762 - memory_profile6_log - INFO -    342                             

2018-04-29 13:22:39,762 - memory_profile6_log - INFO -    343    654.5 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 13:22:39,763 - memory_profile6_log - INFO - 


2018-04-29 13:22:41,141 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 13:22:41,220 - memory_profile6_log - INFO -     p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0    0.001543        1239.605475       485            1249.605475  10960288  1628aad345040e-0b2b0baa7aec7f8-4c322073-e1000-...
1    0.004470          72.776171       168              82.776171  22291119  1613bc4a7ac73-0af9eb163e2375-7d65040b-38400-16...
2    0.001543         384.718661       181             394.718661  10960288  1618a2dbbb9f-0da12bda3-133f7e20-38400-1618a2db...
3    0.000088        2407.461735        14            2417.461735  11911567  1613a83cda155-0c2edb2c45dedd-310a717d-38400-16...
4    0.001543         289.338273       228             299.338273  10960288  1622e1ec6ff15e-0b6bf723a0fb21-8030802-38400-16...
5    0.000088        2637.740683        69            2647.740683  11911567  1616b2c0b4c2d2-0db3e1132c1133-547f0e1a-4a640-1...
6    0.001543         155.294553       413             165.294553  10960288  1612d25f2726b-07eb9362fc5d22-482e0a73-38400-16...
7    0.000088        6740.892857        25            6750.892857  11911567  1618d8380e24a-0af4a6412042c5-1e46677b-38400-16...
8    0.000088        2712.798345        82            2722.798345  11911567  161476433b2a5-0855276a0c9606-167c2c34-38400-16...
9    0.001543        1083.999727        71            1093.999727  10960288  16298ec6d04561-0d34c035b60989-51693374-140000-...
10   0.000088         869.792627       341             879.792627  11911567  1628b5101bd170-0aaa1a9ed98fb4-2f233868-4df28-1...
11   0.001543         238.595157      1129             248.595157  10960288  1610cb3a8c1348-0c7cec1541e7ac-4323461-100200-1...
12   0.001543         144.669136       114             154.669136  10960288  16130c0d6563b8-091947d71b8641-76313118-3d10d-1...
13   0.001543         509.021036        72             519.021036  10960288  1611c4ae4c2239-0fd14eaf0808ba-5768397b-100200-...
14   0.004470         167.484887        73             177.484887  22291119  16137d86cfc93-0a4e58c10a7965-39626377-38400-16...
15   0.000088        3435.617685      2635            3445.617685  11911567  16116a35abf7b-02ebd04ab0ab75-4323461-100200-16...
16   0.001543        1837.258279       275            1847.258279  10960288  1627fb69adcf-0580e12c76435f-72203a15-38400-162...
17   0.000088        6740.892857        15            6750.892857  11911567  162294766a319c-0d16b05bb32617-18741b1f-38400-1...
18   0.001543        1610.180397        59            1620.180397  10960288  161f36169fce02-03becb8d4c95b1-b353461-15f900-1...
19   0.000088         802.487245        42             812.487245  11911567  161452195ee549-0bb068a0c-4f2c7b27-3d10d-161452...
2018-04-29 13:22:41,223 - memory_profile6_log - INFO - 

2018-04-29 13:22:41,365 - memory_profile6_log - INFO - size of big_frame_hist: 111.10 MB
2018-04-29 13:22:41,484 - memory_profile6_log - INFO - size of big_frame: 102.25 MB
2018-04-29 13:22:41,503 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-10
2018-04-29 13:31:32,665 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:31:32,668 - memory_profile6_log - INFO - date_generated: 
2018-04-29 13:31:32,668 - memory_profile6_log - INFO -  
2018-04-29 13:31:32,670 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 28, 13, 31, 32, 669000)]
2018-04-29 13:31:32,671 - memory_profile6_log - INFO - 

2018-04-29 13:31:32,671 - memory_profile6_log - INFO - using current date: 2018-04-29 13:31:32.669000
2018-04-29 13:31:32,671 - memory_profile6_log - INFO - using start date: 2018-04-28 00:00:00
2018-04-29 13:31:32,673 - memory_profile6_log - INFO - using end date: 2018-04-28
2018-04-29 13:31:32,803 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 13:31:32,809 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-28
2018-04-29 13:31:35,388 - memory_profile6_log - INFO - size of df: 24.0 Byte
2018-04-29 13:31:35,390 - memory_profile6_log - INFO - 2018-04-28 data is empty!
2018-04-29 13:31:35,391 - memory_profile6_log - INFO - tframe for date: 2018-04-28 is empty
2018-04-29 13:31:35,392 - memory_profile6_log - INFO - len datalist: 0
2018-04-29 13:31:35,394 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 13:31:35,397 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 13:31:35,397 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 13:31:35,398 - memory_profile6_log - INFO - ================================================

2018-04-29 13:31:35,401 - memory_profile6_log - INFO -    295     86.7 MiB     86.7 MiB   @profile

2018-04-29 13:31:35,401 - memory_profile6_log - INFO -    296                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 13:31:35,403 - memory_profile6_log - INFO -    297     86.7 MiB      0.0 MiB       bq_client = client

2018-04-29 13:31:35,404 - memory_profile6_log - INFO -    298     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 13:31:35,404 - memory_profile6_log - INFO -    299                             

2018-04-29 13:31:35,405 - memory_profile6_log - INFO -    300     86.7 MiB      0.0 MiB       datalist = []

2018-04-29 13:31:35,407 - memory_profile6_log - INFO -    301     86.7 MiB      0.0 MiB       datalist_hist = []

2018-04-29 13:31:35,407 - memory_profile6_log - INFO -    302                             

2018-04-29 13:31:35,411 - memory_profile6_log - INFO -    303     86.7 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 13:31:35,413 - memory_profile6_log - INFO -    304     90.2 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 13:31:35,414 - memory_profile6_log - INFO -    305     90.2 MiB      3.5 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 13:31:35,414 - memory_profile6_log - INFO -    306     90.2 MiB      0.0 MiB           if tframe is not None:

2018-04-29 13:31:35,415 - memory_profile6_log - INFO -    307                                         if not tframe.empty:

2018-04-29 13:31:35,417 - memory_profile6_log - INFO -    308                                             X_split = np.array_split(tframe, 5)

2018-04-29 13:31:35,417 - memory_profile6_log - INFO -    309                                             logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 13:31:35,417 - memory_profile6_log - INFO -    310                                             logger.info("Appending history data...")

2018-04-29 13:31:35,421 - memory_profile6_log - INFO -    311                                             for ix in range(len(X_split)):

2018-04-29 13:31:35,421 - memory_profile6_log - INFO -    312                                                 # ~ loading history

2018-04-29 13:31:35,423 - memory_profile6_log - INFO -    313                                                 """

2018-04-29 13:31:35,424 - memory_profile6_log - INFO -    314                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 13:31:35,424 - memory_profile6_log - INFO -    315                                                 """

2018-04-29 13:31:35,426 - memory_profile6_log - INFO -    316                                                 logger.info("processing batch-%d", ix)

2018-04-29 13:31:35,427 - memory_profile6_log - INFO -    317                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 13:31:35,427 - memory_profile6_log - INFO -    318                                                 logger.info("creating list history data...")

2018-04-29 13:31:35,427 - memory_profile6_log - INFO -    319                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 13:31:35,430 - memory_profile6_log - INFO -    320                                                 lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 13:31:35,431 - memory_profile6_log - INFO -    321                             

2018-04-29 13:31:35,433 - memory_profile6_log - INFO -    322                                                 logger.info("call history data...")

2018-04-29 13:31:35,434 - memory_profile6_log - INFO -    323                                                 h_frame = mh.loadDSHistory(lhistory)

2018-04-29 13:31:35,434 - memory_profile6_log - INFO -    324                             

2018-04-29 13:31:35,436 - memory_profile6_log - INFO -    325                                                 # me = os.getpid()

2018-04-29 13:31:35,437 - memory_profile6_log - INFO -    326                                                 # kill_proc_tree(me)

2018-04-29 13:31:35,438 - memory_profile6_log - INFO -    327                             

2018-04-29 13:31:35,440 - memory_profile6_log - INFO -    328                                                 logger.info("done collecting history data, appending now...")

2018-04-29 13:31:35,443 - memory_profile6_log - INFO -    329                                                 for m in h_frame:

2018-04-29 13:31:35,444 - memory_profile6_log - INFO -    330                                                     if m is not None:

2018-04-29 13:31:35,444 - memory_profile6_log - INFO -    331                                                         if len(m) > 0:

2018-04-29 13:31:35,447 - memory_profile6_log - INFO -    332                                                             datalist_hist.append(pd.DataFrame(m))

2018-04-29 13:31:35,447 - memory_profile6_log - INFO -    333                                                 del h_frame

2018-04-29 13:31:35,448 - memory_profile6_log - INFO -    334                                                 del lhistory

2018-04-29 13:31:35,450 - memory_profile6_log - INFO -    335                             

2018-04-29 13:31:35,450 - memory_profile6_log - INFO -    336                                             logger.info("Appending training data...")

2018-04-29 13:31:35,450 - memory_profile6_log - INFO -    337                                             datalist.append(tframe)

2018-04-29 13:31:35,451 - memory_profile6_log - INFO -    338                                     else: 

2018-04-29 13:31:35,454 - memory_profile6_log - INFO -    339     90.2 MiB      0.0 MiB               logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 13:31:35,456 - memory_profile6_log - INFO -    340     90.2 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 13:31:35,457 - memory_profile6_log - INFO -    341     90.2 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 13:31:35,457 - memory_profile6_log - INFO -    342                             

2018-04-29 13:31:35,457 - memory_profile6_log - INFO -    343     90.2 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 13:31:35,459 - memory_profile6_log - INFO - 


2018-04-29 13:31:35,460 - memory_profile6_log - INFO - Training cannot be empty..
2018-04-29 13:31:35,460 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 13:31:35,460 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 13:31:35,461 - memory_profile6_log - INFO - ================================================

2018-04-29 13:31:35,464 - memory_profile6_log - INFO -    345     86.6 MiB     86.6 MiB   @profile

2018-04-29 13:31:35,464 - memory_profile6_log - INFO -    346                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 13:31:35,467 - memory_profile6_log - INFO -    347     86.7 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 13:31:35,467 - memory_profile6_log - INFO -    348     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 13:31:35,467 - memory_profile6_log - INFO -    349                             

2018-04-29 13:31:35,467 - memory_profile6_log - INFO -    350                                 # ~~~ Begin collecting data ~~~

2018-04-29 13:31:35,469 - memory_profile6_log - INFO -    351     86.7 MiB      0.0 MiB       t0 = time.time()

2018-04-29 13:31:35,470 - memory_profile6_log - INFO -    352     90.2 MiB      3.6 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 13:31:35,470 - memory_profile6_log - INFO -    353     90.2 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 13:31:35,471 - memory_profile6_log - INFO -    354     90.2 MiB      0.0 MiB           logger.info("Training cannot be empty..")

2018-04-29 13:31:35,473 - memory_profile6_log - INFO -    355     90.2 MiB      0.0 MiB           return False

2018-04-29 13:31:35,473 - memory_profile6_log - INFO -    356                                 big_frame_hist = pd.concat(datalist_hist)

2018-04-29 13:31:35,473 - memory_profile6_log - INFO -    357                                 print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 13:31:35,476 - memory_profile6_log - INFO -    358                                 logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 13:31:35,476 - memory_profile6_log - INFO -    359                             

2018-04-29 13:31:35,477 - memory_profile6_log - INFO -    360                                 big_frame = pd.concat(datalist)

2018-04-29 13:31:35,477 - memory_profile6_log - INFO -    361                                 logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 13:31:35,480 - memory_profile6_log - INFO -    362                                 del datalist

2018-04-29 13:31:35,480 - memory_profile6_log - INFO -    363                             

2018-04-29 13:31:35,480 - memory_profile6_log - INFO -    364                                 big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 13:31:35,482 - memory_profile6_log - INFO -    365                             

2018-04-29 13:31:35,482 - memory_profile6_log - INFO -    366                                 # ~ get current news interest ~

2018-04-29 13:31:35,483 - memory_profile6_log - INFO -    367                                 if not cd:

2018-04-29 13:31:35,483 - memory_profile6_log - INFO -    368                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 13:31:35,483 - memory_profile6_log - INFO -    369                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 13:31:35,486 - memory_profile6_log - INFO -    370                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 13:31:35,487 - memory_profile6_log - INFO -    371                                 else:

2018-04-29 13:31:35,490 - memory_profile6_log - INFO -    372                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 13:31:35,490 - memory_profile6_log - INFO -    373                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 13:31:35,490 - memory_profile6_log - INFO -    374                             

2018-04-29 13:31:35,492 - memory_profile6_log - INFO -    375                                     # safe handling of query parameter

2018-04-29 13:31:35,492 - memory_profile6_log - INFO -    376                                     query_params = [

2018-04-29 13:31:35,493 - memory_profile6_log - INFO -    377                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 13:31:35,493 - memory_profile6_log - INFO -    378                                     ]

2018-04-29 13:31:35,494 - memory_profile6_log - INFO -    379                             

2018-04-29 13:31:35,494 - memory_profile6_log - INFO -    380                                     job_config.query_parameters = query_params

2018-04-29 13:31:35,500 - memory_profile6_log - INFO -    381                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 13:31:35,500 - memory_profile6_log - INFO -    382                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 13:31:35,502 - memory_profile6_log - INFO -    383                             

2018-04-29 13:31:35,503 - memory_profile6_log - INFO -    384                                 current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 13:31:35,503 - memory_profile6_log - INFO -    385                                 current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 13:31:35,503 - memory_profile6_log - INFO -    386                                                                        format='%Y-%m-%d', errors='coerce')

2018-04-29 13:31:35,506 - memory_profile6_log - INFO -    387                                 logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 13:31:35,506 - memory_profile6_log - INFO -    388                                 train_time = time.time() - t0

2018-04-29 13:31:35,506 - memory_profile6_log - INFO -    389                                 logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 13:31:35,506 - memory_profile6_log - INFO -    390                             

2018-04-29 13:31:35,506 - memory_profile6_log - INFO -    391                                 return big_frame, current_frame, big_frame_hist

2018-04-29 13:31:35,507 - memory_profile6_log - INFO - 


2018-04-29 13:31:35,507 - memory_profile6_log - INFO - Train return False, please cek your data availability
2018-04-29 13:32:09,723 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:32:09,726 - memory_profile6_log - INFO - date_generated: 
2018-04-29 13:32:09,726 - memory_profile6_log - INFO -  
2018-04-29 13:32:09,727 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 28, 13, 32, 9, 727000)]
2018-04-29 13:32:09,727 - memory_profile6_log - INFO - 

2018-04-29 13:32:09,729 - memory_profile6_log - INFO - using current date: 2018-04-29 13:32:09.727000
2018-04-29 13:32:09,729 - memory_profile6_log - INFO - using start date: 2018-04-28 00:00:00
2018-04-29 13:32:09,730 - memory_profile6_log - INFO - using end date: 2018-04-28
2018-04-29 13:32:09,871 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 13:32:09,875 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-28
2018-04-29 13:32:13,559 - memory_profile6_log - INFO - size of df: 24.0 Byte
2018-04-29 13:32:13,561 - memory_profile6_log - INFO - 2018-04-28 data is empty!
2018-04-29 13:32:13,562 - memory_profile6_log - INFO - tframe for date: 2018-04-28 is empty
2018-04-29 13:32:13,562 - memory_profile6_log - INFO - len datalist: 0
2018-04-29 13:32:13,565 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 13:32:13,565 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 13:32:13,566 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 13:32:13,568 - memory_profile6_log - INFO - ================================================

2018-04-29 13:32:13,569 - memory_profile6_log - INFO -    295     86.4 MiB     86.4 MiB   @profile

2018-04-29 13:32:13,569 - memory_profile6_log - INFO -    296                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 13:32:13,569 - memory_profile6_log - INFO -    297     86.4 MiB      0.0 MiB       bq_client = client

2018-04-29 13:32:13,571 - memory_profile6_log - INFO -    298     86.4 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 13:32:13,572 - memory_profile6_log - INFO -    299                             

2018-04-29 13:32:13,572 - memory_profile6_log - INFO -    300     86.4 MiB      0.0 MiB       datalist = []

2018-04-29 13:32:13,572 - memory_profile6_log - INFO -    301     86.4 MiB      0.0 MiB       datalist_hist = []

2018-04-29 13:32:13,573 - memory_profile6_log - INFO -    302                             

2018-04-29 13:32:13,573 - memory_profile6_log - INFO -    303     86.4 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 13:32:13,575 - memory_profile6_log - INFO -    304     90.1 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 13:32:13,575 - memory_profile6_log - INFO -    305     90.1 MiB      3.7 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 13:32:13,576 - memory_profile6_log - INFO -    306     90.1 MiB      0.0 MiB           if tframe is not None:

2018-04-29 13:32:13,578 - memory_profile6_log - INFO -    307                                         if not tframe.empty:

2018-04-29 13:32:13,578 - memory_profile6_log - INFO -    308                                             X_split = np.array_split(tframe, 5)

2018-04-29 13:32:13,579 - memory_profile6_log - INFO -    309                                             logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 13:32:13,581 - memory_profile6_log - INFO -    310                                             logger.info("Appending history data...")

2018-04-29 13:32:13,582 - memory_profile6_log - INFO -    311                                             for ix in range(len(X_split)):

2018-04-29 13:32:13,582 - memory_profile6_log - INFO -    312                                                 # ~ loading history

2018-04-29 13:32:13,584 - memory_profile6_log - INFO -    313                                                 """

2018-04-29 13:32:13,585 - memory_profile6_log - INFO -    314                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 13:32:13,586 - memory_profile6_log - INFO -    315                                                 """

2018-04-29 13:32:13,589 - memory_profile6_log - INFO -    316                                                 logger.info("processing batch-%d", ix)

2018-04-29 13:32:13,592 - memory_profile6_log - INFO -    317                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 13:32:13,592 - memory_profile6_log - INFO -    318                                                 logger.info("creating list history data...")

2018-04-29 13:32:13,595 - memory_profile6_log - INFO -    319                                                 lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 13:32:13,595 - memory_profile6_log - INFO -    320                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 13:32:13,596 - memory_profile6_log - INFO -    321                             

2018-04-29 13:32:13,598 - memory_profile6_log - INFO -    322                                                 logger.info("call history data...")

2018-04-29 13:32:13,601 - memory_profile6_log - INFO -    323                                                 h_frame = mh.loadDSHistory(lhistory)

2018-04-29 13:32:13,602 - memory_profile6_log - INFO -    324                             

2018-04-29 13:32:13,604 - memory_profile6_log - INFO -    325                                                 # me = os.getpid()

2018-04-29 13:32:13,605 - memory_profile6_log - INFO -    326                                                 # kill_proc_tree(me)

2018-04-29 13:32:13,607 - memory_profile6_log - INFO -    327                             

2018-04-29 13:32:13,607 - memory_profile6_log - INFO -    328                                                 logger.info("done collecting history data, appending now...")

2018-04-29 13:32:13,608 - memory_profile6_log - INFO -    329                                                 for m in h_frame:

2018-04-29 13:32:13,608 - memory_profile6_log - INFO -    330                                                     if m is not None:

2018-04-29 13:32:13,611 - memory_profile6_log - INFO -    331                                                         if len(m) > 0:

2018-04-29 13:32:13,614 - memory_profile6_log - INFO -    332                                                             datalist_hist.append(pd.DataFrame(m))

2018-04-29 13:32:13,615 - memory_profile6_log - INFO -    333                                                 del h_frame

2018-04-29 13:32:13,615 - memory_profile6_log - INFO -    334                                                 del lhistory

2018-04-29 13:32:13,615 - memory_profile6_log - INFO -    335                             

2018-04-29 13:32:13,617 - memory_profile6_log - INFO -    336                                             logger.info("Appending training data...")

2018-04-29 13:32:13,618 - memory_profile6_log - INFO -    337                                             datalist.append(tframe)

2018-04-29 13:32:13,619 - memory_profile6_log - INFO -    338                                     else: 

2018-04-29 13:32:13,619 - memory_profile6_log - INFO -    339     90.1 MiB      0.0 MiB               logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 13:32:13,622 - memory_profile6_log - INFO -    340     90.1 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 13:32:13,624 - memory_profile6_log - INFO -    341     90.1 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 13:32:13,625 - memory_profile6_log - INFO -    342                             

2018-04-29 13:32:13,625 - memory_profile6_log - INFO -    343     90.1 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 13:32:13,628 - memory_profile6_log - INFO - 


2018-04-29 13:32:13,628 - memory_profile6_log - INFO - Training cannot be empty..
2018-04-29 13:32:13,628 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 13:32:13,628 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 13:32:13,630 - memory_profile6_log - INFO - ================================================

2018-04-29 13:32:13,630 - memory_profile6_log - INFO -    345     86.2 MiB     86.2 MiB   @profile

2018-04-29 13:32:13,631 - memory_profile6_log - INFO -    346                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 13:32:13,631 - memory_profile6_log - INFO -    347     86.4 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 13:32:13,631 - memory_profile6_log - INFO -    348     86.4 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 13:32:13,634 - memory_profile6_log - INFO -    349                             

2018-04-29 13:32:13,634 - memory_profile6_log - INFO -    350                                 # ~~~ Begin collecting data ~~~

2018-04-29 13:32:13,637 - memory_profile6_log - INFO -    351     86.4 MiB      0.0 MiB       t0 = time.time()

2018-04-29 13:32:13,638 - memory_profile6_log - INFO -    352     90.1 MiB      3.8 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 13:32:13,638 - memory_profile6_log - INFO -    353     90.1 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 13:32:13,638 - memory_profile6_log - INFO -    354     90.1 MiB      0.0 MiB           logger.info("Training cannot be empty..")

2018-04-29 13:32:13,638 - memory_profile6_log - INFO -    355     90.1 MiB      0.0 MiB           return False

2018-04-29 13:32:13,638 - memory_profile6_log - INFO -    356                                 big_frame_hist = pd.concat(datalist_hist)

2018-04-29 13:32:13,640 - memory_profile6_log - INFO -    357                                 print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 13:32:13,640 - memory_profile6_log - INFO -    358                                 logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 13:32:13,641 - memory_profile6_log - INFO -    359                             

2018-04-29 13:32:13,641 - memory_profile6_log - INFO -    360                                 big_frame = pd.concat(datalist)

2018-04-29 13:32:13,641 - memory_profile6_log - INFO -    361                                 logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 13:32:13,641 - memory_profile6_log - INFO -    362                                 del datalist

2018-04-29 13:32:13,641 - memory_profile6_log - INFO -    363                             

2018-04-29 13:32:13,648 - memory_profile6_log - INFO -    364                                 big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 13:32:13,651 - memory_profile6_log - INFO -    365                             

2018-04-29 13:32:13,653 - memory_profile6_log - INFO -    366                                 # ~ get current news interest ~

2018-04-29 13:32:13,654 - memory_profile6_log - INFO -    367                                 if not cd:

2018-04-29 13:32:13,657 - memory_profile6_log - INFO -    368                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 13:32:13,657 - memory_profile6_log - INFO -    369                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 13:32:13,658 - memory_profile6_log - INFO -    370                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 13:32:13,660 - memory_profile6_log - INFO -    371                                 else:

2018-04-29 13:32:13,661 - memory_profile6_log - INFO -    372                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 13:32:13,661 - memory_profile6_log - INFO -    373                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 13:32:13,663 - memory_profile6_log - INFO -    374                             

2018-04-29 13:32:13,664 - memory_profile6_log - INFO -    375                                     # safe handling of query parameter

2018-04-29 13:32:13,664 - memory_profile6_log - INFO -    376                                     query_params = [

2018-04-29 13:32:13,664 - memory_profile6_log - INFO -    377                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 13:32:13,667 - memory_profile6_log - INFO -    378                                     ]

2018-04-29 13:32:13,668 - memory_profile6_log - INFO -    379                             

2018-04-29 13:32:13,670 - memory_profile6_log - INFO -    380                                     job_config.query_parameters = query_params

2018-04-29 13:32:13,671 - memory_profile6_log - INFO -    381                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 13:32:13,671 - memory_profile6_log - INFO -    382                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 13:32:13,671 - memory_profile6_log - INFO -    383                             

2018-04-29 13:32:13,671 - memory_profile6_log - INFO -    384                                 current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 13:32:13,673 - memory_profile6_log - INFO -    385                                 current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 13:32:13,673 - memory_profile6_log - INFO -    386                                                                        format='%Y-%m-%d', errors='coerce')

2018-04-29 13:32:13,673 - memory_profile6_log - INFO -    387                                 logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 13:32:13,674 - memory_profile6_log - INFO -    388                                 train_time = time.time() - t0

2018-04-29 13:32:13,674 - memory_profile6_log - INFO -    389                                 logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 13:32:13,674 - memory_profile6_log - INFO -    390                             

2018-04-29 13:32:13,674 - memory_profile6_log - INFO -    391                                 return big_frame, current_frame, big_frame_hist

2018-04-29 13:32:13,680 - memory_profile6_log - INFO - 


2018-04-29 13:32:13,681 - memory_profile6_log - INFO - Train return False, please cek your data availability
2018-04-29 13:32:20,326 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:32:20,329 - memory_profile6_log - INFO - date_generated: 
2018-04-29 13:32:20,329 - memory_profile6_log - INFO -  
2018-04-29 13:32:20,331 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 28, 13, 32, 20, 330000)]
2018-04-29 13:32:20,332 - memory_profile6_log - INFO - 

2018-04-29 13:32:20,332 - memory_profile6_log - INFO - using current date: 2018-04-29 13:32:20.330000
2018-04-29 13:32:20,332 - memory_profile6_log - INFO - using start date: 2018-04-28 00:00:00
2018-04-29 13:32:20,334 - memory_profile6_log - INFO - using end date: 2018-04-28
2018-04-29 13:32:20,461 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 13:32:20,466 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-28
2018-04-29 13:32:24,036 - memory_profile6_log - INFO - size of df: 24.0 Byte
2018-04-29 13:32:24,038 - memory_profile6_log - INFO - 2018-04-28 data is empty!
2018-04-29 13:32:24,039 - memory_profile6_log - INFO - tframe for date: 2018-04-28 is empty
2018-04-29 13:32:24,042 - memory_profile6_log - INFO - len datalist: 0
2018-04-29 13:32:24,042 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 13:32:24,045 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 13:32:24,046 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 13:32:24,048 - memory_profile6_log - INFO - ================================================

2018-04-29 13:32:24,049 - memory_profile6_log - INFO -    295     86.4 MiB     86.4 MiB   @profile

2018-04-29 13:32:24,052 - memory_profile6_log - INFO -    296                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 13:32:24,053 - memory_profile6_log - INFO -    297     86.4 MiB      0.0 MiB       bq_client = client

2018-04-29 13:32:24,056 - memory_profile6_log - INFO -    298     86.4 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 13:32:24,059 - memory_profile6_log - INFO -    299                             

2018-04-29 13:32:24,061 - memory_profile6_log - INFO -    300     86.4 MiB      0.0 MiB       datalist = []

2018-04-29 13:32:24,062 - memory_profile6_log - INFO -    301     86.4 MiB      0.0 MiB       datalist_hist = []

2018-04-29 13:32:24,063 - memory_profile6_log - INFO -    302                             

2018-04-29 13:32:24,063 - memory_profile6_log - INFO -    303     86.4 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 13:32:24,065 - memory_profile6_log - INFO -    304     90.1 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 13:32:24,066 - memory_profile6_log - INFO -    305     90.1 MiB      3.7 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 13:32:24,069 - memory_profile6_log - INFO -    306     90.1 MiB      0.0 MiB           if tframe is not None:

2018-04-29 13:32:24,069 - memory_profile6_log - INFO -    307                                         if not tframe.empty:

2018-04-29 13:32:24,071 - memory_profile6_log - INFO -    308                                             X_split = np.array_split(tframe, 5)

2018-04-29 13:32:24,072 - memory_profile6_log - INFO -    309                                             logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 13:32:24,072 - memory_profile6_log - INFO -    310                                             logger.info("Appending history data...")

2018-04-29 13:32:24,072 - memory_profile6_log - INFO -    311                                             for ix in range(len(X_split)):

2018-04-29 13:32:24,073 - memory_profile6_log - INFO -    312                                                 # ~ loading history

2018-04-29 13:32:24,075 - memory_profile6_log - INFO -    313                                                 """

2018-04-29 13:32:24,075 - memory_profile6_log - INFO -    314                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 13:32:24,076 - memory_profile6_log - INFO -    315                                                 """

2018-04-29 13:32:24,076 - memory_profile6_log - INFO -    316                                                 logger.info("processing batch-%d", ix)

2018-04-29 13:32:24,079 - memory_profile6_log - INFO -    317                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 13:32:24,081 - memory_profile6_log - INFO -    318                                                 logger.info("creating list history data...")

2018-04-29 13:32:24,082 - memory_profile6_log - INFO -    319                                                 lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 13:32:24,082 - memory_profile6_log - INFO -    320                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 13:32:24,085 - memory_profile6_log - INFO -    321                             

2018-04-29 13:32:24,085 - memory_profile6_log - INFO -    322                                                 logger.info("call history data...")

2018-04-29 13:32:24,085 - memory_profile6_log - INFO -    323                                                 h_frame = mh.loadDSHistory(lhistory)

2018-04-29 13:32:24,086 - memory_profile6_log - INFO -    324                             

2018-04-29 13:32:24,086 - memory_profile6_log - INFO -    325                                                 # me = os.getpid()

2018-04-29 13:32:24,088 - memory_profile6_log - INFO -    326                                                 # kill_proc_tree(me)

2018-04-29 13:32:24,088 - memory_profile6_log - INFO -    327                             

2018-04-29 13:32:24,092 - memory_profile6_log - INFO -    328                                                 logger.info("done collecting history data, appending now...")

2018-04-29 13:32:24,094 - memory_profile6_log - INFO -    329                                                 for m in h_frame:

2018-04-29 13:32:24,095 - memory_profile6_log - INFO -    330                                                     if m is not None:

2018-04-29 13:32:24,095 - memory_profile6_log - INFO -    331                                                         if len(m) > 0:

2018-04-29 13:32:24,096 - memory_profile6_log - INFO -    332                                                             datalist_hist.append(pd.DataFrame(m))

2018-04-29 13:32:24,098 - memory_profile6_log - INFO -    333                                                 del h_frame

2018-04-29 13:32:24,098 - memory_profile6_log - INFO -    334                                                 del lhistory

2018-04-29 13:32:24,101 - memory_profile6_log - INFO -    335                             

2018-04-29 13:32:24,101 - memory_profile6_log - INFO -    336                                             logger.info("Appending training data...")

2018-04-29 13:32:24,104 - memory_profile6_log - INFO -    337                                             datalist.append(tframe)

2018-04-29 13:32:24,105 - memory_profile6_log - INFO -    338                                     else: 

2018-04-29 13:32:24,107 - memory_profile6_log - INFO -    339     90.1 MiB      0.0 MiB               logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 13:32:24,108 - memory_profile6_log - INFO -    340     90.1 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 13:32:24,111 - memory_profile6_log - INFO -    341     90.1 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 13:32:24,112 - memory_profile6_log - INFO -    342                             

2018-04-29 13:32:24,114 - memory_profile6_log - INFO -    343     90.1 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 13:32:24,115 - memory_profile6_log - INFO - 


2018-04-29 13:32:24,117 - memory_profile6_log - INFO - Training cannot be empty..
2018-04-29 13:32:24,118 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 13:32:24,118 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 13:32:24,118 - memory_profile6_log - INFO - ================================================

2018-04-29 13:32:24,118 - memory_profile6_log - INFO -    345     86.2 MiB     86.2 MiB   @profile

2018-04-29 13:32:24,121 - memory_profile6_log - INFO -    346                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 13:32:24,121 - memory_profile6_log - INFO -    347     86.4 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 13:32:24,124 - memory_profile6_log - INFO -    348     86.4 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 13:32:24,124 - memory_profile6_log - INFO -    349                             

2018-04-29 13:32:24,127 - memory_profile6_log - INFO -    350                                 # ~~~ Begin collecting data ~~~

2018-04-29 13:32:24,127 - memory_profile6_log - INFO -    351     86.4 MiB      0.0 MiB       t0 = time.time()

2018-04-29 13:32:24,128 - memory_profile6_log - INFO -    352     90.1 MiB      3.7 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 13:32:24,128 - memory_profile6_log - INFO -    353     90.1 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 13:32:24,128 - memory_profile6_log - INFO -    354     90.1 MiB      0.0 MiB           logger.info("Training cannot be empty..")

2018-04-29 13:32:24,128 - memory_profile6_log - INFO -    355     90.1 MiB      0.0 MiB           return False

2018-04-29 13:32:24,130 - memory_profile6_log - INFO -    356                                 big_frame_hist = pd.concat(datalist_hist)

2018-04-29 13:32:24,130 - memory_profile6_log - INFO -    357                                 print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 13:32:24,130 - memory_profile6_log - INFO -    358                                 logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 13:32:24,134 - memory_profile6_log - INFO -    359                             

2018-04-29 13:32:24,134 - memory_profile6_log - INFO -    360                                 big_frame = pd.concat(datalist)

2018-04-29 13:32:24,135 - memory_profile6_log - INFO -    361                                 logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 13:32:24,137 - memory_profile6_log - INFO -    362                                 del datalist

2018-04-29 13:32:24,137 - memory_profile6_log - INFO -    363                             

2018-04-29 13:32:24,138 - memory_profile6_log - INFO -    364                                 big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 13:32:24,138 - memory_profile6_log - INFO -    365                             

2018-04-29 13:32:24,138 - memory_profile6_log - INFO -    366                                 # ~ get current news interest ~

2018-04-29 13:32:24,138 - memory_profile6_log - INFO -    367                                 if not cd:

2018-04-29 13:32:24,140 - memory_profile6_log - INFO -    368                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 13:32:24,141 - memory_profile6_log - INFO -    369                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 13:32:24,141 - memory_profile6_log - INFO -    370                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 13:32:24,145 - memory_profile6_log - INFO -    371                                 else:

2018-04-29 13:32:24,148 - memory_profile6_log - INFO -    372                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 13:32:24,148 - memory_profile6_log - INFO -    373                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 13:32:24,148 - memory_profile6_log - INFO -    374                             

2018-04-29 13:32:24,150 - memory_profile6_log - INFO -    375                                     # safe handling of query parameter

2018-04-29 13:32:24,151 - memory_profile6_log - INFO -    376                                     query_params = [

2018-04-29 13:32:24,153 - memory_profile6_log - INFO -    377                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 13:32:24,153 - memory_profile6_log - INFO -    378                                     ]

2018-04-29 13:32:24,154 - memory_profile6_log - INFO -    379                             

2018-04-29 13:32:24,154 - memory_profile6_log - INFO -    380                                     job_config.query_parameters = query_params

2018-04-29 13:32:24,157 - memory_profile6_log - INFO -    381                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 13:32:24,158 - memory_profile6_log - INFO -    382                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 13:32:24,160 - memory_profile6_log - INFO -    383                             

2018-04-29 13:32:24,161 - memory_profile6_log - INFO -    384                                 current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 13:32:24,161 - memory_profile6_log - INFO -    385                                 current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 13:32:24,161 - memory_profile6_log - INFO -    386                                                                        format='%Y-%m-%d', errors='coerce')

2018-04-29 13:32:24,163 - memory_profile6_log - INFO -    387                                 logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 13:32:24,163 - memory_profile6_log - INFO -    388                                 train_time = time.time() - t0

2018-04-29 13:32:24,163 - memory_profile6_log - INFO -    389                                 logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 13:32:24,164 - memory_profile6_log - INFO -    390                             

2018-04-29 13:32:24,164 - memory_profile6_log - INFO -    391                                 return big_frame, current_frame, big_frame_hist

2018-04-29 13:32:24,164 - memory_profile6_log - INFO - 


2018-04-29 13:32:24,167 - memory_profile6_log - INFO - Train return False, please cek your data availability
2018-04-29 13:32:36,563 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:32:36,566 - memory_profile6_log - INFO - date_generated: 
2018-04-29 13:32:36,566 - memory_profile6_log - INFO -  
2018-04-29 13:32:36,566 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 28, 13, 32, 36, 567000)]
2018-04-29 13:32:36,568 - memory_profile6_log - INFO - 

2018-04-29 13:32:36,568 - memory_profile6_log - INFO - using current date: 2018-04-29 13:32:36.567000
2018-04-29 13:32:36,568 - memory_profile6_log - INFO - using start date: 2018-04-28 00:00:00
2018-04-29 13:32:36,568 - memory_profile6_log - INFO - using end date: 2018-04-28
2018-04-29 13:32:36,691 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 13:32:36,694 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-28
2018-04-29 13:32:39,292 - memory_profile6_log - INFO - size of df: 24.0 Byte
2018-04-29 13:32:39,292 - memory_profile6_log - INFO - 2018-04-28 data is empty!
2018-04-29 13:32:39,293 - memory_profile6_log - INFO - tframe for date: 2018-04-28 is empty
2018-04-29 13:32:39,296 - memory_profile6_log - INFO - len datalist: 0
2018-04-29 13:32:39,296 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 13:32:39,298 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 13:32:39,299 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 13:32:39,299 - memory_profile6_log - INFO - ================================================

2018-04-29 13:32:39,301 - memory_profile6_log - INFO -    295     86.4 MiB     86.4 MiB   @profile

2018-04-29 13:32:39,302 - memory_profile6_log - INFO -    296                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 13:32:39,302 - memory_profile6_log - INFO -    297     86.4 MiB      0.0 MiB       bq_client = client

2018-04-29 13:32:39,303 - memory_profile6_log - INFO -    298     86.4 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 13:32:39,303 - memory_profile6_log - INFO -    299                             

2018-04-29 13:32:39,305 - memory_profile6_log - INFO -    300     86.4 MiB      0.0 MiB       datalist = []

2018-04-29 13:32:39,305 - memory_profile6_log - INFO -    301     86.4 MiB      0.0 MiB       datalist_hist = []

2018-04-29 13:32:39,306 - memory_profile6_log - INFO -    302                             

2018-04-29 13:32:39,306 - memory_profile6_log - INFO -    303     86.4 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 13:32:39,308 - memory_profile6_log - INFO -    304     90.1 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 13:32:39,309 - memory_profile6_log - INFO -    305     90.1 MiB      3.7 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 13:32:39,309 - memory_profile6_log - INFO -    306     90.1 MiB      0.0 MiB           if tframe is not None:

2018-04-29 13:32:39,309 - memory_profile6_log - INFO -    307                                         if not tframe.empty:

2018-04-29 13:32:39,312 - memory_profile6_log - INFO -    308                                             X_split = np.array_split(tframe, 5)

2018-04-29 13:32:39,312 - memory_profile6_log - INFO -    309                                             logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 13:32:39,312 - memory_profile6_log - INFO -    310                                             logger.info("Appending history data...")

2018-04-29 13:32:39,313 - memory_profile6_log - INFO -    311                                             for ix in range(len(X_split)):

2018-04-29 13:32:39,315 - memory_profile6_log - INFO -    312                                                 # ~ loading history

2018-04-29 13:32:39,315 - memory_profile6_log - INFO -    313                                                 """

2018-04-29 13:32:39,316 - memory_profile6_log - INFO -    314                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 13:32:39,318 - memory_profile6_log - INFO -    315                                                 """

2018-04-29 13:32:39,319 - memory_profile6_log - INFO -    316                                                 logger.info("processing batch-%d", ix)

2018-04-29 13:32:39,319 - memory_profile6_log - INFO -    317                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 13:32:39,322 - memory_profile6_log - INFO -    318                                                 logger.info("creating list history data...")

2018-04-29 13:32:39,323 - memory_profile6_log - INFO -    319                                                 lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 13:32:39,325 - memory_profile6_log - INFO -    320                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 13:32:39,325 - memory_profile6_log - INFO -    321                             

2018-04-29 13:32:39,326 - memory_profile6_log - INFO -    322                                                 logger.info("call history data...")

2018-04-29 13:32:39,328 - memory_profile6_log - INFO -    323                                                 h_frame = mh.loadDSHistory(lhistory)

2018-04-29 13:32:39,328 - memory_profile6_log - INFO -    324                             

2018-04-29 13:32:39,329 - memory_profile6_log - INFO -    325                                                 # me = os.getpid()

2018-04-29 13:32:39,331 - memory_profile6_log - INFO -    326                                                 # kill_proc_tree(me)

2018-04-29 13:32:39,331 - memory_profile6_log - INFO -    327                             

2018-04-29 13:32:39,335 - memory_profile6_log - INFO -    328                                                 logger.info("done collecting history data, appending now...")

2018-04-29 13:32:39,336 - memory_profile6_log - INFO -    329                                                 for m in h_frame:

2018-04-29 13:32:39,338 - memory_profile6_log - INFO -    330                                                     if m is not None:

2018-04-29 13:32:39,338 - memory_profile6_log - INFO -    331                                                         if len(m) > 0:

2018-04-29 13:32:39,339 - memory_profile6_log - INFO -    332                                                             datalist_hist.append(pd.DataFrame(m))

2018-04-29 13:32:39,341 - memory_profile6_log - INFO -    333                                                 del h_frame

2018-04-29 13:32:39,341 - memory_profile6_log - INFO -    334                                                 del lhistory

2018-04-29 13:32:39,342 - memory_profile6_log - INFO -    335                             

2018-04-29 13:32:39,342 - memory_profile6_log - INFO -    336                                             logger.info("Appending training data...")

2018-04-29 13:32:39,345 - memory_profile6_log - INFO -    337                                             datalist.append(tframe)

2018-04-29 13:32:39,348 - memory_profile6_log - INFO -    338                                     else: 

2018-04-29 13:32:39,348 - memory_profile6_log - INFO -    339     90.1 MiB      0.0 MiB               logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 13:32:39,348 - memory_profile6_log - INFO -    340     90.1 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 13:32:39,349 - memory_profile6_log - INFO -    341     90.1 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 13:32:39,349 - memory_profile6_log - INFO -    342                             

2018-04-29 13:32:39,351 - memory_profile6_log - INFO -    343     90.1 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 13:32:39,351 - memory_profile6_log - INFO - 


2018-04-29 13:32:39,352 - memory_profile6_log - INFO - Training cannot be empty..
2018-04-29 13:32:39,355 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 13:32:39,355 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 13:32:39,358 - memory_profile6_log - INFO - ================================================

2018-04-29 13:32:39,358 - memory_profile6_log - INFO -    345     86.2 MiB     86.2 MiB   @profile

2018-04-29 13:32:39,358 - memory_profile6_log - INFO -    346                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 13:32:39,358 - memory_profile6_log - INFO -    347     86.4 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 13:32:39,359 - memory_profile6_log - INFO -    348     86.4 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 13:32:39,359 - memory_profile6_log - INFO -    349                             

2018-04-29 13:32:39,362 - memory_profile6_log - INFO -    350                                 # ~~~ Begin collecting data ~~~

2018-04-29 13:32:39,362 - memory_profile6_log - INFO -    351     86.4 MiB      0.0 MiB       t0 = time.time()

2018-04-29 13:32:39,364 - memory_profile6_log - INFO -    352     90.1 MiB      3.7 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 13:32:39,364 - memory_profile6_log - INFO -    353     90.1 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 13:32:39,367 - memory_profile6_log - INFO -    354     90.1 MiB      0.0 MiB           logger.info("Training cannot be empty..")

2018-04-29 13:32:39,368 - memory_profile6_log - INFO -    355     90.1 MiB      0.0 MiB           return False

2018-04-29 13:32:39,368 - memory_profile6_log - INFO -    356                                 big_frame_hist = pd.concat(datalist_hist)

2018-04-29 13:32:39,371 - memory_profile6_log - INFO -    357                                 print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 13:32:39,371 - memory_profile6_log - INFO -    358                                 logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 13:32:39,371 - memory_profile6_log - INFO -    359                             

2018-04-29 13:32:39,372 - memory_profile6_log - INFO -    360                                 big_frame = pd.concat(datalist)

2018-04-29 13:32:39,372 - memory_profile6_log - INFO -    361                                 logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 13:32:39,372 - memory_profile6_log - INFO -    362                                 del datalist

2018-04-29 13:32:39,372 - memory_profile6_log - INFO -    363                             

2018-04-29 13:32:39,374 - memory_profile6_log - INFO -    364                                 big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 13:32:39,374 - memory_profile6_log - INFO -    365                             

2018-04-29 13:32:39,374 - memory_profile6_log - INFO -    366                                 # ~ get current news interest ~

2018-04-29 13:32:39,375 - memory_profile6_log - INFO -    367                                 if not cd:

2018-04-29 13:32:39,375 - memory_profile6_log - INFO -    368                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 13:32:39,375 - memory_profile6_log - INFO -    369                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 13:32:39,375 - memory_profile6_log - INFO -    370                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 13:32:39,378 - memory_profile6_log - INFO -    371                                 else:

2018-04-29 13:32:39,381 - memory_profile6_log - INFO -    372                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 13:32:39,381 - memory_profile6_log - INFO -    373                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 13:32:39,381 - memory_profile6_log - INFO -    374                             

2018-04-29 13:32:39,381 - memory_profile6_log - INFO -    375                                     # safe handling of query parameter

2018-04-29 13:32:39,382 - memory_profile6_log - INFO -    376                                     query_params = [

2018-04-29 13:32:39,382 - memory_profile6_log - INFO -    377                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 13:32:39,384 - memory_profile6_log - INFO -    378                                     ]

2018-04-29 13:32:39,384 - memory_profile6_log - INFO -    379                             

2018-04-29 13:32:39,384 - memory_profile6_log - INFO -    380                                     job_config.query_parameters = query_params

2018-04-29 13:32:39,385 - memory_profile6_log - INFO -    381                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 13:32:39,387 - memory_profile6_log - INFO -    382                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 13:32:39,387 - memory_profile6_log - INFO -    383                             

2018-04-29 13:32:39,390 - memory_profile6_log - INFO -    384                                 current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 13:32:39,391 - memory_profile6_log - INFO -    385                                 current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 13:32:39,391 - memory_profile6_log - INFO -    386                                                                        format='%Y-%m-%d', errors='coerce')

2018-04-29 13:32:39,391 - memory_profile6_log - INFO -    387                                 logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 13:32:39,392 - memory_profile6_log - INFO -    388                                 train_time = time.time() - t0

2018-04-29 13:32:39,392 - memory_profile6_log - INFO -    389                                 logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 13:32:39,394 - memory_profile6_log - INFO -    390                             

2018-04-29 13:32:39,394 - memory_profile6_log - INFO -    391                                 return big_frame, current_frame, big_frame_hist

2018-04-29 13:32:39,394 - memory_profile6_log - INFO - 


2018-04-29 13:32:39,395 - memory_profile6_log - INFO - Train return False, please cek your data availability
2018-04-29 13:33:57,421 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:33:57,424 - memory_profile6_log - INFO - date_generated: 
2018-04-29 13:33:57,424 - memory_profile6_log - INFO -  
2018-04-29 13:33:57,426 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 9, 0, 0)]
2018-04-29 13:33:57,426 - memory_profile6_log - INFO - 

2018-04-29 13:33:57,427 - memory_profile6_log - INFO - using current date: 2018-04-29 13:33:57.424000
2018-04-29 13:33:57,427 - memory_profile6_log - INFO - using start date: 2018-04-09 00:00:00
2018-04-29 13:33:57,427 - memory_profile6_log - INFO - using end date: 2018-04-09
2018-04-29 13:33:57,562 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 13:33:57,566 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-09
2018-04-29 13:34:14,615 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:34:14,618 - memory_profile6_log - INFO - date_generated: 
2018-04-29 13:34:14,618 - memory_profile6_log - INFO -  
2018-04-29 13:34:14,619 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 13:34:14,619 - memory_profile6_log - INFO - 

2018-04-29 13:34:14,619 - memory_profile6_log - INFO - using current date: 2018-04-29 13:34:14.619000
2018-04-29 13:34:14,621 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 13:34:14,621 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 13:34:14,743 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 13:34:14,746 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 13:35:23,510 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 13:35:23,512 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 13:35:23,552 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 13:35:23,552 - memory_profile6_log - INFO - Appending history data...
2018-04-29 13:35:23,555 - memory_profile6_log - INFO - processing batch-0
2018-04-29 13:35:23,555 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:35:23,596 - memory_profile6_log - INFO - call history data...
2018-04-29 13:35:51,823 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:35:52,523 - memory_profile6_log - INFO - processing batch-1
2018-04-29 13:35:52,525 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:35:52,532 - memory_profile6_log - INFO - call history data...
2018-04-29 13:36:19,740 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:36:20,434 - memory_profile6_log - INFO - processing batch-2
2018-04-29 13:36:20,437 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:36:20,444 - memory_profile6_log - INFO - call history data...
2018-04-29 13:36:46,828 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:36:47,533 - memory_profile6_log - INFO - processing batch-3
2018-04-29 13:36:47,535 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:36:47,542 - memory_profile6_log - INFO - call history data...
2018-04-29 13:37:14,426 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:37:15,117 - memory_profile6_log - INFO - processing batch-4
2018-04-29 13:37:15,118 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:37:15,128 - memory_profile6_log - INFO - call history data...
2018-04-29 13:37:41,414 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:37:42,118 - memory_profile6_log - INFO - Appending training data...
2018-04-29 13:37:42,119 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 13:37:42,121 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 13:37:42,122 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 13:37:42,124 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 13:37:42,125 - memory_profile6_log - INFO - ================================================

2018-04-29 13:37:42,127 - memory_profile6_log - INFO -    295     87.2 MiB     87.2 MiB   @profile

2018-04-29 13:37:42,127 - memory_profile6_log - INFO -    296                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 13:37:42,128 - memory_profile6_log - INFO -    297     87.2 MiB      0.0 MiB       bq_client = client

2018-04-29 13:37:42,131 - memory_profile6_log - INFO -    298     87.2 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 13:37:42,131 - memory_profile6_log - INFO -    299                             

2018-04-29 13:37:42,134 - memory_profile6_log - INFO -    300     87.2 MiB      0.0 MiB       datalist = []

2018-04-29 13:37:42,135 - memory_profile6_log - INFO -    301     87.2 MiB      0.0 MiB       datalist_hist = []

2018-04-29 13:37:42,137 - memory_profile6_log - INFO -    302                             

2018-04-29 13:37:42,138 - memory_profile6_log - INFO -    303     87.2 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 13:37:42,138 - memory_profile6_log - INFO -    304    351.3 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 13:37:42,141 - memory_profile6_log - INFO -    305    338.7 MiB    251.5 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 13:37:42,144 - memory_profile6_log - INFO -    306    338.7 MiB      0.0 MiB           if tframe is not None:

2018-04-29 13:37:42,144 - memory_profile6_log - INFO -    307    338.7 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 13:37:42,145 - memory_profile6_log - INFO -    308    346.8 MiB      8.1 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 13:37:42,147 - memory_profile6_log - INFO -    309    346.8 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 13:37:42,147 - memory_profile6_log - INFO -    310    346.8 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 13:37:42,148 - memory_profile6_log - INFO -    311    351.3 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 13:37:42,148 - memory_profile6_log - INFO -    312                                                 # ~ loading history

2018-04-29 13:37:42,151 - memory_profile6_log - INFO -    313                                                 """

2018-04-29 13:37:42,151 - memory_profile6_log - INFO -    314                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 13:37:42,153 - memory_profile6_log - INFO -    315                                                 """

2018-04-29 13:37:42,154 - memory_profile6_log - INFO -    316    351.1 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 13:37:42,154 - memory_profile6_log - INFO -    317                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 13:37:42,154 - memory_profile6_log - INFO -    318    351.1 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 13:37:42,154 - memory_profile6_log - INFO -    319    351.1 MiB      0.5 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 13:37:42,155 - memory_profile6_log - INFO -    320                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 13:37:42,157 - memory_profile6_log - INFO -    321                             

2018-04-29 13:37:42,157 - memory_profile6_log - INFO -    322    351.1 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 13:37:42,158 - memory_profile6_log - INFO -    323    351.2 MiB      3.2 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 13:37:42,161 - memory_profile6_log - INFO -    324                             

2018-04-29 13:37:42,164 - memory_profile6_log - INFO -    325                                                 # me = os.getpid()

2018-04-29 13:37:42,164 - memory_profile6_log - INFO -    326                                                 # kill_proc_tree(me)

2018-04-29 13:37:42,164 - memory_profile6_log - INFO -    327                             

2018-04-29 13:37:42,165 - memory_profile6_log - INFO -    328    351.2 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 13:37:42,167 - memory_profile6_log - INFO -    329    351.3 MiB      0.0 MiB                       for m in h_frame:

2018-04-29 13:37:42,167 - memory_profile6_log - INFO -    330    351.3 MiB      0.0 MiB                           if m is not None:

2018-04-29 13:37:42,167 - memory_profile6_log - INFO -    331    351.3 MiB      0.0 MiB                               if len(m) > 0:

2018-04-29 13:37:42,168 - memory_profile6_log - INFO -    332    351.3 MiB      0.7 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 13:37:42,170 - memory_profile6_log - INFO -    333    351.3 MiB      0.0 MiB                       del h_frame

2018-04-29 13:37:42,173 - memory_profile6_log - INFO -    334    351.3 MiB      0.0 MiB                       del lhistory

2018-04-29 13:37:42,177 - memory_profile6_log - INFO -    335                             

2018-04-29 13:37:42,177 - memory_profile6_log - INFO -    336    351.3 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 13:37:42,178 - memory_profile6_log - INFO -    337    351.3 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 13:37:42,180 - memory_profile6_log - INFO -    338                                     else: 

2018-04-29 13:37:42,180 - memory_profile6_log - INFO -    339                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 13:37:42,184 - memory_profile6_log - INFO -    340    351.3 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 13:37:42,184 - memory_profile6_log - INFO -    341    351.3 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 13:37:42,187 - memory_profile6_log - INFO -    342                             

2018-04-29 13:37:42,187 - memory_profile6_log - INFO -    343    351.3 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 13:37:42,188 - memory_profile6_log - INFO - 


2018-04-29 13:37:43,318 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 13:37:43,388 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0   0.000646        1844.325967        32            1854.325967  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
1   0.000646        1844.325967        16            1854.325967  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
2   0.000646        1844.325967        32            1854.325967  10960288  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
3   0.000646        1844.325967        16            1854.325967  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
4   0.000646        1844.325967        32            1854.325967  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
5        NaN      333823.000000       256          333833.000000  10959239  161129f951435-0d660a18dbcd87-173a7640-100200-1...
6   0.000646        1844.325967        16            1854.325967  10960288  1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...
0   0.000646        1844.325967         8            1854.325967  10960288  161e219cd891c8-0e882943705df4-15290f5f-38400-1...
1   0.000646        1844.325967        16            1854.325967  10960288  1626fd160d9114-055d801ae6b39d-66265f05-38400-1...
2   0.000646        1844.325967         8            1854.325967  10960288  16169731ce618-00742c5e307396-5b4d6a36-38400-16...
3   0.000646        1844.325967        16            1854.325967  10960288  161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...
4   0.000646        1844.325967         8            1854.325967  10960288  1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...
5   0.000646        1844.325967        16            1854.325967  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
0   0.000646        1844.325967         4            1854.325967  10960288  1612d88e55ab8-09af9658f30a46-65117b23-38400-16...
1   0.000646        1844.325967         8            1854.325967  10960288  16149f0a1611-00a034775-63446032-38400-16149f0a...
2   0.000646        1844.325967         4            1854.325967  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
3   0.000646        1844.325967         4            1854.325967  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
4   0.000646        1844.325967         4            1854.325967  10960288  16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...
5   0.000646        1844.325967         8            1854.325967  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
6   0.000646        1844.325967         4            1854.325967  10960288  16132f1045f30b-0613b72b33445e-5d640103-38400-1...
2018-04-29 13:37:43,388 - memory_profile6_log - INFO - 

2018-04-29 13:37:43,398 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 13:37:43,469 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 13:37:43,480 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 13:45:24,711 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:45:24,714 - memory_profile6_log - INFO - date_generated: 
2018-04-29 13:45:24,716 - memory_profile6_log - INFO -  
2018-04-29 13:45:24,716 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 13:45:24,717 - memory_profile6_log - INFO - 

2018-04-29 13:45:24,717 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 13:45:24,717 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 13:45:24,719 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 13:45:24,862 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 13:45:24,865 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 13:45:51,473 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:47:02,813 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:47:23,256 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:47:55,134 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:48:42,676 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:49:07,651 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:49:07,654 - memory_profile6_log - INFO - 2018-04-29
2018-04-29 13:49:07,654 - memory_profile6_log - INFO - 

2018-04-29 13:49:44,980 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:49:44,983 - memory_profile6_log - INFO - date_generated: 
2018-04-29 13:49:44,983 - memory_profile6_log - INFO -  
2018-04-29 13:49:44,983 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 13:49:44,983 - memory_profile6_log - INFO - 

2018-04-29 13:49:44,984 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 13:49:44,984 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 13:49:44,986 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 13:49:45,119 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 13:49:45,124 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 13:50:56,196 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 13:50:56,197 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 13:50:56,240 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 13:50:56,240 - memory_profile6_log - INFO - Appending history data...
2018-04-29 13:50:56,242 - memory_profile6_log - INFO - processing batch-0
2018-04-29 13:50:56,242 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:50:56,285 - memory_profile6_log - INFO - call history data...
2018-04-29 13:51:26,808 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:51:27,529 - memory_profile6_log - INFO - processing batch-1
2018-04-29 13:51:27,530 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:51:27,539 - memory_profile6_log - INFO - call history data...
2018-04-29 13:51:58,483 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:51:59,184 - memory_profile6_log - INFO - processing batch-2
2018-04-29 13:51:59,184 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:51:59,193 - memory_profile6_log - INFO - call history data...
2018-04-29 13:52:27,194 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:52:27,888 - memory_profile6_log - INFO - processing batch-3
2018-04-29 13:52:27,890 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:52:27,898 - memory_profile6_log - INFO - call history data...
2018-04-29 13:52:55,315 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:52:56,012 - memory_profile6_log - INFO - processing batch-4
2018-04-29 13:52:56,013 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:52:56,022 - memory_profile6_log - INFO - call history data...
2018-04-29 13:53:23,374 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 13:53:24,069 - memory_profile6_log - INFO - Appending training data...
2018-04-29 13:53:24,072 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 13:53:24,072 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 13:53:24,075 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 13:53:24,075 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 13:53:24,076 - memory_profile6_log - INFO - ================================================

2018-04-29 13:53:24,078 - memory_profile6_log - INFO -    295     86.7 MiB     86.7 MiB   @profile

2018-04-29 13:53:24,079 - memory_profile6_log - INFO -    296                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 13:53:24,082 - memory_profile6_log - INFO -    297     86.7 MiB      0.0 MiB       bq_client = client

2018-04-29 13:53:24,085 - memory_profile6_log - INFO -    298     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 13:53:24,085 - memory_profile6_log - INFO -    299                             

2018-04-29 13:53:24,086 - memory_profile6_log - INFO -    300     86.7 MiB      0.0 MiB       datalist = []

2018-04-29 13:53:24,088 - memory_profile6_log - INFO -    301     86.7 MiB      0.0 MiB       datalist_hist = []

2018-04-29 13:53:24,088 - memory_profile6_log - INFO -    302                             

2018-04-29 13:53:24,088 - memory_profile6_log - INFO -    303     86.7 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 13:53:24,089 - memory_profile6_log - INFO -    304    351.5 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 13:53:24,089 - memory_profile6_log - INFO -    305    339.3 MiB    252.6 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 13:53:24,091 - memory_profile6_log - INFO -    306    339.3 MiB      0.0 MiB           if tframe is not None:

2018-04-29 13:53:24,094 - memory_profile6_log - INFO -    307    339.3 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 13:53:24,094 - memory_profile6_log - INFO -    308    347.1 MiB      7.8 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 13:53:24,095 - memory_profile6_log - INFO -    309    347.1 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 13:53:24,096 - memory_profile6_log - INFO -    310    347.1 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 13:53:24,096 - memory_profile6_log - INFO -    311    351.5 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 13:53:24,098 - memory_profile6_log - INFO -    312                                                 # ~ loading history

2018-04-29 13:53:24,098 - memory_profile6_log - INFO -    313                                                 """

2018-04-29 13:53:24,099 - memory_profile6_log - INFO -    314                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 13:53:24,101 - memory_profile6_log - INFO -    315                                                 """

2018-04-29 13:53:24,105 - memory_profile6_log - INFO -    316    351.4 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 13:53:24,105 - memory_profile6_log - INFO -    317                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 13:53:24,107 - memory_profile6_log - INFO -    318    351.4 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 13:53:24,108 - memory_profile6_log - INFO -    319    351.4 MiB      0.8 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 13:53:24,111 - memory_profile6_log - INFO -    320                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 13:53:24,111 - memory_profile6_log - INFO -    321                             

2018-04-29 13:53:24,115 - memory_profile6_log - INFO -    322    351.4 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 13:53:24,115 - memory_profile6_log - INFO -    323    351.5 MiB      2.9 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 13:53:24,118 - memory_profile6_log - INFO -    324                             

2018-04-29 13:53:24,118 - memory_profile6_log - INFO -    325                                                 # me = os.getpid()

2018-04-29 13:53:24,118 - memory_profile6_log - INFO -    326                                                 # kill_proc_tree(me)

2018-04-29 13:53:24,119 - memory_profile6_log - INFO -    327                             

2018-04-29 13:53:24,121 - memory_profile6_log - INFO -    328    351.5 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 13:53:24,121 - memory_profile6_log - INFO -    329    351.5 MiB      0.0 MiB                       for m in h_frame:

2018-04-29 13:53:24,121 - memory_profile6_log - INFO -    330    351.5 MiB      0.0 MiB                           if m is not None:

2018-04-29 13:53:24,125 - memory_profile6_log - INFO -    331    351.5 MiB      0.0 MiB                               if len(m) > 0:

2018-04-29 13:53:24,125 - memory_profile6_log - INFO -    332    351.5 MiB      0.8 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 13:53:24,128 - memory_profile6_log - INFO -    333    351.5 MiB      0.0 MiB                       del h_frame

2018-04-29 13:53:24,128 - memory_profile6_log - INFO -    334    351.5 MiB      0.0 MiB                       del lhistory

2018-04-29 13:53:24,128 - memory_profile6_log - INFO -    335                             

2018-04-29 13:53:24,130 - memory_profile6_log - INFO -    336    351.5 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 13:53:24,130 - memory_profile6_log - INFO -    337    351.5 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 13:53:24,131 - memory_profile6_log - INFO -    338                                     else: 

2018-04-29 13:53:24,131 - memory_profile6_log - INFO -    339                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 13:53:24,132 - memory_profile6_log - INFO -    340    351.5 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 13:53:24,134 - memory_profile6_log - INFO -    341    351.5 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 13:53:24,138 - memory_profile6_log - INFO -    342                             

2018-04-29 13:53:24,138 - memory_profile6_log - INFO -    343    351.5 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 13:53:24,140 - memory_profile6_log - INFO - 


2018-04-29 13:53:25,259 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 13:53:25,328 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0   0.000646        1844.325967        16            1854.325967  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
1   0.000646        1844.325967        32            1854.325967  10960288  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
2   0.000646        1844.325967        16            1854.325967  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
3   0.000646        1844.325967        16            1854.325967  10960288  1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...
4        NaN      333823.000000       256          333833.000000  10959239  161129f951435-0d660a18dbcd87-173a7640-100200-1...
5   0.000646        1844.325967        32            1854.325967  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
6   0.000646        1844.325967        32            1854.325967  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
0   0.000646        1844.325967        16            1854.325967  10960288  1626fd160d9114-055d801ae6b39d-66265f05-38400-1...
1   0.000646        1844.325967        16            1854.325967  10960288  161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...
2   0.000646        1844.325967         8            1854.325967  10960288  161e219cd891c8-0e882943705df4-15290f5f-38400-1...
3   0.000646        1844.325967        16            1854.325967  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
4   0.000646        1844.325967         8            1854.325967  10960288  1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...
5   0.000646        1844.325967         8            1854.325967  10960288  16169731ce618-00742c5e307396-5b4d6a36-38400-16...
0   0.000646        1844.325967         4            1854.325967  10960288  16132f1045f30b-0613b72b33445e-5d640103-38400-1...
1   0.000646        1844.325967         4            1854.325967  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
2   0.000646        1844.325967         4            1854.325967  10960288  16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...
3   0.000646        1844.325967         8            1854.325967  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
4   0.000646        1844.325967         8            1854.325967  10960288  16149f0a1611-00a034775-63446032-38400-16149f0a...
5   0.000646        1844.325967         4            1854.325967  10960288  1612d88e55ab8-09af9658f30a46-65117b23-38400-16...
6   0.000646        1844.325967         4            1854.325967  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
2018-04-29 13:53:25,332 - memory_profile6_log - INFO - 

2018-04-29 13:53:25,342 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 13:53:25,411 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 13:53:25,424 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 13:54:15,888 - memory_profile6_log - INFO - size of df: 58.30 MB
2018-04-29 13:54:15,890 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-29 13:54:15,969 - memory_profile6_log - INFO - size of current_frame: 58.30 MB
2018-04-29 13:54:15,970 - memory_profile6_log - INFO - loading time of: 484420 total genuine-current interest data ~ take 270.875s
2018-04-29 13:54:15,980 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 13:54:15,982 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 13:54:15,983 - memory_profile6_log - INFO - ================================================

2018-04-29 13:54:15,987 - memory_profile6_log - INFO -    345     86.6 MiB     86.6 MiB   @profile

2018-04-29 13:54:15,989 - memory_profile6_log - INFO -    346                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 13:54:15,989 - memory_profile6_log - INFO -    347     86.7 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 13:54:15,992 - memory_profile6_log - INFO -    348     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 13:54:15,993 - memory_profile6_log - INFO -    349                             

2018-04-29 13:54:15,996 - memory_profile6_log - INFO -    350                                 # ~~~ Begin collecting data ~~~

2018-04-29 13:54:15,997 - memory_profile6_log - INFO -    351     86.7 MiB      0.0 MiB       t0 = time.time()

2018-04-29 13:54:16,000 - memory_profile6_log - INFO -    352    351.5 MiB    264.8 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 13:54:16,002 - memory_profile6_log - INFO -    353    351.5 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 13:54:16,003 - memory_profile6_log - INFO -    354                                     logger.info("Training cannot be empty..")

2018-04-29 13:54:16,005 - memory_profile6_log - INFO -    355                                     return False

2018-04-29 13:54:16,006 - memory_profile6_log - INFO -    356    350.9 MiB     -0.6 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 13:54:16,006 - memory_profile6_log - INFO -    357    351.1 MiB      0.2 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 13:54:16,006 - memory_profile6_log - INFO -    358    351.1 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 13:54:16,006 - memory_profile6_log - INFO -    359                             

2018-04-29 13:54:16,006 - memory_profile6_log - INFO -    360    356.9 MiB      5.8 MiB       big_frame = pd.concat(datalist)

2018-04-29 13:54:16,007 - memory_profile6_log - INFO -    361    356.9 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 13:54:16,007 - memory_profile6_log - INFO -    362    351.1 MiB     -5.8 MiB       del datalist

2018-04-29 13:54:16,009 - memory_profile6_log - INFO -    363                             

2018-04-29 13:54:16,013 - memory_profile6_log - INFO -    364    351.1 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 13:54:16,013 - memory_profile6_log - INFO -    365                             

2018-04-29 13:54:16,015 - memory_profile6_log - INFO -    366                                 # ~ get current news interest ~

2018-04-29 13:54:16,016 - memory_profile6_log - INFO -    367    351.1 MiB      0.0 MiB       if not cd:

2018-04-29 13:54:16,016 - memory_profile6_log - INFO -    368                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 13:54:16,017 - memory_profile6_log - INFO -    369                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 13:54:16,019 - memory_profile6_log - INFO -    370                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 13:54:16,020 - memory_profile6_log - INFO -    371                                 else:

2018-04-29 13:54:16,020 - memory_profile6_log - INFO -    372    351.1 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 13:54:16,023 - memory_profile6_log - INFO -    373    351.1 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 13:54:16,023 - memory_profile6_log - INFO -    374                             

2018-04-29 13:54:16,028 - memory_profile6_log - INFO -    375                                     # safe handling of query parameter

2018-04-29 13:54:16,029 - memory_profile6_log - INFO -    376                                     query_params = [

2018-04-29 13:54:16,029 - memory_profile6_log - INFO -    377    351.1 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 13:54:16,030 - memory_profile6_log - INFO -    378                                     ]

2018-04-29 13:54:16,030 - memory_profile6_log - INFO -    379                             

2018-04-29 13:54:16,032 - memory_profile6_log - INFO -    380    351.1 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-29 13:54:16,032 - memory_profile6_log - INFO -    381    419.4 MiB     68.3 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 13:54:16,035 - memory_profile6_log - INFO -    382    419.4 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 13:54:16,036 - memory_profile6_log - INFO -    383                             

2018-04-29 13:54:16,038 - memory_profile6_log - INFO -    384    419.5 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 13:54:16,039 - memory_profile6_log - INFO -    385    419.5 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 13:54:16,039 - memory_profile6_log - INFO -    386    419.5 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 13:54:16,039 - memory_profile6_log - INFO -    387    419.5 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 13:54:16,040 - memory_profile6_log - INFO -    388    419.5 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 13:54:16,040 - memory_profile6_log - INFO -    389    419.5 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 13:54:16,042 - memory_profile6_log - INFO -    390                             

2018-04-29 13:54:16,046 - memory_profile6_log - INFO -    391    419.5 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 13:54:16,046 - memory_profile6_log - INFO - 


2018-04-29 13:54:16,052 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 13:54:16,085 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 13:54:16,085 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-29 13:54:16,086 - memory_profile6_log - INFO - apply on: 5000 total history data(D(t))
2018-04-29 13:54:17,134 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 13:54:17,134 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 13:55:00,436 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-29 13:55:00,464 - memory_profile6_log - INFO - Total train time: 44.380s
2018-04-29 13:55:00,467 - memory_profile6_log - INFO - memory left before cleaning: 70.200 percent memory...
2018-04-29 13:55:00,467 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-29 13:55:00,469 - memory_profile6_log - INFO - deleting df_dut...
2018-04-29 13:55:00,470 - memory_profile6_log - INFO - deleting df_dt...
2018-04-29 13:55:00,470 - memory_profile6_log - INFO - deleting df_input...
2018-04-29 13:55:00,479 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-29 13:55:00,480 - memory_profile6_log - INFO - deleting df_current...
2018-04-29 13:55:00,482 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-29 13:55:00,493 - memory_profile6_log - INFO - deleting model_fit...
2018-04-29 13:55:00,493 - memory_profile6_log - INFO - deleting result...
2018-04-29 13:55:00,513 - memory_profile6_log - INFO - deleting model_transform...
2018-04-29 13:55:00,513 - memory_profile6_log - INFO - memory left after cleaning: 70.000 percent memory...
2018-04-29 13:55:00,516 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-29 13:55:00,516 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-29 13:55:00,684 - memory_profile6_log - INFO - Saving fitted_models as history...
2018-04-29 13:55:00,767 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 10
2018-04-29 13:55:00,769 - memory_profile6_log - INFO - processing batch-0
2018-04-29 13:55:00,964 - memory_profile6_log - INFO - processing batch-1
2018-04-29 13:55:01,153 - memory_profile6_log - INFO - processing batch-2
2018-04-29 13:55:01,331 - memory_profile6_log - INFO - processing batch-3
2018-04-29 13:55:01,516 - memory_profile6_log - INFO - processing batch-4
2018-04-29 13:55:01,703 - memory_profile6_log - INFO - processing batch-5
2018-04-29 13:55:01,884 - memory_profile6_log - INFO - processing batch-6
2018-04-29 13:55:02,066 - memory_profile6_log - INFO - processing batch-7
2018-04-29 13:55:02,260 - memory_profile6_log - INFO - processing batch-8
2018-04-29 13:55:02,453 - memory_profile6_log - INFO - processing batch-9
2018-04-29 13:55:02,637 - memory_profile6_log - INFO - deleting BR...
2018-04-29 13:55:02,638 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-04-29 13:55:02,648 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 13:55:02,648 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 13:55:02,648 - memory_profile6_log - INFO - ================================================

2018-04-29 13:55:02,650 - memory_profile6_log - INFO -    113    419.5 MiB    419.5 MiB   @profile

2018-04-29 13:55:02,651 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-29 13:55:02,651 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-29 13:55:02,651 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-29 13:55:02,651 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-29 13:55:02,651 - memory_profile6_log - INFO -    118                                      saveto="datastore"):

2018-04-29 13:55:02,654 - memory_profile6_log - INFO -    119                                 """

2018-04-29 13:55:02,654 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-29 13:55:02,657 - memory_profile6_log - INFO -    121                                 """

2018-04-29 13:55:02,658 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-29 13:55:02,660 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-29 13:55:02,661 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-29 13:55:02,661 - memory_profile6_log - INFO -    125    419.5 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-29 13:55:02,661 - memory_profile6_log - INFO -    126    427.5 MiB      8.0 MiB       df_dut = df_input.copy(deep=True)

2018-04-29 13:55:02,665 - memory_profile6_log - INFO -    127    427.5 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 13:55:02,667 - memory_profile6_log - INFO -    128                             

2018-04-29 13:55:02,667 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-29 13:55:02,668 - memory_profile6_log - INFO -    130    434.7 MiB      7.3 MiB       df_dt = df_current.copy(deep=True)

2018-04-29 13:55:02,668 - memory_profile6_log - INFO -    131    434.7 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 13:55:02,670 - memory_profile6_log - INFO -    132                             

2018-04-29 13:55:02,671 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-29 13:55:02,671 - memory_profile6_log - INFO -    134    434.7 MiB      0.0 MiB       t0 = time.time()

2018-04-29 13:55:02,671 - memory_profile6_log - INFO -    135    434.7 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-29 13:55:02,673 - memory_profile6_log - INFO -    136    434.7 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-29 13:55:02,678 - memory_profile6_log - INFO -    137    434.7 MiB      0.0 MiB       logger.info("apply on: %d total history data(D(t))", len(df_hist))

2018-04-29 13:55:02,680 - memory_profile6_log - INFO -    138                             

2018-04-29 13:55:02,681 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-29 13:55:02,681 - memory_profile6_log - INFO -    140    434.7 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-29 13:55:02,684 - memory_profile6_log - INFO -    141                             

2018-04-29 13:55:02,684 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-29 13:55:02,686 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-29 13:55:02,686 - memory_profile6_log - INFO -    144    436.8 MiB      2.1 MiB       NB = BR.processX(df_dut)

2018-04-29 13:55:02,687 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-29 13:55:02,690 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-29 13:55:02,691 - memory_profile6_log - INFO -    147    446.7 MiB      9.8 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-29 13:55:02,693 - memory_profile6_log - INFO -    148                                 """

2018-04-29 13:55:02,694 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-29 13:55:02,694 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-29 13:55:02,696 - memory_profile6_log - INFO -    151                                 """

2018-04-29 13:55:02,697 - memory_profile6_log - INFO -    152    446.7 MiB      0.0 MiB       fitby_sigmant = True

2018-04-29 13:55:02,697 - memory_profile6_log - INFO -    153    446.7 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-29 13:55:02,697 - memory_profile6_log - INFO -    154    446.7 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-29 13:55:02,698 - memory_profile6_log - INFO -    155    456.4 MiB      9.7 MiB                            'is_general']]

2018-04-29 13:55:02,703 - memory_profile6_log - INFO -    156    456.4 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-29 13:55:02,704 - memory_profile6_log - INFO -    157    456.4 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-29 13:55:02,707 - memory_profile6_log - INFO -    158    484.7 MiB     28.3 MiB                          verbose=False)

2018-04-29 13:55:02,709 - memory_profile6_log - INFO -    159    484.7 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-29 13:55:02,710 - memory_profile6_log - INFO -    160    484.7 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-29 13:55:02,710 - memory_profile6_log - INFO -    161                             

2018-04-29 13:55:02,711 - memory_profile6_log - INFO -    162                                 # ~~ and Transform ~~

2018-04-29 13:55:02,711 - memory_profile6_log - INFO -    163                                 #   handling current news interest == current date

2018-04-29 13:55:02,716 - memory_profile6_log - INFO -    164    484.7 MiB      0.0 MiB       if df_dt.empty:

2018-04-29 13:55:02,717 - memory_profile6_log - INFO -    165                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-29 13:55:02,719 - memory_profile6_log - INFO -    166                                     return None

2018-04-29 13:55:02,719 - memory_profile6_log - INFO -    167    484.9 MiB      0.2 MiB       NB = BR.processX(df_dt)

2018-04-29 13:55:02,720 - memory_profile6_log - INFO -    168    495.4 MiB     10.6 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-29 13:55:02,720 - memory_profile6_log - INFO -    169                             

2018-04-29 13:55:02,721 - memory_profile6_log - INFO -    170    495.4 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-29 13:55:02,721 - memory_profile6_log - INFO -    171    494.5 MiB     -0.9 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-29 13:55:02,723 - memory_profile6_log - INFO -    172    494.5 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-29 13:55:02,726 - memory_profile6_log - INFO -    173    494.5 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-29 13:55:02,726 - memory_profile6_log - INFO -    174    513.5 MiB     19.0 MiB                                                     verbose=False)

2018-04-29 13:55:02,729 - memory_profile6_log - INFO -    175                                 # ~~~ filter is general and specific topic ~~~

2018-04-29 13:55:02,729 - memory_profile6_log - INFO -    176                                 # the idea is just we need to rerank every topic according

2018-04-29 13:55:02,730 - memory_profile6_log - INFO -    177                                 # user_id and and is_general by p0_posterior

2018-04-29 13:55:02,730 - memory_profile6_log - INFO -    178    513.5 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-29 13:55:02,730 - memory_profile6_log - INFO -    179    515.4 MiB      1.9 MiB                                     'is_general']].groupby(['topic_id',

2018-04-29 13:55:02,730 - memory_profile6_log - INFO -    180    513.5 MiB     -1.9 MiB                                                             'is_general']

2018-04-29 13:55:02,730 - memory_profile6_log - INFO -    181                                                                                      ).size().to_frame().reset_index()

2018-04-29 13:55:02,730 - memory_profile6_log - INFO -    182                             

2018-04-29 13:55:02,733 - memory_profile6_log - INFO -    183                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-29 13:55:02,733 - memory_profile6_log - INFO -    184                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-29 13:55:02,734 - memory_profile6_log - INFO -    185    513.5 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-29 13:55:02,734 - memory_profile6_log - INFO -    186                             

2018-04-29 13:55:02,734 - memory_profile6_log - INFO -    187                                 # ~ start by provide rank for each topic type ~

2018-04-29 13:55:02,734 - memory_profile6_log - INFO -    188    534.6 MiB     21.1 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-29 13:55:02,739 - memory_profile6_log - INFO -    189    538.8 MiB      4.1 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-29 13:55:02,740 - memory_profile6_log - INFO -    190                             

2018-04-29 13:55:02,740 - memory_profile6_log - INFO -    191                                 # ~ set threshold to filter output

2018-04-29 13:55:02,743 - memory_profile6_log - INFO -    192    538.8 MiB      0.0 MiB       if threshold > 0:

2018-04-29 13:55:02,743 - memory_profile6_log - INFO -    193    538.8 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-29 13:55:02,743 - memory_profile6_log - INFO -    194    538.8 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-29 13:55:02,743 - memory_profile6_log - INFO -    195    537.7 MiB     -1.1 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-29 13:55:02,743 - memory_profile6_log - INFO -    196                             

2018-04-29 13:55:02,744 - memory_profile6_log - INFO -    197    537.7 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 13:55:02,744 - memory_profile6_log - INFO -    198    537.7 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-29 13:55:02,744 - memory_profile6_log - INFO -    199                             

2018-04-29 13:55:02,746 - memory_profile6_log - INFO -    200    537.7 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 13:55:02,746 - memory_profile6_log - INFO -    201                             

2018-04-29 13:55:02,746 - memory_profile6_log - INFO -    202    537.7 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-29 13:55:02,746 - memory_profile6_log - INFO -    203    537.7 MiB      0.0 MiB       del df_dut

2018-04-29 13:55:02,750 - memory_profile6_log - INFO -    204    537.7 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-29 13:55:02,750 - memory_profile6_log - INFO -    205    537.7 MiB      0.0 MiB       del df_dt

2018-04-29 13:55:02,752 - memory_profile6_log - INFO -    206    537.7 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-29 13:55:02,753 - memory_profile6_log - INFO -    207    537.7 MiB      0.0 MiB       del df_input

2018-04-29 13:55:02,753 - memory_profile6_log - INFO -    208    537.7 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-29 13:55:02,753 - memory_profile6_log - INFO -    209    528.9 MiB     -8.8 MiB       del df_input_X

2018-04-29 13:55:02,753 - memory_profile6_log - INFO -    210    528.9 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-29 13:55:02,753 - memory_profile6_log - INFO -    211    528.9 MiB      0.0 MiB       del df_current

2018-04-29 13:55:02,755 - memory_profile6_log - INFO -    212    528.9 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-29 13:55:02,755 - memory_profile6_log - INFO -    213    528.9 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-29 13:55:02,755 - memory_profile6_log - INFO -    214    528.9 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-29 13:55:02,756 - memory_profile6_log - INFO -    215    505.6 MiB    -23.3 MiB       del model_fit

2018-04-29 13:55:02,756 - memory_profile6_log - INFO -    216    505.6 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-29 13:55:02,756 - memory_profile6_log - INFO -    217    505.6 MiB      0.0 MiB       del result

2018-04-29 13:55:02,760 - memory_profile6_log - INFO -    218    505.6 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-29 13:55:02,763 - memory_profile6_log - INFO -    219                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 13:55:02,763 - memory_profile6_log - INFO -    220                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 13:55:02,763 - memory_profile6_log - INFO -    221                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 13:55:02,763 - memory_profile6_log - INFO -    222    505.6 MiB      0.0 MiB       if savetrain:

2018-04-29 13:55:02,765 - memory_profile6_log - INFO -    223    511.0 MiB      5.4 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-29 13:55:02,766 - memory_profile6_log - INFO -    224    511.0 MiB      0.0 MiB           del model_transform

2018-04-29 13:55:02,766 - memory_profile6_log - INFO -    225    511.0 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-29 13:55:02,767 - memory_profile6_log - INFO -    226    511.0 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 13:55:02,767 - memory_profile6_log - INFO -    227                             

2018-04-29 13:55:02,769 - memory_profile6_log - INFO -    228    511.0 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-29 13:55:02,769 - memory_profile6_log - INFO -    229                                     # ~ Place your code to save the training model here ~

2018-04-29 13:55:02,772 - memory_profile6_log - INFO -    230    511.0 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-29 13:55:02,773 - memory_profile6_log - INFO -    231    511.0 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-29 13:55:02,773 - memory_profile6_log - INFO -    232    511.0 MiB      0.0 MiB               if multproc:

2018-04-29 13:55:02,776 - memory_profile6_log - INFO -    233    488.8 MiB    -22.2 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-29 13:55:02,776 - memory_profile6_log - INFO -    234                             

2018-04-29 13:55:02,776 - memory_profile6_log - INFO -    235                                             # ~ save fitted models ~

2018-04-29 13:55:02,778 - memory_profile6_log - INFO -    236    488.8 MiB      0.0 MiB                   logger.info("Saving fitted_models as history...")

2018-04-29 13:55:02,778 - memory_profile6_log - INFO -    237    488.9 MiB      0.1 MiB                   save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 13:55:02,778 - memory_profile6_log - INFO -    238    502.6 MiB     13.7 MiB                   fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-29 13:55:02,779 - memory_profile6_log - INFO -    239                             

2018-04-29 13:55:02,779 - memory_profile6_log - INFO -    240    507.1 MiB      4.5 MiB                   X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-29 13:55:02,779 - memory_profile6_log - INFO -    241    507.1 MiB      0.0 MiB                   logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-29 13:55:02,779 - memory_profile6_log - INFO -    242    508.5 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 13:55:02,783 - memory_profile6_log - INFO -    243    508.5 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 13:55:02,786 - memory_profile6_log - INFO -    244    508.5 MiB      1.5 MiB                       mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-29 13:55:02,786 - memory_profile6_log - INFO -    245                             

2018-04-29 13:55:02,786 - memory_profile6_log - INFO -    246    508.5 MiB      0.0 MiB                   del X_split

2018-04-29 13:55:02,786 - memory_profile6_log - INFO -    247    508.5 MiB      0.0 MiB                   del BR

2018-04-29 13:55:02,788 - memory_profile6_log - INFO -    248    508.5 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-29 13:55:02,788 - memory_profile6_log - INFO -    249                             

2018-04-29 13:55:02,789 - memory_profile6_log - INFO -    250    508.5 MiB      0.0 MiB                   del save_sigma_nt

2018-04-29 13:55:02,789 - memory_profile6_log - INFO -    251    508.5 MiB      0.0 MiB                   logger.info("deleting save_sigma_nt...")

2018-04-29 13:55:02,793 - memory_profile6_log - INFO -    252                                             

2018-04-29 13:55:02,793 - memory_profile6_log - INFO -    253                                     elif str(saveto).lower() == "elastic":

2018-04-29 13:55:02,796 - memory_profile6_log - INFO -    254                                         logger.info("Using ElasticSearch as storage...")

2018-04-29 13:55:02,796 - memory_profile6_log - INFO -    255                                         mh.saveElasticS(model_transformsv)

2018-04-29 13:55:02,796 - memory_profile6_log - INFO -    256                             

2018-04-29 13:55:02,798 - memory_profile6_log - INFO -    257                                     # need save sigma_nt for daily train

2018-04-29 13:55:02,798 - memory_profile6_log - INFO -    258                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-29 13:55:02,798 - memory_profile6_log - INFO -    259                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-29 13:55:02,799 - memory_profile6_log - INFO -    260    508.5 MiB      0.0 MiB           if start_date and end_date:

2018-04-29 13:55:02,801 - memory_profile6_log - INFO -    261                                         if not fitby_sigmant:

2018-04-29 13:55:02,801 - memory_profile6_log - INFO -    262                                             logging.info("Saving sigma Nt...")

2018-04-29 13:55:02,802 - memory_profile6_log - INFO -    263                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 13:55:02,802 - memory_profile6_log - INFO -    264                                             save_sigma_nt['start_date'] = start_date

2018-04-29 13:55:02,802 - memory_profile6_log - INFO -    265                                             save_sigma_nt['end_date'] = end_date

2018-04-29 13:55:02,805 - memory_profile6_log - INFO -    266                                             print save_sigma_nt.head(5)

2018-04-29 13:55:02,809 - memory_profile6_log - INFO -    267                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-29 13:55:02,809 - memory_profile6_log - INFO -    268                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-29 13:55:02,811 - memory_profile6_log - INFO -    269    508.5 MiB      0.0 MiB       return

2018-04-29 13:55:02,812 - memory_profile6_log - INFO - 


2018-04-29 13:55:02,812 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-29 13:58:43,009 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 13:58:43,013 - memory_profile6_log - INFO - date_generated: 
2018-04-29 13:58:43,013 - memory_profile6_log - INFO -  
2018-04-29 13:58:43,013 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 13:58:43,013 - memory_profile6_log - INFO - 

2018-04-29 13:58:43,015 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 13:58:43,015 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 13:58:43,016 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 13:58:43,154 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 13:58:43,157 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 13:59:55,194 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 13:59:55,196 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 13:59:55,236 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 13:59:55,239 - memory_profile6_log - INFO - Appending history data...
2018-04-29 13:59:55,240 - memory_profile6_log - INFO - processing batch-0
2018-04-29 13:59:55,240 - memory_profile6_log - INFO - creating list history data...
2018-04-29 13:59:55,319 - memory_profile6_log - INFO - call history data...
2018-04-29 14:00:53,813 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:00:55,125 - memory_profile6_log - INFO - processing batch-1
2018-04-29 14:00:55,127 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:00:55,174 - memory_profile6_log - INFO - call history data...
2018-04-29 14:01:51,164 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:01:52,454 - memory_profile6_log - INFO - processing batch-2
2018-04-29 14:01:52,456 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:01:52,503 - memory_profile6_log - INFO - call history data...
2018-04-29 14:02:48,006 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:02:49,318 - memory_profile6_log - INFO - processing batch-3
2018-04-29 14:02:49,319 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:02:49,371 - memory_profile6_log - INFO - call history data...
2018-04-29 14:03:43,608 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:03:45,026 - memory_profile6_log - INFO - processing batch-4
2018-04-29 14:03:45,028 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:03:45,076 - memory_profile6_log - INFO - call history data...
2018-04-29 14:04:39,763 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:04:41,144 - memory_profile6_log - INFO - Appending training data...
2018-04-29 14:04:41,144 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 14:04:41,145 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 14:04:41,154 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 14:04:41,155 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 14:04:41,157 - memory_profile6_log - INFO - ================================================

2018-04-29 14:04:41,161 - memory_profile6_log - INFO -    295     86.7 MiB     86.7 MiB   @profile

2018-04-29 14:04:41,164 - memory_profile6_log - INFO -    296                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 14:04:41,165 - memory_profile6_log - INFO -    297     86.7 MiB      0.0 MiB       bq_client = client

2018-04-29 14:04:41,167 - memory_profile6_log - INFO -    298     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 14:04:41,168 - memory_profile6_log - INFO -    299                             

2018-04-29 14:04:41,170 - memory_profile6_log - INFO -    300     86.7 MiB      0.0 MiB       datalist = []

2018-04-29 14:04:41,171 - memory_profile6_log - INFO -    301     86.7 MiB      0.0 MiB       datalist_hist = []

2018-04-29 14:04:41,171 - memory_profile6_log - INFO -    302                             

2018-04-29 14:04:41,171 - memory_profile6_log - INFO -    303     86.7 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 14:04:41,174 - memory_profile6_log - INFO -    304    444.9 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 14:04:41,176 - memory_profile6_log - INFO -    305    339.0 MiB    252.3 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 14:04:41,178 - memory_profile6_log - INFO -    306    339.0 MiB      0.0 MiB           if tframe is not None:

2018-04-29 14:04:41,180 - memory_profile6_log - INFO -    307    339.0 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 14:04:41,180 - memory_profile6_log - INFO -    308    346.5 MiB      7.5 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 14:04:41,180 - memory_profile6_log - INFO -    309    346.5 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 14:04:41,181 - memory_profile6_log - INFO -    310    346.5 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 14:04:41,181 - memory_profile6_log - INFO -    311    444.9 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 14:04:41,184 - memory_profile6_log - INFO -    312                                                 # ~ loading history

2018-04-29 14:04:41,187 - memory_profile6_log - INFO -    313                                                 """

2018-04-29 14:04:41,188 - memory_profile6_log - INFO -    314                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 14:04:41,190 - memory_profile6_log - INFO -    315                                                 """

2018-04-29 14:04:41,190 - memory_profile6_log - INFO -    316    428.3 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 14:04:41,191 - memory_profile6_log - INFO -    317                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 14:04:41,191 - memory_profile6_log - INFO -    318    428.3 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 14:04:41,194 - memory_profile6_log - INFO -    319                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 14:04:41,194 - memory_profile6_log - INFO -    320    429.0 MiB      5.1 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 14:04:41,197 - memory_profile6_log - INFO -    321                             

2018-04-29 14:04:41,197 - memory_profile6_log - INFO -    322    429.0 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 14:04:41,197 - memory_profile6_log - INFO -    323    475.2 MiB    256.6 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 14:04:41,198 - memory_profile6_log - INFO -    324                             

2018-04-29 14:04:41,200 - memory_profile6_log - INFO -    325                                                 # me = os.getpid()

2018-04-29 14:04:41,200 - memory_profile6_log - INFO -    326                                                 # kill_proc_tree(me)

2018-04-29 14:04:41,201 - memory_profile6_log - INFO -    327                             

2018-04-29 14:04:41,201 - memory_profile6_log - INFO -    328    475.2 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 14:04:41,206 - memory_profile6_log - INFO -    329    476.4 MiB     -1.4 MiB                       for m in h_frame:

2018-04-29 14:04:41,207 - memory_profile6_log - INFO -    330    476.4 MiB     -1.4 MiB                           if m is not None:

2018-04-29 14:04:41,207 - memory_profile6_log - INFO -    331    476.4 MiB     -1.4 MiB                               if len(m) > 0:

2018-04-29 14:04:41,209 - memory_profile6_log - INFO -    332    476.4 MiB      3.0 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 14:04:41,210 - memory_profile6_log - INFO -    333    444.9 MiB   -166.6 MiB                       del h_frame

2018-04-29 14:04:41,210 - memory_profile6_log - INFO -    334    444.9 MiB     -1.2 MiB                       del lhistory

2018-04-29 14:04:41,211 - memory_profile6_log - INFO -    335                             

2018-04-29 14:04:41,213 - memory_profile6_log - INFO -    336    444.9 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 14:04:41,213 - memory_profile6_log - INFO -    337    444.9 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 14:04:41,217 - memory_profile6_log - INFO -    338                                     else: 

2018-04-29 14:04:41,217 - memory_profile6_log - INFO -    339                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 14:04:41,219 - memory_profile6_log - INFO -    340    444.9 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 14:04:41,220 - memory_profile6_log - INFO -    341    444.9 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 14:04:41,220 - memory_profile6_log - INFO -    342                             

2018-04-29 14:04:41,221 - memory_profile6_log - INFO -    343    444.9 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 14:04:41,223 - memory_profile6_log - INFO - 


2018-04-29 14:04:42,394 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 14:04:42,469 - memory_profile6_log - INFO -     p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0    0.001473         269.647011         4             279.647011  22291119  16174e7bea6105-01e4bf88dc23d8-80f0045-38400-16...
1    0.000646        3688.651934        48            3698.651934  10960288  1613f71c69c80-0616565e91cf83-6b6f613a-38400-16...
2    0.000646        3688.651934        61            3698.651934  10960288  1612d17ab2361-0a61468d795199-43426071-64320-16...
3    0.001473         269.647011        16             279.647011  22291119  1612d3cf24eb0-03b5c67043a8fb-4962427c-38400-16...
4    0.000055        5563.716667         4            5573.716667  11911567  1613746d53017-007a2bf6f22f28-70261016-38400-16...
5    0.000646        1844.325967         8            1854.325967  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
6    0.000055        5563.716667        57            5573.716667  11911567  162c08c688718-0a56519df6356a-776c4f30-38400-16...
7    0.000055        5563.716667        36            5573.716667  11911567  16135d850fb2e1-039c66e4be45428-4d707656-2c600-...
8    0.000055        5563.716667        15            5573.716667  11911567  161a8dd819211-08b69608c04698-15680144-4df28-16...
9    0.001473         269.647011         4             279.647011  22291119  161316a171411a-0feb060c12d58f-73261116-38400-1...
10   0.001473         269.647011         4             279.647011  22291119  16130674cb93f8-0e322948df367c-282b543f-38400-1...
11   0.001473         269.647011         4             279.647011  22291119  1611b7182ba1d8-09456ef3a53157-5f6c3a73-ff000-1...
12   0.001473         269.647011        32             279.647011  22291119  161a333c831e-00b875a76c6048-28313a6c-38400-161...
13   0.000646        7377.303867        74            7387.303867  10960288  1613335453c267-06848318f5de208-76313118-3d10d-...
14   0.000646        1844.325967         4            1854.325967  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
15   0.000646        1844.325967         4            1854.325967  10960288  161e5a454f6161-064229b4591715-61643d25-38400-1...
16   0.000646        3688.651934        24            3698.651934  10960288  1612d9f9772101-0b70a16f870af8-452b452b-38400-1...
17   0.000646        1844.325967        79            1854.325967  10960288  1612d0e57287b-00705a12b256f1-7666d1c-38400-161...
18   0.000646        1844.325967        16            1854.325967  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
19   0.000646        1844.325967        75            1854.325967  10960288  1612f72ab9c12f-067c6547446aa9-68600a3e-49a10-1...
2018-04-29 14:04:42,470 - memory_profile6_log - INFO - 

2018-04-29 14:04:42,553 - memory_profile6_log - INFO - size of big_frame_hist: 69.87 MB
2018-04-29 14:04:42,622 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 14:04:42,634 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 14:05:34,625 - memory_profile6_log - INFO - size of df: 58.30 MB
2018-04-29 14:05:34,627 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-29 14:05:34,727 - memory_profile6_log - INFO - size of current_frame: 58.30 MB
2018-04-29 14:05:34,730 - memory_profile6_log - INFO - loading time of: 484420 total genuine-current interest data ~ take 411.597s
2018-04-29 14:05:34,744 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 14:05:34,746 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 14:05:34,746 - memory_profile6_log - INFO - ================================================

2018-04-29 14:05:34,749 - memory_profile6_log - INFO -    345     86.6 MiB     86.6 MiB   @profile

2018-04-29 14:05:34,750 - memory_profile6_log - INFO -    346                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 14:05:34,753 - memory_profile6_log - INFO -    347     86.7 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 14:05:34,753 - memory_profile6_log - INFO -    348     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 14:05:34,755 - memory_profile6_log - INFO -    349                             

2018-04-29 14:05:34,756 - memory_profile6_log - INFO -    350                                 # ~~~ Begin collecting data ~~~

2018-04-29 14:05:34,756 - memory_profile6_log - INFO -    351     86.7 MiB      0.0 MiB       t0 = time.time()

2018-04-29 14:05:34,760 - memory_profile6_log - INFO -    352    441.8 MiB    355.1 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 14:05:34,762 - memory_profile6_log - INFO -    353    441.8 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 14:05:34,762 - memory_profile6_log - INFO -    354                                     logger.info("Training cannot be empty..")

2018-04-29 14:05:34,763 - memory_profile6_log - INFO -    355                                     return False

2018-04-29 14:05:34,763 - memory_profile6_log - INFO -    356    457.4 MiB     15.6 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 14:05:34,763 - memory_profile6_log - INFO -    357    457.7 MiB      0.2 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 14:05:34,763 - memory_profile6_log - INFO -    358    457.7 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 14:05:34,765 - memory_profile6_log - INFO -    359                             

2018-04-29 14:05:34,765 - memory_profile6_log - INFO -    360    463.7 MiB      6.1 MiB       big_frame = pd.concat(datalist)

2018-04-29 14:05:34,766 - memory_profile6_log - INFO -    361    463.7 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 14:05:34,766 - memory_profile6_log - INFO -    362    457.9 MiB     -5.8 MiB       del datalist

2018-04-29 14:05:34,769 - memory_profile6_log - INFO -    363                             

2018-04-29 14:05:34,770 - memory_profile6_log - INFO -    364    457.9 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 14:05:34,772 - memory_profile6_log - INFO -    365                             

2018-04-29 14:05:34,772 - memory_profile6_log - INFO -    366                                 # ~ get current news interest ~

2018-04-29 14:05:34,773 - memory_profile6_log - INFO -    367    457.9 MiB      0.0 MiB       if not cd:

2018-04-29 14:05:34,773 - memory_profile6_log - INFO -    368                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 14:05:34,773 - memory_profile6_log - INFO -    369                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 14:05:34,773 - memory_profile6_log - INFO -    370                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 14:05:34,773 - memory_profile6_log - INFO -    371                                 else:

2018-04-29 14:05:34,775 - memory_profile6_log - INFO -    372    457.9 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 14:05:34,776 - memory_profile6_log - INFO -    373    457.9 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 14:05:34,776 - memory_profile6_log - INFO -    374                             

2018-04-29 14:05:34,776 - memory_profile6_log - INFO -    375                                     # safe handling of query parameter

2018-04-29 14:05:34,778 - memory_profile6_log - INFO -    376                                     query_params = [

2018-04-29 14:05:34,782 - memory_profile6_log - INFO -    377    457.9 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 14:05:34,782 - memory_profile6_log - INFO -    378                                     ]

2018-04-29 14:05:34,783 - memory_profile6_log - INFO -    379                             

2018-04-29 14:05:34,783 - memory_profile6_log - INFO -    380    457.9 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-29 14:05:34,785 - memory_profile6_log - INFO -    381    515.6 MiB     57.8 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 14:05:34,786 - memory_profile6_log - INFO -    382    515.6 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 14:05:34,788 - memory_profile6_log - INFO -    383                             

2018-04-29 14:05:34,788 - memory_profile6_log - INFO -    384    515.7 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 14:05:34,789 - memory_profile6_log - INFO -    385    515.7 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 14:05:34,790 - memory_profile6_log - INFO -    386    515.7 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 14:05:34,792 - memory_profile6_log - INFO -    387    515.7 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 14:05:34,792 - memory_profile6_log - INFO -    388    515.7 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 14:05:34,793 - memory_profile6_log - INFO -    389    515.7 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 14:05:34,795 - memory_profile6_log - INFO -    390                             

2018-04-29 14:05:34,795 - memory_profile6_log - INFO -    391    515.7 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 14:05:34,796 - memory_profile6_log - INFO - 


2018-04-29 14:05:34,799 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 14:05:34,832 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 14:05:34,834 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-29 14:05:34,835 - memory_profile6_log - INFO - apply on: 253995 total history data(D(t))
2018-04-29 14:05:35,839 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 14:05:35,842 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 14:06:20,223 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-29 14:06:20,259 - memory_profile6_log - INFO - Total train time: 45.426s
2018-04-29 14:06:20,259 - memory_profile6_log - INFO - memory left before cleaning: 70.500 percent memory...
2018-04-29 14:06:20,260 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-29 14:06:20,263 - memory_profile6_log - INFO - deleting df_dut...
2018-04-29 14:06:20,263 - memory_profile6_log - INFO - deleting df_dt...
2018-04-29 14:06:20,266 - memory_profile6_log - INFO - deleting df_input...
2018-04-29 14:06:20,279 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-29 14:06:20,279 - memory_profile6_log - INFO - deleting df_current...
2018-04-29 14:06:20,282 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-29 14:06:20,295 - memory_profile6_log - INFO - deleting model_fit...
2018-04-29 14:06:20,296 - memory_profile6_log - INFO - deleting result...
2018-04-29 14:06:20,319 - memory_profile6_log - INFO - deleting model_transform...
2018-04-29 14:06:20,319 - memory_profile6_log - INFO - memory left after cleaning: 70.400 percent memory...
2018-04-29 14:06:20,321 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-29 14:06:20,322 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-29 14:06:20,515 - memory_profile6_log - INFO - Saving fitted_models as history...
2018-04-29 14:06:20,604 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 10
2018-04-29 14:06:20,605 - memory_profile6_log - INFO - processing batch-0
2018-04-29 14:06:20,796 - memory_profile6_log - INFO - processing batch-1
2018-04-29 14:06:20,986 - memory_profile6_log - INFO - processing batch-2
2018-04-29 14:06:21,171 - memory_profile6_log - INFO - processing batch-3
2018-04-29 14:06:21,364 - memory_profile6_log - INFO - processing batch-4
2018-04-29 14:06:21,555 - memory_profile6_log - INFO - processing batch-5
2018-04-29 14:06:21,737 - memory_profile6_log - INFO - processing batch-6
2018-04-29 14:06:21,921 - memory_profile6_log - INFO - processing batch-7
2018-04-29 14:06:22,112 - memory_profile6_log - INFO - processing batch-8
2018-04-29 14:06:22,306 - memory_profile6_log - INFO - processing batch-9
2018-04-29 14:06:22,503 - memory_profile6_log - INFO - deleting BR...
2018-04-29 14:06:22,505 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-04-29 14:06:22,515 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 14:06:22,515 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 14:06:22,516 - memory_profile6_log - INFO - ================================================

2018-04-29 14:06:22,516 - memory_profile6_log - INFO -    113    513.2 MiB    513.2 MiB   @profile

2018-04-29 14:06:22,517 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-29 14:06:22,517 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-29 14:06:22,523 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-29 14:06:22,523 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-29 14:06:22,525 - memory_profile6_log - INFO -    118                                      saveto="datastore"):

2018-04-29 14:06:22,525 - memory_profile6_log - INFO -    119                                 """

2018-04-29 14:06:22,525 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-29 14:06:22,526 - memory_profile6_log - INFO -    121                                 """

2018-04-29 14:06:22,526 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-29 14:06:22,526 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-29 14:06:22,528 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-29 14:06:22,528 - memory_profile6_log - INFO -    125    513.2 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-29 14:06:22,529 - memory_profile6_log - INFO -    126    521.2 MiB      8.0 MiB       df_dut = df_input.copy(deep=True)

2018-04-29 14:06:22,529 - memory_profile6_log - INFO -    127    521.2 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 14:06:22,529 - memory_profile6_log - INFO -    128                             

2018-04-29 14:06:22,529 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-29 14:06:22,529 - memory_profile6_log - INFO -    130    528.3 MiB      7.1 MiB       df_dt = df_current.copy(deep=True)

2018-04-29 14:06:22,533 - memory_profile6_log - INFO -    131    528.3 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 14:06:22,535 - memory_profile6_log - INFO -    132                             

2018-04-29 14:06:22,536 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-29 14:06:22,536 - memory_profile6_log - INFO -    134    528.3 MiB      0.0 MiB       t0 = time.time()

2018-04-29 14:06:22,536 - memory_profile6_log - INFO -    135    528.3 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-29 14:06:22,536 - memory_profile6_log - INFO -    136    528.3 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-29 14:06:22,536 - memory_profile6_log - INFO -    137    528.3 MiB      0.0 MiB       logger.info("apply on: %d total history data(D(t))", len(df_hist))

2018-04-29 14:06:22,538 - memory_profile6_log - INFO -    138                             

2018-04-29 14:06:22,538 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-29 14:06:22,539 - memory_profile6_log - INFO -    140    528.3 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-29 14:06:22,539 - memory_profile6_log - INFO -    141                             

2018-04-29 14:06:22,539 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-29 14:06:22,540 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-29 14:06:22,540 - memory_profile6_log - INFO -    144    530.2 MiB      1.9 MiB       NB = BR.processX(df_dut)

2018-04-29 14:06:22,540 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-29 14:06:22,542 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-29 14:06:22,542 - memory_profile6_log - INFO -    147    540.1 MiB      9.9 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-29 14:06:22,542 - memory_profile6_log - INFO -    148                                 """

2018-04-29 14:06:22,546 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-29 14:06:22,546 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-29 14:06:22,549 - memory_profile6_log - INFO -    151                                 """

2018-04-29 14:06:22,549 - memory_profile6_log - INFO -    152    540.1 MiB      0.0 MiB       fitby_sigmant = True

2018-04-29 14:06:22,549 - memory_profile6_log - INFO -    153    540.1 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-29 14:06:22,549 - memory_profile6_log - INFO -    154    540.1 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-29 14:06:22,549 - memory_profile6_log - INFO -    155    549.8 MiB      9.7 MiB                            'is_general']]

2018-04-29 14:06:22,551 - memory_profile6_log - INFO -    156    549.8 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-29 14:06:22,551 - memory_profile6_log - INFO -    157    549.8 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-29 14:06:22,551 - memory_profile6_log - INFO -    158    575.9 MiB     26.1 MiB                          verbose=False)

2018-04-29 14:06:22,552 - memory_profile6_log - INFO -    159    575.9 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-29 14:06:22,552 - memory_profile6_log - INFO -    160    575.9 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-29 14:06:22,552 - memory_profile6_log - INFO -    161                             

2018-04-29 14:06:22,552 - memory_profile6_log - INFO -    162                                 # ~~ and Transform ~~

2018-04-29 14:06:22,552 - memory_profile6_log - INFO -    163                                 #   handling current news interest == current date

2018-04-29 14:06:22,553 - memory_profile6_log - INFO -    164    575.9 MiB      0.0 MiB       if df_dt.empty:

2018-04-29 14:06:22,553 - memory_profile6_log - INFO -    165                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-29 14:06:22,553 - memory_profile6_log - INFO -    166                                     return None

2018-04-29 14:06:22,555 - memory_profile6_log - INFO -    167    576.2 MiB      0.3 MiB       NB = BR.processX(df_dt)

2018-04-29 14:06:22,558 - memory_profile6_log - INFO -    168    586.7 MiB     10.6 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-29 14:06:22,559 - memory_profile6_log - INFO -    169                             

2018-04-29 14:06:22,561 - memory_profile6_log - INFO -    170    586.7 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-29 14:06:22,561 - memory_profile6_log - INFO -    171    585.8 MiB     -0.9 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-29 14:06:22,561 - memory_profile6_log - INFO -    172    585.8 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-29 14:06:22,562 - memory_profile6_log - INFO -    173    585.8 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-29 14:06:22,562 - memory_profile6_log - INFO -    174    604.5 MiB     18.7 MiB                                                     verbose=False)

2018-04-29 14:06:22,562 - memory_profile6_log - INFO -    175                                 # ~~~ filter is general and specific topic ~~~

2018-04-29 14:06:22,562 - memory_profile6_log - INFO -    176                                 # the idea is just we need to rerank every topic according

2018-04-29 14:06:22,562 - memory_profile6_log - INFO -    177                                 # user_id and and is_general by p0_posterior

2018-04-29 14:06:22,563 - memory_profile6_log - INFO -    178    604.5 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-29 14:06:22,563 - memory_profile6_log - INFO -    179    606.5 MiB      1.9 MiB                                     'is_general']].groupby(['topic_id',

2018-04-29 14:06:22,565 - memory_profile6_log - INFO -    180    604.6 MiB     -1.9 MiB                                                             'is_general']

2018-04-29 14:06:22,565 - memory_profile6_log - INFO -    181                                                                                      ).size().to_frame().reset_index()

2018-04-29 14:06:22,565 - memory_profile6_log - INFO -    182                             

2018-04-29 14:06:22,565 - memory_profile6_log - INFO -    183                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-29 14:06:22,566 - memory_profile6_log - INFO -    184                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-29 14:06:22,569 - memory_profile6_log - INFO -    185    604.6 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-29 14:06:22,572 - memory_profile6_log - INFO -    186                             

2018-04-29 14:06:22,572 - memory_profile6_log - INFO -    187                                 # ~ start by provide rank for each topic type ~

2018-04-29 14:06:22,573 - memory_profile6_log - INFO -    188    608.9 MiB      4.4 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-29 14:06:22,573 - memory_profile6_log - INFO -    189    614.6 MiB      5.6 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-29 14:06:22,573 - memory_profile6_log - INFO -    190                             

2018-04-29 14:06:22,575 - memory_profile6_log - INFO -    191                                 # ~ set threshold to filter output

2018-04-29 14:06:22,575 - memory_profile6_log - INFO -    192    614.6 MiB      0.0 MiB       if threshold > 0:

2018-04-29 14:06:22,575 - memory_profile6_log - INFO -    193    614.6 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-29 14:06:22,575 - memory_profile6_log - INFO -    194    614.6 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-29 14:06:22,575 - memory_profile6_log - INFO -    195    614.6 MiB      0.0 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-29 14:06:22,575 - memory_profile6_log - INFO -    196                             

2018-04-29 14:06:22,576 - memory_profile6_log - INFO -    197    614.6 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 14:06:22,576 - memory_profile6_log - INFO -    198    614.6 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-29 14:06:22,578 - memory_profile6_log - INFO -    199                             

2018-04-29 14:06:22,582 - memory_profile6_log - INFO -    200    614.6 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 14:06:22,582 - memory_profile6_log - INFO -    201                             

2018-04-29 14:06:22,584 - memory_profile6_log - INFO -    202    614.6 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-29 14:06:22,584 - memory_profile6_log - INFO -    203    614.6 MiB      0.0 MiB       del df_dut

2018-04-29 14:06:22,584 - memory_profile6_log - INFO -    204    614.6 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-29 14:06:22,585 - memory_profile6_log - INFO -    205    614.6 MiB      0.0 MiB       del df_dt

2018-04-29 14:06:22,585 - memory_profile6_log - INFO -    206    614.6 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-29 14:06:22,585 - memory_profile6_log - INFO -    207    614.6 MiB      0.0 MiB       del df_input

2018-04-29 14:06:22,585 - memory_profile6_log - INFO -    208    614.6 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-29 14:06:22,585 - memory_profile6_log - INFO -    209    605.8 MiB     -8.8 MiB       del df_input_X

2018-04-29 14:06:22,585 - memory_profile6_log - INFO -    210    605.8 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-29 14:06:22,586 - memory_profile6_log - INFO -    211    605.8 MiB      0.0 MiB       del df_current

2018-04-29 14:06:22,586 - memory_profile6_log - INFO -    212    605.8 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-29 14:06:22,586 - memory_profile6_log - INFO -    213    605.8 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-29 14:06:22,588 - memory_profile6_log - INFO -    214    605.8 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-29 14:06:22,588 - memory_profile6_log - INFO -    215    582.6 MiB    -23.3 MiB       del model_fit

2018-04-29 14:06:22,589 - memory_profile6_log - INFO -    216    582.6 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-29 14:06:22,592 - memory_profile6_log - INFO -    217    582.6 MiB      0.0 MiB       del result

2018-04-29 14:06:22,594 - memory_profile6_log - INFO -    218    582.6 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-29 14:06:22,595 - memory_profile6_log - INFO -    219                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 14:06:22,595 - memory_profile6_log - INFO -    220                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 14:06:22,595 - memory_profile6_log - INFO -    221                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 14:06:22,595 - memory_profile6_log - INFO -    222    582.6 MiB      0.0 MiB       if savetrain:

2018-04-29 14:06:22,595 - memory_profile6_log - INFO -    223    588.0 MiB      5.4 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-29 14:06:22,596 - memory_profile6_log - INFO -    224    588.0 MiB      0.0 MiB           del model_transform

2018-04-29 14:06:22,598 - memory_profile6_log - INFO -    225    588.0 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-29 14:06:22,598 - memory_profile6_log - INFO -    226    588.0 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 14:06:22,598 - memory_profile6_log - INFO -    227                             

2018-04-29 14:06:22,599 - memory_profile6_log - INFO -    228    588.0 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-29 14:06:22,599 - memory_profile6_log - INFO -    229                                     # ~ Place your code to save the training model here ~

2018-04-29 14:06:22,601 - memory_profile6_log - INFO -    230    588.0 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-29 14:06:22,601 - memory_profile6_log - INFO -    231    588.0 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-29 14:06:22,605 - memory_profile6_log - INFO -    232    588.0 MiB      0.0 MiB               if multproc:

2018-04-29 14:06:22,605 - memory_profile6_log - INFO -    233    566.4 MiB    -21.6 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-29 14:06:22,605 - memory_profile6_log - INFO -    234                             

2018-04-29 14:06:22,607 - memory_profile6_log - INFO -    235                                             # ~ save fitted models ~

2018-04-29 14:06:22,608 - memory_profile6_log - INFO -    236    566.4 MiB      0.0 MiB                   logger.info("Saving fitted_models as history...")

2018-04-29 14:06:22,608 - memory_profile6_log - INFO -    237    566.6 MiB      0.2 MiB                   save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 14:06:22,608 - memory_profile6_log - INFO -    238    581.0 MiB     14.4 MiB                   fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-29 14:06:22,608 - memory_profile6_log - INFO -    239                             

2018-04-29 14:06:22,608 - memory_profile6_log - INFO -    240    591.4 MiB     10.4 MiB                   X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-29 14:06:22,609 - memory_profile6_log - INFO -    241    591.4 MiB      0.0 MiB                   logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-29 14:06:22,609 - memory_profile6_log - INFO -    242    594.1 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 14:06:22,609 - memory_profile6_log - INFO -    243    594.0 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 14:06:22,611 - memory_profile6_log - INFO -    244    594.1 MiB      2.7 MiB                       mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-29 14:06:22,611 - memory_profile6_log - INFO -    245                             

2018-04-29 14:06:22,611 - memory_profile6_log - INFO -    246    592.5 MiB     -1.6 MiB                   del X_split

2018-04-29 14:06:22,611 - memory_profile6_log - INFO -    247    592.5 MiB      0.0 MiB                   del BR

2018-04-29 14:06:22,617 - memory_profile6_log - INFO -    248    592.5 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-29 14:06:22,618 - memory_profile6_log - INFO -    249                             

2018-04-29 14:06:22,618 - memory_profile6_log - INFO -    250    592.5 MiB      0.0 MiB                   del save_sigma_nt

2018-04-29 14:06:22,619 - memory_profile6_log - INFO -    251    592.5 MiB      0.0 MiB                   logger.info("deleting save_sigma_nt...")

2018-04-29 14:06:22,619 - memory_profile6_log - INFO -    252                                             

2018-04-29 14:06:22,621 - memory_profile6_log - INFO -    253                                     elif str(saveto).lower() == "elastic":

2018-04-29 14:06:22,621 - memory_profile6_log - INFO -    254                                         logger.info("Using ElasticSearch as storage...")

2018-04-29 14:06:22,625 - memory_profile6_log - INFO -    255                                         mh.saveElasticS(model_transformsv)

2018-04-29 14:06:22,628 - memory_profile6_log - INFO -    256                             

2018-04-29 14:06:22,630 - memory_profile6_log - INFO -    257                                     # need save sigma_nt for daily train

2018-04-29 14:06:22,634 - memory_profile6_log - INFO -    258                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-29 14:06:22,634 - memory_profile6_log - INFO -    259                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-29 14:06:22,638 - memory_profile6_log - INFO -    260    592.5 MiB      0.0 MiB           if start_date and end_date:

2018-04-29 14:06:22,642 - memory_profile6_log - INFO -    261                                         if not fitby_sigmant:

2018-04-29 14:06:22,642 - memory_profile6_log - INFO -    262                                             logging.info("Saving sigma Nt...")

2018-04-29 14:06:22,644 - memory_profile6_log - INFO -    263                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 14:06:22,644 - memory_profile6_log - INFO -    264                                             save_sigma_nt['start_date'] = start_date

2018-04-29 14:06:22,644 - memory_profile6_log - INFO -    265                                             save_sigma_nt['end_date'] = end_date

2018-04-29 14:06:22,645 - memory_profile6_log - INFO -    266                                             print save_sigma_nt.head(5)

2018-04-29 14:06:22,645 - memory_profile6_log - INFO -    267                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-29 14:06:22,647 - memory_profile6_log - INFO -    268                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-29 14:06:22,650 - memory_profile6_log - INFO -    269    592.5 MiB      0.0 MiB       return

2018-04-29 14:06:22,651 - memory_profile6_log - INFO - 


2018-04-29 14:06:22,654 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-29 14:06:35,957 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 14:06:35,960 - memory_profile6_log - INFO - date_generated: 
2018-04-29 14:06:35,960 - memory_profile6_log - INFO -  
2018-04-29 14:06:35,960 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 14:06:35,960 - memory_profile6_log - INFO - 

2018-04-29 14:06:35,961 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 14:06:35,961 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 14:06:35,963 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 14:06:36,118 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 14:06:36,122 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 14:07:48,753 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 14:07:48,756 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 14:07:48,805 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 14:07:48,805 - memory_profile6_log - INFO - Appending history data...
2018-04-29 14:07:48,806 - memory_profile6_log - INFO - processing batch-0
2018-04-29 14:07:48,809 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:07:48,891 - memory_profile6_log - INFO - call history data...
2018-04-29 14:08:43,818 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:08:45,161 - memory_profile6_log - INFO - processing batch-1
2018-04-29 14:08:45,163 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:08:45,207 - memory_profile6_log - INFO - call history data...
2018-04-29 14:09:37,678 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:09:39,020 - memory_profile6_log - INFO - processing batch-2
2018-04-29 14:09:39,022 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:09:39,069 - memory_profile6_log - INFO - call history data...
2018-04-29 14:10:34,839 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:10:36,135 - memory_profile6_log - INFO - processing batch-3
2018-04-29 14:10:36,137 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:10:36,184 - memory_profile6_log - INFO - call history data...
2018-04-29 14:11:33,484 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:11:34,808 - memory_profile6_log - INFO - processing batch-4
2018-04-29 14:11:34,809 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:11:34,857 - memory_profile6_log - INFO - call history data...
2018-04-29 14:12:30,743 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:12:32,032 - memory_profile6_log - INFO - Appending training data...
2018-04-29 14:12:32,035 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 14:12:32,036 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 14:12:32,045 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 14:12:32,046 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 14:12:32,046 - memory_profile6_log - INFO - ================================================

2018-04-29 14:12:32,052 - memory_profile6_log - INFO -    295     86.6 MiB     86.6 MiB   @profile

2018-04-29 14:12:32,053 - memory_profile6_log - INFO -    296                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 14:12:32,055 - memory_profile6_log - INFO -    297     86.6 MiB      0.0 MiB       bq_client = client

2018-04-29 14:12:32,056 - memory_profile6_log - INFO -    298     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 14:12:32,058 - memory_profile6_log - INFO -    299                             

2018-04-29 14:12:32,059 - memory_profile6_log - INFO -    300     86.6 MiB      0.0 MiB       datalist = []

2018-04-29 14:12:32,059 - memory_profile6_log - INFO -    301     86.6 MiB      0.0 MiB       datalist_hist = []

2018-04-29 14:12:32,059 - memory_profile6_log - INFO -    302                             

2018-04-29 14:12:32,061 - memory_profile6_log - INFO -    303     86.6 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 14:12:32,062 - memory_profile6_log - INFO -    304    445.6 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 14:12:32,065 - memory_profile6_log - INFO -    305    339.1 MiB    252.5 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 14:12:32,065 - memory_profile6_log - INFO -    306    339.1 MiB      0.0 MiB           if tframe is not None:

2018-04-29 14:12:32,068 - memory_profile6_log - INFO -    307    339.1 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 14:12:32,068 - memory_profile6_log - INFO -    308    346.5 MiB      7.4 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 14:12:32,069 - memory_profile6_log - INFO -    309    346.5 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 14:12:32,069 - memory_profile6_log - INFO -    310    346.5 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 14:12:32,071 - memory_profile6_log - INFO -    311    445.6 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 14:12:32,072 - memory_profile6_log - INFO -    312                                                 # ~ loading history

2018-04-29 14:12:32,072 - memory_profile6_log - INFO -    313                                                 """

2018-04-29 14:12:32,072 - memory_profile6_log - INFO -    314                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 14:12:32,076 - memory_profile6_log - INFO -    315                                                 """

2018-04-29 14:12:32,078 - memory_profile6_log - INFO -    316    430.1 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 14:12:32,078 - memory_profile6_log - INFO -    317                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 14:12:32,078 - memory_profile6_log - INFO -    318    430.1 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 14:12:32,079 - memory_profile6_log - INFO -    319                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 14:12:32,081 - memory_profile6_log - INFO -    320    430.4 MiB      4.1 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 14:12:32,082 - memory_profile6_log - INFO -    321                             

2018-04-29 14:12:32,082 - memory_profile6_log - INFO -    322    430.4 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 14:12:32,084 - memory_profile6_log - INFO -    323    475.9 MiB    254.1 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 14:12:32,084 - memory_profile6_log - INFO -    324                             

2018-04-29 14:12:32,085 - memory_profile6_log - INFO -    325                                                 # me = os.getpid()

2018-04-29 14:12:32,088 - memory_profile6_log - INFO -    326                                                 # kill_proc_tree(me)

2018-04-29 14:12:32,089 - memory_profile6_log - INFO -    327                             

2018-04-29 14:12:32,091 - memory_profile6_log - INFO -    328    475.9 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 14:12:32,092 - memory_profile6_log - INFO -    329    476.7 MiB     -1.2 MiB                       for m in h_frame:

2018-04-29 14:12:32,092 - memory_profile6_log - INFO -    330    476.7 MiB     -1.2 MiB                           if m is not None:

2018-04-29 14:12:32,092 - memory_profile6_log - INFO -    331    476.7 MiB     -1.2 MiB                               if len(m) > 0:

2018-04-29 14:12:32,094 - memory_profile6_log - INFO -    332    476.7 MiB      3.3 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 14:12:32,095 - memory_profile6_log - INFO -    333    445.6 MiB   -162.3 MiB                       del h_frame

2018-04-29 14:12:32,095 - memory_profile6_log - INFO -    334    445.6 MiB     -1.3 MiB                       del lhistory

2018-04-29 14:12:32,095 - memory_profile6_log - INFO -    335                             

2018-04-29 14:12:32,098 - memory_profile6_log - INFO -    336    445.6 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 14:12:32,099 - memory_profile6_log - INFO -    337    445.6 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 14:12:32,101 - memory_profile6_log - INFO -    338                                     else: 

2018-04-29 14:12:32,101 - memory_profile6_log - INFO -    339                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 14:12:32,102 - memory_profile6_log - INFO -    340    445.6 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 14:12:32,104 - memory_profile6_log - INFO -    341    445.6 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 14:12:32,105 - memory_profile6_log - INFO -    342                             

2018-04-29 14:12:32,107 - memory_profile6_log - INFO -    343    445.6 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 14:12:32,109 - memory_profile6_log - INFO - 


2018-04-29 14:12:33,322 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 14:12:33,391 - memory_profile6_log - INFO -     p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0    0.000055        5563.716667        27            5573.716667  11911567  1612d9eb7fb333-0a1c17d0ba82e08-2c5f3268-2c600-...
1    0.000646        1844.325967         4            1854.325967  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
2    0.000646        1844.325967        28            1854.325967  10960288  1613cff5e9a99-028b356bcbf06b-734c435e-38400-16...
3    0.000646        1844.325967        16            1854.325967  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
4    0.001473         269.647011         4             279.647011  22291119  1612da8805b14b-0d324c1d36a4c7-39626377-2c740-1...
5    0.000646        1844.325967        25            1854.325967  10960288  161300e0f14a6-098c84d7864a32-57436a33-49a10-16...
6    0.000646        1844.325967       471            1854.325967  10960288  1610ca98d49149-0e61ac652d8ef-4323461-100200-16...
7    0.001473         269.647011        32             279.647011  22291119  1612d5d4a71111-05f5aba4c6e5bd-4a21472c-38400-1...
8    0.000646        1844.325967        23            1854.325967  10960288  161ab955e681ea-00cd2d4e919ec9-1e613a1c-38400-1...
9    0.001473         269.647011         4             279.647011  22291119  161add17c1de-03b340f8c7ee56-100d451f-38400-161...
10   0.000646        3688.651934        14            3698.651934  10960288  1617d6299713e-0638ae7886797c-44432908-38400-16...
11   0.000646        1844.325967        15            1854.325967  10960288  1612e8ce767c6-0e0b4eacdd5b13-45464657-38400-16...
12   0.000646        3688.651934        48            3698.651934  10960288  1613f71c69c80-0616565e91cf83-6b6f613a-38400-16...
13   0.001473         269.647011         8             279.647011  22291119  1612da003b5c7-09558ff4a105eb-134d5241-38400-16...
14   0.001473         269.647011         4             279.647011  22291119  16131b0ebd884-03aa3df1384c1c-7465070d-29b80-16...
15   0.000055        5563.716667         8            5573.716667  11911567  16130e2fea06e-08a4d37af-5c717549-38400-16130e2...
16   0.001473         269.647011         8             279.647011  22291119  161e954cb63110-0ad144257-227b4115-38400-161e95...
17   0.000646        1844.325967        32            1854.325967  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
18   0.001473         269.647011         4             279.647011  22291119  1614aa078305e-0fbe0328ace70f-7b261512-29b80-16...
19   0.000646        1844.325967        38            1854.325967  10960288  161316bf16fdb-0abca8bfb1328c-6e69682e-38400-16...
2018-04-29 14:12:33,394 - memory_profile6_log - INFO - 

2018-04-29 14:12:33,476 - memory_profile6_log - INFO - size of big_frame_hist: 69.87 MB
2018-04-29 14:12:33,546 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 14:12:33,559 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 14:13:21,872 - memory_profile6_log - INFO - size of df: 58.30 MB
2018-04-29 14:13:21,874 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-29 14:13:21,960 - memory_profile6_log - INFO - size of current_frame: 58.30 MB
2018-04-29 14:13:21,961 - memory_profile6_log - INFO - loading time of: 484420 total genuine-current interest data ~ take 405.873s
2018-04-29 14:13:21,979 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 14:13:21,980 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 14:13:21,982 - memory_profile6_log - INFO - ================================================

2018-04-29 14:13:21,983 - memory_profile6_log - INFO -    345     86.5 MiB     86.5 MiB   @profile

2018-04-29 14:13:21,983 - memory_profile6_log - INFO -    346                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 14:13:21,984 - memory_profile6_log - INFO -    347     86.6 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 14:13:21,986 - memory_profile6_log - INFO -    348     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 14:13:21,987 - memory_profile6_log - INFO -    349                             

2018-04-29 14:13:21,990 - memory_profile6_log - INFO -    350                                 # ~~~ Begin collecting data ~~~

2018-04-29 14:13:21,993 - memory_profile6_log - INFO -    351     86.6 MiB      0.0 MiB       t0 = time.time()

2018-04-29 14:13:21,994 - memory_profile6_log - INFO -    352    442.0 MiB    355.4 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 14:13:21,996 - memory_profile6_log - INFO -    353    442.0 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 14:13:21,996 - memory_profile6_log - INFO -    354                                     logger.info("Training cannot be empty..")

2018-04-29 14:13:21,996 - memory_profile6_log - INFO -    355                                     return False

2018-04-29 14:13:21,997 - memory_profile6_log - INFO -    356    456.7 MiB     14.8 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 14:13:21,997 - memory_profile6_log - INFO -    357    457.0 MiB      0.3 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 14:13:21,997 - memory_profile6_log - INFO -    358    457.0 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 14:13:21,999 - memory_profile6_log - INFO -    359                             

2018-04-29 14:13:21,999 - memory_profile6_log - INFO -    360    463.1 MiB      6.1 MiB       big_frame = pd.concat(datalist)

2018-04-29 14:13:22,003 - memory_profile6_log - INFO -    361    463.1 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 14:13:22,005 - memory_profile6_log - INFO -    362    457.3 MiB     -5.8 MiB       del datalist

2018-04-29 14:13:22,005 - memory_profile6_log - INFO -    363                             

2018-04-29 14:13:22,006 - memory_profile6_log - INFO -    364    457.3 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 14:13:22,006 - memory_profile6_log - INFO -    365                             

2018-04-29 14:13:22,006 - memory_profile6_log - INFO -    366                                 # ~ get current news interest ~

2018-04-29 14:13:22,006 - memory_profile6_log - INFO -    367    457.3 MiB      0.0 MiB       if not cd:

2018-04-29 14:13:22,006 - memory_profile6_log - INFO -    368                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 14:13:22,007 - memory_profile6_log - INFO -    369                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 14:13:22,009 - memory_profile6_log - INFO -    370                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 14:13:22,009 - memory_profile6_log - INFO -    371                                 else:

2018-04-29 14:13:22,010 - memory_profile6_log - INFO -    372    457.3 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 14:13:22,015 - memory_profile6_log - INFO -    373    457.3 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 14:13:22,016 - memory_profile6_log - INFO -    374                             

2018-04-29 14:13:22,016 - memory_profile6_log - INFO -    375                                     # safe handling of query parameter

2018-04-29 14:13:22,016 - memory_profile6_log - INFO -    376                                     query_params = [

2018-04-29 14:13:22,019 - memory_profile6_log - INFO -    377    457.3 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 14:13:22,020 - memory_profile6_log - INFO -    378                                     ]

2018-04-29 14:13:22,022 - memory_profile6_log - INFO -    379                             

2018-04-29 14:13:22,022 - memory_profile6_log - INFO -    380    457.3 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-29 14:13:22,022 - memory_profile6_log - INFO -    381    516.1 MiB     58.8 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 14:13:22,025 - memory_profile6_log - INFO -    382    516.1 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 14:13:22,026 - memory_profile6_log - INFO -    383                             

2018-04-29 14:13:22,026 - memory_profile6_log - INFO -    384    516.1 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 14:13:22,029 - memory_profile6_log - INFO -    385    516.1 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 14:13:22,029 - memory_profile6_log - INFO -    386    516.1 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 14:13:22,029 - memory_profile6_log - INFO -    387    516.1 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 14:13:22,029 - memory_profile6_log - INFO -    388    516.1 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 14:13:22,030 - memory_profile6_log - INFO -    389    516.1 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 14:13:22,030 - memory_profile6_log - INFO -    390                             

2018-04-29 14:13:22,032 - memory_profile6_log - INFO -    391    516.1 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 14:13:22,032 - memory_profile6_log - INFO - 


2018-04-29 14:13:22,036 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 14:13:22,065 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 14:13:22,066 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-29 14:13:22,069 - memory_profile6_log - INFO - apply on: 253995 total history data(D(t))
2018-04-29 14:13:23,122 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 14:13:23,124 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 14:13:23,221 - memory_profile6_log - INFO - Unexpected error:
2018-04-29 14:13:23,226 - memory_profile6_log - INFO -  
2018-04-29 14:13:23,226 - memory_profile6_log - INFO - cannot do a non-empty take from an empty axes.
2018-04-29 14:13:23,229 - memory_profile6_log - INFO - 

2018-04-29 14:15:18,052 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 14:15:18,055 - memory_profile6_log - INFO - date_generated: 
2018-04-29 14:15:18,055 - memory_profile6_log - INFO -  
2018-04-29 14:15:18,055 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 14:15:18,055 - memory_profile6_log - INFO - 

2018-04-29 14:15:18,055 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 14:15:18,056 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 14:15:18,058 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 14:15:18,198 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 14:15:18,201 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 14:16:29,242 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 14:16:29,243 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 14:16:29,282 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 14:16:29,285 - memory_profile6_log - INFO - Appending history data...
2018-04-29 14:16:29,286 - memory_profile6_log - INFO - processing batch-0
2018-04-29 14:16:29,286 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:16:29,364 - memory_profile6_log - INFO - call history data...
2018-04-29 14:17:27,111 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:17:28,398 - memory_profile6_log - INFO - processing batch-1
2018-04-29 14:17:28,398 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:17:28,450 - memory_profile6_log - INFO - call history data...
2018-04-29 14:18:23,572 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:18:24,868 - memory_profile6_log - INFO - processing batch-2
2018-04-29 14:18:24,868 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:18:24,917 - memory_profile6_log - INFO - call history data...
2018-04-29 14:19:18,844 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:19:20,194 - memory_profile6_log - INFO - processing batch-3
2018-04-29 14:19:20,194 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:19:20,246 - memory_profile6_log - INFO - call history data...
2018-04-29 14:20:14,894 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:20:16,197 - memory_profile6_log - INFO - processing batch-4
2018-04-29 14:20:16,197 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:20:16,246 - memory_profile6_log - INFO - call history data...
2018-04-29 14:21:09,266 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:21:10,602 - memory_profile6_log - INFO - Appending training data...
2018-04-29 14:21:10,604 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 14:21:10,604 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 14:21:10,614 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 14:21:10,615 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 14:21:10,615 - memory_profile6_log - INFO - ================================================

2018-04-29 14:21:10,617 - memory_profile6_log - INFO -    295     86.7 MiB     86.7 MiB   @profile

2018-04-29 14:21:10,618 - memory_profile6_log - INFO -    296                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 14:21:10,618 - memory_profile6_log - INFO -    297     86.7 MiB      0.0 MiB       bq_client = client

2018-04-29 14:21:10,619 - memory_profile6_log - INFO -    298     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 14:21:10,621 - memory_profile6_log - INFO -    299                             

2018-04-29 14:21:10,621 - memory_profile6_log - INFO -    300     86.7 MiB      0.0 MiB       datalist = []

2018-04-29 14:21:10,624 - memory_profile6_log - INFO -    301     86.7 MiB      0.0 MiB       datalist_hist = []

2018-04-29 14:21:10,625 - memory_profile6_log - INFO -    302                             

2018-04-29 14:21:10,627 - memory_profile6_log - INFO -    303     86.7 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 14:21:10,628 - memory_profile6_log - INFO -    304    446.9 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 14:21:10,628 - memory_profile6_log - INFO -    305    339.0 MiB    252.3 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 14:21:10,628 - memory_profile6_log - INFO -    306    339.0 MiB      0.0 MiB           if tframe is not None:

2018-04-29 14:21:10,630 - memory_profile6_log - INFO -    307    339.0 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 14:21:10,630 - memory_profile6_log - INFO -    308    346.6 MiB      7.6 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 14:21:10,631 - memory_profile6_log - INFO -    309    346.6 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 14:21:10,631 - memory_profile6_log - INFO -    310    346.6 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 14:21:10,632 - memory_profile6_log - INFO -    311    446.9 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 14:21:10,637 - memory_profile6_log - INFO -    312                                                 # ~ loading history

2018-04-29 14:21:10,638 - memory_profile6_log - INFO -    313                                                 """

2018-04-29 14:21:10,640 - memory_profile6_log - INFO -    314                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 14:21:10,641 - memory_profile6_log - INFO -    315                                                 """

2018-04-29 14:21:10,641 - memory_profile6_log - INFO -    316    429.0 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 14:21:10,642 - memory_profile6_log - INFO -    317                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 14:21:10,644 - memory_profile6_log - INFO -    318    429.0 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 14:21:10,644 - memory_profile6_log - INFO -    319                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 14:21:10,644 - memory_profile6_log - INFO -    320    429.4 MiB      4.4 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 14:21:10,644 - memory_profile6_log - INFO -    321                             

2018-04-29 14:21:10,648 - memory_profile6_log - INFO -    322    429.4 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 14:21:10,648 - memory_profile6_log - INFO -    323    475.8 MiB    252.4 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 14:21:10,651 - memory_profile6_log - INFO -    324                             

2018-04-29 14:21:10,653 - memory_profile6_log - INFO -    325                                                 # me = os.getpid()

2018-04-29 14:21:10,653 - memory_profile6_log - INFO -    326                                                 # kill_proc_tree(me)

2018-04-29 14:21:10,654 - memory_profile6_log - INFO -    327                             

2018-04-29 14:21:10,654 - memory_profile6_log - INFO -    328    475.8 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 14:21:10,655 - memory_profile6_log - INFO -    329    477.0 MiB     -1.4 MiB                       for m in h_frame:

2018-04-29 14:21:10,657 - memory_profile6_log - INFO -    330    476.9 MiB     -1.4 MiB                           if m is not None:

2018-04-29 14:21:10,658 - memory_profile6_log - INFO -    331    476.9 MiB     -1.4 MiB                               if len(m) > 0:

2018-04-29 14:21:10,661 - memory_profile6_log - INFO -    332    477.0 MiB      2.4 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 14:21:10,661 - memory_profile6_log - INFO -    333    446.9 MiB   -160.4 MiB                       del h_frame

2018-04-29 14:21:10,663 - memory_profile6_log - INFO -    334    446.9 MiB      0.0 MiB                       del lhistory

2018-04-29 14:21:10,664 - memory_profile6_log - INFO -    335                             

2018-04-29 14:21:10,664 - memory_profile6_log - INFO -    336    446.9 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 14:21:10,664 - memory_profile6_log - INFO -    337    446.9 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 14:21:10,665 - memory_profile6_log - INFO -    338                                     else: 

2018-04-29 14:21:10,668 - memory_profile6_log - INFO -    339                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 14:21:10,670 - memory_profile6_log - INFO -    340    446.9 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 14:21:10,671 - memory_profile6_log - INFO -    341    446.9 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 14:21:10,671 - memory_profile6_log - INFO -    342                             

2018-04-29 14:21:10,673 - memory_profile6_log - INFO -    343    446.9 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 14:21:10,674 - memory_profile6_log - INFO - 


2018-04-29 14:21:11,822 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 14:21:11,892 - memory_profile6_log - INFO -     p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0    0.001473         269.647011         8             279.647011  22291119  1613ad7e0b851-0fc417524ceb79-70261314-4df28-16...
1    0.001473         269.647011         8             279.647011  22291119  1612db41e4cd-0b9acce13-5c71754f-38400-1612db41...
2    0.000646        1844.325967         9            1854.325967  10960288  161378cbe8ea3-04c9e0e16b5ee-510e3b3d-38400-161...
3    0.000646        3688.651934         8            3698.651934  10960288  161976e0d293a-0b965799941492-164f5c5c-55188-16...
4    0.000646        1844.325967        14            1854.325967  10960288  16241fb0c0f0-0ed5c3ec3e65be-7821727e-55188-162...
5    0.001473         269.647011         8             279.647011  22291119  16276de434b17-03652a4cf66c21-75640303-38400-16...
6    0.000055        5563.716667        21            5573.716667  11911567  161382058d651-0055fb1f849841-4d61342f-38400-16...
7    0.000055        5563.716667       471            5573.716667  11911567  1610ca98d49149-0e61ac652d8ef-4323461-100200-16...
8    0.001473         269.647011        16             279.647011  22291119  16130deb9eab4-0f795f55592649-d4f4822-38400-161...
9    0.000646        1844.325967        16            1854.325967  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
10   0.000055        5563.716667        50            5573.716667  11911567  1612d71088d1c3-083723c07e39b5-48616602-38400-1...
11   0.000646        1844.325967        18            1854.325967  10960288  1618f963fcd60-05fe908efd3acd-282b503d-38400-16...
12   0.000646        1844.325967        16            1854.325967  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
13   0.000646        1844.325967       335            1854.325967  10960288  1614a78deff48f-03731aa5504b8a-4353468-100200-1...
14   0.001473         269.647011         4             279.647011  22291119  16130c70ae446-0b4f4fedaf144b-797d262d-38400-16...
15   0.001473         269.647011         4             279.647011  22291119  1616efe1de7365-024418da2c9cdb-667e1364-3d10d-1...
16   0.000646        1844.325967        21            1854.325967  10960288  1615c1a1aab234-08acb35bb06629-57416238-38400-1...
17   0.001473         269.647011        32             279.647011  22291119  1612d5d4a71111-05f5aba4c6e5bd-4a21472c-38400-1...
18   0.001473         269.647011         4             279.647011  22291119  16131b0ebd884-03aa3df1384c1c-7465070d-29b80-16...
19   0.000646        3688.651934         8            3698.651934  10960288  161bdf8fd45b1-07281a95fb4268-a6c5a75-38400-161...
2018-04-29 14:21:11,894 - memory_profile6_log - INFO - 

2018-04-29 14:21:11,994 - memory_profile6_log - INFO - size of big_frame_hist: 69.87 MB
2018-04-29 14:21:12,065 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 14:21:12,076 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-04-29 14:27:32,642 - memory_profile6_log - INFO - size of df: 509.09 MB
2018-04-29 14:27:32,644 - memory_profile6_log - INFO - getting total: 2090090 training data(current date interest)
2018-04-29 14:27:33,273 - memory_profile6_log - INFO - size of current_frame: 525.04 MB
2018-04-29 14:27:33,275 - memory_profile6_log - INFO - loading time of: 2344085 total genuine-current interest data ~ take 735.097s
2018-04-29 14:27:33,292 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 14:27:33,293 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 14:27:33,295 - memory_profile6_log - INFO - ================================================

2018-04-29 14:27:33,296 - memory_profile6_log - INFO -    345     86.6 MiB     86.6 MiB   @profile

2018-04-29 14:27:33,296 - memory_profile6_log - INFO -    346                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 14:27:33,299 - memory_profile6_log - INFO -    347     86.7 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 14:27:33,299 - memory_profile6_log - INFO -    348     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 14:27:33,299 - memory_profile6_log - INFO -    349                             

2018-04-29 14:27:33,301 - memory_profile6_log - INFO -    350                                 # ~~~ Begin collecting data ~~~

2018-04-29 14:27:33,302 - memory_profile6_log - INFO -    351     86.7 MiB      0.0 MiB       t0 = time.time()

2018-04-29 14:27:33,302 - memory_profile6_log - INFO -    352    442.2 MiB    355.5 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 14:27:33,305 - memory_profile6_log - INFO -    353    442.2 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 14:27:33,305 - memory_profile6_log - INFO -    354                                     logger.info("Training cannot be empty..")

2018-04-29 14:27:33,308 - memory_profile6_log - INFO -    355                                     return False

2018-04-29 14:27:33,309 - memory_profile6_log - INFO -    356    457.5 MiB     15.3 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 14:27:33,309 - memory_profile6_log - INFO -    357    457.8 MiB      0.3 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 14:27:33,309 - memory_profile6_log - INFO -    358    457.8 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 14:27:33,309 - memory_profile6_log - INFO -    359                             

2018-04-29 14:27:33,309 - memory_profile6_log - INFO -    360    463.8 MiB      6.1 MiB       big_frame = pd.concat(datalist)

2018-04-29 14:27:33,311 - memory_profile6_log - INFO -    361    463.8 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 14:27:33,312 - memory_profile6_log - INFO -    362    458.0 MiB     -5.8 MiB       del datalist

2018-04-29 14:27:33,312 - memory_profile6_log - INFO -    363                             

2018-04-29 14:27:33,312 - memory_profile6_log - INFO -    364    458.0 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 14:27:33,318 - memory_profile6_log - INFO -    365                             

2018-04-29 14:27:33,319 - memory_profile6_log - INFO -    366                                 # ~ get current news interest ~

2018-04-29 14:27:33,319 - memory_profile6_log - INFO -    367    458.0 MiB      0.0 MiB       if not cd:

2018-04-29 14:27:33,321 - memory_profile6_log - INFO -    368    458.0 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-04-29 14:27:33,321 - memory_profile6_log - INFO -    369    963.7 MiB    505.7 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 14:27:33,323 - memory_profile6_log - INFO -    370    963.7 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 14:27:33,323 - memory_profile6_log - INFO -    371                                 else:

2018-04-29 14:27:33,323 - memory_profile6_log - INFO -    372                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 14:27:33,325 - memory_profile6_log - INFO -    373                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 14:27:33,325 - memory_profile6_log - INFO -    374                             

2018-04-29 14:27:33,325 - memory_profile6_log - INFO -    375                                     # safe handling of query parameter

2018-04-29 14:27:33,329 - memory_profile6_log - INFO -    376                                     query_params = [

2018-04-29 14:27:33,329 - memory_profile6_log - INFO -    377                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 14:27:33,332 - memory_profile6_log - INFO -    378                                     ]

2018-04-29 14:27:33,332 - memory_profile6_log - INFO -    379                             

2018-04-29 14:27:33,334 - memory_profile6_log - INFO -    380                                     job_config.query_parameters = query_params

2018-04-29 14:27:33,334 - memory_profile6_log - INFO -    381                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 14:27:33,335 - memory_profile6_log - INFO -    382                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 14:27:33,335 - memory_profile6_log - INFO -    383                             

2018-04-29 14:27:33,335 - memory_profile6_log - INFO -    384    979.7 MiB     16.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 14:27:33,335 - memory_profile6_log - INFO -    385    979.7 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 14:27:33,336 - memory_profile6_log - INFO -    386    979.7 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 14:27:33,338 - memory_profile6_log - INFO -    387    979.7 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 14:27:33,342 - memory_profile6_log - INFO -    388    979.7 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 14:27:33,342 - memory_profile6_log - INFO -    389    979.7 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 14:27:33,342 - memory_profile6_log - INFO -    390                             

2018-04-29 14:27:33,344 - memory_profile6_log - INFO -    391    979.7 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 14:27:33,344 - memory_profile6_log - INFO - 


2018-04-29 14:27:33,348 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 14:27:33,467 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 14:27:33,469 - memory_profile6_log - INFO - transform on: 2090090 total current data(D(t))
2018-04-29 14:27:33,470 - memory_profile6_log - INFO - apply on: 253995 total history data(D(t))
2018-04-29 14:27:34,509 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 14:27:34,510 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 14:28:20,569 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-29 14:28:20,598 - memory_profile6_log - INFO - Total train time: 47.131s
2018-04-29 14:28:20,599 - memory_profile6_log - INFO - memory left before cleaning: 75.000 percent memory...
2018-04-29 14:28:20,601 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-29 14:28:20,602 - memory_profile6_log - INFO - deleting df_dut...
2018-04-29 14:28:20,604 - memory_profile6_log - INFO - deleting df_dt...
2018-04-29 14:28:20,605 - memory_profile6_log - INFO - deleting df_input...
2018-04-29 14:28:20,690 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-29 14:28:20,690 - memory_profile6_log - INFO - deleting df_current...
2018-04-29 14:28:20,693 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-29 14:28:20,704 - memory_profile6_log - INFO - deleting model_fit...
2018-04-29 14:28:20,706 - memory_profile6_log - INFO - deleting result...
2018-04-29 14:28:20,723 - memory_profile6_log - INFO - deleting model_transform...
2018-04-29 14:28:20,724 - memory_profile6_log - INFO - memory left after cleaning: 74.300 percent memory...
2018-04-29 14:28:20,726 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-29 14:28:20,727 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-29 14:28:20,891 - memory_profile6_log - INFO - Saving fitted_models as history...
2018-04-29 14:28:20,980 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 10
2018-04-29 14:28:20,982 - memory_profile6_log - INFO - processing batch-0
2018-04-29 14:28:21,183 - memory_profile6_log - INFO - processing batch-1
2018-04-29 14:28:21,365 - memory_profile6_log - INFO - processing batch-2
2018-04-29 14:28:21,548 - memory_profile6_log - INFO - processing batch-3
2018-04-29 14:28:21,726 - memory_profile6_log - INFO - processing batch-4
2018-04-29 14:28:21,913 - memory_profile6_log - INFO - processing batch-5
2018-04-29 14:28:22,101 - memory_profile6_log - INFO - processing batch-6
2018-04-29 14:28:22,292 - memory_profile6_log - INFO - processing batch-7
2018-04-29 14:28:22,484 - memory_profile6_log - INFO - processing batch-8
2018-04-29 14:28:22,671 - memory_profile6_log - INFO - processing batch-9
2018-04-29 14:28:22,859 - memory_profile6_log - INFO - deleting BR...
2018-04-29 14:28:22,861 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-04-29 14:28:22,871 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 14:28:22,872 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 14:28:22,875 - memory_profile6_log - INFO - ================================================

2018-04-29 14:28:22,875 - memory_profile6_log - INFO -    113    974.3 MiB    974.3 MiB   @profile

2018-04-29 14:28:22,878 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-29 14:28:22,878 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-29 14:28:22,878 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-29 14:28:22,880 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-29 14:28:22,880 - memory_profile6_log - INFO -    118                                      saveto="datastore"):

2018-04-29 14:28:22,881 - memory_profile6_log - INFO -    119                                 """

2018-04-29 14:28:22,881 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-29 14:28:22,881 - memory_profile6_log - INFO -    121                                 """

2018-04-29 14:28:22,885 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-29 14:28:22,887 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-29 14:28:22,888 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-29 14:28:22,888 - memory_profile6_log - INFO -    125    974.3 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-29 14:28:22,888 - memory_profile6_log - INFO -    126    982.5 MiB      8.1 MiB       df_dut = df_input.copy(deep=True)

2018-04-29 14:28:22,890 - memory_profile6_log - INFO -    127    982.5 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 14:28:22,890 - memory_profile6_log - INFO -    128                             

2018-04-29 14:28:22,891 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-29 14:28:22,892 - memory_profile6_log - INFO -    130   1048.2 MiB     65.8 MiB       df_dt = df_current.copy(deep=True)

2018-04-29 14:28:22,892 - memory_profile6_log - INFO -    131   1048.2 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 14:28:22,892 - memory_profile6_log - INFO -    132                             

2018-04-29 14:28:22,894 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-29 14:28:22,894 - memory_profile6_log - INFO -    134   1048.2 MiB      0.0 MiB       t0 = time.time()

2018-04-29 14:28:22,894 - memory_profile6_log - INFO -    135   1048.2 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-29 14:28:22,898 - memory_profile6_log - INFO -    136   1048.2 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-29 14:28:22,898 - memory_profile6_log - INFO -    137   1048.2 MiB      0.0 MiB       logger.info("apply on: %d total history data(D(t))", len(df_hist))

2018-04-29 14:28:22,901 - memory_profile6_log - INFO -    138                             

2018-04-29 14:28:22,901 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-29 14:28:22,903 - memory_profile6_log - INFO -    140   1048.2 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-29 14:28:22,903 - memory_profile6_log - INFO -    141                             

2018-04-29 14:28:22,904 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-29 14:28:22,904 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-29 14:28:22,904 - memory_profile6_log - INFO -    144   1048.8 MiB      0.6 MiB       NB = BR.processX(df_dut)

2018-04-29 14:28:22,904 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-29 14:28:22,905 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-29 14:28:22,905 - memory_profile6_log - INFO -    147   1058.7 MiB      9.9 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-29 14:28:22,908 - memory_profile6_log - INFO -    148                                 """

2018-04-29 14:28:22,911 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-29 14:28:22,911 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-29 14:28:22,913 - memory_profile6_log - INFO -    151                                 """

2018-04-29 14:28:22,914 - memory_profile6_log - INFO -    152   1058.7 MiB      0.0 MiB       fitby_sigmant = True

2018-04-29 14:28:22,914 - memory_profile6_log - INFO -    153   1058.7 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-29 14:28:22,917 - memory_profile6_log - INFO -    154   1058.7 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-29 14:28:22,921 - memory_profile6_log - INFO -    155   1068.4 MiB      9.7 MiB                            'is_general']]

2018-04-29 14:28:22,924 - memory_profile6_log - INFO -    156   1068.4 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-29 14:28:22,926 - memory_profile6_log - INFO -    157   1068.4 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-29 14:28:22,927 - memory_profile6_log - INFO -    158   1096.0 MiB     27.6 MiB                          verbose=False)

2018-04-29 14:28:22,928 - memory_profile6_log - INFO -    159   1096.0 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-29 14:28:22,930 - memory_profile6_log - INFO -    160   1096.0 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-29 14:28:22,930 - memory_profile6_log - INFO -    161                             

2018-04-29 14:28:22,934 - memory_profile6_log - INFO -    162                                 # ~~ and Transform ~~

2018-04-29 14:28:22,934 - memory_profile6_log - INFO -    163                                 #   handling current news interest == current date

2018-04-29 14:28:22,937 - memory_profile6_log - INFO -    164   1096.0 MiB      0.0 MiB       if df_dt.empty:

2018-04-29 14:28:22,937 - memory_profile6_log - INFO -    165                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-29 14:28:22,937 - memory_profile6_log - INFO -    166                                     return None

2018-04-29 14:28:22,938 - memory_profile6_log - INFO -    167   1102.5 MiB      6.5 MiB       NB = BR.processX(df_dt)

2018-04-29 14:28:22,940 - memory_profile6_log - INFO -    168   1200.5 MiB     98.0 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-29 14:28:22,940 - memory_profile6_log - INFO -    169                             

2018-04-29 14:28:22,944 - memory_profile6_log - INFO -    170   1200.5 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-29 14:28:22,946 - memory_profile6_log - INFO -    171   1272.6 MiB     72.0 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-29 14:28:22,947 - memory_profile6_log - INFO -    172   1272.6 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-29 14:28:22,947 - memory_profile6_log - INFO -    173   1272.6 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-29 14:28:22,948 - memory_profile6_log - INFO -    174   1287.4 MiB     14.9 MiB                                                     verbose=False)

2018-04-29 14:28:22,950 - memory_profile6_log - INFO -    175                                 # ~~~ filter is general and specific topic ~~~

2018-04-29 14:28:22,950 - memory_profile6_log - INFO -    176                                 # the idea is just we need to rerank every topic according

2018-04-29 14:28:22,950 - memory_profile6_log - INFO -    177                                 # user_id and and is_general by p0_posterior

2018-04-29 14:28:22,950 - memory_profile6_log - INFO -    178   1287.4 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-29 14:28:22,951 - memory_profile6_log - INFO -    179   1289.4 MiB      1.9 MiB                                     'is_general']].groupby(['topic_id',

2018-04-29 14:28:22,951 - memory_profile6_log - INFO -    180   1287.4 MiB     -1.9 MiB                                                             'is_general']

2018-04-29 14:28:22,956 - memory_profile6_log - INFO -    181                                                                                      ).size().to_frame().reset_index()

2018-04-29 14:28:22,957 - memory_profile6_log - INFO -    182                             

2018-04-29 14:28:22,959 - memory_profile6_log - INFO -    183                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-29 14:28:22,961 - memory_profile6_log - INFO -    184                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-29 14:28:22,966 - memory_profile6_log - INFO -    185   1287.5 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-29 14:28:22,967 - memory_profile6_log - INFO -    186                             

2018-04-29 14:28:22,967 - memory_profile6_log - INFO -    187                                 # ~ start by provide rank for each topic type ~

2018-04-29 14:28:22,967 - memory_profile6_log - INFO -    188   1290.6 MiB      3.1 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-29 14:28:22,969 - memory_profile6_log - INFO -    189   1296.4 MiB      5.8 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-29 14:28:22,969 - memory_profile6_log - INFO -    190                             

2018-04-29 14:28:22,970 - memory_profile6_log - INFO -    191                                 # ~ set threshold to filter output

2018-04-29 14:28:22,970 - memory_profile6_log - INFO -    192   1296.4 MiB      0.0 MiB       if threshold > 0:

2018-04-29 14:28:22,970 - memory_profile6_log - INFO -    193   1296.4 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-29 14:28:22,970 - memory_profile6_log - INFO -    194   1296.4 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-29 14:28:22,970 - memory_profile6_log - INFO -    195   1296.0 MiB     -0.3 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-29 14:28:22,970 - memory_profile6_log - INFO -    196                             

2018-04-29 14:28:22,971 - memory_profile6_log - INFO -    197   1296.0 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 14:28:22,971 - memory_profile6_log - INFO -    198   1296.0 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-29 14:28:22,971 - memory_profile6_log - INFO -    199                             

2018-04-29 14:28:22,973 - memory_profile6_log - INFO -    200   1296.0 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 14:28:22,974 - memory_profile6_log - INFO -    201                             

2018-04-29 14:28:22,983 - memory_profile6_log - INFO -    202   1296.0 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-29 14:28:22,984 - memory_profile6_log - INFO -    203   1296.0 MiB      0.0 MiB       del df_dut

2018-04-29 14:28:22,986 - memory_profile6_log - INFO -    204   1296.0 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-29 14:28:22,986 - memory_profile6_log - INFO -    205   1296.0 MiB      0.0 MiB       del df_dt

2018-04-29 14:28:22,987 - memory_profile6_log - INFO -    206   1296.0 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-29 14:28:22,989 - memory_profile6_log - INFO -    207   1296.0 MiB      0.0 MiB       del df_input

2018-04-29 14:28:22,992 - memory_profile6_log - INFO -    208   1296.0 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-29 14:28:22,993 - memory_profile6_log - INFO -    209   1214.3 MiB    -81.7 MiB       del df_input_X

2018-04-29 14:28:22,994 - memory_profile6_log - INFO -    210   1214.3 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-29 14:28:22,994 - memory_profile6_log - INFO -    211   1214.3 MiB      0.0 MiB       del df_current

2018-04-29 14:28:22,996 - memory_profile6_log - INFO -    212   1214.3 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-29 14:28:22,996 - memory_profile6_log - INFO -    213   1214.3 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-29 14:28:22,996 - memory_profile6_log - INFO -    214   1214.3 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-29 14:28:22,996 - memory_profile6_log - INFO -    215   1191.0 MiB    -23.3 MiB       del model_fit

2018-04-29 14:28:22,997 - memory_profile6_log - INFO -    216   1191.0 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-29 14:28:22,997 - memory_profile6_log - INFO -    217   1191.0 MiB      0.0 MiB       del result

2018-04-29 14:28:23,000 - memory_profile6_log - INFO -    218   1191.0 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-29 14:28:23,003 - memory_profile6_log - INFO -    219                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 14:28:23,005 - memory_profile6_log - INFO -    220                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 14:28:23,006 - memory_profile6_log - INFO -    221                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 14:28:23,007 - memory_profile6_log - INFO -    222   1191.0 MiB      0.0 MiB       if savetrain:

2018-04-29 14:28:23,009 - memory_profile6_log - INFO -    223   1196.4 MiB      5.4 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-29 14:28:23,010 - memory_profile6_log - INFO -    224   1196.4 MiB      0.0 MiB           del model_transform

2018-04-29 14:28:23,010 - memory_profile6_log - INFO -    225   1196.4 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-29 14:28:23,010 - memory_profile6_log - INFO -    226   1196.4 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 14:28:23,012 - memory_profile6_log - INFO -    227                             

2018-04-29 14:28:23,015 - memory_profile6_log - INFO -    228   1196.4 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-29 14:28:23,016 - memory_profile6_log - INFO -    229                                     # ~ Place your code to save the training model here ~

2018-04-29 14:28:23,017 - memory_profile6_log - INFO -    230   1196.4 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-29 14:28:23,019 - memory_profile6_log - INFO -    231   1196.4 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-29 14:28:23,019 - memory_profile6_log - INFO -    232   1196.4 MiB      0.0 MiB               if multproc:

2018-04-29 14:28:23,020 - memory_profile6_log - INFO -    233   1176.9 MiB    -19.6 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-29 14:28:23,020 - memory_profile6_log - INFO -    234                             

2018-04-29 14:28:23,022 - memory_profile6_log - INFO -    235                                             # ~ save fitted models ~

2018-04-29 14:28:23,023 - memory_profile6_log - INFO -    236   1176.9 MiB      0.0 MiB                   logger.info("Saving fitted_models as history...")

2018-04-29 14:28:23,023 - memory_profile6_log - INFO -    237   1177.1 MiB      0.2 MiB                   save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 14:28:23,029 - memory_profile6_log - INFO -    238   1192.3 MiB     15.2 MiB                   fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-29 14:28:23,029 - memory_profile6_log - INFO -    239                             

2018-04-29 14:28:23,030 - memory_profile6_log - INFO -    240   1203.1 MiB     10.9 MiB                   X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-29 14:28:23,032 - memory_profile6_log - INFO -    241   1203.1 MiB      0.0 MiB                   logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-29 14:28:23,035 - memory_profile6_log - INFO -    242   1205.1 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 14:28:23,036 - memory_profile6_log - INFO -    243   1205.1 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 14:28:23,036 - memory_profile6_log - INFO -    244   1205.1 MiB      1.9 MiB                       mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-29 14:28:23,036 - memory_profile6_log - INFO -    245                             

2018-04-29 14:28:23,036 - memory_profile6_log - INFO -    246   1205.1 MiB      0.0 MiB                   del X_split

2018-04-29 14:28:23,036 - memory_profile6_log - INFO -    247   1205.1 MiB      0.0 MiB                   del BR

2018-04-29 14:28:23,036 - memory_profile6_log - INFO -    248   1205.1 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-29 14:28:23,036 - memory_profile6_log - INFO -    249                             

2018-04-29 14:28:23,036 - memory_profile6_log - INFO -    250   1205.1 MiB      0.0 MiB                   del save_sigma_nt

2018-04-29 14:28:23,036 - memory_profile6_log - INFO -    251   1205.1 MiB      0.0 MiB                   logger.info("deleting save_sigma_nt...")

2018-04-29 14:28:23,036 - memory_profile6_log - INFO -    252                                             

2018-04-29 14:28:23,042 - memory_profile6_log - INFO -    253                                     elif str(saveto).lower() == "elastic":

2018-04-29 14:28:23,046 - memory_profile6_log - INFO -    254                                         logger.info("Using ElasticSearch as storage...")

2018-04-29 14:28:23,046 - memory_profile6_log - INFO -    255                                         mh.saveElasticS(model_transformsv)

2018-04-29 14:28:23,046 - memory_profile6_log - INFO -    256                             

2018-04-29 14:28:23,048 - memory_profile6_log - INFO -    257                                     # need save sigma_nt for daily train

2018-04-29 14:28:23,049 - memory_profile6_log - INFO -    258                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-29 14:28:23,049 - memory_profile6_log - INFO -    259                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-29 14:28:23,052 - memory_profile6_log - INFO -    260   1205.1 MiB      0.0 MiB           if start_date and end_date:

2018-04-29 14:28:23,053 - memory_profile6_log - INFO -    261                                         if not fitby_sigmant:

2018-04-29 14:28:23,053 - memory_profile6_log - INFO -    262                                             logging.info("Saving sigma Nt...")

2018-04-29 14:28:23,055 - memory_profile6_log - INFO -    263                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 14:28:23,056 - memory_profile6_log - INFO -    264                                             save_sigma_nt['start_date'] = start_date

2018-04-29 14:28:23,056 - memory_profile6_log - INFO -    265                                             save_sigma_nt['end_date'] = end_date

2018-04-29 14:28:23,059 - memory_profile6_log - INFO -    266                                             print save_sigma_nt.head(5)

2018-04-29 14:28:23,061 - memory_profile6_log - INFO -    267                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-29 14:28:23,061 - memory_profile6_log - INFO -    268                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-29 14:28:23,062 - memory_profile6_log - INFO -    269   1205.1 MiB      0.0 MiB       return

2018-04-29 14:28:23,062 - memory_profile6_log - INFO - 


2018-04-29 14:28:23,065 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-29 14:33:34,835 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 14:33:34,839 - memory_profile6_log - INFO - date_generated: 
2018-04-29 14:33:34,839 - memory_profile6_log - INFO -  
2018-04-29 14:33:34,841 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 9, 0, 0)]
2018-04-29 14:33:34,841 - memory_profile6_log - INFO - 

2018-04-29 14:33:34,841 - memory_profile6_log - INFO - using current date: 2018-04-10
2018-04-29 14:33:34,842 - memory_profile6_log - INFO - using start date: 2018-04-09 00:00:00
2018-04-29 14:33:34,842 - memory_profile6_log - INFO - using end date: 2018-04-09
2018-04-29 14:33:34,967 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 14:33:34,971 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-09
2018-04-29 14:35:28,645 - memory_profile6_log - INFO - size of df: 102.25 MB
2018-04-29 14:35:28,647 - memory_profile6_log - INFO - getting total: 403532 training data(genuine interest) for date: 2018-04-09
2018-04-29 14:35:28,707 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 14:35:28,707 - memory_profile6_log - INFO - Appending history data...
2018-04-29 14:35:28,709 - memory_profile6_log - INFO - processing batch-0
2018-04-29 14:35:28,710 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:35:28,815 - memory_profile6_log - INFO - call history data...
2018-04-29 14:36:39,262 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:36:41,038 - memory_profile6_log - INFO - processing batch-1
2018-04-29 14:36:41,039 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:36:41,117 - memory_profile6_log - INFO - call history data...
2018-04-29 14:37:49,342 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:37:50,984 - memory_profile6_log - INFO - processing batch-2
2018-04-29 14:37:50,986 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:37:51,061 - memory_profile6_log - INFO - call history data...
2018-04-29 14:40:24,117 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 14:40:24,119 - memory_profile6_log - INFO - date_generated: 
2018-04-29 14:40:24,121 - memory_profile6_log - INFO -  
2018-04-29 14:40:24,121 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 9, 0, 0)]
2018-04-29 14:40:24,121 - memory_profile6_log - INFO - 

2018-04-29 14:40:24,121 - memory_profile6_log - INFO - using current date: 2018-04-10
2018-04-29 14:40:24,121 - memory_profile6_log - INFO - using start date: 2018-04-09 00:00:00
2018-04-29 14:40:24,121 - memory_profile6_log - INFO - using end date: 2018-04-09
2018-04-29 14:40:24,256 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 14:40:24,259 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-09
2018-04-29 14:42:17,904 - memory_profile6_log - INFO - size of df: 102.25 MB
2018-04-29 14:42:17,905 - memory_profile6_log - INFO - getting total: 403532 training data(genuine interest) for date: 2018-04-09
2018-04-29 14:42:17,967 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 14:42:17,967 - memory_profile6_log - INFO - Appending history data...
2018-04-29 14:42:17,969 - memory_profile6_log - INFO - processing batch-0
2018-04-29 14:42:17,970 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:42:18,075 - memory_profile6_log - INFO - call history data...
2018-04-29 14:43:27,187 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:43:28,825 - memory_profile6_log - INFO - processing batch-1
2018-04-29 14:43:28,826 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:43:28,900 - memory_profile6_log - INFO - call history data...
2018-04-29 14:44:36,298 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:44:37,921 - memory_profile6_log - INFO - processing batch-2
2018-04-29 14:44:37,923 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:44:37,999 - memory_profile6_log - INFO - call history data...
2018-04-29 14:45:53,348 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:45:55,006 - memory_profile6_log - INFO - processing batch-3
2018-04-29 14:45:55,006 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:45:55,085 - memory_profile6_log - INFO - call history data...
2018-04-29 14:47:08,486 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:47:10,188 - memory_profile6_log - INFO - processing batch-4
2018-04-29 14:47:10,190 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:47:10,259 - memory_profile6_log - INFO - call history data...
2018-04-29 14:48:27,240 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 14:48:28,890 - memory_profile6_log - INFO - Appending training data...
2018-04-29 14:48:28,891 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 14:48:28,892 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 14:48:28,905 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 14:48:28,907 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 14:48:28,908 - memory_profile6_log - INFO - ================================================

2018-04-29 14:48:28,913 - memory_profile6_log - INFO -    295     86.8 MiB     86.8 MiB   @profile

2018-04-29 14:48:28,914 - memory_profile6_log - INFO -    296                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 14:48:28,915 - memory_profile6_log - INFO -    297     86.8 MiB      0.0 MiB       bq_client = client

2018-04-29 14:48:28,917 - memory_profile6_log - INFO -    298     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 14:48:28,917 - memory_profile6_log - INFO -    299                             

2018-04-29 14:48:28,918 - memory_profile6_log - INFO -    300     86.8 MiB      0.0 MiB       datalist = []

2018-04-29 14:48:28,920 - memory_profile6_log - INFO -    301     86.8 MiB      0.0 MiB       datalist_hist = []

2018-04-29 14:48:28,921 - memory_profile6_log - INFO -    302                             

2018-04-29 14:48:28,921 - memory_profile6_log - INFO -    303     86.8 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 14:48:28,924 - memory_profile6_log - INFO -    304    654.5 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 14:48:28,924 - memory_profile6_log - INFO -    305    393.2 MiB    306.4 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 14:48:28,927 - memory_profile6_log - INFO -    306    393.2 MiB      0.0 MiB           if tframe is not None:

2018-04-29 14:48:28,930 - memory_profile6_log - INFO -    307    393.2 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 14:48:28,931 - memory_profile6_log - INFO -    308    406.3 MiB     13.1 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 14:48:28,933 - memory_profile6_log - INFO -    309    406.3 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 14:48:28,936 - memory_profile6_log - INFO -    310    406.3 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 14:48:28,937 - memory_profile6_log - INFO -    311    654.5 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 14:48:28,937 - memory_profile6_log - INFO -    312                                                 # ~ loading history

2018-04-29 14:48:28,938 - memory_profile6_log - INFO -    313                                                 """

2018-04-29 14:48:28,940 - memory_profile6_log - INFO -    314                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 14:48:28,940 - memory_profile6_log - INFO -    315                                                 """

2018-04-29 14:48:28,943 - memory_profile6_log - INFO -    316    630.0 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 14:48:28,944 - memory_profile6_log - INFO -    317                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 14:48:28,946 - memory_profile6_log - INFO -    318    630.0 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 14:48:28,947 - memory_profile6_log - INFO -    319                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 14:48:28,947 - memory_profile6_log - INFO -    320    631.4 MiB      8.1 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 14:48:28,948 - memory_profile6_log - INFO -    321                             

2018-04-29 14:48:28,950 - memory_profile6_log - INFO -    322    631.4 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 14:48:28,950 - memory_profile6_log - INFO -    323    706.2 MiB    536.0 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 14:48:28,950 - memory_profile6_log - INFO -    324                             

2018-04-29 14:48:28,953 - memory_profile6_log - INFO -    325                                                 # me = os.getpid()

2018-04-29 14:48:28,953 - memory_profile6_log - INFO -    326                                                 # kill_proc_tree(me)

2018-04-29 14:48:28,956 - memory_profile6_log - INFO -    327                             

2018-04-29 14:48:28,956 - memory_profile6_log - INFO -    328    706.2 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 14:48:28,957 - memory_profile6_log - INFO -    329    708.4 MiB     -1.0 MiB                       for m in h_frame:

2018-04-29 14:48:28,959 - memory_profile6_log - INFO -    330    708.4 MiB     -1.0 MiB                           if m is not None:

2018-04-29 14:48:28,960 - memory_profile6_log - INFO -    331    708.4 MiB     -1.0 MiB                               if len(m) > 0:

2018-04-29 14:48:28,964 - memory_profile6_log - INFO -    332    708.4 MiB      9.6 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 14:48:28,964 - memory_profile6_log - INFO -    333    655.1 MiB   -291.4 MiB                       del h_frame

2018-04-29 14:48:28,967 - memory_profile6_log - INFO -    334    654.5 MiB    -15.0 MiB                       del lhistory

2018-04-29 14:48:28,967 - memory_profile6_log - INFO -    335                             

2018-04-29 14:48:28,969 - memory_profile6_log - INFO -    336    654.5 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 14:48:28,970 - memory_profile6_log - INFO -    337    654.5 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 14:48:28,970 - memory_profile6_log - INFO -    338                                     else: 

2018-04-29 14:48:28,970 - memory_profile6_log - INFO -    339                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 14:48:28,971 - memory_profile6_log - INFO -    340    654.5 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 14:48:28,971 - memory_profile6_log - INFO -    341    654.5 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 14:48:28,979 - memory_profile6_log - INFO -    342                             

2018-04-29 14:48:28,980 - memory_profile6_log - INFO -    343    654.5 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 14:48:28,982 - memory_profile6_log - INFO - 


2018-04-29 14:48:30,170 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 14:48:30,242 - memory_profile6_log - INFO -     p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0    0.001543         376.207606       303             386.207606  10960288  1610d37360f208-09f8dc8d4f6437-4323461-100200-1...
1    0.000646       18443.259669        47           18453.259669  10960288  161ac1f818c291-0dd09ed3def2f7-3b60490d-100200-...
2    0.001543         115.330640       143             125.330640  10960288  16215b1fa84f4-0244194f6a3c3f-c343567-e1000-162...
3    0.001543         276.015458       675             286.015458  10960288  16281b8933da15-0391070f0a64e6-b34356b-1fa400-1...
4    0.000088        3435.617685      2635            3445.617685  11911567  16116a35abf7b-02ebd04ab0ab75-4323461-100200-16...
5    0.001543        3007.139656       234            3017.139656  10960288  161e4fe8217546-0d3d07a15ce9da-b353461-100200-1...
6    0.000088        6740.892857        15            6750.892857  11911567  162294766a319c-0d16b05bb32617-18741b1f-38400-1...
7    0.001543         445.737339       185             455.737339  10960288  16137e7988e400-0e4f9d96053ce-2c5f3268-4a640-16...
8    0.001543         381.996451       331             391.996451  10960288  16130047ada52-0ada44117-14d422a-29b80-16130047...
9    0.001543         105.812247       762             115.812247  10960288  1612d18da84ab-04e53015f12048-770c003f-38400-16...
10   0.001543        1148.403873      2879            1158.403873  10960288  1610ca10ab6151-09e6699899a0ec-4323461-100200-1...
11   0.004470         382.074899        32             392.074899  22291119  1626b3c178312f-08f9ac741a7b3f-71261e1a-38400-1...
12   0.001543         451.839870        99             461.839870  10960288  16113e91df674a-0aa88425265e92-4323461-100200-1...
13   0.001543         882.303128        54             892.303128  10960288  161f466e32911-0b032e180a22a5-444f4d5b-38400-16...
14   0.000088        6169.630751        59            6179.630751  11911567  162a944840211a-008b91166f77268-17347840-c0000-...
15   0.001543       15204.105398       176           15214.105398  10960288  162413b89ac2fc-0d7cbff1c76a7-4323461-100200-16...
16   0.001543         257.539864       417             267.539864  10960288  161b69c3161ab-0448ac078c2e7-3a2a047e-38400-161...
17   0.000088        2414.648188       134            2424.648188  11911567  161fb5419215b-09e7c04c9d1dc-103b0a74-38400-161...
18   0.000646       11065.955801       250           11075.955801  10960288  1610c92e11ec4a-0dcda7bb5f421e-4323461-100200-1...
19   0.001543        3664.951456        33            3674.951456  10960288  16223975899307-07f89d079f2be9-3e3d5f01-100200-...
2018-04-29 14:48:30,243 - memory_profile6_log - INFO - 

2018-04-29 14:48:30,368 - memory_profile6_log - INFO - size of big_frame_hist: 111.10 MB
2018-04-29 14:48:30,474 - memory_profile6_log - INFO - size of big_frame: 102.25 MB
2018-04-29 14:48:30,494 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-04-29 14:54:33,487 - memory_profile6_log - INFO - size of df: 509.09 MB
2018-04-29 14:54:33,490 - memory_profile6_log - INFO - getting total: 2090090 training data(current date interest)
2018-04-29 14:54:34,127 - memory_profile6_log - INFO - size of current_frame: 525.04 MB
2018-04-29 14:54:34,128 - memory_profile6_log - INFO - loading time of: 2493622 total genuine-current interest data ~ take 849.894s
2018-04-29 14:54:34,157 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 14:54:34,157 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 14:54:34,158 - memory_profile6_log - INFO - ================================================

2018-04-29 14:54:34,161 - memory_profile6_log - INFO -    345     86.7 MiB     86.7 MiB   @profile

2018-04-29 14:54:34,161 - memory_profile6_log - INFO -    346                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 14:54:34,163 - memory_profile6_log - INFO -    347     86.8 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 14:54:34,164 - memory_profile6_log - INFO -    348     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 14:54:34,164 - memory_profile6_log - INFO -    349                             

2018-04-29 14:54:34,165 - memory_profile6_log - INFO -    350                                 # ~~~ Begin collecting data ~~~

2018-04-29 14:54:34,167 - memory_profile6_log - INFO -    351     86.8 MiB      0.0 MiB       t0 = time.time()

2018-04-29 14:54:34,168 - memory_profile6_log - INFO -    352    646.5 MiB    559.7 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 14:54:34,170 - memory_profile6_log - INFO -    353    646.5 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 14:54:34,171 - memory_profile6_log - INFO -    354                                     logger.info("Training cannot be empty..")

2018-04-29 14:54:34,171 - memory_profile6_log - INFO -    355                                     return False

2018-04-29 14:54:34,173 - memory_profile6_log - INFO -    356    671.0 MiB     24.6 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 14:54:34,174 - memory_profile6_log - INFO -    357    671.3 MiB      0.3 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 14:54:34,174 - memory_profile6_log - INFO -    358    671.3 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 14:54:34,174 - memory_profile6_log - INFO -    359                             

2018-04-29 14:54:34,174 - memory_profile6_log - INFO -    360    680.8 MiB      9.5 MiB       big_frame = pd.concat(datalist)

2018-04-29 14:54:34,176 - memory_profile6_log - INFO -    361    680.8 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 14:54:34,176 - memory_profile6_log - INFO -    362    671.6 MiB     -9.2 MiB       del datalist

2018-04-29 14:54:34,177 - memory_profile6_log - INFO -    363                             

2018-04-29 14:54:34,177 - memory_profile6_log - INFO -    364    671.6 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 14:54:34,180 - memory_profile6_log - INFO -    365                             

2018-04-29 14:54:34,181 - memory_profile6_log - INFO -    366                                 # ~ get current news interest ~

2018-04-29 14:54:34,184 - memory_profile6_log - INFO -    367    671.6 MiB      0.0 MiB       if not cd:

2018-04-29 14:54:34,184 - memory_profile6_log - INFO -    368    671.6 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-04-29 14:54:34,184 - memory_profile6_log - INFO -    369   1098.3 MiB    426.7 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 14:54:34,186 - memory_profile6_log - INFO -    370   1098.3 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 14:54:34,186 - memory_profile6_log - INFO -    371                                 else:

2018-04-29 14:54:34,187 - memory_profile6_log - INFO -    372                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 14:54:34,187 - memory_profile6_log - INFO -    373                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 14:54:34,187 - memory_profile6_log - INFO -    374                             

2018-04-29 14:54:34,188 - memory_profile6_log - INFO -    375                                     # safe handling of query parameter

2018-04-29 14:54:34,193 - memory_profile6_log - INFO -    376                                     query_params = [

2018-04-29 14:54:34,194 - memory_profile6_log - INFO -    377                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 14:54:34,194 - memory_profile6_log - INFO -    378                                     ]

2018-04-29 14:54:34,194 - memory_profile6_log - INFO -    379                             

2018-04-29 14:54:34,196 - memory_profile6_log - INFO -    380                                     job_config.query_parameters = query_params

2018-04-29 14:54:34,197 - memory_profile6_log - INFO -    381                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 14:54:34,197 - memory_profile6_log - INFO -    382                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 14:54:34,197 - memory_profile6_log - INFO -    383                             

2018-04-29 14:54:34,197 - memory_profile6_log - INFO -    384   1114.2 MiB     16.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 14:54:34,198 - memory_profile6_log - INFO -    385   1114.2 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 14:54:34,198 - memory_profile6_log - INFO -    386   1114.3 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 14:54:34,200 - memory_profile6_log - INFO -    387   1114.3 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 14:54:34,203 - memory_profile6_log - INFO -    388   1114.3 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 14:54:34,203 - memory_profile6_log - INFO -    389   1114.3 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 14:54:34,204 - memory_profile6_log - INFO -    390                             

2018-04-29 14:54:34,204 - memory_profile6_log - INFO -    391   1114.3 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 14:54:34,206 - memory_profile6_log - INFO - 


2018-04-29 14:54:34,210 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 14:54:34,344 - memory_profile6_log - INFO - train on: 403532 total genuine interest data(D(u, t))
2018-04-29 14:54:34,345 - memory_profile6_log - INFO - transform on: 2090090 total current data(D(t))
2018-04-29 14:54:34,346 - memory_profile6_log - INFO - apply on: 403532 total history data(D(t))
2018-04-29 14:54:35,934 - memory_profile6_log - INFO - Len of model_fit: 403532
2018-04-29 14:54:35,936 - memory_profile6_log - INFO - Len of df_dut: 403532
2018-04-29 14:55:37,686 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-29 14:55:37,730 - memory_profile6_log - INFO - Total train time: 63.386s
2018-04-29 14:55:37,732 - memory_profile6_log - INFO - memory left before cleaning: 75.200 percent memory...
2018-04-29 14:55:37,733 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-29 14:55:37,733 - memory_profile6_log - INFO - deleting df_dut...
2018-04-29 14:55:37,734 - memory_profile6_log - INFO - deleting df_dt...
2018-04-29 14:55:37,736 - memory_profile6_log - INFO - deleting df_input...
2018-04-29 14:55:37,819 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-29 14:55:37,821 - memory_profile6_log - INFO - deleting df_current...
2018-04-29 14:55:37,822 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-29 14:55:37,841 - memory_profile6_log - INFO - deleting model_fit...
2018-04-29 14:55:37,842 - memory_profile6_log - INFO - deleting result...
2018-04-29 14:55:37,871 - memory_profile6_log - INFO - deleting model_transform...
2018-04-29 14:55:37,871 - memory_profile6_log - INFO - memory left after cleaning: 74.600 percent memory...
2018-04-29 14:55:37,874 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-29 14:55:37,875 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-29 14:55:38,059 - memory_profile6_log - INFO - Saving fitted_models as history...
2018-04-29 14:55:38,183 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 10
2018-04-29 14:55:38,184 - memory_profile6_log - INFO - processing batch-0
2018-04-29 14:55:38,372 - memory_profile6_log - INFO - processing batch-1
2018-04-29 14:55:38,569 - memory_profile6_log - INFO - processing batch-2
2018-04-29 14:55:38,752 - memory_profile6_log - INFO - processing batch-3
2018-04-29 14:55:38,937 - memory_profile6_log - INFO - processing batch-4
2018-04-29 14:55:39,131 - memory_profile6_log - INFO - processing batch-5
2018-04-29 14:55:39,312 - memory_profile6_log - INFO - processing batch-6
2018-04-29 14:55:39,497 - memory_profile6_log - INFO - processing batch-7
2018-04-29 14:55:39,683 - memory_profile6_log - INFO - processing batch-8
2018-04-29 14:55:39,871 - memory_profile6_log - INFO - processing batch-9
2018-04-29 14:55:40,062 - memory_profile6_log - INFO - deleting BR...
2018-04-29 14:55:40,065 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-04-29 14:55:40,098 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 14:55:40,098 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 14:55:40,098 - memory_profile6_log - INFO - ================================================

2018-04-29 14:55:40,098 - memory_profile6_log - INFO -    113   1104.2 MiB   1104.2 MiB   @profile

2018-04-29 14:55:40,099 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-29 14:55:40,099 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-29 14:55:40,102 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-29 14:55:40,102 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-29 14:55:40,105 - memory_profile6_log - INFO -    118                                      saveto="datastore"):

2018-04-29 14:55:40,105 - memory_profile6_log - INFO -    119                                 """

2018-04-29 14:55:40,107 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-29 14:55:40,108 - memory_profile6_log - INFO -    121                                 """

2018-04-29 14:55:40,108 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-29 14:55:40,108 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-29 14:55:40,109 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-29 14:55:40,111 - memory_profile6_log - INFO -    125   1104.2 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-29 14:55:40,114 - memory_profile6_log - INFO -    126   1116.6 MiB     12.4 MiB       df_dut = df_input.copy(deep=True)

2018-04-29 14:55:40,115 - memory_profile6_log - INFO -    127   1116.6 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 14:55:40,117 - memory_profile6_log - INFO -    128                             

2018-04-29 14:55:40,117 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-29 14:55:40,118 - memory_profile6_log - INFO -    130   1182.4 MiB     65.8 MiB       df_dt = df_current.copy(deep=True)

2018-04-29 14:55:40,118 - memory_profile6_log - INFO -    131   1182.4 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 14:55:40,119 - memory_profile6_log - INFO -    132                             

2018-04-29 14:55:40,119 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-29 14:55:40,121 - memory_profile6_log - INFO -    134   1182.4 MiB      0.0 MiB       t0 = time.time()

2018-04-29 14:55:40,121 - memory_profile6_log - INFO -    135   1182.4 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-29 14:55:40,121 - memory_profile6_log - INFO -    136   1182.4 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-29 14:55:40,130 - memory_profile6_log - INFO -    137   1182.4 MiB      0.0 MiB       logger.info("apply on: %d total history data(D(t))", len(df_hist))

2018-04-29 14:55:40,130 - memory_profile6_log - INFO -    138                             

2018-04-29 14:55:40,131 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-29 14:55:40,131 - memory_profile6_log - INFO -    140   1182.4 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-29 14:55:40,131 - memory_profile6_log - INFO -    141                             

2018-04-29 14:55:40,131 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-29 14:55:40,132 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-29 14:55:40,132 - memory_profile6_log - INFO -    144   1183.9 MiB      1.4 MiB       NB = BR.processX(df_dut)

2018-04-29 14:55:40,134 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-29 14:55:40,134 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-29 14:55:40,134 - memory_profile6_log - INFO -    147   1199.5 MiB     15.6 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-29 14:55:40,135 - memory_profile6_log - INFO -    148                                 """

2018-04-29 14:55:40,137 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-29 14:55:40,137 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-29 14:55:40,138 - memory_profile6_log - INFO -    151                                 """

2018-04-29 14:55:40,138 - memory_profile6_log - INFO -    152   1199.5 MiB      0.0 MiB       fitby_sigmant = True

2018-04-29 14:55:40,141 - memory_profile6_log - INFO -    153   1199.5 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-29 14:55:40,142 - memory_profile6_log - INFO -    154   1199.5 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-29 14:55:40,144 - memory_profile6_log - INFO -    155   1214.9 MiB     15.4 MiB                            'is_general']]

2018-04-29 14:55:40,145 - memory_profile6_log - INFO -    156   1214.9 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-29 14:55:40,147 - memory_profile6_log - INFO -    157   1214.9 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-29 14:55:40,148 - memory_profile6_log - INFO -    158   1255.9 MiB     41.0 MiB                          verbose=False)

2018-04-29 14:55:40,148 - memory_profile6_log - INFO -    159   1255.9 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-29 14:55:40,148 - memory_profile6_log - INFO -    160   1255.9 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-29 14:55:40,148 - memory_profile6_log - INFO -    161                             

2018-04-29 14:55:40,150 - memory_profile6_log - INFO -    162                                 # ~~ and Transform ~~

2018-04-29 14:55:40,150 - memory_profile6_log - INFO -    163                                 #   handling current news interest == current date

2018-04-29 14:55:40,154 - memory_profile6_log - INFO -    164   1255.9 MiB      0.0 MiB       if df_dt.empty:

2018-04-29 14:55:40,155 - memory_profile6_log - INFO -    165                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-29 14:55:40,155 - memory_profile6_log - INFO -    166                                     return None

2018-04-29 14:55:40,157 - memory_profile6_log - INFO -    167   1262.3 MiB      6.4 MiB       NB = BR.processX(df_dt)

2018-04-29 14:55:40,157 - memory_profile6_log - INFO -    168   1360.1 MiB     97.7 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-29 14:55:40,157 - memory_profile6_log - INFO -    169                             

2018-04-29 14:55:40,157 - memory_profile6_log - INFO -    170   1360.1 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-29 14:55:40,157 - memory_profile6_log - INFO -    171   1426.4 MiB     66.3 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-29 14:55:40,157 - memory_profile6_log - INFO -    172   1426.4 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-29 14:55:40,158 - memory_profile6_log - INFO -    173   1426.4 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-29 14:55:40,158 - memory_profile6_log - INFO -    174   1449.7 MiB     23.4 MiB                                                     verbose=False)

2018-04-29 14:55:40,160 - memory_profile6_log - INFO -    175                                 # ~~~ filter is general and specific topic ~~~

2018-04-29 14:55:40,161 - memory_profile6_log - INFO -    176                                 # the idea is just we need to rerank every topic according

2018-04-29 14:55:40,161 - memory_profile6_log - INFO -    177                                 # user_id and and is_general by p0_posterior

2018-04-29 14:55:40,161 - memory_profile6_log - INFO -    178   1449.7 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-29 14:55:40,161 - memory_profile6_log - INFO -    179   1452.8 MiB      3.1 MiB                                     'is_general']].groupby(['topic_id',

2018-04-29 14:55:40,165 - memory_profile6_log - INFO -    180   1449.8 MiB     -3.1 MiB                                                             'is_general']

2018-04-29 14:55:40,165 - memory_profile6_log - INFO -    181                                                                                      ).size().to_frame().reset_index()

2018-04-29 14:55:40,167 - memory_profile6_log - INFO -    182                             

2018-04-29 14:55:40,167 - memory_profile6_log - INFO -    183                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-29 14:55:40,168 - memory_profile6_log - INFO -    184                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-29 14:55:40,168 - memory_profile6_log - INFO -    185   1449.8 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-29 14:55:40,170 - memory_profile6_log - INFO -    186                             

2018-04-29 14:55:40,170 - memory_profile6_log - INFO -    187                                 # ~ start by provide rank for each topic type ~

2018-04-29 14:55:40,171 - memory_profile6_log - INFO -    188   1451.7 MiB      1.9 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-29 14:55:40,171 - memory_profile6_log - INFO -    189   1459.5 MiB      7.8 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-29 14:55:40,171 - memory_profile6_log - INFO -    190                             

2018-04-29 14:55:40,171 - memory_profile6_log - INFO -    191                                 # ~ set threshold to filter output

2018-04-29 14:55:40,171 - memory_profile6_log - INFO -    192   1459.5 MiB      0.0 MiB       if threshold > 0:

2018-04-29 14:55:40,173 - memory_profile6_log - INFO -    193   1459.5 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-29 14:55:40,173 - memory_profile6_log - INFO -    194   1459.5 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-29 14:55:40,173 - memory_profile6_log - INFO -    195   1457.6 MiB     -1.9 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-29 14:55:40,174 - memory_profile6_log - INFO -    196                             

2018-04-29 14:55:40,174 - memory_profile6_log - INFO -    197   1457.6 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 14:55:40,174 - memory_profile6_log - INFO -    198   1457.6 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-29 14:55:40,177 - memory_profile6_log - INFO -    199                             

2018-04-29 14:55:40,178 - memory_profile6_log - INFO -    200   1457.6 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 14:55:40,180 - memory_profile6_log - INFO -    201                             

2018-04-29 14:55:40,180 - memory_profile6_log - INFO -    202   1457.6 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-29 14:55:40,180 - memory_profile6_log - INFO -    203   1457.6 MiB      0.0 MiB       del df_dut

2018-04-29 14:55:40,181 - memory_profile6_log - INFO -    204   1457.6 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-29 14:55:40,181 - memory_profile6_log - INFO -    205   1457.6 MiB      0.0 MiB       del df_dt

2018-04-29 14:55:40,181 - memory_profile6_log - INFO -    206   1457.6 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-29 14:55:40,223 - memory_profile6_log - INFO -    207   1457.6 MiB      0.0 MiB       del df_input

2018-04-29 14:55:40,226 - memory_profile6_log - INFO -    208   1457.6 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-29 14:55:40,232 - memory_profile6_log - INFO -    209   1375.9 MiB    -81.7 MiB       del df_input_X

2018-04-29 14:55:40,232 - memory_profile6_log - INFO -    210   1375.9 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-29 14:55:40,233 - memory_profile6_log - INFO -    211   1375.9 MiB      0.0 MiB       del df_current

2018-04-29 14:55:40,236 - memory_profile6_log - INFO -    212   1375.9 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-29 14:55:40,237 - memory_profile6_log - INFO -    213   1375.9 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-29 14:55:40,237 - memory_profile6_log - INFO -    214   1375.9 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-29 14:55:40,239 - memory_profile6_log - INFO -    215   1338.9 MiB    -37.0 MiB       del model_fit

2018-04-29 14:55:40,240 - memory_profile6_log - INFO -    216   1338.9 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-29 14:55:40,240 - memory_profile6_log - INFO -    217   1338.9 MiB      0.0 MiB       del result

2018-04-29 14:55:40,240 - memory_profile6_log - INFO -    218   1338.9 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-29 14:55:40,240 - memory_profile6_log - INFO -    219                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 14:55:40,240 - memory_profile6_log - INFO -    220                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 14:55:40,240 - memory_profile6_log - INFO -    221                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 14:55:40,242 - memory_profile6_log - INFO -    222   1338.9 MiB      0.0 MiB       if savetrain:

2018-04-29 14:55:40,242 - memory_profile6_log - INFO -    223   1347.4 MiB      8.5 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-29 14:55:40,242 - memory_profile6_log - INFO -    224   1347.4 MiB      0.0 MiB           del model_transform

2018-04-29 14:55:40,243 - memory_profile6_log - INFO -    225   1347.4 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-29 14:55:40,246 - memory_profile6_log - INFO -    226   1347.4 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 14:55:40,246 - memory_profile6_log - INFO -    227                             

2018-04-29 14:55:40,247 - memory_profile6_log - INFO -    228   1347.4 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-29 14:55:40,247 - memory_profile6_log - INFO -    229                                     # ~ Place your code to save the training model here ~

2018-04-29 14:55:40,249 - memory_profile6_log - INFO -    230   1347.4 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-29 14:55:40,249 - memory_profile6_log - INFO -    231   1347.4 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-29 14:55:40,250 - memory_profile6_log - INFO -    232   1347.4 MiB      0.0 MiB               if multproc:

2018-04-29 14:55:40,250 - memory_profile6_log - INFO -    233   1318.4 MiB    -29.0 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-29 14:55:40,250 - memory_profile6_log - INFO -    234                             

2018-04-29 14:55:40,250 - memory_profile6_log - INFO -    235                                             # ~ save fitted models ~

2018-04-29 14:55:40,253 - memory_profile6_log - INFO -    236   1318.4 MiB      0.0 MiB                   logger.info("Saving fitted_models as history...")

2018-04-29 14:55:40,253 - memory_profile6_log - INFO -    237   1318.5 MiB      0.0 MiB                   save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 14:55:40,253 - memory_profile6_log - INFO -    238   1341.0 MiB     22.6 MiB                   fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-29 14:55:40,253 - memory_profile6_log - INFO -    239                             

2018-04-29 14:55:40,257 - memory_profile6_log - INFO -    240   1356.0 MiB     14.9 MiB                   X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-29 14:55:40,257 - memory_profile6_log - INFO -    241   1356.0 MiB      0.0 MiB                   logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-29 14:55:40,259 - memory_profile6_log - INFO -    242   1357.6 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 14:55:40,259 - memory_profile6_log - INFO -    243   1357.6 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 14:55:40,260 - memory_profile6_log - INFO -    244   1357.6 MiB      1.6 MiB                       mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-29 14:55:40,262 - memory_profile6_log - INFO -    245                             

2018-04-29 14:55:40,262 - memory_profile6_log - INFO -    246   1353.8 MiB     -3.7 MiB                   del X_split

2018-04-29 14:55:40,263 - memory_profile6_log - INFO -    247   1353.8 MiB      0.0 MiB                   del BR

2018-04-29 14:55:40,263 - memory_profile6_log - INFO -    248   1353.8 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-29 14:55:40,263 - memory_profile6_log - INFO -    249                             

2018-04-29 14:55:40,263 - memory_profile6_log - INFO -    250   1353.8 MiB      0.0 MiB                   del save_sigma_nt

2018-04-29 14:55:40,263 - memory_profile6_log - INFO -    251   1353.8 MiB      0.0 MiB                   logger.info("deleting save_sigma_nt...")

2018-04-29 14:55:40,265 - memory_profile6_log - INFO -    252                                             

2018-04-29 14:55:40,265 - memory_profile6_log - INFO -    253                                     elif str(saveto).lower() == "elastic":

2018-04-29 14:55:40,267 - memory_profile6_log - INFO -    254                                         logger.info("Using ElasticSearch as storage...")

2018-04-29 14:55:40,269 - memory_profile6_log - INFO -    255                                         mh.saveElasticS(model_transformsv)

2018-04-29 14:55:40,270 - memory_profile6_log - INFO -    256                             

2018-04-29 14:55:40,272 - memory_profile6_log - INFO -    257                                     # need save sigma_nt for daily train

2018-04-29 14:55:40,272 - memory_profile6_log - INFO -    258                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-29 14:55:40,273 - memory_profile6_log - INFO -    259                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-29 14:55:40,273 - memory_profile6_log - INFO -    260   1353.8 MiB      0.0 MiB           if start_date and end_date:

2018-04-29 14:55:40,273 - memory_profile6_log - INFO -    261                                         if not fitby_sigmant:

2018-04-29 14:55:40,273 - memory_profile6_log - INFO -    262                                             logging.info("Saving sigma Nt...")

2018-04-29 14:55:40,273 - memory_profile6_log - INFO -    263                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 14:55:40,275 - memory_profile6_log - INFO -    264                                             save_sigma_nt['start_date'] = start_date

2018-04-29 14:55:40,275 - memory_profile6_log - INFO -    265                                             save_sigma_nt['end_date'] = end_date

2018-04-29 14:55:40,276 - memory_profile6_log - INFO -    266                                             print save_sigma_nt.head(5)

2018-04-29 14:55:40,276 - memory_profile6_log - INFO -    267                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-29 14:55:40,279 - memory_profile6_log - INFO -    268                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-29 14:55:40,280 - memory_profile6_log - INFO -    269   1353.8 MiB      0.0 MiB       return

2018-04-29 14:55:40,282 - memory_profile6_log - INFO - 


2018-04-29 14:55:40,282 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-29 14:57:45,928 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 14:57:45,931 - memory_profile6_log - INFO - date_generated: 
2018-04-29 14:57:45,931 - memory_profile6_log - INFO -  
2018-04-29 14:57:45,933 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 9, 0, 0)]
2018-04-29 14:57:45,933 - memory_profile6_log - INFO - 

2018-04-29 14:57:45,934 - memory_profile6_log - INFO - using current date: 2018-04-10
2018-04-29 14:57:45,934 - memory_profile6_log - INFO - using start date: 2018-04-09 00:00:00
2018-04-29 14:57:45,934 - memory_profile6_log - INFO - using end date: 2018-04-09
2018-04-29 14:57:46,069 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 14:57:46,072 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-09
2018-04-29 14:59:39,032 - memory_profile6_log - INFO - size of df: 102.25 MB
2018-04-29 14:59:39,032 - memory_profile6_log - INFO - getting total: 403532 training data(genuine interest) for date: 2018-04-09
2018-04-29 14:59:39,088 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 14:59:39,088 - memory_profile6_log - INFO - Appending history data...
2018-04-29 14:59:39,091 - memory_profile6_log - INFO - processing batch-0
2018-04-29 14:59:39,092 - memory_profile6_log - INFO - creating list history data...
2018-04-29 14:59:39,197 - memory_profile6_log - INFO - call history data...
2018-04-29 15:00:55,723 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 15:00:57,375 - memory_profile6_log - INFO - processing batch-1
2018-04-29 15:00:57,377 - memory_profile6_log - INFO - creating list history data...
2018-04-29 15:00:57,450 - memory_profile6_log - INFO - call history data...
2018-04-29 15:02:13,332 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 15:02:14,957 - memory_profile6_log - INFO - processing batch-2
2018-04-29 15:02:14,959 - memory_profile6_log - INFO - creating list history data...
2018-04-29 15:02:15,042 - memory_profile6_log - INFO - call history data...
2018-04-29 15:03:30,634 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 15:03:32,263 - memory_profile6_log - INFO - processing batch-3
2018-04-29 15:03:32,265 - memory_profile6_log - INFO - creating list history data...
2018-04-29 15:03:32,338 - memory_profile6_log - INFO - call history data...
2018-04-29 15:04:48,194 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 15:04:49,855 - memory_profile6_log - INFO - processing batch-4
2018-04-29 15:04:49,857 - memory_profile6_log - INFO - creating list history data...
2018-04-29 15:04:49,930 - memory_profile6_log - INFO - call history data...
2018-04-29 15:06:11,232 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 15:06:12,865 - memory_profile6_log - INFO - Appending training data...
2018-04-29 15:06:12,868 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 15:06:12,868 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 15:06:12,881 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 15:06:12,882 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 15:06:12,884 - memory_profile6_log - INFO - ================================================

2018-04-29 15:06:12,884 - memory_profile6_log - INFO -    295     86.8 MiB     86.8 MiB   @profile

2018-04-29 15:06:12,888 - memory_profile6_log - INFO -    296                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 15:06:12,888 - memory_profile6_log - INFO -    297     86.8 MiB      0.0 MiB       bq_client = client

2018-04-29 15:06:12,891 - memory_profile6_log - INFO -    298     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 15:06:12,891 - memory_profile6_log - INFO -    299                             

2018-04-29 15:06:12,891 - memory_profile6_log - INFO -    300     86.8 MiB      0.0 MiB       datalist = []

2018-04-29 15:06:12,892 - memory_profile6_log - INFO -    301     86.8 MiB      0.0 MiB       datalist_hist = []

2018-04-29 15:06:12,894 - memory_profile6_log - INFO -    302                             

2018-04-29 15:06:12,894 - memory_profile6_log - INFO -    303     86.8 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 15:06:12,898 - memory_profile6_log - INFO -    304    655.4 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 15:06:12,898 - memory_profile6_log - INFO -    305    392.9 MiB    306.1 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 15:06:12,900 - memory_profile6_log - INFO -    306    392.9 MiB      0.0 MiB           if tframe is not None:

2018-04-29 15:06:12,901 - memory_profile6_log - INFO -    307    392.9 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 15:06:12,901 - memory_profile6_log - INFO -    308    406.1 MiB     13.1 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 15:06:12,903 - memory_profile6_log - INFO -    309    406.1 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 15:06:12,904 - memory_profile6_log - INFO -    310    406.1 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 15:06:12,905 - memory_profile6_log - INFO -    311    655.4 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 15:06:12,907 - memory_profile6_log - INFO -    312                                                 # ~ loading history

2018-04-29 15:06:12,908 - memory_profile6_log - INFO -    313                                                 """

2018-04-29 15:06:12,910 - memory_profile6_log - INFO -    314                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 15:06:12,911 - memory_profile6_log - INFO -    315                                                 """

2018-04-29 15:06:12,913 - memory_profile6_log - INFO -    316    628.7 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 15:06:12,913 - memory_profile6_log - INFO -    317                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 15:06:12,914 - memory_profile6_log - INFO -    318    628.7 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 15:06:12,914 - memory_profile6_log - INFO -    319                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 15:06:12,917 - memory_profile6_log - INFO -    320    629.5 MiB      8.5 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 15:06:12,918 - memory_profile6_log - INFO -    321                             

2018-04-29 15:06:12,920 - memory_profile6_log - INFO -    322    629.5 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 15:06:12,921 - memory_profile6_log - INFO -    323    704.9 MiB    534.5 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 15:06:12,921 - memory_profile6_log - INFO -    324                             

2018-04-29 15:06:12,923 - memory_profile6_log - INFO -    325                                                 # me = os.getpid()

2018-04-29 15:06:12,924 - memory_profile6_log - INFO -    326                                                 # kill_proc_tree(me)

2018-04-29 15:06:12,924 - memory_profile6_log - INFO -    327                             

2018-04-29 15:06:12,926 - memory_profile6_log - INFO -    328    704.9 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 15:06:12,928 - memory_profile6_log - INFO -    329    707.0 MiB     -0.9 MiB                       for m in h_frame:

2018-04-29 15:06:12,930 - memory_profile6_log - INFO -    330    707.0 MiB     -0.9 MiB                           if m is not None:

2018-04-29 15:06:12,930 - memory_profile6_log - INFO -    331    707.0 MiB     -0.9 MiB                               if len(m) > 0:

2018-04-29 15:06:12,931 - memory_profile6_log - INFO -    332    707.0 MiB      9.7 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 15:06:12,933 - memory_profile6_log - INFO -    333    655.4 MiB   -290.5 MiB                       del h_frame

2018-04-29 15:06:12,934 - memory_profile6_log - INFO -    334    655.4 MiB    -13.7 MiB                       del lhistory

2018-04-29 15:06:12,934 - memory_profile6_log - INFO -    335                             

2018-04-29 15:06:12,936 - memory_profile6_log - INFO -    336    655.4 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 15:06:12,937 - memory_profile6_log - INFO -    337    655.4 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 15:06:12,938 - memory_profile6_log - INFO -    338                                     else: 

2018-04-29 15:06:12,940 - memory_profile6_log - INFO -    339                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 15:06:12,940 - memory_profile6_log - INFO -    340    655.4 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 15:06:12,941 - memory_profile6_log - INFO -    341    655.4 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 15:06:12,943 - memory_profile6_log - INFO -    342                             

2018-04-29 15:06:12,943 - memory_profile6_log - INFO -    343    655.4 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 15:06:12,944 - memory_profile6_log - INFO - 


2018-04-29 15:06:14,108 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 15:06:14,207 - memory_profile6_log - INFO -     p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0    0.001543         394.687612       718             404.687612  10960288  1612f7bb03410e-0524566d1e88ff-2a0b185d-38400-1...
1    0.001543         502.346467       311             512.346467  10960288  16275ef9b5ad-0d25ed18f50a1c-3b670e20-4df28-162...
2    0.001543        1610.180397        59            1620.180397  10960288  161f36169fce02-03becb8d4c95b1-b353461-15f900-1...
3    0.000088       13481.785714        51           13491.785714  11911567  161c17f16b2aea-07a42b07ed0344-a35346f-15f900-1...
4    0.004470         104.752541       220             114.752541  22291119  1612de3ae04200-06b613a5de42a28-2c5f3268-2c600-...
5    0.001543        1086.583319      1401            1096.583319  10960288  1616ac78a3e5b5-09b19c5de4820b-4c534c69-100200-...
6    0.004470         182.437807       178             192.437807  22291119  161abf106841e-0a1f11083ce818-6a6d3d24-38400-16...
7    0.001543        7329.902913        88            7339.902913  10960288  162a823de4d3a-0188a0cd2dbc66-556b3f73-100200-1...
8    0.001543         137.700488       173             147.700488  10960288  161a8d02707a-00e15b9bbe51de-7023786e-38400-161...
9    0.001543         757.502989        83             767.502989  10960288  162000bced2108-0b3e765f648e59-5e615b69-38400-1...
10   0.004470          73.211957       167              83.211957  22291119  1620fd2047f140-01c64ab93e299-7126121a-38400-16...
11   0.000088         793.046218        85             803.046218  11911567  1617837a69987-0c6dc2d5fbd368-367b2a70-38400-16...
12   0.001543         291.317353       500             301.317353  10960288  161e597faed5c7-0a6a04984b835d8-1362684a-100200...
13   0.001543        2670.969656       226            2680.969656  10960288               9b58f159-de79-43c4-a658-b6d19f4e0e28
14   0.001543        5530.427277       944            5540.427277  10960288  162188fedc4102f-08353c9a5317cc-b353461-100200-...
15   0.004470         191.037449        64             201.037449  22291119  1613090be5a44b-0c62feb84-2f10403b-13c680-16130...
16   0.000088        2106.529018        32            2116.529018  11911567  16162a4f8de32-00d4e9007f223f-1a520605-38400-16...
17   0.001543          91.623786        60             101.623786  10960288  1612dcfd0171f5-00dd8987eb7be2-5c7f6035-38400-1...
18   0.001543         137.060738       728             147.060738  10960288  1610cafeaae255-08b80070e796c6-4323461-100200-1...
19   0.000088        1053.264509        32            1063.264509  11911567  1622d0156341d8-00514d785e0b658-17357b40-c0000-...
2018-04-29 15:06:14,209 - memory_profile6_log - INFO - 

2018-04-29 15:06:14,335 - memory_profile6_log - INFO - size of big_frame_hist: 111.10 MB
2018-04-29 15:06:14,451 - memory_profile6_log - INFO - size of big_frame: 102.25 MB
2018-04-29 15:06:14,470 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-04-29 15:12:32,046 - memory_profile6_log - INFO - size of df: 509.09 MB
2018-04-29 15:12:32,048 - memory_profile6_log - INFO - getting total: 2090090 training data(current date interest)
2018-04-29 15:12:32,690 - memory_profile6_log - INFO - size of current_frame: 525.04 MB
2018-04-29 15:12:32,693 - memory_profile6_log - INFO - loading time of: 2493622 total genuine-current interest data ~ take 886.646s
2018-04-29 15:12:32,717 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 15:12:32,717 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 15:12:32,719 - memory_profile6_log - INFO - ================================================

2018-04-29 15:12:32,720 - memory_profile6_log - INFO -    345     86.7 MiB     86.7 MiB   @profile

2018-04-29 15:12:32,720 - memory_profile6_log - INFO -    346                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 15:12:32,721 - memory_profile6_log - INFO -    347     86.8 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 15:12:32,721 - memory_profile6_log - INFO -    348     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 15:12:32,723 - memory_profile6_log - INFO -    349                             

2018-04-29 15:12:32,723 - memory_profile6_log - INFO -    350                                 # ~~~ Begin collecting data ~~~

2018-04-29 15:12:32,723 - memory_profile6_log - INFO -    351     86.8 MiB      0.0 MiB       t0 = time.time()

2018-04-29 15:12:32,724 - memory_profile6_log - INFO -    352    648.7 MiB    561.8 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 15:12:32,724 - memory_profile6_log - INFO -    353    648.7 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 15:12:32,726 - memory_profile6_log - INFO -    354                                     logger.info("Training cannot be empty..")

2018-04-29 15:12:32,726 - memory_profile6_log - INFO -    355                                     return False

2018-04-29 15:12:32,730 - memory_profile6_log - INFO -    356    673.1 MiB     24.4 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 15:12:32,730 - memory_profile6_log - INFO -    357    673.2 MiB      0.1 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 15:12:32,730 - memory_profile6_log - INFO -    358    673.2 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 15:12:32,732 - memory_profile6_log - INFO -    359                             

2018-04-29 15:12:32,732 - memory_profile6_log - INFO -    360    682.8 MiB      9.6 MiB       big_frame = pd.concat(datalist)

2018-04-29 15:12:32,733 - memory_profile6_log - INFO -    361    682.8 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 15:12:32,733 - memory_profile6_log - INFO -    362    673.5 MiB     -9.2 MiB       del datalist

2018-04-29 15:12:32,733 - memory_profile6_log - INFO -    363                             

2018-04-29 15:12:32,733 - memory_profile6_log - INFO -    364    673.5 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 15:12:32,734 - memory_profile6_log - INFO -    365                             

2018-04-29 15:12:32,734 - memory_profile6_log - INFO -    366                                 # ~ get current news interest ~

2018-04-29 15:12:32,736 - memory_profile6_log - INFO -    367    673.5 MiB      0.0 MiB       if not cd:

2018-04-29 15:12:32,736 - memory_profile6_log - INFO -    368    673.5 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-04-29 15:12:32,736 - memory_profile6_log - INFO -    369   1098.8 MiB    425.3 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 15:12:32,736 - memory_profile6_log - INFO -    370   1098.8 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 15:12:32,736 - memory_profile6_log - INFO -    371                                 else:

2018-04-29 15:12:32,736 - memory_profile6_log - INFO -    372                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 15:12:32,737 - memory_profile6_log - INFO -    373                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 15:12:32,737 - memory_profile6_log - INFO -    374                             

2018-04-29 15:12:32,739 - memory_profile6_log - INFO -    375                                     # safe handling of query parameter

2018-04-29 15:12:32,743 - memory_profile6_log - INFO -    376                                     query_params = [

2018-04-29 15:12:32,743 - memory_profile6_log - INFO -    377                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 15:12:32,743 - memory_profile6_log - INFO -    378                                     ]

2018-04-29 15:12:32,744 - memory_profile6_log - INFO -    379                             

2018-04-29 15:12:32,744 - memory_profile6_log - INFO -    380                                     job_config.query_parameters = query_params

2018-04-29 15:12:32,746 - memory_profile6_log - INFO -    381                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 15:12:32,746 - memory_profile6_log - INFO -    382                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 15:12:32,746 - memory_profile6_log - INFO -    383                             

2018-04-29 15:12:32,746 - memory_profile6_log - INFO -    384   1114.8 MiB     16.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 15:12:32,746 - memory_profile6_log - INFO -    385   1114.8 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 15:12:32,747 - memory_profile6_log - INFO -    386   1114.8 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 15:12:32,749 - memory_profile6_log - INFO -    387   1114.8 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 15:12:32,750 - memory_profile6_log - INFO -    388   1114.8 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 15:12:32,750 - memory_profile6_log - INFO -    389   1114.8 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 15:12:32,755 - memory_profile6_log - INFO -    390                             

2018-04-29 15:12:32,756 - memory_profile6_log - INFO -    391   1114.8 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 15:12:32,756 - memory_profile6_log - INFO - 


2018-04-29 15:12:32,759 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 15:12:32,905 - memory_profile6_log - INFO - train on: 403532 total genuine interest data(D(u, t))
2018-04-29 15:12:32,907 - memory_profile6_log - INFO - transform on: 2090090 total current data(D(t))
2018-04-29 15:12:32,907 - memory_profile6_log - INFO - apply on: 403532 total history data(D(t))
2018-04-29 15:12:34,657 - memory_profile6_log - INFO - Len of model_fit: 403532
2018-04-29 15:12:34,657 - memory_profile6_log - INFO - Len of df_dut: 403532
2018-04-29 15:13:35,778 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-29 15:13:35,819 - memory_profile6_log - INFO - Total train time: 62.913s
2018-04-29 15:13:35,819 - memory_profile6_log - INFO - memory left before cleaning: 76.700 percent memory...
2018-04-29 15:13:35,822 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-29 15:13:35,822 - memory_profile6_log - INFO - deleting df_dut...
2018-04-29 15:13:35,825 - memory_profile6_log - INFO - deleting df_dt...
2018-04-29 15:13:35,825 - memory_profile6_log - INFO - deleting df_input...
2018-04-29 15:13:35,910 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-29 15:13:35,911 - memory_profile6_log - INFO - deleting df_current...
2018-04-29 15:13:35,913 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-29 15:13:35,933 - memory_profile6_log - INFO - deleting model_fit...
2018-04-29 15:13:35,934 - memory_profile6_log - INFO - deleting result...
2018-04-29 15:13:35,963 - memory_profile6_log - INFO - deleting model_transform...
2018-04-29 15:13:35,966 - memory_profile6_log - INFO - memory left after cleaning: 76.000 percent memory...
2018-04-29 15:13:35,967 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-29 15:13:35,969 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-29 15:13:36,176 - memory_profile6_log - INFO - Saving fitted_models as history...
2018-04-29 15:13:36,292 - memory_profile6_log - INFO - Len of X_split for batch save fitted_models: 10
2018-04-29 15:13:36,293 - memory_profile6_log - INFO - processing batch-0
2018-04-29 15:13:36,484 - memory_profile6_log - INFO - processing batch-1
2018-04-29 15:13:36,671 - memory_profile6_log - INFO - processing batch-2
2018-04-29 15:13:36,858 - memory_profile6_log - INFO - processing batch-3
2018-04-29 15:13:37,039 - memory_profile6_log - INFO - processing batch-4
2018-04-29 15:13:37,239 - memory_profile6_log - INFO - processing batch-5
2018-04-29 15:13:37,428 - memory_profile6_log - INFO - processing batch-6
2018-04-29 15:13:37,609 - memory_profile6_log - INFO - processing batch-7
2018-04-29 15:13:37,795 - memory_profile6_log - INFO - processing batch-8
2018-04-29 15:13:37,980 - memory_profile6_log - INFO - processing batch-9
2018-04-29 15:13:38,176 - memory_profile6_log - INFO - deleting BR...
2018-04-29 15:13:38,177 - memory_profile6_log - INFO - deleting save_sigma_nt...
2018-04-29 15:13:38,193 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 15:13:38,193 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 15:13:38,193 - memory_profile6_log - INFO - ================================================

2018-04-29 15:13:38,194 - memory_profile6_log - INFO -    113   1105.3 MiB   1105.3 MiB   @profile

2018-04-29 15:13:38,194 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-29 15:13:38,194 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-29 15:13:38,194 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-29 15:13:38,196 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-29 15:13:38,196 - memory_profile6_log - INFO -    118                                      saveto="datastore"):

2018-04-29 15:13:38,196 - memory_profile6_log - INFO -    119                                 """

2018-04-29 15:13:38,197 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-29 15:13:38,197 - memory_profile6_log - INFO -    121                                 """

2018-04-29 15:13:38,197 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-29 15:13:38,197 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-29 15:13:38,197 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-29 15:13:38,200 - memory_profile6_log - INFO -    125   1105.3 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-29 15:13:38,200 - memory_profile6_log - INFO -    126   1118.0 MiB     12.7 MiB       df_dut = df_input.copy(deep=True)

2018-04-29 15:13:38,204 - memory_profile6_log - INFO -    127   1118.0 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 15:13:38,204 - memory_profile6_log - INFO -    128                             

2018-04-29 15:13:38,207 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-29 15:13:38,207 - memory_profile6_log - INFO -    130   1183.8 MiB     65.8 MiB       df_dt = df_current.copy(deep=True)

2018-04-29 15:13:38,209 - memory_profile6_log - INFO -    131   1183.8 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 15:13:38,210 - memory_profile6_log - INFO -    132                             

2018-04-29 15:13:38,210 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-29 15:13:38,210 - memory_profile6_log - INFO -    134   1183.8 MiB      0.0 MiB       t0 = time.time()

2018-04-29 15:13:38,210 - memory_profile6_log - INFO -    135   1183.8 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-29 15:13:38,211 - memory_profile6_log - INFO -    136   1183.8 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-29 15:13:38,211 - memory_profile6_log - INFO -    137   1183.8 MiB      0.0 MiB       logger.info("apply on: %d total history data(D(t))", len(df_hist))

2018-04-29 15:13:38,213 - memory_profile6_log - INFO -    138                             

2018-04-29 15:13:38,213 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-29 15:13:38,213 - memory_profile6_log - INFO -    140   1183.8 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-29 15:13:38,217 - memory_profile6_log - INFO -    141                             

2018-04-29 15:13:38,217 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-29 15:13:38,219 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-29 15:13:38,219 - memory_profile6_log - INFO -    144   1186.2 MiB      2.4 MiB       NB = BR.processX(df_dut)

2018-04-29 15:13:38,220 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-29 15:13:38,220 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-29 15:13:38,220 - memory_profile6_log - INFO -    147   1202.1 MiB     16.0 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-29 15:13:38,220 - memory_profile6_log - INFO -    148                                 """

2018-04-29 15:13:38,221 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-29 15:13:38,221 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-29 15:13:38,223 - memory_profile6_log - INFO -    151                                 """

2018-04-29 15:13:38,223 - memory_profile6_log - INFO -    152   1202.1 MiB      0.0 MiB       fitby_sigmant = True

2018-04-29 15:13:38,223 - memory_profile6_log - INFO -    153   1202.1 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-29 15:13:38,223 - memory_profile6_log - INFO -    154   1202.1 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-29 15:13:38,227 - memory_profile6_log - INFO -    155   1217.5 MiB     15.4 MiB                            'is_general']]

2018-04-29 15:13:38,230 - memory_profile6_log - INFO -    156   1217.5 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-29 15:13:38,230 - memory_profile6_log - INFO -    157   1217.5 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-29 15:13:38,230 - memory_profile6_log - INFO -    158   1259.0 MiB     41.5 MiB                          verbose=False)

2018-04-29 15:13:38,232 - memory_profile6_log - INFO -    159   1259.0 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-29 15:13:38,232 - memory_profile6_log - INFO -    160   1259.0 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-29 15:13:38,233 - memory_profile6_log - INFO -    161                             

2018-04-29 15:13:38,233 - memory_profile6_log - INFO -    162                                 # ~~ and Transform ~~

2018-04-29 15:13:38,234 - memory_profile6_log - INFO -    163                                 #   handling current news interest == current date

2018-04-29 15:13:38,234 - memory_profile6_log - INFO -    164   1259.0 MiB      0.0 MiB       if df_dt.empty:

2018-04-29 15:13:38,236 - memory_profile6_log - INFO -    165                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-29 15:13:38,236 - memory_profile6_log - INFO -    166                                     return None

2018-04-29 15:13:38,236 - memory_profile6_log - INFO -    167   1265.3 MiB      6.3 MiB       NB = BR.processX(df_dt)

2018-04-29 15:13:38,239 - memory_profile6_log - INFO -    168   1363.7 MiB     98.5 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-29 15:13:38,240 - memory_profile6_log - INFO -    169                             

2018-04-29 15:13:38,240 - memory_profile6_log - INFO -    170   1363.7 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-29 15:13:38,243 - memory_profile6_log - INFO -    171   1430.1 MiB     66.3 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-29 15:13:38,243 - memory_profile6_log - INFO -    172   1430.1 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-29 15:13:38,243 - memory_profile6_log - INFO -    173   1430.1 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-29 15:13:38,244 - memory_profile6_log - INFO -    174   1454.3 MiB     24.2 MiB                                                     verbose=False)

2018-04-29 15:13:38,244 - memory_profile6_log - INFO -    175                                 # ~~~ filter is general and specific topic ~~~

2018-04-29 15:13:38,246 - memory_profile6_log - INFO -    176                                 # the idea is just we need to rerank every topic according

2018-04-29 15:13:38,246 - memory_profile6_log - INFO -    177                                 # user_id and and is_general by p0_posterior

2018-04-29 15:13:38,246 - memory_profile6_log - INFO -    178   1454.3 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-29 15:13:38,246 - memory_profile6_log - INFO -    179   1457.4 MiB      3.1 MiB                                     'is_general']].groupby(['topic_id',

2018-04-29 15:13:38,246 - memory_profile6_log - INFO -    180   1454.3 MiB     -3.1 MiB                                                             'is_general']

2018-04-29 15:13:38,250 - memory_profile6_log - INFO -    181                                                                                      ).size().to_frame().reset_index()

2018-04-29 15:13:38,253 - memory_profile6_log - INFO -    182                             

2018-04-29 15:13:38,253 - memory_profile6_log - INFO -    183                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-29 15:13:38,253 - memory_profile6_log - INFO -    184                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-29 15:13:38,255 - memory_profile6_log - INFO -    185   1454.4 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-29 15:13:38,255 - memory_profile6_log - INFO -    186                             

2018-04-29 15:13:38,256 - memory_profile6_log - INFO -    187                                 # ~ start by provide rank for each topic type ~

2018-04-29 15:13:38,256 - memory_profile6_log - INFO -    188   1450.3 MiB     -4.1 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-29 15:13:38,257 - memory_profile6_log - INFO -    189   1456.8 MiB      6.5 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-29 15:13:38,260 - memory_profile6_log - INFO -    190                             

2018-04-29 15:13:38,262 - memory_profile6_log - INFO -    191                                 # ~ set threshold to filter output

2018-04-29 15:13:38,263 - memory_profile6_log - INFO -    192   1456.8 MiB      0.0 MiB       if threshold > 0:

2018-04-29 15:13:38,263 - memory_profile6_log - INFO -    193   1456.8 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-29 15:13:38,263 - memory_profile6_log - INFO -    194   1456.8 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-29 15:13:38,263 - memory_profile6_log - INFO -    195   1454.7 MiB     -2.1 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-29 15:13:38,263 - memory_profile6_log - INFO -    196                             

2018-04-29 15:13:38,265 - memory_profile6_log - INFO -    197   1454.7 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 15:13:38,265 - memory_profile6_log - INFO -    198   1454.7 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-29 15:13:38,266 - memory_profile6_log - INFO -    199                             

2018-04-29 15:13:38,267 - memory_profile6_log - INFO -    200   1454.7 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 15:13:38,267 - memory_profile6_log - INFO -    201                             

2018-04-29 15:13:38,269 - memory_profile6_log - INFO -    202   1454.7 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-29 15:13:38,269 - memory_profile6_log - INFO -    203   1454.7 MiB      0.0 MiB       del df_dut

2018-04-29 15:13:38,269 - memory_profile6_log - INFO -    204   1454.7 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-29 15:13:38,269 - memory_profile6_log - INFO -    205   1454.7 MiB      0.0 MiB       del df_dt

2018-04-29 15:13:38,273 - memory_profile6_log - INFO -    206   1454.7 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-29 15:13:38,273 - memory_profile6_log - INFO -    207   1454.7 MiB      0.0 MiB       del df_input

2018-04-29 15:13:38,275 - memory_profile6_log - INFO -    208   1454.7 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-29 15:13:38,276 - memory_profile6_log - INFO -    209   1373.0 MiB    -81.7 MiB       del df_input_X

2018-04-29 15:13:38,276 - memory_profile6_log - INFO -    210   1373.0 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-29 15:13:38,276 - memory_profile6_log - INFO -    211   1373.0 MiB      0.0 MiB       del df_current

2018-04-29 15:13:38,278 - memory_profile6_log - INFO -    212   1373.0 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-29 15:13:38,278 - memory_profile6_log - INFO -    213   1373.0 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-29 15:13:38,279 - memory_profile6_log - INFO -    214   1373.0 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-29 15:13:38,279 - memory_profile6_log - INFO -    215   1336.0 MiB    -37.0 MiB       del model_fit

2018-04-29 15:13:38,279 - memory_profile6_log - INFO -    216   1336.0 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-29 15:13:38,279 - memory_profile6_log - INFO -    217   1336.0 MiB      0.0 MiB       del result

2018-04-29 15:13:38,279 - memory_profile6_log - INFO -    218   1336.0 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-29 15:13:38,279 - memory_profile6_log - INFO -    219                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 15:13:38,280 - memory_profile6_log - INFO -    220                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 15:13:38,280 - memory_profile6_log - INFO -    221                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 15:13:38,282 - memory_profile6_log - INFO -    222   1336.0 MiB      0.0 MiB       if savetrain:

2018-04-29 15:13:38,286 - memory_profile6_log - INFO -    223   1344.5 MiB      8.5 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-29 15:13:38,286 - memory_profile6_log - INFO -    224   1344.5 MiB      0.0 MiB           del model_transform

2018-04-29 15:13:38,288 - memory_profile6_log - INFO -    225   1344.5 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-29 15:13:38,288 - memory_profile6_log - INFO -    226   1344.5 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 15:13:38,289 - memory_profile6_log - INFO -    227                             

2018-04-29 15:13:38,289 - memory_profile6_log - INFO -    228   1344.5 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-29 15:13:38,289 - memory_profile6_log - INFO -    229                                     # ~ Place your code to save the training model here ~

2018-04-29 15:13:38,290 - memory_profile6_log - INFO -    230   1344.5 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-29 15:13:38,292 - memory_profile6_log - INFO -    231   1344.5 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-29 15:13:38,292 - memory_profile6_log - INFO -    232   1344.5 MiB      0.0 MiB               if multproc:

2018-04-29 15:13:38,296 - memory_profile6_log - INFO -    233   1315.3 MiB    -29.2 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-29 15:13:38,296 - memory_profile6_log - INFO -    234                             

2018-04-29 15:13:38,299 - memory_profile6_log - INFO -    235                                             # ~ save fitted models ~

2018-04-29 15:13:38,299 - memory_profile6_log - INFO -    236   1315.3 MiB      0.0 MiB                   logger.info("Saving fitted_models as history...")

2018-04-29 15:13:38,301 - memory_profile6_log - INFO -    237   1315.4 MiB      0.0 MiB                   save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 15:13:38,302 - memory_profile6_log - INFO -    238   1338.4 MiB     23.1 MiB                   fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-29 15:13:38,302 - memory_profile6_log - INFO -    239                             

2018-04-29 15:13:38,302 - memory_profile6_log - INFO -    240   1351.9 MiB     13.5 MiB                   X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-29 15:13:38,302 - memory_profile6_log - INFO -    241   1351.9 MiB      0.0 MiB                   logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-29 15:13:38,302 - memory_profile6_log - INFO -    242   1353.2 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 15:13:38,303 - memory_profile6_log - INFO -    243   1353.2 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 15:13:38,306 - memory_profile6_log - INFO -    244   1353.2 MiB      1.2 MiB                       mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-29 15:13:38,308 - memory_profile6_log - INFO -    245                             

2018-04-29 15:13:38,309 - memory_profile6_log - INFO -    246   1349.5 MiB     -3.7 MiB                   del X_split

2018-04-29 15:13:38,309 - memory_profile6_log - INFO -    247   1349.5 MiB      0.0 MiB                   del BR

2018-04-29 15:13:38,311 - memory_profile6_log - INFO -    248   1349.5 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-29 15:13:38,311 - memory_profile6_log - INFO -    249                             

2018-04-29 15:13:38,312 - memory_profile6_log - INFO -    250   1349.5 MiB      0.0 MiB                   del save_sigma_nt

2018-04-29 15:13:38,312 - memory_profile6_log - INFO -    251   1349.5 MiB      0.0 MiB                   logger.info("deleting save_sigma_nt...")

2018-04-29 15:13:38,312 - memory_profile6_log - INFO -    252                                             

2018-04-29 15:13:38,312 - memory_profile6_log - INFO -    253                                     elif str(saveto).lower() == "elastic":

2018-04-29 15:13:38,312 - memory_profile6_log - INFO -    254                                         logger.info("Using ElasticSearch as storage...")

2018-04-29 15:13:38,312 - memory_profile6_log - INFO -    255                                         mh.saveElasticS(model_transformsv)

2018-04-29 15:13:38,313 - memory_profile6_log - INFO -    256                             

2018-04-29 15:13:38,313 - memory_profile6_log - INFO -    257                                     # need save sigma_nt for daily train

2018-04-29 15:13:38,313 - memory_profile6_log - INFO -    258                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-29 15:13:38,315 - memory_profile6_log - INFO -    259                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-29 15:13:38,315 - memory_profile6_log - INFO -    260   1349.5 MiB      0.0 MiB           if start_date and end_date:

2018-04-29 15:13:38,319 - memory_profile6_log - INFO -    261                                         if not fitby_sigmant:

2018-04-29 15:13:38,321 - memory_profile6_log - INFO -    262                                             logging.info("Saving sigma Nt...")

2018-04-29 15:13:38,322 - memory_profile6_log - INFO -    263                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 15:13:38,323 - memory_profile6_log - INFO -    264                                             save_sigma_nt['start_date'] = start_date

2018-04-29 15:13:38,325 - memory_profile6_log - INFO -    265                                             save_sigma_nt['end_date'] = end_date

2018-04-29 15:13:38,326 - memory_profile6_log - INFO -    266                                             print save_sigma_nt.head(5)

2018-04-29 15:13:38,326 - memory_profile6_log - INFO -    267                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-29 15:13:38,328 - memory_profile6_log - INFO -    268                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-29 15:13:38,328 - memory_profile6_log - INFO -    269   1349.5 MiB      0.0 MiB       return

2018-04-29 15:13:38,331 - memory_profile6_log - INFO - 


2018-04-29 15:13:38,331 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-29 15:55:57,732 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 15:55:57,733 - memory_profile6_log - INFO - date_generated: 
2018-04-29 15:55:57,734 - memory_profile6_log - INFO -  
2018-04-29 15:55:57,734 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 15:55:57,736 - memory_profile6_log - INFO - 

2018-04-29 15:55:57,736 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 15:55:57,736 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 15:55:57,736 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 15:55:57,857 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 15:55:57,859 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 15:56:34,727 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 15:56:34,732 - memory_profile6_log - INFO - date_generated: 
2018-04-29 15:56:34,732 - memory_profile6_log - INFO -  
2018-04-29 15:56:34,732 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 15:56:34,733 - memory_profile6_log - INFO - 

2018-04-29 15:56:34,733 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 15:56:34,733 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 15:56:34,733 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 15:56:34,854 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 15:56:34,857 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 15:57:40,752 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 15:57:40,755 - memory_profile6_log - INFO - date_generated: 
2018-04-29 15:57:40,755 - memory_profile6_log - INFO -  
2018-04-29 15:57:40,756 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 15:57:40,756 - memory_profile6_log - INFO - 

2018-04-29 15:57:40,756 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 15:57:40,756 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 15:57:40,757 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 15:57:40,881 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 15:57:40,884 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 15:58:48,711 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 15:58:48,713 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 15:58:48,760 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 15:58:48,762 - memory_profile6_log - INFO - Appending history data...
2018-04-29 15:58:48,763 - memory_profile6_log - INFO - processing batch-0
2018-04-29 15:58:48,765 - memory_profile6_log - INFO - creating list history data...
2018-04-29 15:58:48,844 - memory_profile6_log - INFO - call history data...
2018-04-29 15:59:45,088 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 15:59:46,328 - memory_profile6_log - INFO - processing batch-1
2018-04-29 15:59:46,328 - memory_profile6_log - INFO - creating list history data...
2018-04-29 15:59:46,378 - memory_profile6_log - INFO - call history data...
2018-04-29 16:00:40,875 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:00:42,095 - memory_profile6_log - INFO - processing batch-2
2018-04-29 16:00:42,096 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:00:42,151 - memory_profile6_log - INFO - call history data...
2018-04-29 16:01:39,690 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:01:40,913 - memory_profile6_log - INFO - processing batch-3
2018-04-29 16:01:40,914 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:01:40,966 - memory_profile6_log - INFO - call history data...
2018-04-29 16:02:36,318 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:02:37,552 - memory_profile6_log - INFO - processing batch-4
2018-04-29 16:02:37,555 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:02:37,605 - memory_profile6_log - INFO - call history data...
2018-04-29 16:03:32,013 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:03:33,252 - memory_profile6_log - INFO - Appending training data...
2018-04-29 16:03:33,253 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 16:03:33,255 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 16:03:33,263 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 16:03:33,263 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 16:03:33,266 - memory_profile6_log - INFO - ================================================

2018-04-29 16:03:33,266 - memory_profile6_log - INFO -    298     86.9 MiB     86.9 MiB   @profile

2018-04-29 16:03:33,267 - memory_profile6_log - INFO -    299                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 16:03:33,269 - memory_profile6_log - INFO -    300     86.9 MiB      0.0 MiB       bq_client = client

2018-04-29 16:03:33,269 - memory_profile6_log - INFO -    301     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 16:03:33,270 - memory_profile6_log - INFO -    302                             

2018-04-29 16:03:33,273 - memory_profile6_log - INFO -    303     86.9 MiB      0.0 MiB       datalist = []

2018-04-29 16:03:33,273 - memory_profile6_log - INFO -    304     86.9 MiB      0.0 MiB       datalist_hist = []

2018-04-29 16:03:33,276 - memory_profile6_log - INFO -    305                             

2018-04-29 16:03:33,276 - memory_profile6_log - INFO -    306     86.9 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 16:03:33,278 - memory_profile6_log - INFO -    307    445.9 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 16:03:33,279 - memory_profile6_log - INFO -    308    339.6 MiB    252.6 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 16:03:33,280 - memory_profile6_log - INFO -    309    339.6 MiB      0.0 MiB           if tframe is not None:

2018-04-29 16:03:33,282 - memory_profile6_log - INFO -    310    339.6 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 16:03:33,283 - memory_profile6_log - INFO -    311    347.1 MiB      7.5 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 16:03:33,285 - memory_profile6_log - INFO -    312    347.1 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 16:03:33,286 - memory_profile6_log - INFO -    313    347.1 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 16:03:33,286 - memory_profile6_log - INFO -    314    445.9 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 16:03:33,289 - memory_profile6_log - INFO -    315                                                 # ~ loading history

2018-04-29 16:03:33,289 - memory_profile6_log - INFO -    316                                                 """

2018-04-29 16:03:33,292 - memory_profile6_log - INFO -    317                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 16:03:33,292 - memory_profile6_log - INFO -    318                                                 """

2018-04-29 16:03:33,295 - memory_profile6_log - INFO -    319    430.2 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 16:03:33,296 - memory_profile6_log - INFO -    320                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 16:03:33,298 - memory_profile6_log - INFO -    321    430.2 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 16:03:33,299 - memory_profile6_log - INFO -    322                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 16:03:33,299 - memory_profile6_log - INFO -    323    430.8 MiB      4.0 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 16:03:33,301 - memory_profile6_log - INFO -    324                             

2018-04-29 16:03:33,302 - memory_profile6_log - INFO -    325    430.8 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 16:03:33,302 - memory_profile6_log - INFO -    326    475.4 MiB    251.8 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 16:03:33,305 - memory_profile6_log - INFO -    327                             

2018-04-29 16:03:33,306 - memory_profile6_log - INFO -    328                                                 # me = os.getpid()

2018-04-29 16:03:33,308 - memory_profile6_log - INFO -    329                                                 # kill_proc_tree(me)

2018-04-29 16:03:33,309 - memory_profile6_log - INFO -    330                             

2018-04-29 16:03:33,309 - memory_profile6_log - INFO -    331    475.4 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 16:03:33,309 - memory_profile6_log - INFO -    332    476.4 MiB     -0.9 MiB                       for m in h_frame:

2018-04-29 16:03:33,311 - memory_profile6_log - INFO -    333    476.4 MiB     -0.9 MiB                           if m is not None:

2018-04-29 16:03:33,312 - memory_profile6_log - INFO -    334    476.4 MiB     -0.9 MiB                               if len(m) > 0:

2018-04-29 16:03:33,312 - memory_profile6_log - INFO -    335    476.4 MiB      2.8 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 16:03:33,315 - memory_profile6_log - INFO -    336    445.9 MiB   -160.7 MiB                       del h_frame

2018-04-29 16:03:33,315 - memory_profile6_log - INFO -    337    445.9 MiB      0.0 MiB                       del lhistory

2018-04-29 16:03:33,318 - memory_profile6_log - INFO -    338                             

2018-04-29 16:03:33,319 - memory_profile6_log - INFO -    339    445.9 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 16:03:33,319 - memory_profile6_log - INFO -    340    445.9 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 16:03:33,319 - memory_profile6_log - INFO -    341                                     else: 

2018-04-29 16:03:33,321 - memory_profile6_log - INFO -    342                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 16:03:33,322 - memory_profile6_log - INFO -    343    445.9 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 16:03:33,322 - memory_profile6_log - INFO -    344    445.9 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 16:03:33,325 - memory_profile6_log - INFO -    345                             

2018-04-29 16:03:33,326 - memory_profile6_log - INFO -    346    445.9 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 16:03:33,328 - memory_profile6_log - INFO - 


2018-04-29 16:03:34,430 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 16:03:34,500 - memory_profile6_log - INFO -     p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0    0.000646        1844.325967        72            1854.325967  10960288  1627c8436d652-070435fe3262f2-15606626-38400-16...
1    0.000055        5563.716667        29            5573.716667  11911567  1622db65f7379-02d6dcab4ac38a-6b3d6c-2c880-1622...
2    0.000646        1844.325967        21            1854.325967  10960288  1625b257b8a6c-0032929dc13715-6e17495f-29b80-16...
3    0.000646        1844.325967        48            1854.325967  10960288  1613c2f0cd9da-032377262d89-467c6e3a-38400-1613...
4    0.000646        1844.325967         4            1854.325967  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
5    0.001473         269.647011         8             279.647011  22291119  1612dd4baba55-0e3d53e928bbdd-1a331550-38400-16...
6    0.000055        5563.716667        79            5573.716667  11911567  161a2573db38a-0834b483951503-7023786e-38400-16...
7    0.001473         269.647011         4             279.647011  22291119  1612d7e198144-0691c46a6-7a720f25-38400-1612d7e...
8    0.001473         269.647011         4             279.647011  22291119  1612d10d34c0-0f502e986b12-73261116-38400-1612d...
9    0.001473         269.647011        32             279.647011  22291119  161a333c831e-00b875a76c6048-28313a6c-38400-161...
10   0.000646        1844.325967        18            1854.325967  10960288  1612d67486c12-0240b99fbdedf6-3a0e0140-38400-16...
11   0.000055        5563.716667         4            5573.716667  11911567  1613746d53017-007a2bf6f22f28-70261016-38400-16...
12   0.000646        1844.325967        23            1854.325967  10960288  161abd353a066-0eb2535f986277-4e684743-38400-16...
13   0.000646        1844.325967        34            1854.325967  10960288  161b31ba3c0146-091f7870d34799-5f47693b-38400-1...
14   0.000055        5563.716667        18            5573.716667  11911567  1612dc48c2f6c-02034120b12382-5944693b-38400-16...
15   0.001473         269.647011        16             279.647011  22291119  1622eb5b2bc8d-08b70a147f1bcf-655a7c2d-38400-16...
16   0.000646        1844.325967        17            1854.325967  10960288  1610cb732313c-0c6252e2dec4ba-4323461-100200-16...
17   0.001473         269.647011         4             279.647011  22291119  1616efe1de7365-024418da2c9cdb-667e1364-3d10d-1...
18   0.000646        1844.325967         9            1854.325967  10960288  1613026b3ea26e-03f94b4cbf2756-7703033f-38400-1...
19   0.001473         269.647011        32             279.647011  22291119  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
2018-04-29 16:03:34,502 - memory_profile6_log - INFO - 

2018-04-29 16:03:34,584 - memory_profile6_log - INFO - size of big_frame_hist: 69.87 MB
2018-04-29 16:03:34,653 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 16:03:34,665 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 16:04:24,134 - memory_profile6_log - INFO - size of df: 58.30 MB
2018-04-29 16:04:24,135 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-29 16:04:24,216 - memory_profile6_log - INFO - size of current_frame: 58.30 MB
2018-04-29 16:04:24,217 - memory_profile6_log - INFO - loading time of: 484420 total genuine-current interest data ~ take 403.358s
2018-04-29 16:04:24,233 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 16:04:24,233 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 16:04:24,234 - memory_profile6_log - INFO - ================================================

2018-04-29 16:04:24,236 - memory_profile6_log - INFO -    348     86.8 MiB     86.8 MiB   @profile

2018-04-29 16:04:24,236 - memory_profile6_log - INFO -    349                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 16:04:24,239 - memory_profile6_log - INFO -    350     86.9 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 16:04:24,239 - memory_profile6_log - INFO -    351     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 16:04:24,240 - memory_profile6_log - INFO -    352                             

2018-04-29 16:04:24,242 - memory_profile6_log - INFO -    353                                 # ~~~ Begin collecting data ~~~

2018-04-29 16:04:24,243 - memory_profile6_log - INFO -    354     86.9 MiB      0.0 MiB       t0 = time.time()

2018-04-29 16:04:24,243 - memory_profile6_log - INFO -    355    444.7 MiB    357.8 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 16:04:24,246 - memory_profile6_log - INFO -    356    444.7 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 16:04:24,247 - memory_profile6_log - INFO -    357                                     logger.info("Training cannot be empty..")

2018-04-29 16:04:24,250 - memory_profile6_log - INFO -    358                                     return False

2018-04-29 16:04:24,250 - memory_profile6_log - INFO -    359    460.0 MiB     15.3 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 16:04:24,252 - memory_profile6_log - INFO -    360    460.3 MiB      0.3 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 16:04:24,253 - memory_profile6_log - INFO -    361    460.3 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 16:04:24,253 - memory_profile6_log - INFO -    362                             

2018-04-29 16:04:24,253 - memory_profile6_log - INFO -    363    466.4 MiB      6.1 MiB       big_frame = pd.concat(datalist)

2018-04-29 16:04:24,256 - memory_profile6_log - INFO -    364    466.4 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 16:04:24,256 - memory_profile6_log - INFO -    365    460.6 MiB     -5.8 MiB       del datalist

2018-04-29 16:04:24,257 - memory_profile6_log - INFO -    366                             

2018-04-29 16:04:24,257 - memory_profile6_log - INFO -    367    460.6 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 16:04:24,259 - memory_profile6_log - INFO -    368                             

2018-04-29 16:04:24,259 - memory_profile6_log - INFO -    369                                 # ~ get current news interest ~

2018-04-29 16:04:24,259 - memory_profile6_log - INFO -    370    460.6 MiB      0.0 MiB       if not cd:

2018-04-29 16:04:24,259 - memory_profile6_log - INFO -    371                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 16:04:24,259 - memory_profile6_log - INFO -    372                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 16:04:24,260 - memory_profile6_log - INFO -    373                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 16:04:24,260 - memory_profile6_log - INFO -    374                                 else:

2018-04-29 16:04:24,262 - memory_profile6_log - INFO -    375    460.6 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 16:04:24,262 - memory_profile6_log - INFO -    376    460.6 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 16:04:24,263 - memory_profile6_log - INFO -    377                             

2018-04-29 16:04:24,263 - memory_profile6_log - INFO -    378                                     # safe handling of query parameter

2018-04-29 16:04:24,263 - memory_profile6_log - INFO -    379                                     query_params = [

2018-04-29 16:04:24,263 - memory_profile6_log - INFO -    380    460.6 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 16:04:24,263 - memory_profile6_log - INFO -    381                                     ]

2018-04-29 16:04:24,269 - memory_profile6_log - INFO -    382                             

2018-04-29 16:04:24,269 - memory_profile6_log - INFO -    383    460.6 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-29 16:04:24,270 - memory_profile6_log - INFO -    384    516.1 MiB     55.5 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 16:04:24,270 - memory_profile6_log - INFO -    385    516.1 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 16:04:24,272 - memory_profile6_log - INFO -    386                             

2018-04-29 16:04:24,273 - memory_profile6_log - INFO -    387    516.1 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 16:04:24,273 - memory_profile6_log - INFO -    388    516.1 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 16:04:24,275 - memory_profile6_log - INFO -    389    516.1 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 16:04:24,275 - memory_profile6_log - INFO -    390    516.1 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 16:04:24,276 - memory_profile6_log - INFO -    391    516.1 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 16:04:24,276 - memory_profile6_log - INFO -    392    516.1 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 16:04:24,279 - memory_profile6_log - INFO -    393                             

2018-04-29 16:04:24,279 - memory_profile6_log - INFO -    394    516.1 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 16:04:24,280 - memory_profile6_log - INFO - 


2018-04-29 16:04:24,283 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 16:04:24,315 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 16:04:24,315 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-29 16:04:24,316 - memory_profile6_log - INFO - apply on: 253995 total history...)
2018-04-29 16:04:25,375 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 16:04:25,377 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 16:04:25,891 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-29 16:04:25,892 - memory_profile6_log - INFO - 

2018-04-29 16:04:25,892 - memory_profile6_log - INFO - len of current fitted models: 253995
2018-04-29 16:04:25,894 - memory_profile6_log - INFO - 

2018-04-29 16:04:25,894 - memory_profile6_log - INFO - len of history fitted models: 253995
2018-04-29 16:04:25,895 - memory_profile6_log - INFO - 

2018-04-29 16:11:05,427 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 16:11:05,430 - memory_profile6_log - INFO - date_generated: 
2018-04-29 16:11:05,430 - memory_profile6_log - INFO -  
2018-04-29 16:11:05,430 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 16:11:05,430 - memory_profile6_log - INFO - 

2018-04-29 16:11:05,431 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 16:11:05,433 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 16:11:05,433 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 16:11:05,575 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 16:11:05,578 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 16:12:14,964 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 16:12:14,966 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 16:12:15,009 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 16:12:15,009 - memory_profile6_log - INFO - Appending history data...
2018-04-29 16:12:15,012 - memory_profile6_log - INFO - processing batch-0
2018-04-29 16:12:15,013 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:12:15,055 - memory_profile6_log - INFO - call history data...
2018-04-29 16:12:43,990 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:12:44,632 - memory_profile6_log - INFO - processing batch-1
2018-04-29 16:12:44,634 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:12:44,641 - memory_profile6_log - INFO - call history data...
2018-04-29 16:13:12,963 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:13:13,635 - memory_profile6_log - INFO - processing batch-2
2018-04-29 16:13:13,637 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:13:13,644 - memory_profile6_log - INFO - call history data...
2018-04-29 16:13:42,684 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:13:43,338 - memory_profile6_log - INFO - processing batch-3
2018-04-29 16:13:43,341 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:13:43,348 - memory_profile6_log - INFO - call history data...
2018-04-29 16:14:14,796 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:14:15,448 - memory_profile6_log - INFO - processing batch-4
2018-04-29 16:14:15,450 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:14:15,457 - memory_profile6_log - INFO - call history data...
2018-04-29 16:14:47,148 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:14:47,799 - memory_profile6_log - INFO - Appending training data...
2018-04-29 16:14:47,801 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 16:14:47,802 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 16:14:47,803 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 16:14:47,805 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 16:14:47,805 - memory_profile6_log - INFO - ================================================

2018-04-29 16:14:47,806 - memory_profile6_log - INFO -    302     86.6 MiB     86.6 MiB   @profile

2018-04-29 16:14:47,808 - memory_profile6_log - INFO -    303                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 16:14:47,809 - memory_profile6_log - INFO -    304     86.6 MiB      0.0 MiB       bq_client = client

2018-04-29 16:14:47,812 - memory_profile6_log - INFO -    305     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 16:14:47,813 - memory_profile6_log - INFO -    306                             

2018-04-29 16:14:47,815 - memory_profile6_log - INFO -    307     86.6 MiB      0.0 MiB       datalist = []

2018-04-29 16:14:47,815 - memory_profile6_log - INFO -    308     86.6 MiB      0.0 MiB       datalist_hist = []

2018-04-29 16:14:47,816 - memory_profile6_log - INFO -    309                             

2018-04-29 16:14:47,818 - memory_profile6_log - INFO -    310     86.6 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 16:14:47,818 - memory_profile6_log - INFO -    311    351.7 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 16:14:47,822 - memory_profile6_log - INFO -    312    339.7 MiB    253.1 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 16:14:47,825 - memory_profile6_log - INFO -    313    339.7 MiB      0.0 MiB           if tframe is not None:

2018-04-29 16:14:47,825 - memory_profile6_log - INFO -    314    339.7 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 16:14:47,826 - memory_profile6_log - INFO -    315    347.3 MiB      7.6 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 16:14:47,828 - memory_profile6_log - INFO -    316    347.3 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 16:14:47,829 - memory_profile6_log - INFO -    317    347.3 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 16:14:47,832 - memory_profile6_log - INFO -    318    351.7 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 16:14:47,834 - memory_profile6_log - INFO -    319                                                 # ~ loading history

2018-04-29 16:14:47,835 - memory_profile6_log - INFO -    320                                                 """

2018-04-29 16:14:47,835 - memory_profile6_log - INFO -    321                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 16:14:47,836 - memory_profile6_log - INFO -    322                                                 """

2018-04-29 16:14:47,838 - memory_profile6_log - INFO -    323    351.4 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 16:14:47,838 - memory_profile6_log - INFO -    324                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 16:14:47,839 - memory_profile6_log - INFO -    325    351.4 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 16:14:47,842 - memory_profile6_log - INFO -    326    351.4 MiB      0.9 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 16:14:47,844 - memory_profile6_log - INFO -    327                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 16:14:47,845 - memory_profile6_log - INFO -    328                             

2018-04-29 16:14:47,845 - memory_profile6_log - INFO -    329    351.4 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 16:14:47,846 - memory_profile6_log - INFO -    330    351.6 MiB      2.8 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 16:14:47,848 - memory_profile6_log - INFO -    331                             

2018-04-29 16:14:47,848 - memory_profile6_log - INFO -    332                                                 # me = os.getpid()

2018-04-29 16:14:47,849 - memory_profile6_log - INFO -    333                                                 # kill_proc_tree(me)

2018-04-29 16:14:47,852 - memory_profile6_log - INFO -    334                             

2018-04-29 16:14:47,854 - memory_profile6_log - INFO -    335    351.6 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 16:14:47,855 - memory_profile6_log - INFO -    336    351.7 MiB      0.0 MiB                       for m in h_frame:

2018-04-29 16:14:47,857 - memory_profile6_log - INFO -    337    351.7 MiB      0.0 MiB                           if m is not None:

2018-04-29 16:14:47,858 - memory_profile6_log - INFO -    338    351.7 MiB      0.0 MiB                               if len(m) > 0:

2018-04-29 16:14:47,858 - memory_profile6_log - INFO -    339    351.7 MiB      0.7 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 16:14:47,859 - memory_profile6_log - INFO -    340    351.7 MiB      0.0 MiB                       del h_frame

2018-04-29 16:14:47,862 - memory_profile6_log - INFO -    341    351.7 MiB      0.0 MiB                       del lhistory

2018-04-29 16:14:47,865 - memory_profile6_log - INFO -    342                             

2018-04-29 16:14:47,865 - memory_profile6_log - INFO -    343    351.7 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 16:14:47,867 - memory_profile6_log - INFO -    344    351.7 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 16:14:47,868 - memory_profile6_log - INFO -    345                                     else: 

2018-04-29 16:14:47,869 - memory_profile6_log - INFO -    346                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 16:14:47,871 - memory_profile6_log - INFO -    347    351.7 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 16:14:47,874 - memory_profile6_log - INFO -    348    351.7 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 16:14:47,875 - memory_profile6_log - INFO -    349                             

2018-04-29 16:14:47,875 - memory_profile6_log - INFO -    350    351.7 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 16:14:47,877 - memory_profile6_log - INFO - 


2018-04-29 16:14:48,982 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 16:14:49,051 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0        NaN      333823.000000       256          333833.000000  10959239  161129f951435-0d660a18dbcd87-173a7640-100200-1...
1   0.000646        1844.325967        32            1854.325967  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
2   0.000646        1844.325967        16            1854.325967  10960288  1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...
3   0.000646        1844.325967        32            1854.325967  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
4   0.000646        1844.325967        32            1854.325967  10960288  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
5   0.000646        1844.325967        16            1854.325967  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
6   0.000646        1844.325967        16            1854.325967  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
0   0.000646        1844.325967         8            1854.325967  10960288  16169731ce618-00742c5e307396-5b4d6a36-38400-16...
1   0.000646        1844.325967        16            1854.325967  10960288  161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...
2   0.000646        1844.325967        16            1854.325967  10960288  1626fd160d9114-055d801ae6b39d-66265f05-38400-1...
3   0.000646        1844.325967        16            1854.325967  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
4   0.000646        1844.325967         8            1854.325967  10960288  1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...
5   0.000646        1844.325967         8            1854.325967  10960288  161e219cd891c8-0e882943705df4-15290f5f-38400-1...
0   0.000646        1844.325967         4            1854.325967  10960288  16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...
1   0.000646        1844.325967         4            1854.325967  10960288  1612d88e55ab8-09af9658f30a46-65117b23-38400-16...
2   0.000646        1844.325967         8            1854.325967  10960288  16149f0a1611-00a034775-63446032-38400-16149f0a...
3   0.000646        1844.325967         8            1854.325967  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
4   0.000646        1844.325967         4            1854.325967  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
5   0.000646        1844.325967         4            1854.325967  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
6   0.000646        1844.325967         4            1854.325967  10960288  16132f1045f30b-0613b72b33445e-5d640103-38400-1...
2018-04-29 16:14:49,052 - memory_profile6_log - INFO - 

2018-04-29 16:14:49,062 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 16:14:49,134 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 16:14:49,148 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 16:15:39,512 - memory_profile6_log - INFO - size of df: 58.30 MB
2018-04-29 16:15:39,513 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-29 16:15:39,575 - memory_profile6_log - INFO - size of current_frame: 58.30 MB
2018-04-29 16:15:39,575 - memory_profile6_log - INFO - loading time of: 484420 total genuine-current interest data ~ take 274.026s
2018-04-29 16:15:39,582 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 16:15:39,582 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 16:15:39,582 - memory_profile6_log - INFO - ================================================

2018-04-29 16:15:39,585 - memory_profile6_log - INFO -    352     86.5 MiB     86.5 MiB   @profile

2018-04-29 16:15:39,586 - memory_profile6_log - INFO -    353                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 16:15:39,588 - memory_profile6_log - INFO -    354     86.6 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 16:15:39,588 - memory_profile6_log - INFO -    355     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 16:15:39,589 - memory_profile6_log - INFO -    356                             

2018-04-29 16:15:39,589 - memory_profile6_log - INFO -    357                                 # ~~~ Begin collecting data ~~~

2018-04-29 16:15:39,591 - memory_profile6_log - INFO -    358     86.6 MiB      0.0 MiB       t0 = time.time()

2018-04-29 16:15:39,592 - memory_profile6_log - INFO -    359    351.7 MiB    265.1 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 16:15:39,592 - memory_profile6_log - INFO -    360    351.7 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 16:15:39,592 - memory_profile6_log - INFO -    361                                     logger.info("Training cannot be empty..")

2018-04-29 16:15:39,594 - memory_profile6_log - INFO -    362                                     return False

2018-04-29 16:15:39,595 - memory_profile6_log - INFO -    363    349.8 MiB     -1.9 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 16:15:39,598 - memory_profile6_log - INFO -    364    350.0 MiB      0.2 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 16:15:39,598 - memory_profile6_log - INFO -    365    350.0 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 16:15:39,598 - memory_profile6_log - INFO -    366                             

2018-04-29 16:15:39,599 - memory_profile6_log - INFO -    367    355.8 MiB      5.8 MiB       big_frame = pd.concat(datalist)

2018-04-29 16:15:39,601 - memory_profile6_log - INFO -    368    355.8 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 16:15:39,601 - memory_profile6_log - INFO -    369    350.0 MiB     -5.8 MiB       del datalist

2018-04-29 16:15:39,601 - memory_profile6_log - INFO -    370                             

2018-04-29 16:15:39,605 - memory_profile6_log - INFO -    371    350.0 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 16:15:39,605 - memory_profile6_log - INFO -    372                             

2018-04-29 16:15:39,608 - memory_profile6_log - INFO -    373                                 # ~ get current news interest ~

2018-04-29 16:15:39,608 - memory_profile6_log - INFO -    374    350.0 MiB      0.0 MiB       if not cd:

2018-04-29 16:15:39,609 - memory_profile6_log - INFO -    375                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 16:15:39,609 - memory_profile6_log - INFO -    376                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 16:15:39,611 - memory_profile6_log - INFO -    377                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 16:15:39,612 - memory_profile6_log - INFO -    378                                 else:

2018-04-29 16:15:39,612 - memory_profile6_log - INFO -    379    350.0 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 16:15:39,615 - memory_profile6_log - INFO -    380    350.0 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 16:15:39,618 - memory_profile6_log - INFO -    381                             

2018-04-29 16:15:39,619 - memory_profile6_log - INFO -    382                                     # safe handling of query parameter

2018-04-29 16:15:39,621 - memory_profile6_log - INFO -    383                                     query_params = [

2018-04-29 16:15:39,621 - memory_profile6_log - INFO -    384    350.0 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 16:15:39,621 - memory_profile6_log - INFO -    385                                     ]

2018-04-29 16:15:39,621 - memory_profile6_log - INFO -    386                             

2018-04-29 16:15:39,621 - memory_profile6_log - INFO -    387    350.0 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-29 16:15:39,622 - memory_profile6_log - INFO -    388    420.0 MiB     70.0 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 16:15:39,622 - memory_profile6_log - INFO -    389    420.0 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 16:15:39,622 - memory_profile6_log - INFO -    390                             

2018-04-29 16:15:39,625 - memory_profile6_log - INFO -    391    420.1 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 16:15:39,625 - memory_profile6_log - INFO -    392    420.1 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 16:15:39,628 - memory_profile6_log - INFO -    393    420.1 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 16:15:39,628 - memory_profile6_log - INFO -    394    420.1 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 16:15:39,630 - memory_profile6_log - INFO -    395    420.1 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 16:15:39,630 - memory_profile6_log - INFO -    396    420.1 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 16:15:39,631 - memory_profile6_log - INFO -    397                             

2018-04-29 16:15:39,631 - memory_profile6_log - INFO -    398    420.1 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 16:15:39,631 - memory_profile6_log - INFO - 


2018-04-29 16:15:39,634 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 16:15:39,665 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 16:15:39,667 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-29 16:15:39,668 - memory_profile6_log - INFO - apply on: 5000 total history...)
2018-04-29 16:15:40,720 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 16:15:40,720 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 16:15:40,940 - memory_profile6_log - INFO - model_fit dtypes:

2018-04-29 16:15:40,946 - memory_profile6_log - INFO - date              datetime64[ns, UTC]
user_id                        object
topic_id                       object
num_x                           int64
num_y                           int64
is_general                       bool
date_all_click                  int64
Ntotal                          int64
sigma_Nt                        int64
joinprob_ci                   float64
p_cat_ci                      float64
posterior                     float64
dtype: object
2018-04-29 16:15:40,947 - memory_profile6_log - INFO - 

2018-04-29 16:15:40,947 - memory_profile6_log - INFO - fitted_models_hist dtypes:

2018-04-29 16:15:40,953 - memory_profile6_log - INFO - p0_cat_ci                float64
pt_posterior_x_Nt        float64
sigma_Nt                   int64
smoothed_pt_posterior    float64
topic_id                  object
user_id                   object
dtype: object
2018-04-29 16:15:40,956 - memory_profile6_log - INFO - 

2018-04-29 16:15:41,191 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-29 16:15:41,193 - memory_profile6_log - INFO - 

2018-04-29 16:15:41,194 - memory_profile6_log - INFO - len of current fitted models: 253995
2018-04-29 16:15:41,194 - memory_profile6_log - INFO - 

2018-04-29 16:15:41,196 - memory_profile6_log - INFO - len of history fitted models: 5000
2018-04-29 16:15:41,197 - memory_profile6_log - INFO - 

2018-04-29 16:29:00,503 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 16:29:00,505 - memory_profile6_log - INFO - date_generated: 
2018-04-29 16:29:00,506 - memory_profile6_log - INFO -  
2018-04-29 16:29:00,506 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 16:29:00,506 - memory_profile6_log - INFO - 

2018-04-29 16:29:00,506 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 16:29:00,507 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 16:29:00,507 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 16:29:00,637 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 16:29:00,641 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 16:29:22,244 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 16:29:22,249 - memory_profile6_log - INFO - date_generated: 
2018-04-29 16:29:22,249 - memory_profile6_log - INFO -  
2018-04-29 16:29:22,250 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 16:29:22,250 - memory_profile6_log - INFO - 

2018-04-29 16:29:22,250 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 16:29:22,250 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 16:29:22,252 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 16:29:22,381 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 16:29:22,388 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 16:30:30,180 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 16:30:30,180 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 16:30:30,224 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 16:30:30,226 - memory_profile6_log - INFO - Appending history data...
2018-04-29 16:30:30,227 - memory_profile6_log - INFO - processing batch-0
2018-04-29 16:30:30,229 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:30:30,270 - memory_profile6_log - INFO - call history data...
2018-04-29 16:31:00,420 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:31:01,144 - memory_profile6_log - INFO - processing batch-1
2018-04-29 16:31:01,145 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:31:01,154 - memory_profile6_log - INFO - call history data...
2018-04-29 16:31:29,710 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:31:30,361 - memory_profile6_log - INFO - processing batch-2
2018-04-29 16:31:30,362 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:31:30,371 - memory_profile6_log - INFO - call history data...
2018-04-29 16:32:01,180 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:32:01,851 - memory_profile6_log - INFO - processing batch-3
2018-04-29 16:32:01,852 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:32:01,859 - memory_profile6_log - INFO - call history data...
2018-04-29 16:32:30,960 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:32:31,664 - memory_profile6_log - INFO - processing batch-4
2018-04-29 16:32:31,665 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:32:31,674 - memory_profile6_log - INFO - call history data...
2018-04-29 16:33:01,066 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:33:01,744 - memory_profile6_log - INFO - Appending training data...
2018-04-29 16:33:01,746 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 16:33:01,747 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 16:33:01,749 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 16:33:01,750 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 16:33:01,750 - memory_profile6_log - INFO - ================================================

2018-04-29 16:33:01,753 - memory_profile6_log - INFO -    302     86.9 MiB     86.9 MiB   @profile

2018-04-29 16:33:01,753 - memory_profile6_log - INFO -    303                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 16:33:01,756 - memory_profile6_log - INFO -    304     86.9 MiB      0.0 MiB       bq_client = client

2018-04-29 16:33:01,757 - memory_profile6_log - INFO -    305     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 16:33:01,759 - memory_profile6_log - INFO -    306                             

2018-04-29 16:33:01,759 - memory_profile6_log - INFO -    307     86.9 MiB      0.0 MiB       datalist = []

2018-04-29 16:33:01,762 - memory_profile6_log - INFO -    308     86.9 MiB      0.0 MiB       datalist_hist = []

2018-04-29 16:33:01,762 - memory_profile6_log - INFO -    309                             

2018-04-29 16:33:01,763 - memory_profile6_log - INFO -    310     86.9 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 16:33:01,763 - memory_profile6_log - INFO -    311    349.9 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 16:33:01,766 - memory_profile6_log - INFO -    312    338.0 MiB    251.0 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 16:33:01,767 - memory_profile6_log - INFO -    313    338.0 MiB      0.0 MiB           if tframe is not None:

2018-04-29 16:33:01,769 - memory_profile6_log - INFO -    314    338.0 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 16:33:01,769 - memory_profile6_log - INFO -    315    346.2 MiB      8.3 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 16:33:01,769 - memory_profile6_log - INFO -    316    346.2 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 16:33:01,770 - memory_profile6_log - INFO -    317    346.2 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 16:33:01,772 - memory_profile6_log - INFO -    318    349.8 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 16:33:01,773 - memory_profile6_log - INFO -    319                                                 # ~ loading history

2018-04-29 16:33:01,773 - memory_profile6_log - INFO -    320                                                 """

2018-04-29 16:33:01,775 - memory_profile6_log - INFO -    321                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 16:33:01,776 - memory_profile6_log - INFO -    322                                                 """

2018-04-29 16:33:01,779 - memory_profile6_log - INFO -    323    349.6 MiB      0.1 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 16:33:01,779 - memory_profile6_log - INFO -    324                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 16:33:01,779 - memory_profile6_log - INFO -    325    349.6 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 16:33:01,780 - memory_profile6_log - INFO -    326    349.6 MiB      0.5 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 16:33:01,782 - memory_profile6_log - INFO -    327                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 16:33:01,782 - memory_profile6_log - INFO -    328                             

2018-04-29 16:33:01,783 - memory_profile6_log - INFO -    329    349.6 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 16:33:01,786 - memory_profile6_log - INFO -    330    349.7 MiB      2.3 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 16:33:01,786 - memory_profile6_log - INFO -    331                             

2018-04-29 16:33:01,788 - memory_profile6_log - INFO -    332                                                 # me = os.getpid()

2018-04-29 16:33:01,789 - memory_profile6_log - INFO -    333                                                 # kill_proc_tree(me)

2018-04-29 16:33:01,789 - memory_profile6_log - INFO -    334                             

2018-04-29 16:33:01,790 - memory_profile6_log - INFO -    335    349.7 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 16:33:01,790 - memory_profile6_log - INFO -    336    349.8 MiB      0.0 MiB                       for m in h_frame:

2018-04-29 16:33:01,792 - memory_profile6_log - INFO -    337    349.8 MiB      0.0 MiB                           if m is not None:

2018-04-29 16:33:01,792 - memory_profile6_log - INFO -    338    349.8 MiB      0.0 MiB                               if len(m) > 0:

2018-04-29 16:33:01,792 - memory_profile6_log - INFO -    339    349.8 MiB      0.6 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 16:33:01,793 - memory_profile6_log - INFO -    340    349.8 MiB      0.0 MiB                       del h_frame

2018-04-29 16:33:01,795 - memory_profile6_log - INFO -    341    349.8 MiB      0.0 MiB                       del lhistory

2018-04-29 16:33:01,798 - memory_profile6_log - INFO -    342                             

2018-04-29 16:33:01,799 - memory_profile6_log - INFO -    343    349.9 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 16:33:01,801 - memory_profile6_log - INFO -    344    349.9 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 16:33:01,802 - memory_profile6_log - INFO -    345                                     else: 

2018-04-29 16:33:01,803 - memory_profile6_log - INFO -    346                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 16:33:01,803 - memory_profile6_log - INFO -    347    349.9 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 16:33:01,805 - memory_profile6_log - INFO -    348    349.9 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 16:33:01,806 - memory_profile6_log - INFO -    349                             

2018-04-29 16:33:01,809 - memory_profile6_log - INFO -    350    349.9 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 16:33:01,809 - memory_profile6_log - INFO - 


2018-04-29 16:33:02,957 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 16:33:03,026 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0   0.000646        1844.325967        16            1854.325967  10960288  1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...
1   0.000646        1844.325967        32            1854.325967  10960288  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
2   0.000646        1844.325967        16            1854.325967  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
3   0.000646        1844.325967        32            1854.325967  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
4        NaN      333823.000000       256          333833.000000  10959239  161129f951435-0d660a18dbcd87-173a7640-100200-1...
5   0.000646        1844.325967        16            1854.325967  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
6   0.000646        1844.325967        32            1854.325967  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
0   0.000646        1844.325967         8            1854.325967  10960288  1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...
1   0.000646        1844.325967         8            1854.325967  10960288  161e219cd891c8-0e882943705df4-15290f5f-38400-1...
2   0.000646        1844.325967        16            1854.325967  10960288  1626fd160d9114-055d801ae6b39d-66265f05-38400-1...
3   0.000646        1844.325967        16            1854.325967  10960288  161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...
4   0.000646        1844.325967         8            1854.325967  10960288  16169731ce618-00742c5e307396-5b4d6a36-38400-16...
5   0.000646        1844.325967        16            1854.325967  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
0   0.000646        1844.325967         4            1854.325967  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
1   0.000646        1844.325967         4            1854.325967  10960288  1612d88e55ab8-09af9658f30a46-65117b23-38400-16...
2   0.000646        1844.325967         4            1854.325967  10960288  16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...
3   0.000646        1844.325967         4            1854.325967  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
4   0.000646        1844.325967         8            1854.325967  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
5   0.000646        1844.325967         8            1854.325967  10960288  16149f0a1611-00a034775-63446032-38400-16149f0a...
6   0.000646        1844.325967         4            1854.325967  10960288  16132f1045f30b-0613b72b33445e-5d640103-38400-1...
2018-04-29 16:33:03,026 - memory_profile6_log - INFO - 

2018-04-29 16:33:03,036 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 16:33:03,105 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 16:33:03,124 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 16:34:06,776 - memory_profile6_log - INFO - size of df: 58.30 MB
2018-04-29 16:34:06,776 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-29 16:34:06,841 - memory_profile6_log - INFO - size of current_frame: 58.30 MB
2018-04-29 16:34:06,842 - memory_profile6_log - INFO - loading time of: 484420 total genuine-current interest data ~ take 284.482s
2018-04-29 16:34:06,848 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 16:34:06,849 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 16:34:06,851 - memory_profile6_log - INFO - ================================================

2018-04-29 16:34:06,851 - memory_profile6_log - INFO -    352     86.8 MiB     86.8 MiB   @profile

2018-04-29 16:34:06,854 - memory_profile6_log - INFO -    353                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 16:34:06,855 - memory_profile6_log - INFO -    354     86.9 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 16:34:06,855 - memory_profile6_log - INFO -    355     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 16:34:06,855 - memory_profile6_log - INFO -    356                             

2018-04-29 16:34:06,857 - memory_profile6_log - INFO -    357                                 # ~~~ Begin collecting data ~~~

2018-04-29 16:34:06,858 - memory_profile6_log - INFO -    358     86.9 MiB      0.0 MiB       t0 = time.time()

2018-04-29 16:34:06,858 - memory_profile6_log - INFO -    359    349.9 MiB    262.9 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 16:34:06,858 - memory_profile6_log - INFO -    360    349.9 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 16:34:06,859 - memory_profile6_log - INFO -    361                                     logger.info("Training cannot be empty..")

2018-04-29 16:34:06,861 - memory_profile6_log - INFO -    362                                     return False

2018-04-29 16:34:06,865 - memory_profile6_log - INFO -    363    349.0 MiB     -0.8 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 16:34:06,865 - memory_profile6_log - INFO -    364    349.1 MiB      0.1 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 16:34:06,867 - memory_profile6_log - INFO -    365    349.1 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 16:34:06,868 - memory_profile6_log - INFO -    366                             

2018-04-29 16:34:06,868 - memory_profile6_log - INFO -    367    354.9 MiB      5.8 MiB       big_frame = pd.concat(datalist)

2018-04-29 16:34:06,868 - memory_profile6_log - INFO -    368    354.9 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 16:34:06,868 - memory_profile6_log - INFO -    369    349.1 MiB     -5.8 MiB       del datalist

2018-04-29 16:34:06,868 - memory_profile6_log - INFO -    370                             

2018-04-29 16:34:06,871 - memory_profile6_log - INFO -    371    349.1 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 16:34:06,871 - memory_profile6_log - INFO -    372                             

2018-04-29 16:34:06,872 - memory_profile6_log - INFO -    373                                 # ~ get current news interest ~

2018-04-29 16:34:06,874 - memory_profile6_log - INFO -    374    349.1 MiB      0.0 MiB       if not cd:

2018-04-29 16:34:06,875 - memory_profile6_log - INFO -    375                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 16:34:06,875 - memory_profile6_log - INFO -    376                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 16:34:06,875 - memory_profile6_log - INFO -    377                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 16:34:06,875 - memory_profile6_log - INFO -    378                                 else:

2018-04-29 16:34:06,877 - memory_profile6_log - INFO -    379    349.1 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 16:34:06,878 - memory_profile6_log - INFO -    380    349.1 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 16:34:06,878 - memory_profile6_log - INFO -    381                             

2018-04-29 16:34:06,878 - memory_profile6_log - INFO -    382                                     # safe handling of query parameter

2018-04-29 16:34:06,878 - memory_profile6_log - INFO -    383                                     query_params = [

2018-04-29 16:34:06,881 - memory_profile6_log - INFO -    384    349.1 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 16:34:06,884 - memory_profile6_log - INFO -    385                                     ]

2018-04-29 16:34:06,884 - memory_profile6_log - INFO -    386                             

2018-04-29 16:34:06,884 - memory_profile6_log - INFO -    387    349.1 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-29 16:34:06,885 - memory_profile6_log - INFO -    388    420.1 MiB     70.9 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 16:34:06,887 - memory_profile6_log - INFO -    389    420.1 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 16:34:06,888 - memory_profile6_log - INFO -    390                             

2018-04-29 16:34:06,888 - memory_profile6_log - INFO -    391    420.1 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 16:34:06,888 - memory_profile6_log - INFO -    392    420.1 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 16:34:06,890 - memory_profile6_log - INFO -    393    420.1 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 16:34:06,891 - memory_profile6_log - INFO -    394    420.1 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 16:34:06,892 - memory_profile6_log - INFO -    395    420.1 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 16:34:06,894 - memory_profile6_log - INFO -    396    420.1 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 16:34:06,894 - memory_profile6_log - INFO -    397                             

2018-04-29 16:34:06,895 - memory_profile6_log - INFO -    398    420.1 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 16:34:06,897 - memory_profile6_log - INFO - 


2018-04-29 16:34:06,900 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 16:34:06,931 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 16:34:06,934 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-29 16:34:06,934 - memory_profile6_log - INFO - apply on: 5000 total history...)
2018-04-29 16:34:08,019 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 16:34:08,020 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 16:34:08,250 - memory_profile6_log - INFO - model_fit dtypes:

2018-04-29 16:34:08,256 - memory_profile6_log - INFO - date              datetime64[ns, UTC]
user_id                        object
topic_id                       object
num_x                           int64
num_y                           int64
is_general                       bool
date_all_click                  int64
Ntotal                          int64
sigma_Nt                        int64
joinprob_ci                   float64
p_cat_ci                      float64
posterior                     float64
dtype: object
2018-04-29 16:34:08,259 - memory_profile6_log - INFO - 

2018-04-29 16:34:08,259 - memory_profile6_log - INFO - fitted_models_hist dtypes:

2018-04-29 16:34:08,266 - memory_profile6_log - INFO - p0_cat_ci                float64
pt_posterior_x_Nt        float64
sigma_Nt                   int64
smoothed_pt_posterior    float64
topic_id                  object
user_id                   object
dtype: object
2018-04-29 16:34:08,266 - memory_profile6_log - INFO - 

2018-04-29 16:34:08,841 - memory_profile6_log - INFO - Len of fitted_models on main class: 253995
2018-04-29 16:34:08,842 - memory_profile6_log - INFO - 

2018-04-29 16:34:08,844 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-29 16:34:08,845 - memory_profile6_log - INFO - 

2018-04-29 16:34:08,845 - memory_profile6_log - INFO - Fitted models before concat:

2018-04-29 16:34:08,871 - memory_profile6_log - INFO -                                 user_id    topic_id  pt_posterior_x_Nt  p0_cat_ci  smoothed_pt_posterior
0  107e02cd-54af-40fd-a602-e6105250ae8c  1046474724        1902.125356   0.000090            1912.125356
1  107e02cd-54af-40fd-a602-e6105250ae8c  1060896301        1346.060484   0.000065            1356.060484
2  107e02cd-54af-40fd-a602-e6105250ae8c   188400148         144.387111   0.036200             154.387111
3  107e02cd-54af-40fd-a602-e6105250ae8c    22553543          71.961844   0.107319              81.961844
4  107e02cd-54af-40fd-a602-e6105250ae8c    22596701         122.085700   0.040442             132.085700
2018-04-29 16:34:08,871 - memory_profile6_log - INFO - 

2018-04-29 16:34:08,872 - memory_profile6_log - INFO - len of current fitted models: 253995
2018-04-29 16:34:08,874 - memory_profile6_log - INFO - 

2018-04-29 16:34:08,875 - memory_profile6_log - INFO - len of history fitted models: 5000
2018-04-29 16:34:08,875 - memory_profile6_log - INFO - 

2018-04-29 16:34:08,918 - memory_profile6_log - INFO - len of fitted models after concat: 258995
2018-04-29 16:34:08,920 - memory_profile6_log - INFO - 

2018-04-29 16:34:08,921 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-29 16:34:08,921 - memory_profile6_log - INFO - 

2018-04-29 16:39:51,529 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 16:39:51,532 - memory_profile6_log - INFO - date_generated: 
2018-04-29 16:39:51,532 - memory_profile6_log - INFO -  
2018-04-29 16:39:51,532 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 16:39:51,532 - memory_profile6_log - INFO - 

2018-04-29 16:39:51,532 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 16:39:51,533 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 16:39:51,535 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 16:39:51,654 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 16:39:51,657 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 16:40:59,770 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 16:40:59,772 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 16:40:59,815 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 16:40:59,815 - memory_profile6_log - INFO - Appending history data...
2018-04-29 16:40:59,816 - memory_profile6_log - INFO - processing batch-0
2018-04-29 16:40:59,819 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:40:59,864 - memory_profile6_log - INFO - call history data...
2018-04-29 16:41:28,305 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:41:30,196 - memory_profile6_log - INFO - processing batch-1
2018-04-29 16:41:30,197 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:41:30,216 - memory_profile6_log - INFO - call history data...
2018-04-29 16:42:02,934 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:42:04,561 - memory_profile6_log - INFO - processing batch-2
2018-04-29 16:42:04,562 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:42:04,582 - memory_profile6_log - INFO - call history data...
2018-04-29 16:42:37,641 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:42:39,323 - memory_profile6_log - INFO - processing batch-3
2018-04-29 16:42:39,326 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:42:39,345 - memory_profile6_log - INFO - call history data...
2018-04-29 16:43:12,641 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:43:14,309 - memory_profile6_log - INFO - processing batch-4
2018-04-29 16:43:14,312 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:43:14,332 - memory_profile6_log - INFO - call history data...
2018-04-29 16:43:47,201 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:43:48,819 - memory_profile6_log - INFO - Appending training data...
2018-04-29 16:43:48,821 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 16:43:48,823 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 16:43:48,826 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 16:43:48,828 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 16:43:48,832 - memory_profile6_log - INFO - ================================================

2018-04-29 16:43:48,835 - memory_profile6_log - INFO -    310     87.0 MiB     87.0 MiB   @profile

2018-04-29 16:43:48,838 - memory_profile6_log - INFO -    311                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 16:43:48,844 - memory_profile6_log - INFO -    312     87.0 MiB      0.0 MiB       bq_client = client

2018-04-29 16:43:48,846 - memory_profile6_log - INFO -    313     87.0 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 16:43:48,848 - memory_profile6_log - INFO -    314                             

2018-04-29 16:43:48,851 - memory_profile6_log - INFO -    315     87.0 MiB      0.0 MiB       datalist = []

2018-04-29 16:43:48,855 - memory_profile6_log - INFO -    316     87.0 MiB      0.0 MiB       datalist_hist = []

2018-04-29 16:43:48,858 - memory_profile6_log - INFO -    317                             

2018-04-29 16:43:48,859 - memory_profile6_log - INFO -    318     87.0 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 16:43:48,861 - memory_profile6_log - INFO -    319    349.5 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 16:43:48,867 - memory_profile6_log - INFO -    320    338.2 MiB    251.1 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 16:43:48,868 - memory_profile6_log - INFO -    321    338.2 MiB      0.0 MiB           if tframe is not None:

2018-04-29 16:43:48,871 - memory_profile6_log - INFO -    322    338.2 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 16:43:48,872 - memory_profile6_log - INFO -    323    346.4 MiB      8.3 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 16:43:48,878 - memory_profile6_log - INFO -    324    346.4 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 16:43:48,880 - memory_profile6_log - INFO -    325    346.4 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 16:43:48,882 - memory_profile6_log - INFO -    326    349.5 MiB     -0.3 MiB                   for ix in range(len(X_split)):

2018-04-29 16:43:48,884 - memory_profile6_log - INFO -    327                                                 # ~ loading history

2018-04-29 16:43:48,888 - memory_profile6_log - INFO -    328                                                 """

2018-04-29 16:43:48,891 - memory_profile6_log - INFO -    329                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 16:43:48,892 - memory_profile6_log - INFO -    330                                                 """

2018-04-29 16:43:48,894 - memory_profile6_log - INFO -    331    349.4 MiB     -0.2 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 16:43:48,898 - memory_profile6_log - INFO -    332                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 16:43:48,903 - memory_profile6_log - INFO -    333    349.4 MiB     -0.3 MiB                       logger.info("creating list history data...")

2018-04-29 16:43:48,904 - memory_profile6_log - INFO -    334    349.4 MiB      0.2 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 16:43:48,911 - memory_profile6_log - INFO -    335                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 16:43:48,917 - memory_profile6_log - INFO -    336                             

2018-04-29 16:43:48,921 - memory_profile6_log - INFO -    337    349.4 MiB     -0.3 MiB                       logger.info("call history data...")

2018-04-29 16:43:48,923 - memory_profile6_log - INFO -    338    349.5 MiB      1.3 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 16:43:48,926 - memory_profile6_log - INFO -    339                             

2018-04-29 16:43:48,930 - memory_profile6_log - INFO -    340                                                 # me = os.getpid()

2018-04-29 16:43:48,934 - memory_profile6_log - INFO -    341                                                 # kill_proc_tree(me)

2018-04-29 16:43:48,937 - memory_profile6_log - INFO -    342                             

2018-04-29 16:43:48,941 - memory_profile6_log - INFO -    343    349.5 MiB     -0.2 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 16:43:48,944 - memory_profile6_log - INFO -    344    349.5 MiB    -83.3 MiB                       for m in h_frame:

2018-04-29 16:43:48,946 - memory_profile6_log - INFO -    345    349.5 MiB    -83.4 MiB                           if m is not None:

2018-04-29 16:43:48,947 - memory_profile6_log - INFO -    346    349.5 MiB    -83.4 MiB                               if len(m) > 0:

2018-04-29 16:43:48,953 - memory_profile6_log - INFO -    347    349.5 MiB    -82.6 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 16:43:48,954 - memory_profile6_log - INFO -    348    349.5 MiB     -0.3 MiB                       del h_frame

2018-04-29 16:43:48,957 - memory_profile6_log - INFO -    349    349.5 MiB     -0.3 MiB                       del lhistory

2018-04-29 16:43:48,959 - memory_profile6_log - INFO -    350                             

2018-04-29 16:43:48,964 - memory_profile6_log - INFO -    351    349.5 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 16:43:48,967 - memory_profile6_log - INFO -    352    349.5 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 16:43:48,970 - memory_profile6_log - INFO -    353                                     else: 

2018-04-29 16:43:48,971 - memory_profile6_log - INFO -    354                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 16:43:48,976 - memory_profile6_log - INFO -    355    349.5 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 16:43:48,979 - memory_profile6_log - INFO -    356    349.5 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 16:43:48,982 - memory_profile6_log - INFO -    357                             

2018-04-29 16:43:48,983 - memory_profile6_log - INFO -    358    349.5 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 16:43:48,987 - memory_profile6_log - INFO - 


2018-04-29 16:43:51,846 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 16:43:52,006 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0   0.000646        1844.325967        16            1854.325967  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
1   0.000646        1844.325967        16            1854.325967  10960288  1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...
2   0.000646        1844.325967        16            1854.325967  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
3   0.000646        1844.325967        32            1854.325967  10960288  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
4   0.000646        1844.325967        32            1854.325967  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
5        NaN      333823.000000       256          333833.000000  10959239  161129f951435-0d660a18dbcd87-173a7640-100200-1...
6   0.000646        1844.325967        32            1854.325967  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
0   0.000646        1844.325967         8            1854.325967  10960288  1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...
1   0.000646        1844.325967        16            1854.325967  10960288  161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...
2   0.000646        1844.325967         8            1854.325967  10960288  161e219cd891c8-0e882943705df4-15290f5f-38400-1...
3   0.000646        1844.325967         8            1854.325967  10960288  16169731ce618-00742c5e307396-5b4d6a36-38400-16...
4   0.000646        1844.325967        16            1854.325967  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
5   0.000646        1844.325967        16            1854.325967  10960288  1626fd160d9114-055d801ae6b39d-66265f05-38400-1...
0   0.000646        1844.325967         4            1854.325967  10960288  16132f1045f30b-0613b72b33445e-5d640103-38400-1...
1   0.000646        1844.325967         8            1854.325967  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
2   0.000646        1844.325967         4            1854.325967  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
3   0.000646        1844.325967         4            1854.325967  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
4   0.000646        1844.325967         4            1854.325967  10960288  16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...
5   0.000646        1844.325967         4            1854.325967  10960288  1612d88e55ab8-09af9658f30a46-65117b23-38400-16...
6   0.000646        1844.325967         8            1854.325967  10960288  16149f0a1611-00a034775-63446032-38400-16149f0a...
2018-04-29 16:43:52,009 - memory_profile6_log - INFO - 

2018-04-29 16:43:52,045 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 16:43:52,226 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 16:43:52,250 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 16:45:25,917 - memory_profile6_log - INFO - size of df: 58.30 MB
2018-04-29 16:45:25,921 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-29 16:45:26,062 - memory_profile6_log - INFO - size of current_frame: 58.30 MB
2018-04-29 16:45:26,065 - memory_profile6_log - INFO - loading time of: 484420 total genuine-current interest data ~ take 334.430s
2018-04-29 16:45:26,075 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 16:45:26,076 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 16:45:26,078 - memory_profile6_log - INFO - ================================================

2018-04-29 16:45:26,078 - memory_profile6_log - INFO -    360     86.9 MiB     86.9 MiB   @profile

2018-04-29 16:45:26,078 - memory_profile6_log - INFO -    361                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 16:45:26,079 - memory_profile6_log - INFO -    362     87.0 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 16:45:26,081 - memory_profile6_log - INFO -    363     87.0 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 16:45:26,086 - memory_profile6_log - INFO -    364                             

2018-04-29 16:45:26,091 - memory_profile6_log - INFO -    365                                 # ~~~ Begin collecting data ~~~

2018-04-29 16:45:26,092 - memory_profile6_log - INFO -    366     87.0 MiB      0.0 MiB       t0 = time.time()

2018-04-29 16:45:26,092 - memory_profile6_log - INFO -    367    349.5 MiB    262.5 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 16:45:26,096 - memory_profile6_log - INFO -    368    349.5 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 16:45:26,098 - memory_profile6_log - INFO -    369                                     logger.info("Training cannot be empty..")

2018-04-29 16:45:26,108 - memory_profile6_log - INFO -    370                                     return False

2018-04-29 16:45:26,111 - memory_profile6_log - INFO -    371    348.6 MiB     -1.0 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 16:45:26,114 - memory_profile6_log - INFO -    372    348.6 MiB      0.1 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 16:45:26,125 - memory_profile6_log - INFO -    373    348.6 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 16:45:26,140 - memory_profile6_log - INFO -    374                             

2018-04-29 16:45:26,141 - memory_profile6_log - INFO -    375    354.7 MiB      6.1 MiB       big_frame = pd.concat(datalist)

2018-04-29 16:45:26,154 - memory_profile6_log - INFO -    376    354.7 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 16:45:26,157 - memory_profile6_log - INFO -    377    348.9 MiB     -5.8 MiB       del datalist

2018-04-29 16:45:26,160 - memory_profile6_log - INFO -    378                             

2018-04-29 16:45:26,164 - memory_profile6_log - INFO -    379    348.9 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 16:45:26,167 - memory_profile6_log - INFO -    380                             

2018-04-29 16:45:26,167 - memory_profile6_log - INFO -    381                                 # ~ get current news interest ~

2018-04-29 16:45:26,170 - memory_profile6_log - INFO -    382    348.9 MiB      0.0 MiB       if not cd:

2018-04-29 16:45:26,171 - memory_profile6_log - INFO -    383                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 16:45:26,177 - memory_profile6_log - INFO -    384                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 16:45:26,180 - memory_profile6_log - INFO -    385                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 16:45:26,181 - memory_profile6_log - INFO -    386                                 else:

2018-04-29 16:45:26,184 - memory_profile6_log - INFO -    387    348.9 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 16:45:26,187 - memory_profile6_log - INFO -    388    348.9 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 16:45:26,190 - memory_profile6_log - INFO -    389                             

2018-04-29 16:45:26,190 - memory_profile6_log - INFO -    390                                     # safe handling of query parameter

2018-04-29 16:45:26,193 - memory_profile6_log - INFO -    391                                     query_params = [

2018-04-29 16:45:26,194 - memory_profile6_log - INFO -    392    348.9 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 16:45:26,194 - memory_profile6_log - INFO -    393                                     ]

2018-04-29 16:45:26,196 - memory_profile6_log - INFO -    394                             

2018-04-29 16:45:26,201 - memory_profile6_log - INFO -    395    348.9 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-29 16:45:26,203 - memory_profile6_log - INFO -    396    419.7 MiB     70.8 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 16:45:26,204 - memory_profile6_log - INFO -    397    419.7 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 16:45:26,207 - memory_profile6_log - INFO -    398                             

2018-04-29 16:45:26,209 - memory_profile6_log - INFO -    399    419.7 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 16:45:26,214 - memory_profile6_log - INFO -    400    419.7 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 16:45:26,216 - memory_profile6_log - INFO -    401    419.7 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 16:45:26,217 - memory_profile6_log - INFO -    402    419.7 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 16:45:26,217 - memory_profile6_log - INFO -    403    419.7 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 16:45:26,220 - memory_profile6_log - INFO -    404    419.7 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 16:45:26,220 - memory_profile6_log - INFO -    405                             

2018-04-29 16:45:26,223 - memory_profile6_log - INFO -    406    419.7 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 16:45:26,227 - memory_profile6_log - INFO - 


2018-04-29 16:45:26,237 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 16:45:26,299 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 16:45:26,302 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-29 16:45:26,303 - memory_profile6_log - INFO - apply on: 5000 total history...)
2018-04-29 16:48:57,874 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 16:48:57,881 - memory_profile6_log - INFO - date_generated: 
2018-04-29 16:48:57,881 - memory_profile6_log - INFO -  
2018-04-29 16:48:57,881 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 16:48:57,884 - memory_profile6_log - INFO - 

2018-04-29 16:48:57,884 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 16:48:57,885 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 16:48:57,887 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 16:48:58,243 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 16:48:58,252 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 16:51:22,716 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 16:51:22,719 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 16:51:22,799 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 16:51:22,802 - memory_profile6_log - INFO - Appending history data...
2018-04-29 16:51:22,805 - memory_profile6_log - INFO - processing batch-0
2018-04-29 16:51:22,806 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:51:22,900 - memory_profile6_log - INFO - call history data...
2018-04-29 16:52:00,471 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:52:02,181 - memory_profile6_log - INFO - processing batch-1
2018-04-29 16:52:02,184 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:52:02,203 - memory_profile6_log - INFO - call history data...
2018-04-29 16:52:34,967 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:52:36,621 - memory_profile6_log - INFO - processing batch-2
2018-04-29 16:52:36,624 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:52:36,641 - memory_profile6_log - INFO - call history data...
2018-04-29 16:53:07,974 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:53:09,618 - memory_profile6_log - INFO - processing batch-3
2018-04-29 16:53:09,619 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:53:09,638 - memory_profile6_log - INFO - call history data...
2018-04-29 16:53:41,885 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:53:43,536 - memory_profile6_log - INFO - processing batch-4
2018-04-29 16:53:43,539 - memory_profile6_log - INFO - creating list history data...
2018-04-29 16:53:43,562 - memory_profile6_log - INFO - call history data...
2018-04-29 16:54:13,644 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 16:54:14,295 - memory_profile6_log - INFO - Appending training data...
2018-04-29 16:54:14,296 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 16:54:14,299 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 16:54:14,299 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 16:54:14,301 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 16:54:14,302 - memory_profile6_log - INFO - ================================================

2018-04-29 16:54:14,302 - memory_profile6_log - INFO -    311     86.5 MiB     86.5 MiB   @profile

2018-04-29 16:54:14,303 - memory_profile6_log - INFO -    312                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 16:54:14,306 - memory_profile6_log - INFO -    313     86.5 MiB      0.0 MiB       bq_client = client

2018-04-29 16:54:14,306 - memory_profile6_log - INFO -    314     86.5 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 16:54:14,309 - memory_profile6_log - INFO -    315                             

2018-04-29 16:54:14,309 - memory_profile6_log - INFO -    316     86.5 MiB      0.0 MiB       datalist = []

2018-04-29 16:54:14,311 - memory_profile6_log - INFO -    317     86.5 MiB      0.0 MiB       datalist_hist = []

2018-04-29 16:54:14,312 - memory_profile6_log - INFO -    318                             

2018-04-29 16:54:14,312 - memory_profile6_log - INFO -    319     86.5 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 16:54:14,312 - memory_profile6_log - INFO -    320    350.5 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 16:54:14,313 - memory_profile6_log - INFO -    321    338.8 MiB    252.3 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 16:54:14,316 - memory_profile6_log - INFO -    322    338.8 MiB      0.0 MiB           if tframe is not None:

2018-04-29 16:54:14,316 - memory_profile6_log - INFO -    323    338.8 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 16:54:14,318 - memory_profile6_log - INFO -    324    346.3 MiB      7.6 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 16:54:14,319 - memory_profile6_log - INFO -    325    346.3 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 16:54:14,319 - memory_profile6_log - INFO -    326    346.3 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 16:54:14,321 - memory_profile6_log - INFO -    327    350.4 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 16:54:14,321 - memory_profile6_log - INFO -    328                                                 # ~ loading history

2018-04-29 16:54:14,322 - memory_profile6_log - INFO -    329                                                 """

2018-04-29 16:54:14,322 - memory_profile6_log - INFO -    330                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 16:54:14,323 - memory_profile6_log - INFO -    331                                                 """

2018-04-29 16:54:14,325 - memory_profile6_log - INFO -    332    350.2 MiB      0.1 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 16:54:14,326 - memory_profile6_log - INFO -    333                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 16:54:14,328 - memory_profile6_log - INFO -    334    350.2 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 16:54:14,328 - memory_profile6_log - INFO -    335    350.2 MiB      1.1 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 16:54:14,329 - memory_profile6_log - INFO -    336                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 16:54:14,329 - memory_profile6_log - INFO -    337                             

2018-04-29 16:54:14,332 - memory_profile6_log - INFO -    338    350.2 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 16:54:14,332 - memory_profile6_log - INFO -    339    350.4 MiB      2.2 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 16:54:14,332 - memory_profile6_log - INFO -    340                             

2018-04-29 16:54:14,334 - memory_profile6_log - INFO -    341                                                 # me = os.getpid()

2018-04-29 16:54:14,338 - memory_profile6_log - INFO -    342                                                 # kill_proc_tree(me)

2018-04-29 16:54:14,338 - memory_profile6_log - INFO -    343                             

2018-04-29 16:54:14,342 - memory_profile6_log - INFO -    344    350.4 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 16:54:14,342 - memory_profile6_log - INFO -    345    350.4 MiB      0.0 MiB                       for m in h_frame:

2018-04-29 16:54:14,342 - memory_profile6_log - INFO -    346    350.4 MiB      0.0 MiB                           if m is not None:

2018-04-29 16:54:14,344 - memory_profile6_log - INFO -    347    350.4 MiB      0.0 MiB                               if len(m) > 0:

2018-04-29 16:54:14,345 - memory_profile6_log - INFO -    348    350.4 MiB      0.8 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 16:54:14,346 - memory_profile6_log - INFO -    349    350.4 MiB      0.0 MiB                       del h_frame

2018-04-29 16:54:14,348 - memory_profile6_log - INFO -    350    350.4 MiB      0.0 MiB                       del lhistory

2018-04-29 16:54:14,349 - memory_profile6_log - INFO -    351                             

2018-04-29 16:54:14,351 - memory_profile6_log - INFO -    352    350.5 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 16:54:14,351 - memory_profile6_log - INFO -    353    350.5 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 16:54:14,352 - memory_profile6_log - INFO -    354                                     else: 

2018-04-29 16:54:14,354 - memory_profile6_log - INFO -    355                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 16:54:14,355 - memory_profile6_log - INFO -    356    350.5 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 16:54:14,358 - memory_profile6_log - INFO -    357    350.5 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 16:54:14,358 - memory_profile6_log - INFO -    358                             

2018-04-29 16:54:14,359 - memory_profile6_log - INFO -    359    350.5 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 16:54:14,359 - memory_profile6_log - INFO - 


2018-04-29 16:54:15,466 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 16:54:15,536 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0   0.000646        1844.325967        16            1854.325967  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
1   0.000646        1844.325967        16            1854.325967  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
2   0.000646        1844.325967        32            1854.325967  10960288  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
3   0.000646        1844.325967        32            1854.325967  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
4   0.000646        1844.325967        32            1854.325967  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
5   0.000646        1844.325967        16            1854.325967  10960288  1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...
6        NaN      333823.000000       256          333833.000000  10959239  161129f951435-0d660a18dbcd87-173a7640-100200-1...
0   0.000646        1844.325967         8            1854.325967  10960288  161e219cd891c8-0e882943705df4-15290f5f-38400-1...
1   0.000646        1844.325967        16            1854.325967  10960288  1626fd160d9114-055d801ae6b39d-66265f05-38400-1...
2   0.000646        1844.325967         8            1854.325967  10960288  1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...
3   0.000646        1844.325967         8            1854.325967  10960288  16169731ce618-00742c5e307396-5b4d6a36-38400-16...
4   0.000646        1844.325967        16            1854.325967  10960288  161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...
5   0.000646        1844.325967        16            1854.325967  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
0   0.000646        1844.325967         8            1854.325967  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
1   0.000646        1844.325967         4            1854.325967  10960288  1612d88e55ab8-09af9658f30a46-65117b23-38400-16...
2   0.000646        1844.325967         8            1854.325967  10960288  16149f0a1611-00a034775-63446032-38400-16149f0a...
3   0.000646        1844.325967         4            1854.325967  10960288  16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...
4   0.000646        1844.325967         4            1854.325967  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
5   0.000646        1844.325967         4            1854.325967  10960288  16132f1045f30b-0613b72b33445e-5d640103-38400-1...
6   0.000646        1844.325967         4            1854.325967  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
2018-04-29 16:54:15,536 - memory_profile6_log - INFO - 

2018-04-29 16:54:15,546 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 16:54:15,621 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 16:54:15,634 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 16:55:04,036 - memory_profile6_log - INFO - size of df: 58.30 MB
2018-04-29 16:55:04,038 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-29 16:55:04,099 - memory_profile6_log - INFO - size of current_frame: 58.30 MB
2018-04-29 16:55:04,101 - memory_profile6_log - INFO - loading time of: 484420 total genuine-current interest data ~ take 365.913s
2018-04-29 16:55:04,111 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 16:55:04,112 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 16:55:04,114 - memory_profile6_log - INFO - ================================================

2018-04-29 16:55:04,115 - memory_profile6_log - INFO -    361     86.4 MiB     86.4 MiB   @profile

2018-04-29 16:55:04,117 - memory_profile6_log - INFO -    362                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 16:55:04,118 - memory_profile6_log - INFO -    363     86.5 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 16:55:04,121 - memory_profile6_log - INFO -    364     86.5 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 16:55:04,121 - memory_profile6_log - INFO -    365                             

2018-04-29 16:55:04,121 - memory_profile6_log - INFO -    366                                 # ~~~ Begin collecting data ~~~

2018-04-29 16:55:04,122 - memory_profile6_log - INFO -    367     86.5 MiB      0.0 MiB       t0 = time.time()

2018-04-29 16:55:04,124 - memory_profile6_log - INFO -    368    350.5 MiB    264.0 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 16:55:04,124 - memory_profile6_log - INFO -    369    350.5 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 16:55:04,125 - memory_profile6_log - INFO -    370                                     logger.info("Training cannot be empty..")

2018-04-29 16:55:04,127 - memory_profile6_log - INFO -    371                                     return False

2018-04-29 16:55:04,127 - memory_profile6_log - INFO -    372    349.1 MiB     -1.3 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 16:55:04,128 - memory_profile6_log - INFO -    373    349.3 MiB      0.2 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 16:55:04,128 - memory_profile6_log - INFO -    374    349.3 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 16:55:04,131 - memory_profile6_log - INFO -    375                             

2018-04-29 16:55:04,134 - memory_profile6_log - INFO -    376    355.1 MiB      5.8 MiB       big_frame = pd.concat(datalist)

2018-04-29 16:55:04,134 - memory_profile6_log - INFO -    377    355.1 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 16:55:04,135 - memory_profile6_log - INFO -    378    349.3 MiB     -5.8 MiB       del datalist

2018-04-29 16:55:04,138 - memory_profile6_log - INFO -    379                             

2018-04-29 16:55:04,138 - memory_profile6_log - INFO -    380    349.3 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 16:55:04,140 - memory_profile6_log - INFO -    381                             

2018-04-29 16:55:04,141 - memory_profile6_log - INFO -    382                                 # ~ get current news interest ~

2018-04-29 16:55:04,142 - memory_profile6_log - INFO -    383    349.3 MiB      0.0 MiB       if not cd:

2018-04-29 16:55:04,144 - memory_profile6_log - INFO -    384                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 16:55:04,144 - memory_profile6_log - INFO -    385                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 16:55:04,144 - memory_profile6_log - INFO -    386                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 16:55:04,145 - memory_profile6_log - INFO -    387                                 else:

2018-04-29 16:55:04,147 - memory_profile6_log - INFO -    388    349.3 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 16:55:04,147 - memory_profile6_log - INFO -    389    349.3 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 16:55:04,148 - memory_profile6_log - INFO -    390                             

2018-04-29 16:55:04,148 - memory_profile6_log - INFO -    391                                     # safe handling of query parameter

2018-04-29 16:55:04,148 - memory_profile6_log - INFO -    392                                     query_params = [

2018-04-29 16:55:04,148 - memory_profile6_log - INFO -    393    349.3 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 16:55:04,150 - memory_profile6_log - INFO -    394                                     ]

2018-04-29 16:55:04,154 - memory_profile6_log - INFO -    395                             

2018-04-29 16:55:04,154 - memory_profile6_log - INFO -    396    349.3 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-29 16:55:04,155 - memory_profile6_log - INFO -    397    420.1 MiB     70.8 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 16:55:04,157 - memory_profile6_log - INFO -    398    420.1 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 16:55:04,157 - memory_profile6_log - INFO -    399                             

2018-04-29 16:55:04,157 - memory_profile6_log - INFO -    400    420.1 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 16:55:04,157 - memory_profile6_log - INFO -    401    420.1 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 16:55:04,160 - memory_profile6_log - INFO -    402    420.1 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 16:55:04,161 - memory_profile6_log - INFO -    403    420.1 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 16:55:04,161 - memory_profile6_log - INFO -    404    420.1 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 16:55:04,161 - memory_profile6_log - INFO -    405    420.1 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 16:55:04,164 - memory_profile6_log - INFO -    406                             

2018-04-29 16:55:04,164 - memory_profile6_log - INFO -    407    420.1 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 16:55:04,168 - memory_profile6_log - INFO - 


2018-04-29 16:55:04,171 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 16:55:04,200 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 16:55:04,200 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-29 16:55:04,203 - memory_profile6_log - INFO - apply on: 5000 total history...)
2018-04-29 16:55:04,428 - memory_profile6_log - INFO - len of uniques_fit_hist: 
2018-04-29 16:55:04,430 - memory_profile6_log - INFO -  
2018-04-29 16:55:04,431 - memory_profile6_log - INFO - 5000
2018-04-29 16:55:04,434 - memory_profile6_log - INFO - 

2018-04-29 16:55:04,436 - memory_profile6_log - INFO - uniques_fit_hist:

2018-04-29 16:55:04,509 - memory_profile6_log - INFO -                                               user_id  sigma_Nt
0   16290be16f412b-0b174766972ba1-78227b65-38400-1...        16
1   1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...        16
2   161890d29c1d5-0e69b9c4731723-2406423c-38400-16...        32
3   1612da17a01324-0b4ae4e577dca7-58596970-38400-1...        32
4   16140595aef60-0f425b909e43ea-5768397b-100200-1...        32
5   1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...        16
6   161129f951435-0d660a18dbcd87-173a7640-100200-1...       256
0   161e219cd891c8-0e882943705df4-15290f5f-38400-1...         8
1   1626fd160d9114-055d801ae6b39d-66265f05-38400-1...        16
2   1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...         8
3   16169731ce618-00742c5e307396-5b4d6a36-38400-16...         8
4   161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...        16
5   1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...        16
0   1613f4d321b10-00841d4071241b-42686c23-38400-16...         8
1   1612d88e55ab8-09af9658f30a46-65117b23-38400-16...         4
2   16149f0a1611-00a034775-63446032-38400-16149f0a...         8
3   16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...         4
4   16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...         4
5   16132f1045f30b-0613b72b33445e-5d640103-38400-1...         4
6   1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...         4
0   1610ca98d49149-0e61ac652d8ef-4323461-100200-16...       471
1   16273bde6812b-084c1eff4c3363-42584954-38400-16...        33
2   161e5a454f6161-064229b4591715-61643d25-38400-1...         4
3   1626193fe8054-0dba227898b329-75650b03-38400-16...         4
4   1621f5b8e9b1-0128731ade18b5-2e483333-38400-162...        66
5   1624301817b205-0f6738c6297604-684f2d56-38400-1...         4
6   1612e03d3fd5d-08eab60d8f3d32-a4e5b49-38400-161...        33
0   16257d034e22b8-0cb575c67a4be78-1d451b27-fa000-...        15
1   1621e49418f49-0d6f8597daa543-4b496639-38400-16...        52
2   1612e8ce767c6-0e0b4eacdd5b13-45464657-38400-16...        15
..                                                ...       ...
3   1612fb1a15735-020f35175f6dd5-45654c3a-38400-16...        43
4   1613186a6bebe-0899e00791177-2c18273a-38400-161...        30
5   161d23e558715-09f1e96db21b58-1e061919-38400-16...        41
0   16141b03760d-0228f75551037a-726b623e-38400-161...        42
1   161376faec61f-0d2c1d9ede3adc-5c617622-38400-16...        44
2   161d1b2e498308-0f48d8032a1c19-2620205a-3d10d-1...        15
3   1612d5b869d12-02f1a1eaffe4d-4b643e7e-38400-161...        17
4   1613fdfbed91a2-04890805f9eed18-6f30253b-2c600-...        18
5   1613a92918a8-0d33d223cef553-114e2d5d-64140-161...        19
6   16131273eeea55-0bc3d7d469b3f-15297b10-3d10d-16...        21
0   1616ad1a959555-0d9ab606c6b589-453c4d32-4a640-1...        22
1   16141c4bcefc9-0958eed353b901-646e0a3e-38400-16...        22
2   1612da9b46c118-0b9b0791246fad-4b476b3b-38400-1...        22
3   161422bc4861f-052e3f9302d3b3-7d26745b-38400-16...        24
4   1612d14cd3653-0e00119350fb19-7703023a-38400-16...        46
5   16183eaa8bb80-03fa1ac4c491db-592b3b5c-38400-16...        24
6   16155d99105143-05bfbfdb8-6a697342-38400-16155d...        23
0   1612dc9c1d04e-0b4b0ed06c0eff-4d60643c-29b80-16...        14
1   161dc51f5b82-0c490fa7e3be9f-16702366-38400-161...        25
2   161325c8c3f26-04c492744c92c4-6e38331d-2c880-16...        14
3   1615a3074e06-0ae8be715-f400144-38400-1615a3074e3e        14
4   161413ffbfa100-01159c226cdab5-f465175-38400-16...        14
5   1614be2c2f30-01d03541d14aec-50655a08-38400-161...        14
0   162418d41739d-07c0823ef9d4a58-2f222d58-2c600-1...        25
1   1614aa25de1609-0d17261ba945a3-33b0722-4a640-16...        10
2   1619caa77db166-03b18ccc6-6e5d1c4a-38400-1619ca...        10
3   1613fba31b8c3-0690745c06a243-6e55342d-38400-16...        37
4   1612d2b4f371a-024ccb56cb028d-22a5812-38400-161...        64
5   1612db724803d-0872de26b-59712a6c-38400-1612db7...        29
6   1612d64081b0-028625bf07ab7-5978623d-38400-1612...        20

[5000 rows x 2 columns]
2018-04-29 16:55:04,510 - memory_profile6_log - INFO - 

2018-04-29 16:55:04,517 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate: 
2018-04-29 16:55:04,519 - memory_profile6_log - INFO -  
2018-04-29 16:55:04,523 - memory_profile6_log - INFO - 3753
2018-04-29 16:55:04,528 - memory_profile6_log - INFO - 

2018-04-29 16:55:04,529 - memory_profile6_log - INFO - uniques_fit_hist:

2018-04-29 16:55:04,601 - memory_profile6_log - INFO -                                               user_id  sigma_Nt
0   16290be16f412b-0b174766972ba1-78227b65-38400-1...        16
1   1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...        16
2   161890d29c1d5-0e69b9c4731723-2406423c-38400-16...        32
3   1612da17a01324-0b4ae4e577dca7-58596970-38400-1...        32
4   16140595aef60-0f425b909e43ea-5768397b-100200-1...        32
5   1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...        16
6   161129f951435-0d660a18dbcd87-173a7640-100200-1...       256
0   161e219cd891c8-0e882943705df4-15290f5f-38400-1...         8
1   1626fd160d9114-055d801ae6b39d-66265f05-38400-1...        16
2   1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...         8
3   16169731ce618-00742c5e307396-5b4d6a36-38400-16...         8
4   161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...        16
5   1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...        16
0   1613f4d321b10-00841d4071241b-42686c23-38400-16...         8
1   1612d88e55ab8-09af9658f30a46-65117b23-38400-16...         4
2   16149f0a1611-00a034775-63446032-38400-16149f0a...         8
3   16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...         4
4   16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...         4
5   16132f1045f30b-0613b72b33445e-5d640103-38400-1...         4
6   1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...         4
0   1610ca98d49149-0e61ac652d8ef-4323461-100200-16...       471
1   16273bde6812b-084c1eff4c3363-42584954-38400-16...        33
2   161e5a454f6161-064229b4591715-61643d25-38400-1...         4
3   1626193fe8054-0dba227898b329-75650b03-38400-16...         4
4   1621f5b8e9b1-0128731ade18b5-2e483333-38400-162...        66
5   1624301817b205-0f6738c6297604-684f2d56-38400-1...         4
6   1612e03d3fd5d-08eab60d8f3d32-a4e5b49-38400-161...        33
0   16257d034e22b8-0cb575c67a4be78-1d451b27-fa000-...        15
1   1621e49418f49-0d6f8597daa543-4b496639-38400-16...        52
2   1612e8ce767c6-0e0b4eacdd5b13-45464657-38400-16...        15
..                                                ...       ...
2   1617393822170-0072ba8096a026-4507440e-2c880-16...         8
4   16258005d03d1-07d7d0a4afb7c5-6c585013-38400-16...         8
5   161471adf26c7-03b31f4e1da6d7-7933452b-49a10-16...         8
2   1612d9378ceb1-0fccf82b476fd9-282b543f-38400-16...         4
3   1613830c19746-0bbc73dc2-10157579-38400-1613830...         4
2   161494230c766-07e2dfc2f38f0b-6f337f6d-38400-16...        33
3   16137ace4ed1ff-0a7c55846c7e79-59706a3f-38400-1...        30
0   16131e09fdbb6-08b5ba401ad8d8-4346500b-38400-16...        26
4   1617f7e7d6765-0df630faf-410a670f-38400-1617f7e...        13
2   161477a820960-0456909f230cbc-5b65171a-38400-16...        13
6   1613028a106128-0169ca14063a61-58596970-38400-1...        13
0   1627b8912298c-05b1a0e9ebe016-6149071e-96000-16...        13
1   1615fdf1b7b305-0bab7f28a269c68-6b7e027f-3d10d-...        13
4   162000bced2108-0b3e765f648e59-5e615b69-38400-1...        13
3   1612d396ff3b9-09cc160072e2ae-524a6333-55188-16...         9
0   1612fb32d0db3-05006fe1eb0964-f4b0b15-38400-161...        22
2   16266151da687-05fe10a72-221f3b01-3d10d-1626615...        22
5   1616a422ea2a3-08ebd602631f82-47144065-38400-16...        11
1   162a542b04319f-03ce6f10c22dd6-7c277a75-49a10-1...        12
2   1613c72d67cf8-00ac6c573a9be6-13365f51-38400-16...        12
6   16249414f9719b-0593b54318f6e9-5e4d6f38-38400-1...        12
6   1612d9c84d05b-078aa885ec464-1456653c-38400-161...        92
4   161874b1a4414-0f718a13161522-124e2a53-38400-16...        27
1   1618eeefe1e9b-0595b2e4c-3730a44-38400-1618eeef...        62
4   1612cf2255851-02c696926-69287075-38400-1612cf2...        25
3   1619c7df013112-07d824e63cc2db-204a2f45-38400-1...        56
5   161318a0b7bbb-061acaca0b5b7e-362a0770-38400-16...        14
5   16270725c4fa8-0d14bbbfae6e24-67265a0f-38400-16...        20
3   16242598f9a188-0e5551733cd82e-5e426d3d-38400-1...        10
4   1612d2b4f371a-024ccb56cb028d-22a5812-38400-161...        64

[3753 rows x 2 columns]
2018-04-29 16:55:04,604 - memory_profile6_log - INFO - 

2018-04-29 16:55:05,443 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 16:55:05,444 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 16:55:05,663 - memory_profile6_log - INFO - model_fit dtypes:

2018-04-29 16:55:05,668 - memory_profile6_log - INFO - date              datetime64[ns, UTC]
user_id                        object
topic_id                       object
num_x                           int64
num_y                           int64
is_general                       bool
date_all_click                  int64
Ntotal                          int64
sigma_Nt                        int64
joinprob_ci                   float64
p_cat_ci                      float64
posterior                     float64
dtype: object
2018-04-29 16:55:05,671 - memory_profile6_log - INFO - 

2018-04-29 16:55:05,671 - memory_profile6_log - INFO - fitted_models_hist dtypes:

2018-04-29 16:55:05,677 - memory_profile6_log - INFO - p0_cat_ci                float64
pt_posterior_x_Nt        float64
sigma_Nt                   int64
smoothed_pt_posterior    float64
topic_id                  object
user_id                   object
dtype: object
2018-04-29 16:55:05,680 - memory_profile6_log - INFO - 

2018-04-29 16:55:06,213 - memory_profile6_log - INFO - Len of fitted_models on main class: 253995
2018-04-29 16:55:06,213 - memory_profile6_log - INFO - 

2018-04-29 16:55:06,216 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-29 16:55:06,217 - memory_profile6_log - INFO - 

2018-04-29 16:55:06,219 - memory_profile6_log - INFO - Fitted models before concat:

2018-04-29 16:55:06,244 - memory_profile6_log - INFO -                                 user_id    topic_id  pt_posterior_x_Nt  p0_cat_ci  smoothed_pt_posterior
0  107e02cd-54af-40fd-a602-e6105250ae8c  1046474724        1902.125356   0.000090            1912.125356
1  107e02cd-54af-40fd-a602-e6105250ae8c  1060896301        1346.060484   0.000065            1356.060484
2  107e02cd-54af-40fd-a602-e6105250ae8c   188400148         144.387111   0.036200             154.387111
3  107e02cd-54af-40fd-a602-e6105250ae8c    22553543          71.961844   0.107319              81.961844
4  107e02cd-54af-40fd-a602-e6105250ae8c    22596701         122.085700   0.040442             132.085700
2018-04-29 16:55:06,246 - memory_profile6_log - INFO - 

2018-04-29 16:55:06,246 - memory_profile6_log - INFO - len of current fitted models: 253995
2018-04-29 16:55:06,247 - memory_profile6_log - INFO - 

2018-04-29 16:55:06,249 - memory_profile6_log - INFO - len of history fitted models: 5000
2018-04-29 16:55:06,250 - memory_profile6_log - INFO - 

2018-04-29 16:55:06,289 - memory_profile6_log - INFO - len of fitted models after concat: 258995
2018-04-29 16:55:06,292 - memory_profile6_log - INFO - 

2018-04-29 16:55:06,292 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-29 16:55:06,293 - memory_profile6_log - INFO - 

2018-04-29 17:05:49,724 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 17:05:49,727 - memory_profile6_log - INFO - date_generated: 
2018-04-29 17:05:49,729 - memory_profile6_log - INFO -  
2018-04-29 17:05:49,729 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 17:05:49,729 - memory_profile6_log - INFO - 

2018-04-29 17:05:49,729 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 17:05:49,730 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 17:05:49,730 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 17:05:49,852 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 17:05:49,857 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 17:06:59,009 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 17:06:59,012 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 17:06:59,053 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 17:06:59,055 - memory_profile6_log - INFO - Appending history data...
2018-04-29 17:06:59,056 - memory_profile6_log - INFO - processing batch-0
2018-04-29 17:06:59,056 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:06:59,095 - memory_profile6_log - INFO - call history data...
2018-04-29 17:07:26,825 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:07:27,505 - memory_profile6_log - INFO - processing batch-1
2018-04-29 17:07:27,506 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:07:27,513 - memory_profile6_log - INFO - call history data...
2018-04-29 17:07:56,138 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:07:56,802 - memory_profile6_log - INFO - processing batch-2
2018-04-29 17:07:56,803 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:07:56,809 - memory_profile6_log - INFO - call history data...
2018-04-29 17:08:24,138 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:08:24,789 - memory_profile6_log - INFO - processing batch-3
2018-04-29 17:08:24,790 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:08:24,799 - memory_profile6_log - INFO - call history data...
2018-04-29 17:08:55,641 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:08:56,342 - memory_profile6_log - INFO - processing batch-4
2018-04-29 17:08:56,345 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:08:56,352 - memory_profile6_log - INFO - call history data...
2018-04-29 17:09:29,720 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:09:30,401 - memory_profile6_log - INFO - Appending training data...
2018-04-29 17:09:30,401 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 17:09:30,404 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 17:09:30,405 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 17:09:30,407 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 17:09:30,407 - memory_profile6_log - INFO - ================================================

2018-04-29 17:09:30,408 - memory_profile6_log - INFO -    309     86.7 MiB     86.7 MiB   @profile

2018-04-29 17:09:30,410 - memory_profile6_log - INFO -    310                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 17:09:30,411 - memory_profile6_log - INFO -    311     86.7 MiB      0.0 MiB       bq_client = client

2018-04-29 17:09:30,413 - memory_profile6_log - INFO -    312     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 17:09:30,414 - memory_profile6_log - INFO -    313                             

2018-04-29 17:09:30,414 - memory_profile6_log - INFO -    314     86.7 MiB      0.0 MiB       datalist = []

2018-04-29 17:09:30,415 - memory_profile6_log - INFO -    315     86.7 MiB      0.0 MiB       datalist_hist = []

2018-04-29 17:09:30,417 - memory_profile6_log - INFO -    316                             

2018-04-29 17:09:30,417 - memory_profile6_log - INFO -    317     86.7 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 17:09:30,418 - memory_profile6_log - INFO -    318    351.4 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 17:09:30,420 - memory_profile6_log - INFO -    319    338.8 MiB    252.1 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 17:09:30,421 - memory_profile6_log - INFO -    320    338.8 MiB      0.0 MiB           if tframe is not None:

2018-04-29 17:09:30,423 - memory_profile6_log - INFO -    321    338.8 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 17:09:30,424 - memory_profile6_log - INFO -    322    347.1 MiB      8.3 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 17:09:30,424 - memory_profile6_log - INFO -    323    347.1 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 17:09:30,426 - memory_profile6_log - INFO -    324    347.1 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 17:09:30,426 - memory_profile6_log - INFO -    325    351.4 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 17:09:30,427 - memory_profile6_log - INFO -    326                                                 # ~ loading history

2018-04-29 17:09:30,427 - memory_profile6_log - INFO -    327                                                 """

2018-04-29 17:09:30,430 - memory_profile6_log - INFO -    328                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 17:09:30,430 - memory_profile6_log - INFO -    329                                                 """

2018-04-29 17:09:30,434 - memory_profile6_log - INFO -    330    351.2 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 17:09:30,436 - memory_profile6_log - INFO -    331                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 17:09:30,437 - memory_profile6_log - INFO -    332    351.2 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 17:09:30,437 - memory_profile6_log - INFO -    333    351.2 MiB      0.5 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 17:09:30,438 - memory_profile6_log - INFO -    334                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 17:09:30,440 - memory_profile6_log - INFO -    335                             

2018-04-29 17:09:30,440 - memory_profile6_log - INFO -    336    351.2 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 17:09:30,441 - memory_profile6_log - INFO -    337    351.3 MiB      3.1 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 17:09:30,443 - memory_profile6_log - INFO -    338                             

2018-04-29 17:09:30,446 - memory_profile6_log - INFO -    339                                                 # me = os.getpid()

2018-04-29 17:09:30,447 - memory_profile6_log - INFO -    340                                                 # kill_proc_tree(me)

2018-04-29 17:09:30,447 - memory_profile6_log - INFO -    341                             

2018-04-29 17:09:30,448 - memory_profile6_log - INFO -    342    351.3 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 17:09:30,450 - memory_profile6_log - INFO -    343    351.4 MiB     -0.1 MiB                       for m in h_frame:

2018-04-29 17:09:30,450 - memory_profile6_log - INFO -    344    351.4 MiB     -0.1 MiB                           if m is not None:

2018-04-29 17:09:30,451 - memory_profile6_log - INFO -    345    351.4 MiB     -0.1 MiB                               if len(m) > 0:

2018-04-29 17:09:30,453 - memory_profile6_log - INFO -    346    351.4 MiB      0.6 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 17:09:30,453 - memory_profile6_log - INFO -    347    351.4 MiB      0.0 MiB                       del h_frame

2018-04-29 17:09:30,456 - memory_profile6_log - INFO -    348    351.4 MiB      0.0 MiB                       del lhistory

2018-04-29 17:09:30,457 - memory_profile6_log - INFO -    349                             

2018-04-29 17:09:30,459 - memory_profile6_log - INFO -    350    351.4 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 17:09:30,459 - memory_profile6_log - INFO -    351    351.4 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 17:09:30,460 - memory_profile6_log - INFO -    352                                     else: 

2018-04-29 17:09:30,460 - memory_profile6_log - INFO -    353                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 17:09:30,461 - memory_profile6_log - INFO -    354    351.4 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 17:09:30,461 - memory_profile6_log - INFO -    355    351.4 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 17:09:30,463 - memory_profile6_log - INFO -    356                             

2018-04-29 17:09:30,464 - memory_profile6_log - INFO -    357    351.4 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 17:09:30,466 - memory_profile6_log - INFO - 


2018-04-29 17:09:31,588 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 17:09:31,661 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0        NaN      333823.000000       256          333833.000000  10959239  161129f951435-0d660a18dbcd87-173a7640-100200-1...
1   0.000646        1844.325967        16            1854.325967  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
2   0.000646        1844.325967        16            1854.325967  10960288  1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...
3   0.000646        1844.325967        32            1854.325967  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
4   0.000646        1844.325967        32            1854.325967  10960288  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
5   0.000646        1844.325967        16            1854.325967  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
6   0.000646        1844.325967        32            1854.325967  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
0   0.000646        1844.325967        16            1854.325967  10960288  161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...
1   0.000646        1844.325967        16            1854.325967  10960288  1626fd160d9114-055d801ae6b39d-66265f05-38400-1...
2   0.000646        1844.325967         8            1854.325967  10960288  161e219cd891c8-0e882943705df4-15290f5f-38400-1...
3   0.000646        1844.325967         8            1854.325967  10960288  16169731ce618-00742c5e307396-5b4d6a36-38400-16...
4   0.000646        1844.325967        16            1854.325967  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
5   0.000646        1844.325967         8            1854.325967  10960288  1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...
0   0.000646        1844.325967         4            1854.325967  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
1   0.000646        1844.325967         4            1854.325967  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
2   0.000646        1844.325967         8            1854.325967  10960288  16149f0a1611-00a034775-63446032-38400-16149f0a...
3   0.000646        1844.325967         8            1854.325967  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
4   0.000646        1844.325967         4            1854.325967  10960288  16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...
5   0.000646        1844.325967         4            1854.325967  10960288  16132f1045f30b-0613b72b33445e-5d640103-38400-1...
6   0.000646        1844.325967         4            1854.325967  10960288  1612d88e55ab8-09af9658f30a46-65117b23-38400-16...
2018-04-29 17:09:31,661 - memory_profile6_log - INFO - 

2018-04-29 17:09:31,671 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 17:09:31,742 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 17:09:31,756 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 17:10:20,163 - memory_profile6_log - INFO - size of df: 58.30 MB
2018-04-29 17:10:20,164 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-29 17:10:20,230 - memory_profile6_log - INFO - size of current_frame: 58.30 MB
2018-04-29 17:10:20,230 - memory_profile6_log - INFO - loading time of: 484420 total genuine-current interest data ~ take 270.399s
2018-04-29 17:10:20,237 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 17:10:20,239 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 17:10:20,240 - memory_profile6_log - INFO - ================================================

2018-04-29 17:10:20,243 - memory_profile6_log - INFO -    359     86.6 MiB     86.6 MiB   @profile

2018-04-29 17:10:20,244 - memory_profile6_log - INFO -    360                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 17:10:20,246 - memory_profile6_log - INFO -    361     86.7 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 17:10:20,246 - memory_profile6_log - INFO -    362     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 17:10:20,247 - memory_profile6_log - INFO -    363                             

2018-04-29 17:10:20,247 - memory_profile6_log - INFO -    364                                 # ~~~ Begin collecting data ~~~

2018-04-29 17:10:20,249 - memory_profile6_log - INFO -    365     86.7 MiB      0.0 MiB       t0 = time.time()

2018-04-29 17:10:20,250 - memory_profile6_log - INFO -    366    351.4 MiB    264.7 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 17:10:20,252 - memory_profile6_log - INFO -    367    351.4 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 17:10:20,253 - memory_profile6_log - INFO -    368                                     logger.info("Training cannot be empty..")

2018-04-29 17:10:20,253 - memory_profile6_log - INFO -    369                                     return False

2018-04-29 17:10:20,255 - memory_profile6_log - INFO -    370    348.0 MiB     -3.3 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 17:10:20,255 - memory_profile6_log - INFO -    371    348.1 MiB      0.1 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 17:10:20,256 - memory_profile6_log - INFO -    372    348.1 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 17:10:20,256 - memory_profile6_log - INFO -    373                             

2018-04-29 17:10:20,256 - memory_profile6_log - INFO -    374    353.9 MiB      5.8 MiB       big_frame = pd.concat(datalist)

2018-04-29 17:10:20,256 - memory_profile6_log - INFO -    375    353.9 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 17:10:20,257 - memory_profile6_log - INFO -    376    348.1 MiB     -5.8 MiB       del datalist

2018-04-29 17:10:20,259 - memory_profile6_log - INFO -    377                             

2018-04-29 17:10:20,259 - memory_profile6_log - INFO -    378    348.1 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 17:10:20,263 - memory_profile6_log - INFO -    379                             

2018-04-29 17:10:20,263 - memory_profile6_log - INFO -    380                                 # ~ get current news interest ~

2018-04-29 17:10:20,265 - memory_profile6_log - INFO -    381    348.1 MiB      0.0 MiB       if not cd:

2018-04-29 17:10:20,266 - memory_profile6_log - INFO -    382                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 17:10:20,266 - memory_profile6_log - INFO -    383                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 17:10:20,266 - memory_profile6_log - INFO -    384                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 17:10:20,269 - memory_profile6_log - INFO -    385                                 else:

2018-04-29 17:10:20,269 - memory_profile6_log - INFO -    386    348.1 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 17:10:20,270 - memory_profile6_log - INFO -    387    348.1 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 17:10:20,270 - memory_profile6_log - INFO -    388                             

2018-04-29 17:10:20,275 - memory_profile6_log - INFO -    389                                     # safe handling of query parameter

2018-04-29 17:10:20,276 - memory_profile6_log - INFO -    390                                     query_params = [

2018-04-29 17:10:20,276 - memory_profile6_log - INFO -    391    348.1 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 17:10:20,279 - memory_profile6_log - INFO -    392                                     ]

2018-04-29 17:10:20,279 - memory_profile6_log - INFO -    393                             

2018-04-29 17:10:20,279 - memory_profile6_log - INFO -    394    348.1 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-29 17:10:20,279 - memory_profile6_log - INFO -    395    419.3 MiB     71.1 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 17:10:20,279 - memory_profile6_log - INFO -    396    419.3 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 17:10:20,280 - memory_profile6_log - INFO -    397                             

2018-04-29 17:10:20,280 - memory_profile6_log - INFO -    398    419.3 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 17:10:20,280 - memory_profile6_log - INFO -    399    419.3 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 17:10:20,282 - memory_profile6_log - INFO -    400    419.3 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 17:10:20,282 - memory_profile6_log - INFO -    401    419.3 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 17:10:20,282 - memory_profile6_log - INFO -    402    419.3 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 17:10:20,285 - memory_profile6_log - INFO -    403    419.3 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 17:10:20,286 - memory_profile6_log - INFO -    404                             

2018-04-29 17:10:20,288 - memory_profile6_log - INFO -    405    419.3 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 17:10:20,290 - memory_profile6_log - INFO - 


2018-04-29 17:10:20,293 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 17:10:20,325 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 17:10:20,325 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-29 17:10:20,326 - memory_profile6_log - INFO - apply on: 5000 total history...)
2018-04-29 17:10:20,562 - memory_profile6_log - INFO - len of uniques_fit_hist:5000
2018-04-29 17:10:20,571 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:3753
2018-04-29 17:10:21,431 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 17:10:21,433 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 17:10:21,644 - memory_profile6_log - INFO - model_fit dtypes:

2018-04-29 17:10:21,650 - memory_profile6_log - INFO - date              datetime64[ns, UTC]
user_id                        object
topic_id                       object
num_x                           int64
num_y                           int64
is_general                       bool
date_all_click                  int64
Ntotal                          int64
sigma_Nt                        int64
joinprob_ci                   float64
p_cat_ci                      float64
posterior                     float64
dtype: object
2018-04-29 17:10:21,651 - memory_profile6_log - INFO - 

2018-04-29 17:10:21,651 - memory_profile6_log - INFO - fitted_models_hist dtypes:

2018-04-29 17:10:21,657 - memory_profile6_log - INFO - p0_cat_ci                float64
pt_posterior_x_Nt        float64
sigma_Nt                   int64
smoothed_pt_posterior    float64
topic_id                  object
user_id                   object
dtype: object
2018-04-29 17:10:21,658 - memory_profile6_log - INFO - 

2018-04-29 17:10:22,230 - memory_profile6_log - INFO - Len of fitted_models on main class: 253995
2018-04-29 17:10:22,230 - memory_profile6_log - INFO - 

2018-04-29 17:10:22,232 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-29 17:10:22,233 - memory_profile6_log - INFO - 

2018-04-29 17:10:22,234 - memory_profile6_log - INFO - Fitted models before concat:

2018-04-29 17:10:22,259 - memory_profile6_log - INFO -                                 user_id    topic_id  pt_posterior_x_Nt  p0_cat_ci  smoothed_pt_posterior
0  107e02cd-54af-40fd-a602-e6105250ae8c  1046474724         951.062678   0.000090             961.062678
1  107e02cd-54af-40fd-a602-e6105250ae8c  1060896301         673.030242   0.000065             683.030242
2  107e02cd-54af-40fd-a602-e6105250ae8c   188400148          72.193555   0.036200              82.193555
3  107e02cd-54af-40fd-a602-e6105250ae8c    22553543          35.980922   0.107319              45.980922
4  107e02cd-54af-40fd-a602-e6105250ae8c    22596701          61.042850   0.040442              71.042850
2018-04-29 17:10:22,260 - memory_profile6_log - INFO - 

2018-04-29 17:10:22,262 - memory_profile6_log - INFO - len of current fitted models: 253995
2018-04-29 17:10:22,263 - memory_profile6_log - INFO - 

2018-04-29 17:10:22,263 - memory_profile6_log - INFO - len of history fitted models: 5000
2018-04-29 17:10:22,265 - memory_profile6_log - INFO - 

2018-04-29 17:10:22,305 - memory_profile6_log - INFO - len of fitted models after concat: 258995
2018-04-29 17:10:22,306 - memory_profile6_log - INFO - 

2018-04-29 17:10:22,308 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-29 17:10:22,309 - memory_profile6_log - INFO - 

2018-04-29 17:10:22,466 - memory_profile6_log - INFO - Simplifying current fitted models...
2018-04-29 17:10:22,467 - memory_profile6_log - INFO - 

2018-04-29 17:15:48,315 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 17:15:48,318 - memory_profile6_log - INFO - date_generated: 
2018-04-29 17:15:48,319 - memory_profile6_log - INFO -  
2018-04-29 17:15:48,319 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 17:15:48,319 - memory_profile6_log - INFO - 

2018-04-29 17:15:48,321 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 17:15:48,322 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 17:15:48,322 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 17:15:48,459 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 17:15:48,461 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 17:16:55,750 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 17:16:55,752 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 17:16:55,798 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 17:16:55,799 - memory_profile6_log - INFO - Appending history data...
2018-04-29 17:16:55,799 - memory_profile6_log - INFO - processing batch-0
2018-04-29 17:16:55,802 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:16:55,845 - memory_profile6_log - INFO - call history data...
2018-04-29 17:17:23,976 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:17:24,654 - memory_profile6_log - INFO - processing batch-1
2018-04-29 17:17:24,654 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:17:24,664 - memory_profile6_log - INFO - call history data...
2018-04-29 17:17:51,801 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:17:52,447 - memory_profile6_log - INFO - processing batch-2
2018-04-29 17:17:52,447 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:17:52,456 - memory_profile6_log - INFO - call history data...
2018-04-29 17:18:19,555 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:18:20,265 - memory_profile6_log - INFO - processing batch-3
2018-04-29 17:18:20,266 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:18:20,273 - memory_profile6_log - INFO - call history data...
2018-04-29 17:18:50,697 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:18:51,542 - memory_profile6_log - INFO - processing batch-4
2018-04-29 17:18:51,542 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:18:51,551 - memory_profile6_log - INFO - call history data...
2018-04-29 17:19:25,858 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:19:26,565 - memory_profile6_log - INFO - Appending training data...
2018-04-29 17:19:26,566 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 17:19:26,569 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 17:19:26,571 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 17:19:26,572 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 17:19:26,572 - memory_profile6_log - INFO - ================================================

2018-04-29 17:19:26,573 - memory_profile6_log - INFO -    309     86.9 MiB     86.9 MiB   @profile

2018-04-29 17:19:26,575 - memory_profile6_log - INFO -    310                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 17:19:26,578 - memory_profile6_log - INFO -    311     86.9 MiB      0.0 MiB       bq_client = client

2018-04-29 17:19:26,578 - memory_profile6_log - INFO -    312     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 17:19:26,581 - memory_profile6_log - INFO -    313                             

2018-04-29 17:19:26,582 - memory_profile6_log - INFO -    314     86.9 MiB      0.0 MiB       datalist = []

2018-04-29 17:19:26,582 - memory_profile6_log - INFO -    315     86.9 MiB      0.0 MiB       datalist_hist = []

2018-04-29 17:19:26,584 - memory_profile6_log - INFO -    316                             

2018-04-29 17:19:26,584 - memory_profile6_log - INFO -    317     86.9 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 17:19:26,585 - memory_profile6_log - INFO -    318    350.8 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 17:19:26,588 - memory_profile6_log - INFO -    319    338.5 MiB    251.6 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 17:19:26,588 - memory_profile6_log - INFO -    320    338.5 MiB      0.0 MiB           if tframe is not None:

2018-04-29 17:19:26,589 - memory_profile6_log - INFO -    321    338.5 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 17:19:26,589 - memory_profile6_log - INFO -    322    346.8 MiB      8.3 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 17:19:26,592 - memory_profile6_log - INFO -    323    346.8 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 17:19:26,592 - memory_profile6_log - INFO -    324    346.8 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 17:19:26,592 - memory_profile6_log - INFO -    325    350.8 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 17:19:26,595 - memory_profile6_log - INFO -    326                                                 # ~ loading history

2018-04-29 17:19:26,595 - memory_profile6_log - INFO -    327                                                 """

2018-04-29 17:19:26,596 - memory_profile6_log - INFO -    328                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 17:19:26,598 - memory_profile6_log - INFO -    329                                                 """

2018-04-29 17:19:26,599 - memory_profile6_log - INFO -    330    350.7 MiB      0.1 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 17:19:26,601 - memory_profile6_log - INFO -    331                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 17:19:26,601 - memory_profile6_log - INFO -    332    350.7 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 17:19:26,604 - memory_profile6_log - INFO -    333    350.7 MiB      0.5 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 17:19:26,605 - memory_profile6_log - INFO -    334                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 17:19:26,605 - memory_profile6_log - INFO -    335                             

2018-04-29 17:19:26,611 - memory_profile6_log - INFO -    336    350.7 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 17:19:26,611 - memory_profile6_log - INFO -    337    350.8 MiB      2.7 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 17:19:26,615 - memory_profile6_log - INFO -    338                             

2018-04-29 17:19:26,615 - memory_profile6_log - INFO -    339                                                 # me = os.getpid()

2018-04-29 17:19:26,617 - memory_profile6_log - INFO -    340                                                 # kill_proc_tree(me)

2018-04-29 17:19:26,618 - memory_profile6_log - INFO -    341                             

2018-04-29 17:19:26,618 - memory_profile6_log - INFO -    342    350.8 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 17:19:26,621 - memory_profile6_log - INFO -    343    350.8 MiB     -0.0 MiB                       for m in h_frame:

2018-04-29 17:19:26,621 - memory_profile6_log - INFO -    344    350.8 MiB     -0.0 MiB                           if m is not None:

2018-04-29 17:19:26,621 - memory_profile6_log - INFO -    345    350.8 MiB     -0.0 MiB                               if len(m) > 0:

2018-04-29 17:19:26,622 - memory_profile6_log - INFO -    346    350.8 MiB      0.7 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 17:19:26,624 - memory_profile6_log - INFO -    347    350.8 MiB     -0.0 MiB                       del h_frame

2018-04-29 17:19:26,625 - memory_profile6_log - INFO -    348    350.8 MiB      0.0 MiB                       del lhistory

2018-04-29 17:19:26,625 - memory_profile6_log - INFO -    349                             

2018-04-29 17:19:26,627 - memory_profile6_log - INFO -    350    350.8 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 17:19:26,630 - memory_profile6_log - INFO -    351    350.8 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 17:19:26,631 - memory_profile6_log - INFO -    352                                     else: 

2018-04-29 17:19:26,631 - memory_profile6_log - INFO -    353                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 17:19:26,631 - memory_profile6_log - INFO -    354    350.8 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 17:19:26,634 - memory_profile6_log - INFO -    355    350.8 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 17:19:26,634 - memory_profile6_log - INFO -    356                             

2018-04-29 17:19:26,634 - memory_profile6_log - INFO -    357    350.8 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 17:19:26,635 - memory_profile6_log - INFO - 


2018-04-29 17:19:27,713 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 17:19:27,792 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0   0.000646        1844.325967        16            1854.325967  10960288  1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...
1        NaN      333823.000000       256          333833.000000  10959239  161129f951435-0d660a18dbcd87-173a7640-100200-1...
2   0.000646        1844.325967        32            1854.325967  10960288  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
3   0.000646        1844.325967        32            1854.325967  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
4   0.000646        1844.325967        32            1854.325967  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
5   0.000646        1844.325967        16            1854.325967  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
6   0.000646        1844.325967        16            1854.325967  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
0   0.000646        1844.325967        16            1854.325967  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
1   0.000646        1844.325967        16            1854.325967  10960288  1626fd160d9114-055d801ae6b39d-66265f05-38400-1...
2   0.000646        1844.325967        16            1854.325967  10960288  161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...
3   0.000646        1844.325967         8            1854.325967  10960288  161e219cd891c8-0e882943705df4-15290f5f-38400-1...
4   0.000646        1844.325967         8            1854.325967  10960288  16169731ce618-00742c5e307396-5b4d6a36-38400-16...
5   0.000646        1844.325967         8            1854.325967  10960288  1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...
0   0.000646        1844.325967         8            1854.325967  10960288  16149f0a1611-00a034775-63446032-38400-16149f0a...
1   0.000646        1844.325967         4            1854.325967  10960288  16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...
2   0.000646        1844.325967         4            1854.325967  10960288  16132f1045f30b-0613b72b33445e-5d640103-38400-1...
3   0.000646        1844.325967         4            1854.325967  10960288  1612d88e55ab8-09af9658f30a46-65117b23-38400-16...
4   0.000646        1844.325967         4            1854.325967  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
5   0.000646        1844.325967         4            1854.325967  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
6   0.000646        1844.325967         8            1854.325967  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
2018-04-29 17:19:27,793 - memory_profile6_log - INFO - 

2018-04-29 17:19:27,802 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 17:19:27,875 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 17:19:27,887 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 17:20:16,663 - memory_profile6_log - INFO - size of df: 58.30 MB
2018-04-29 17:20:16,664 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-29 17:20:16,727 - memory_profile6_log - INFO - size of current_frame: 58.30 MB
2018-04-29 17:20:16,730 - memory_profile6_log - INFO - loading time of: 484420 total genuine-current interest data ~ take 268.293s
2018-04-29 17:20:16,736 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 17:20:16,736 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 17:20:16,739 - memory_profile6_log - INFO - ================================================

2018-04-29 17:20:16,740 - memory_profile6_log - INFO -    359     86.8 MiB     86.8 MiB   @profile

2018-04-29 17:20:16,742 - memory_profile6_log - INFO -    360                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 17:20:16,743 - memory_profile6_log - INFO -    361     86.9 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 17:20:16,743 - memory_profile6_log - INFO -    362     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 17:20:16,744 - memory_profile6_log - INFO -    363                             

2018-04-29 17:20:16,744 - memory_profile6_log - INFO -    364                                 # ~~~ Begin collecting data ~~~

2018-04-29 17:20:16,747 - memory_profile6_log - INFO -    365     86.9 MiB      0.0 MiB       t0 = time.time()

2018-04-29 17:20:16,750 - memory_profile6_log - INFO -    366    350.8 MiB    263.9 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 17:20:16,750 - memory_profile6_log - INFO -    367    350.8 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 17:20:16,752 - memory_profile6_log - INFO -    368                                     logger.info("Training cannot be empty..")

2018-04-29 17:20:16,753 - memory_profile6_log - INFO -    369                                     return False

2018-04-29 17:20:16,753 - memory_profile6_log - INFO -    370    349.7 MiB     -1.1 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 17:20:16,756 - memory_profile6_log - INFO -    371    349.9 MiB      0.1 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 17:20:16,756 - memory_profile6_log - INFO -    372    349.9 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 17:20:16,756 - memory_profile6_log - INFO -    373                             

2018-04-29 17:20:16,756 - memory_profile6_log - INFO -    374    355.7 MiB      5.8 MiB       big_frame = pd.concat(datalist)

2018-04-29 17:20:16,759 - memory_profile6_log - INFO -    375    355.7 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 17:20:16,759 - memory_profile6_log - INFO -    376    349.9 MiB     -5.8 MiB       del datalist

2018-04-29 17:20:16,766 - memory_profile6_log - INFO -    377                             

2018-04-29 17:20:16,766 - memory_profile6_log - INFO -    378    349.9 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 17:20:16,772 - memory_profile6_log - INFO -    379                             

2018-04-29 17:20:16,773 - memory_profile6_log - INFO -    380                                 # ~ get current news interest ~

2018-04-29 17:20:16,775 - memory_profile6_log - INFO -    381    349.9 MiB      0.0 MiB       if not cd:

2018-04-29 17:20:16,776 - memory_profile6_log - INFO -    382                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 17:20:16,776 - memory_profile6_log - INFO -    383                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 17:20:16,776 - memory_profile6_log - INFO -    384                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 17:20:16,776 - memory_profile6_log - INFO -    385                                 else:

2018-04-29 17:20:16,778 - memory_profile6_log - INFO -    386    349.9 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 17:20:16,779 - memory_profile6_log - INFO -    387    349.9 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 17:20:16,782 - memory_profile6_log - INFO -    388                             

2018-04-29 17:20:16,782 - memory_profile6_log - INFO -    389                                     # safe handling of query parameter

2018-04-29 17:20:16,783 - memory_profile6_log - INFO -    390                                     query_params = [

2018-04-29 17:20:16,785 - memory_profile6_log - INFO -    391    349.9 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 17:20:16,785 - memory_profile6_log - INFO -    392                                     ]

2018-04-29 17:20:16,786 - memory_profile6_log - INFO -    393                             

2018-04-29 17:20:16,786 - memory_profile6_log - INFO -    394    349.9 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-29 17:20:16,786 - memory_profile6_log - INFO -    395    420.6 MiB     70.7 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 17:20:16,789 - memory_profile6_log - INFO -    396    420.6 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 17:20:16,789 - memory_profile6_log - INFO -    397                             

2018-04-29 17:20:16,789 - memory_profile6_log - INFO -    398    420.6 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 17:20:16,790 - memory_profile6_log - INFO -    399    420.6 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 17:20:16,793 - memory_profile6_log - INFO -    400    420.6 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 17:20:16,795 - memory_profile6_log - INFO -    401    420.6 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 17:20:16,796 - memory_profile6_log - INFO -    402    420.6 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 17:20:16,798 - memory_profile6_log - INFO -    403    420.6 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 17:20:16,798 - memory_profile6_log - INFO -    404                             

2018-04-29 17:20:16,799 - memory_profile6_log - INFO -    405    420.6 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 17:20:16,799 - memory_profile6_log - INFO - 


2018-04-29 17:20:16,802 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 17:20:16,832 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 17:20:16,832 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-29 17:20:16,834 - memory_profile6_log - INFO - apply on: 5000 total history...)
2018-04-29 17:20:17,048 - memory_profile6_log - INFO - len of uniques_fit_hist:5000
2018-04-29 17:20:17,053 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:3753
2018-04-29 17:20:17,861 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 17:20:17,864 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 17:20:18,072 - memory_profile6_log - INFO - model_fit dtypes:

2018-04-29 17:20:18,078 - memory_profile6_log - INFO - date              datetime64[ns, UTC]
user_id                        object
topic_id                       object
num_x                           int64
num_y                           int64
is_general                       bool
date_all_click                  int64
Ntotal                          int64
sigma_Nt                        int64
joinprob_ci                   float64
p_cat_ci                      float64
posterior                     float64
dtype: object
2018-04-29 17:20:18,078 - memory_profile6_log - INFO - 

2018-04-29 17:20:18,079 - memory_profile6_log - INFO - fitted_models_hist dtypes:

2018-04-29 17:20:18,085 - memory_profile6_log - INFO - p0_cat_ci                float64
pt_posterior_x_Nt        float64
sigma_Nt                   int64
smoothed_pt_posterior    float64
topic_id                  object
user_id                   object
dtype: object
2018-04-29 17:20:18,088 - memory_profile6_log - INFO - 

2018-04-29 17:20:18,642 - memory_profile6_log - INFO - Len of fitted_models on main class: 253995
2018-04-29 17:20:18,644 - memory_profile6_log - INFO - 

2018-04-29 17:20:18,644 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-29 17:20:18,644 - memory_profile6_log - INFO - 

2018-04-29 17:20:18,645 - memory_profile6_log - INFO - Fitted models before concat:

2018-04-29 17:20:18,673 - memory_profile6_log - INFO -                                 user_id    topic_id  pt_posterior_x_Nt  p0_cat_ci  smoothed_pt_posterior
0  107e02cd-54af-40fd-a602-e6105250ae8c  1046474724         951.062678   0.000090             961.062678
1  107e02cd-54af-40fd-a602-e6105250ae8c  1060896301         673.030242   0.000065             683.030242
2  107e02cd-54af-40fd-a602-e6105250ae8c   188400148          72.193555   0.036200              82.193555
3  107e02cd-54af-40fd-a602-e6105250ae8c    22553543          35.980922   0.107319              45.980922
4  107e02cd-54af-40fd-a602-e6105250ae8c    22596701          61.042850   0.040442              71.042850
2018-04-29 17:20:18,697 - memory_profile6_log - INFO - 

2018-04-29 17:20:18,697 - memory_profile6_log - INFO - len of current fitted models: 253995
2018-04-29 17:20:18,698 - memory_profile6_log - INFO - 

2018-04-29 17:20:18,700 - memory_profile6_log - INFO - len of history fitted models: 5000
2018-04-29 17:20:18,700 - memory_profile6_log - INFO - 

2018-04-29 17:20:18,742 - memory_profile6_log - INFO - len of fitted models after concat: 258995
2018-04-29 17:20:18,743 - memory_profile6_log - INFO - 

2018-04-29 17:20:18,744 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-29 17:20:18,746 - memory_profile6_log - INFO - 

2018-04-29 17:20:18,905 - memory_profile6_log - INFO - Fitted models after concat:

2018-04-29 17:20:18,924 - memory_profile6_log - INFO -                                 user_id    topic_id  pt_posterior_x_Nt
0  107e02cd-54af-40fd-a602-e6105250ae8c  1046474724         951.062678
1  107e02cd-54af-40fd-a602-e6105250ae8c  1060896301         673.030242
2  107e02cd-54af-40fd-a602-e6105250ae8c   188400148          72.193555
3  107e02cd-54af-40fd-a602-e6105250ae8c    22553543          35.980922
4  107e02cd-54af-40fd-a602-e6105250ae8c    22596701          61.042850
2018-04-29 17:20:18,926 - memory_profile6_log - INFO - 

2018-04-29 17:20:18,927 - memory_profile6_log - INFO - Simplifying current fitted models...
2018-04-29 17:20:18,928 - memory_profile6_log - INFO - 

2018-04-29 17:26:16,216 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 17:26:16,220 - memory_profile6_log - INFO - date_generated: 
2018-04-29 17:26:16,220 - memory_profile6_log - INFO -  
2018-04-29 17:26:16,220 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 17:26:16,221 - memory_profile6_log - INFO - 

2018-04-29 17:26:16,221 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 17:26:16,221 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 17:26:16,223 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 17:26:16,348 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 17:26:16,352 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 17:27:26,122 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 17:27:26,124 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 17:27:26,170 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 17:27:26,171 - memory_profile6_log - INFO - Appending history data...
2018-04-29 17:27:26,171 - memory_profile6_log - INFO - processing batch-0
2018-04-29 17:27:26,174 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:27:26,216 - memory_profile6_log - INFO - call history data...
2018-04-29 17:27:56,654 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:27:57,328 - memory_profile6_log - INFO - processing batch-1
2018-04-29 17:27:57,331 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:27:57,338 - memory_profile6_log - INFO - call history data...
2018-04-29 17:28:25,072 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:28:25,723 - memory_profile6_log - INFO - processing batch-2
2018-04-29 17:28:25,724 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:28:25,732 - memory_profile6_log - INFO - call history data...
2018-04-29 17:28:54,874 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:28:55,533 - memory_profile6_log - INFO - processing batch-3
2018-04-29 17:28:55,536 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:28:55,543 - memory_profile6_log - INFO - call history data...
2018-04-29 17:29:22,792 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:29:23,447 - memory_profile6_log - INFO - processing batch-4
2018-04-29 17:29:23,450 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:29:23,457 - memory_profile6_log - INFO - call history data...
2018-04-29 17:29:50,767 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:29:51,453 - memory_profile6_log - INFO - Appending training data...
2018-04-29 17:29:51,454 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 17:29:51,457 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 17:29:51,459 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 17:29:51,460 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 17:29:51,460 - memory_profile6_log - INFO - ================================================

2018-04-29 17:29:51,463 - memory_profile6_log - INFO -    311     86.9 MiB     86.9 MiB   @profile

2018-04-29 17:29:51,466 - memory_profile6_log - INFO -    312                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 17:29:51,469 - memory_profile6_log - INFO -    313     86.9 MiB      0.0 MiB       bq_client = client

2018-04-29 17:29:51,470 - memory_profile6_log - INFO -    314     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 17:29:51,473 - memory_profile6_log - INFO -    315                             

2018-04-29 17:29:51,473 - memory_profile6_log - INFO -    316     86.9 MiB      0.0 MiB       datalist = []

2018-04-29 17:29:51,474 - memory_profile6_log - INFO -    317     86.9 MiB      0.0 MiB       datalist_hist = []

2018-04-29 17:29:51,476 - memory_profile6_log - INFO -    318                             

2018-04-29 17:29:51,477 - memory_profile6_log - INFO -    319     86.9 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 17:29:51,479 - memory_profile6_log - INFO -    320    351.5 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 17:29:51,480 - memory_profile6_log - INFO -    321    339.6 MiB    252.7 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 17:29:51,480 - memory_profile6_log - INFO -    322    339.6 MiB      0.0 MiB           if tframe is not None:

2018-04-29 17:29:51,482 - memory_profile6_log - INFO -    323    339.6 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 17:29:51,483 - memory_profile6_log - INFO -    324    347.9 MiB      8.3 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 17:29:51,483 - memory_profile6_log - INFO -    325    347.9 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 17:29:51,483 - memory_profile6_log - INFO -    326    347.9 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 17:29:51,484 - memory_profile6_log - INFO -    327    351.5 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 17:29:51,486 - memory_profile6_log - INFO -    328                                                 # ~ loading history

2018-04-29 17:29:51,487 - memory_profile6_log - INFO -    329                                                 """

2018-04-29 17:29:51,489 - memory_profile6_log - INFO -    330                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 17:29:51,490 - memory_profile6_log - INFO -    331                                                 """

2018-04-29 17:29:51,490 - memory_profile6_log - INFO -    332    351.3 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 17:29:51,492 - memory_profile6_log - INFO -    333                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 17:29:51,493 - memory_profile6_log - INFO -    334    351.3 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 17:29:51,493 - memory_profile6_log - INFO -    335    351.3 MiB      0.6 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 17:29:51,493 - memory_profile6_log - INFO -    336                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 17:29:51,494 - memory_profile6_log - INFO -    337                             

2018-04-29 17:29:51,497 - memory_profile6_log - INFO -    338    351.3 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 17:29:51,499 - memory_profile6_log - INFO -    339    351.4 MiB      2.2 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 17:29:51,499 - memory_profile6_log - INFO -    340                             

2018-04-29 17:29:51,500 - memory_profile6_log - INFO -    341                                                 # me = os.getpid()

2018-04-29 17:29:51,500 - memory_profile6_log - INFO -    342                                                 # kill_proc_tree(me)

2018-04-29 17:29:51,500 - memory_profile6_log - INFO -    343                             

2018-04-29 17:29:51,502 - memory_profile6_log - INFO -    344    351.4 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 17:29:51,503 - memory_profile6_log - INFO -    345    351.5 MiB      0.0 MiB                       for m in h_frame:

2018-04-29 17:29:51,503 - memory_profile6_log - INFO -    346    351.5 MiB      0.0 MiB                           if m is not None:

2018-04-29 17:29:51,503 - memory_profile6_log - INFO -    347    351.5 MiB      0.0 MiB                               if len(m) > 0:

2018-04-29 17:29:51,505 - memory_profile6_log - INFO -    348    351.5 MiB      0.8 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 17:29:51,506 - memory_profile6_log - INFO -    349    351.5 MiB      0.0 MiB                       del h_frame

2018-04-29 17:29:51,507 - memory_profile6_log - INFO -    350    351.5 MiB      0.0 MiB                       del lhistory

2018-04-29 17:29:51,509 - memory_profile6_log - INFO -    351                             

2018-04-29 17:29:51,510 - memory_profile6_log - INFO -    352    351.5 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 17:29:51,512 - memory_profile6_log - INFO -    353    351.5 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 17:29:51,513 - memory_profile6_log - INFO -    354                                     else: 

2018-04-29 17:29:51,513 - memory_profile6_log - INFO -    355                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 17:29:51,513 - memory_profile6_log - INFO -    356    351.5 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 17:29:51,515 - memory_profile6_log - INFO -    357    351.5 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 17:29:51,516 - memory_profile6_log - INFO -    358                             

2018-04-29 17:29:51,516 - memory_profile6_log - INFO -    359    351.5 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 17:29:51,519 - memory_profile6_log - INFO - 


2018-04-29 17:29:52,608 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 17:29:52,677 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0   0.000646        1844.325967        32            1854.325967  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
1   0.000646        1844.325967        16            1854.325967  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
2   0.000646        1844.325967        32            1854.325967  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
3   0.000646        1844.325967        16            1854.325967  10960288  1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...
4        NaN      333823.000000       256          333833.000000  10959239  161129f951435-0d660a18dbcd87-173a7640-100200-1...
5   0.000646        1844.325967        32            1854.325967  10960288  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
6   0.000646        1844.325967        16            1854.325967  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
0   0.000646        1844.325967        16            1854.325967  10960288  1626fd160d9114-055d801ae6b39d-66265f05-38400-1...
1   0.000646        1844.325967         8            1854.325967  10960288  16169731ce618-00742c5e307396-5b4d6a36-38400-16...
2   0.000646        1844.325967         8            1854.325967  10960288  161e219cd891c8-0e882943705df4-15290f5f-38400-1...
3   0.000646        1844.325967        16            1854.325967  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
4   0.000646        1844.325967        16            1854.325967  10960288  161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...
5   0.000646        1844.325967         8            1854.325967  10960288  1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...
0   0.000646        1844.325967         8            1854.325967  10960288  16149f0a1611-00a034775-63446032-38400-16149f0a...
1   0.000646        1844.325967         8            1854.325967  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
2   0.000646        1844.325967         4            1854.325967  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
3   0.000646        1844.325967         4            1854.325967  10960288  16132f1045f30b-0613b72b33445e-5d640103-38400-1...
4   0.000646        1844.325967         4            1854.325967  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
5   0.000646        1844.325967         4            1854.325967  10960288  1612d88e55ab8-09af9658f30a46-65117b23-38400-16...
6   0.000646        1844.325967         4            1854.325967  10960288  16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...
2018-04-29 17:29:52,680 - memory_profile6_log - INFO - 

2018-04-29 17:29:52,688 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 17:29:52,760 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 17:29:52,778 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 17:30:41,678 - memory_profile6_log - INFO - size of df: 58.30 MB
2018-04-29 17:30:41,680 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-29 17:30:41,743 - memory_profile6_log - INFO - size of current_frame: 58.30 MB
2018-04-29 17:30:41,743 - memory_profile6_log - INFO - loading time of: 484420 total genuine-current interest data ~ take 265.418s
2018-04-29 17:30:41,750 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 17:30:41,750 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 17:30:41,752 - memory_profile6_log - INFO - ================================================

2018-04-29 17:30:41,752 - memory_profile6_log - INFO -    361     86.7 MiB     86.7 MiB   @profile

2018-04-29 17:30:41,759 - memory_profile6_log - INFO -    362                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 17:30:41,759 - memory_profile6_log - INFO -    363     86.9 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 17:30:41,762 - memory_profile6_log - INFO -    364     86.9 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 17:30:41,763 - memory_profile6_log - INFO -    365                             

2018-04-29 17:30:41,763 - memory_profile6_log - INFO -    366                                 # ~~~ Begin collecting data ~~~

2018-04-29 17:30:41,765 - memory_profile6_log - INFO -    367     86.9 MiB      0.0 MiB       t0 = time.time()

2018-04-29 17:30:41,766 - memory_profile6_log - INFO -    368    351.5 MiB    264.6 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 17:30:41,766 - memory_profile6_log - INFO -    369    351.5 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 17:30:41,769 - memory_profile6_log - INFO -    370                                     logger.info("Training cannot be empty..")

2018-04-29 17:30:41,769 - memory_profile6_log - INFO -    371                                     return False

2018-04-29 17:30:41,772 - memory_profile6_log - INFO -    372    351.6 MiB      0.1 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 17:30:41,773 - memory_profile6_log - INFO -    373    351.7 MiB      0.1 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 17:30:41,773 - memory_profile6_log - INFO -    374    351.7 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 17:30:41,775 - memory_profile6_log - INFO -    375                             

2018-04-29 17:30:41,775 - memory_profile6_log - INFO -    376    357.5 MiB      5.8 MiB       big_frame = pd.concat(datalist)

2018-04-29 17:30:41,775 - memory_profile6_log - INFO -    377    357.5 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 17:30:41,776 - memory_profile6_log - INFO -    378    351.7 MiB     -5.8 MiB       del datalist

2018-04-29 17:30:41,776 - memory_profile6_log - INFO -    379                             

2018-04-29 17:30:41,779 - memory_profile6_log - INFO -    380    351.7 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 17:30:41,782 - memory_profile6_log - INFO -    381                             

2018-04-29 17:30:41,782 - memory_profile6_log - INFO -    382                                 # ~ get current news interest ~

2018-04-29 17:30:41,782 - memory_profile6_log - INFO -    383    351.7 MiB      0.0 MiB       if not cd:

2018-04-29 17:30:41,782 - memory_profile6_log - INFO -    384                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 17:30:41,783 - memory_profile6_log - INFO -    385                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 17:30:41,785 - memory_profile6_log - INFO -    386                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 17:30:41,785 - memory_profile6_log - INFO -    387                                 else:

2018-04-29 17:30:41,786 - memory_profile6_log - INFO -    388    351.7 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 17:30:41,786 - memory_profile6_log - INFO -    389    351.7 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 17:30:41,790 - memory_profile6_log - INFO -    390                             

2018-04-29 17:30:41,790 - memory_profile6_log - INFO -    391                                     # safe handling of query parameter

2018-04-29 17:30:41,792 - memory_profile6_log - INFO -    392                                     query_params = [

2018-04-29 17:30:41,792 - memory_profile6_log - INFO -    393    351.7 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 17:30:41,792 - memory_profile6_log - INFO -    394                                     ]

2018-04-29 17:30:41,793 - memory_profile6_log - INFO -    395                             

2018-04-29 17:30:41,795 - memory_profile6_log - INFO -    396    351.7 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-29 17:30:41,795 - memory_profile6_log - INFO -    397    418.3 MiB     66.6 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 17:30:41,796 - memory_profile6_log - INFO -    398    418.3 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 17:30:41,798 - memory_profile6_log - INFO -    399                             

2018-04-29 17:30:41,799 - memory_profile6_log - INFO -    400    418.3 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 17:30:41,799 - memory_profile6_log - INFO -    401    418.3 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 17:30:41,799 - memory_profile6_log - INFO -    402    418.3 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 17:30:41,802 - memory_profile6_log - INFO -    403    418.3 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 17:30:41,802 - memory_profile6_log - INFO -    404    418.3 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 17:30:41,803 - memory_profile6_log - INFO -    405    418.3 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 17:30:41,803 - memory_profile6_log - INFO -    406                             

2018-04-29 17:30:41,805 - memory_profile6_log - INFO -    407    418.3 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 17:30:41,805 - memory_profile6_log - INFO - 


2018-04-29 17:30:41,809 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 17:30:41,838 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 17:30:41,839 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-29 17:30:41,841 - memory_profile6_log - INFO - apply on: 5000 total history...)
2018-04-29 17:30:42,052 - memory_profile6_log - INFO - len of uniques_fit_hist:5000
2018-04-29 17:30:42,059 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:3753
2018-04-29 17:30:42,857 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 17:30:42,858 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 17:30:43,084 - memory_profile6_log - INFO - model_fit dtypes:

2018-04-29 17:30:43,089 - memory_profile6_log - INFO - date              datetime64[ns, UTC]
user_id                        object
topic_id                       object
num_x                           int64
num_y                           int64
is_general                       bool
date_all_click                  int64
Ntotal                          int64
sigma_Nt                        int64
joinprob_ci                   float64
p_cat_ci                      float64
posterior                     float64
dtype: object
2018-04-29 17:30:43,091 - memory_profile6_log - INFO - 

2018-04-29 17:30:43,092 - memory_profile6_log - INFO - fitted_models_hist dtypes:

2018-04-29 17:30:43,098 - memory_profile6_log - INFO - p0_cat_ci                float64
pt_posterior_x_Nt        float64
sigma_Nt                   int64
smoothed_pt_posterior    float64
topic_id                  object
user_id                   object
dtype: object
2018-04-29 17:30:43,098 - memory_profile6_log - INFO - 

2018-04-29 17:30:43,592 - memory_profile6_log - INFO - Len of fitted_models on main class: 253995
2018-04-29 17:30:43,595 - memory_profile6_log - INFO - 

2018-04-29 17:30:43,595 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-29 17:30:43,596 - memory_profile6_log - INFO - 

2018-04-29 17:30:43,598 - memory_profile6_log - INFO - Fitted models before concat:

2018-04-29 17:30:43,619 - memory_profile6_log - INFO -                                 user_id    topic_id  pt_posterior_x_Nt  smoothed_pt_posterior
0  107e02cd-54af-40fd-a602-e6105250ae8c  1046474724         951.062678             961.062678
1  107e02cd-54af-40fd-a602-e6105250ae8c  1060896301         673.030242             683.030242
2  107e02cd-54af-40fd-a602-e6105250ae8c   188400148          72.193555              82.193555
3  107e02cd-54af-40fd-a602-e6105250ae8c    22553543          35.980922              45.980922
4  107e02cd-54af-40fd-a602-e6105250ae8c    22596701          61.042850              71.042850
2018-04-29 17:30:43,621 - memory_profile6_log - INFO - 

2018-04-29 17:30:43,621 - memory_profile6_log - INFO - len of current fitted models: 253995
2018-04-29 17:30:43,624 - memory_profile6_log - INFO - 

2018-04-29 17:30:43,625 - memory_profile6_log - INFO - len of history fitted models: 5000
2018-04-29 17:30:43,625 - memory_profile6_log - INFO - 

2018-04-29 17:30:43,658 - memory_profile6_log - INFO - len of fitted models after concat: 258995
2018-04-29 17:30:43,660 - memory_profile6_log - INFO - 

2018-04-29 17:30:43,661 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-29 17:30:43,664 - memory_profile6_log - INFO - 

2018-04-29 17:30:43,819 - memory_profile6_log - INFO - Fitted models after concat:

2018-04-29 17:30:43,836 - memory_profile6_log - INFO -                                 user_id    topic_id  pt_posterior_x_Nt
0  107e02cd-54af-40fd-a602-e6105250ae8c  1046474724         951.062678
1  107e02cd-54af-40fd-a602-e6105250ae8c  1060896301         673.030242
2  107e02cd-54af-40fd-a602-e6105250ae8c   188400148          72.193555
3  107e02cd-54af-40fd-a602-e6105250ae8c    22553543          35.980922
4  107e02cd-54af-40fd-a602-e6105250ae8c    22596701          61.042850
2018-04-29 17:30:43,838 - memory_profile6_log - INFO - 

2018-04-29 17:30:43,838 - memory_profile6_log - INFO - Simplifying current fitted models...
2018-04-29 17:30:43,839 - memory_profile6_log - INFO - 

2018-04-29 17:33:33,459 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 17:33:33,460 - memory_profile6_log - INFO - date_generated: 
2018-04-29 17:33:33,460 - memory_profile6_log - INFO -  
2018-04-29 17:33:33,461 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 17:33:33,461 - memory_profile6_log - INFO - 

2018-04-29 17:33:33,461 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 17:33:33,463 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 17:33:33,463 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 17:33:33,586 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 17:33:33,591 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 17:34:47,931 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 17:34:47,933 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 17:34:47,973 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 17:34:47,974 - memory_profile6_log - INFO - Appending history data...
2018-04-29 17:34:47,976 - memory_profile6_log - INFO - processing batch-0
2018-04-29 17:34:47,976 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:34:48,019 - memory_profile6_log - INFO - call history data...
2018-04-29 17:35:16,384 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:35:17,078 - memory_profile6_log - INFO - processing batch-1
2018-04-29 17:35:17,078 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:35:17,086 - memory_profile6_log - INFO - call history data...
2018-04-29 17:35:45,230 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:35:45,890 - memory_profile6_log - INFO - processing batch-2
2018-04-29 17:35:45,891 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:35:45,898 - memory_profile6_log - INFO - call history data...
2018-04-29 17:36:13,418 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:36:14,125 - memory_profile6_log - INFO - processing batch-3
2018-04-29 17:36:14,127 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:36:14,134 - memory_profile6_log - INFO - call history data...
2018-04-29 17:36:41,668 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:36:42,447 - memory_profile6_log - INFO - processing batch-4
2018-04-29 17:36:42,448 - memory_profile6_log - INFO - creating list history data...
2018-04-29 17:36:42,457 - memory_profile6_log - INFO - call history data...
2018-04-29 17:37:10,122 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 17:37:10,829 - memory_profile6_log - INFO - Appending training data...
2018-04-29 17:37:10,832 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 17:37:10,835 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 17:37:10,835 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 17:37:10,836 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 17:37:10,838 - memory_profile6_log - INFO - ================================================

2018-04-29 17:37:10,838 - memory_profile6_log - INFO -    311     87.0 MiB     87.0 MiB   @profile

2018-04-29 17:37:10,838 - memory_profile6_log - INFO -    312                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 17:37:10,839 - memory_profile6_log - INFO -    313     87.0 MiB      0.0 MiB       bq_client = client

2018-04-29 17:37:10,842 - memory_profile6_log - INFO -    314     87.0 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 17:37:10,842 - memory_profile6_log - INFO -    315                             

2018-04-29 17:37:10,844 - memory_profile6_log - INFO -    316     87.0 MiB      0.0 MiB       datalist = []

2018-04-29 17:37:10,844 - memory_profile6_log - INFO -    317     87.0 MiB      0.0 MiB       datalist_hist = []

2018-04-29 17:37:10,845 - memory_profile6_log - INFO -    318                             

2018-04-29 17:37:10,845 - memory_profile6_log - INFO -    319     87.0 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 17:37:10,846 - memory_profile6_log - INFO -    320    351.4 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 17:37:10,846 - memory_profile6_log - INFO -    321    340.4 MiB    253.5 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 17:37:10,848 - memory_profile6_log - INFO -    322    340.4 MiB      0.0 MiB           if tframe is not None:

2018-04-29 17:37:10,848 - memory_profile6_log - INFO -    323    340.4 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 17:37:10,848 - memory_profile6_log - INFO -    324    347.6 MiB      7.2 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 17:37:10,851 - memory_profile6_log - INFO -    325    347.6 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 17:37:10,852 - memory_profile6_log - INFO -    326    347.6 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 17:37:10,855 - memory_profile6_log - INFO -    327    351.4 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 17:37:10,855 - memory_profile6_log - INFO -    328                                                 # ~ loading history

2018-04-29 17:37:10,857 - memory_profile6_log - INFO -    329                                                 """

2018-04-29 17:37:10,858 - memory_profile6_log - INFO -    330                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 17:37:10,858 - memory_profile6_log - INFO -    331                                                 """

2018-04-29 17:37:10,859 - memory_profile6_log - INFO -    332    351.2 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 17:37:10,861 - memory_profile6_log - INFO -    333                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 17:37:10,864 - memory_profile6_log - INFO -    334    351.2 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 17:37:10,865 - memory_profile6_log - INFO -    335    351.2 MiB      0.5 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 17:37:10,867 - memory_profile6_log - INFO -    336                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 17:37:10,868 - memory_profile6_log - INFO -    337                             

2018-04-29 17:37:10,874 - memory_profile6_log - INFO -    338    351.2 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 17:37:10,878 - memory_profile6_log - INFO -    339    351.3 MiB      2.3 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 17:37:10,881 - memory_profile6_log - INFO -    340                             

2018-04-29 17:37:10,885 - memory_profile6_log - INFO -    341                                                 # me = os.getpid()

2018-04-29 17:37:10,891 - memory_profile6_log - INFO -    342                                                 # kill_proc_tree(me)

2018-04-29 17:37:10,892 - memory_profile6_log - INFO -    343                             

2018-04-29 17:37:10,894 - memory_profile6_log - INFO -    344    351.3 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 17:37:10,897 - memory_profile6_log - INFO -    345    351.4 MiB      0.0 MiB                       for m in h_frame:

2018-04-29 17:37:10,901 - memory_profile6_log - INFO -    346    351.4 MiB      0.0 MiB                           if m is not None:

2018-04-29 17:37:10,901 - memory_profile6_log - INFO -    347    351.4 MiB      0.0 MiB                               if len(m) > 0:

2018-04-29 17:37:10,903 - memory_profile6_log - INFO -    348    351.4 MiB      0.9 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 17:37:10,904 - memory_profile6_log - INFO -    349    351.4 MiB      0.0 MiB                       del h_frame

2018-04-29 17:37:10,905 - memory_profile6_log - INFO -    350    351.4 MiB      0.0 MiB                       del lhistory

2018-04-29 17:37:10,907 - memory_profile6_log - INFO -    351                             

2018-04-29 17:37:10,907 - memory_profile6_log - INFO -    352    351.4 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 17:37:10,908 - memory_profile6_log - INFO -    353    351.4 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 17:37:10,908 - memory_profile6_log - INFO -    354                                     else: 

2018-04-29 17:37:10,911 - memory_profile6_log - INFO -    355                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 17:37:10,911 - memory_profile6_log - INFO -    356    351.4 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 17:37:10,915 - memory_profile6_log - INFO -    357    351.4 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 17:37:10,917 - memory_profile6_log - INFO -    358                             

2018-04-29 17:37:10,918 - memory_profile6_log - INFO -    359    351.4 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 17:37:10,920 - memory_profile6_log - INFO - 


2018-04-29 17:37:12,190 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 17:37:12,265 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0   0.000646        1844.325967        32            1854.325967  10960288  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
1   0.000646        1844.325967        16            1854.325967  10960288  1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...
2        NaN      333823.000000       256          333833.000000  10959239  161129f951435-0d660a18dbcd87-173a7640-100200-1...
3   0.000646        1844.325967        16            1854.325967  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
4   0.000646        1844.325967        32            1854.325967  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
5   0.000646        1844.325967        32            1854.325967  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
6   0.000646        1844.325967        16            1854.325967  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
0   0.000646        1844.325967        16            1854.325967  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
1   0.000646        1844.325967         8            1854.325967  10960288  1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...
2   0.000646        1844.325967         8            1854.325967  10960288  16169731ce618-00742c5e307396-5b4d6a36-38400-16...
3   0.000646        1844.325967        16            1854.325967  10960288  161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...
4   0.000646        1844.325967         8            1854.325967  10960288  161e219cd891c8-0e882943705df4-15290f5f-38400-1...
5   0.000646        1844.325967        16            1854.325967  10960288  1626fd160d9114-055d801ae6b39d-66265f05-38400-1...
0   0.000646        1844.325967         4            1854.325967  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
1   0.000646        1844.325967         8            1854.325967  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
2   0.000646        1844.325967         4            1854.325967  10960288  1612d88e55ab8-09af9658f30a46-65117b23-38400-16...
3   0.000646        1844.325967         8            1854.325967  10960288  16149f0a1611-00a034775-63446032-38400-16149f0a...
4   0.000646        1844.325967         4            1854.325967  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
5   0.000646        1844.325967         4            1854.325967  10960288  16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...
6   0.000646        1844.325967         4            1854.325967  10960288  16132f1045f30b-0613b72b33445e-5d640103-38400-1...
2018-04-29 17:37:12,266 - memory_profile6_log - INFO - 

2018-04-29 17:37:12,275 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 17:37:12,344 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 17:37:12,355 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 17:38:09,230 - memory_profile6_log - INFO - size of df: 58.30 MB
2018-04-29 17:38:09,232 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-29 17:38:09,292 - memory_profile6_log - INFO - size of current_frame: 58.30 MB
2018-04-29 17:38:09,293 - memory_profile6_log - INFO - loading time of: 484420 total genuine-current interest data ~ take 275.728s
2018-04-29 17:38:09,299 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 17:38:09,301 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 17:38:09,302 - memory_profile6_log - INFO - ================================================

2018-04-29 17:38:09,303 - memory_profile6_log - INFO -    361     86.9 MiB     86.9 MiB   @profile

2018-04-29 17:38:09,305 - memory_profile6_log - INFO -    362                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 17:38:09,305 - memory_profile6_log - INFO -    363     87.0 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 17:38:09,305 - memory_profile6_log - INFO -    364     87.0 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 17:38:09,308 - memory_profile6_log - INFO -    365                             

2018-04-29 17:38:09,309 - memory_profile6_log - INFO -    366                                 # ~~~ Begin collecting data ~~~

2018-04-29 17:38:09,309 - memory_profile6_log - INFO -    367     87.0 MiB      0.0 MiB       t0 = time.time()

2018-04-29 17:38:09,309 - memory_profile6_log - INFO -    368    351.4 MiB    264.4 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 17:38:09,311 - memory_profile6_log - INFO -    369    351.4 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 17:38:09,311 - memory_profile6_log - INFO -    370                                     logger.info("Training cannot be empty..")

2018-04-29 17:38:09,312 - memory_profile6_log - INFO -    371                                     return False

2018-04-29 17:38:09,312 - memory_profile6_log - INFO -    372    350.9 MiB     -0.5 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 17:38:09,312 - memory_profile6_log - INFO -    373    351.0 MiB      0.1 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 17:38:09,315 - memory_profile6_log - INFO -    374    351.0 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 17:38:09,315 - memory_profile6_log - INFO -    375                             

2018-04-29 17:38:09,316 - memory_profile6_log - INFO -    376    356.8 MiB      5.8 MiB       big_frame = pd.concat(datalist)

2018-04-29 17:38:09,318 - memory_profile6_log - INFO -    377    356.8 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 17:38:09,318 - memory_profile6_log - INFO -    378    351.0 MiB     -5.8 MiB       del datalist

2018-04-29 17:38:09,319 - memory_profile6_log - INFO -    379                             

2018-04-29 17:38:09,319 - memory_profile6_log - INFO -    380    351.0 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 17:38:09,319 - memory_profile6_log - INFO -    381                             

2018-04-29 17:38:09,319 - memory_profile6_log - INFO -    382                                 # ~ get current news interest ~

2018-04-29 17:38:09,319 - memory_profile6_log - INFO -    383    351.0 MiB      0.0 MiB       if not cd:

2018-04-29 17:38:09,321 - memory_profile6_log - INFO -    384                                     logger.info("Collecting training data(current date interest)..")

2018-04-29 17:38:09,322 - memory_profile6_log - INFO -    385                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 17:38:09,322 - memory_profile6_log - INFO -    386                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 17:38:09,326 - memory_profile6_log - INFO -    387                                 else:

2018-04-29 17:38:09,328 - memory_profile6_log - INFO -    388    351.0 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 17:38:09,328 - memory_profile6_log - INFO -    389    351.0 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 17:38:09,328 - memory_profile6_log - INFO -    390                             

2018-04-29 17:38:09,328 - memory_profile6_log - INFO -    391                                     # safe handling of query parameter

2018-04-29 17:38:09,329 - memory_profile6_log - INFO -    392                                     query_params = [

2018-04-29 17:38:09,329 - memory_profile6_log - INFO -    393    351.0 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 17:38:09,331 - memory_profile6_log - INFO -    394                                     ]

2018-04-29 17:38:09,332 - memory_profile6_log - INFO -    395                             

2018-04-29 17:38:09,334 - memory_profile6_log - INFO -    396    351.0 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-29 17:38:09,334 - memory_profile6_log - INFO -    397    418.8 MiB     67.7 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 17:38:09,336 - memory_profile6_log - INFO -    398    418.8 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 17:38:09,336 - memory_profile6_log - INFO -    399                             

2018-04-29 17:38:09,338 - memory_profile6_log - INFO -    400    418.8 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 17:38:09,339 - memory_profile6_log - INFO -    401    418.8 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 17:38:09,341 - memory_profile6_log - INFO -    402    418.8 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 17:38:09,342 - memory_profile6_log - INFO -    403    418.8 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 17:38:09,342 - memory_profile6_log - INFO -    404    418.8 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 17:38:09,342 - memory_profile6_log - INFO -    405    418.8 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 17:38:09,344 - memory_profile6_log - INFO -    406                             

2018-04-29 17:38:09,344 - memory_profile6_log - INFO -    407    418.8 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 17:38:09,345 - memory_profile6_log - INFO - 


2018-04-29 17:38:09,348 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 17:38:09,380 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 17:38:09,381 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-29 17:38:09,381 - memory_profile6_log - INFO - apply on: 5000 total history...)
2018-04-29 17:38:09,598 - memory_profile6_log - INFO - len of uniques_fit_hist:5000
2018-04-29 17:38:09,604 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:3753
2018-04-29 17:38:10,436 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 17:38:10,437 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 17:38:10,645 - memory_profile6_log - INFO - model_fit dtypes:

2018-04-29 17:38:10,651 - memory_profile6_log - INFO - date              datetime64[ns, UTC]
user_id                        object
topic_id                       object
num_x                           int64
num_y                           int64
is_general                       bool
date_all_click                  int64
Ntotal                          int64
sigma_Nt                        int64
joinprob_ci                   float64
p_cat_ci                      float64
posterior                     float64
dtype: object
2018-04-29 17:38:10,651 - memory_profile6_log - INFO - 

2018-04-29 17:38:10,653 - memory_profile6_log - INFO - fitted_models_hist dtypes:

2018-04-29 17:38:10,657 - memory_profile6_log - INFO - p0_cat_ci                float64
pt_posterior_x_Nt        float64
sigma_Nt                   int64
smoothed_pt_posterior    float64
topic_id                  object
user_id                   object
dtype: object
2018-04-29 17:38:10,658 - memory_profile6_log - INFO - 

2018-04-29 17:38:11,155 - memory_profile6_log - INFO - Len of fitted_models on main class: 253995
2018-04-29 17:38:11,157 - memory_profile6_log - INFO - 

2018-04-29 17:38:11,157 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-29 17:38:11,158 - memory_profile6_log - INFO - 

2018-04-29 17:38:11,158 - memory_profile6_log - INFO - Fitted models before concat:

2018-04-29 17:38:11,181 - memory_profile6_log - INFO -                                 user_id    topic_id  pt_posterior_x_Nt  smoothed_pt_posterior
0  107e02cd-54af-40fd-a602-e6105250ae8c  1046474724         951.062678             961.062678
1  107e02cd-54af-40fd-a602-e6105250ae8c  1060896301         673.030242             683.030242
2  107e02cd-54af-40fd-a602-e6105250ae8c   188400148          72.193555              82.193555
3  107e02cd-54af-40fd-a602-e6105250ae8c    22553543          35.980922              45.980922
4  107e02cd-54af-40fd-a602-e6105250ae8c    22596701          61.042850              71.042850
2018-04-29 17:38:11,183 - memory_profile6_log - INFO - 

2018-04-29 17:38:11,184 - memory_profile6_log - INFO - len of current fitted models: 253995
2018-04-29 17:38:11,184 - memory_profile6_log - INFO - 

2018-04-29 17:38:11,186 - memory_profile6_log - INFO - len of history fitted models: 5000
2018-04-29 17:38:11,186 - memory_profile6_log - INFO - 

2018-04-29 17:38:11,219 - memory_profile6_log - INFO - len of fitted models after concat: 258995
2018-04-29 17:38:11,220 - memory_profile6_log - INFO - 

2018-04-29 17:38:11,221 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-29 17:38:11,223 - memory_profile6_log - INFO - 

2018-04-29 17:38:11,371 - memory_profile6_log - INFO - Fitted models after concat:

2018-04-29 17:38:11,390 - memory_profile6_log - INFO -                                 user_id    topic_id  pt_posterior_x_Nt
0  107e02cd-54af-40fd-a602-e6105250ae8c  1046474724         951.062678
1  107e02cd-54af-40fd-a602-e6105250ae8c  1060896301         673.030242
2  107e02cd-54af-40fd-a602-e6105250ae8c   188400148          72.193555
3  107e02cd-54af-40fd-a602-e6105250ae8c    22553543          35.980922
4  107e02cd-54af-40fd-a602-e6105250ae8c    22596701          61.042850
2018-04-29 17:38:11,391 - memory_profile6_log - INFO - 

2018-04-29 17:38:11,407 - memory_profile6_log - INFO - Fitted models after concat-simplify:

2018-04-29 17:38:11,428 - memory_profile6_log - INFO -                                 user_id    topic_id  pt_posterior_x_Nt  smoothed_pt_posterior
0  107e02cd-54af-40fd-a602-e6105250ae8c  1046474724         951.062678             961.062678
1  107e02cd-54af-40fd-a602-e6105250ae8c  1060896301         673.030242             683.030242
2  107e02cd-54af-40fd-a602-e6105250ae8c   188400148          72.193555              82.193555
3  107e02cd-54af-40fd-a602-e6105250ae8c    22553543          35.980922              45.980922
4  107e02cd-54af-40fd-a602-e6105250ae8c    22596701          61.042850              71.042850
2018-04-29 17:38:11,430 - memory_profile6_log - INFO - 

2018-04-29 17:38:11,431 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 253995
2018-04-29 17:38:11,433 - memory_profile6_log - INFO - 

2018-04-29 17:38:11,503 - memory_profile6_log - INFO -                                 user_id    topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci
0  107e02cd-54af-40fd-a602-e6105250ae8c  1046474724         951.062678             961.062678   0.000090
1  107e02cd-54af-40fd-a602-e6105250ae8c  1060896301         673.030242             683.030242   0.000065
2  107e02cd-54af-40fd-a602-e6105250ae8c   188400148          72.193555              82.193555   0.036200
3  107e02cd-54af-40fd-a602-e6105250ae8c    22553543          35.980922              45.980922   0.107319
4  107e02cd-54af-40fd-a602-e6105250ae8c    22596701          61.042850              71.042850   0.040442
5  107e02cd-54af-40fd-a602-e6105250ae8c    22601470          16.743893              26.743893   0.057919
6  107e02cd-54af-40fd-a602-e6105250ae8c    22617325         569.663823             579.663823   0.002035
7  107e02cd-54af-40fd-a602-e6105250ae8c    22661796          78.768995              88.768995   0.019008
8  107e02cd-54af-40fd-a602-e6105250ae8c    27312625         287.778448             297.778448   0.003279
9  107e02cd-54af-40fd-a602-e6105250ae8c    27313228        1290.552835            1300.552835   0.000136
2018-04-29 17:38:11,506 - memory_profile6_log - INFO - 

2018-04-29 17:38:52,265 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-29 17:38:52,295 - memory_profile6_log - INFO - Total train time: 42.916s
2018-04-29 17:38:52,296 - memory_profile6_log - INFO - memory left before cleaning: 76.200 percent memory...
2018-04-29 17:38:52,298 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-29 17:38:52,299 - memory_profile6_log - INFO - deleting df_dut...
2018-04-29 17:38:52,299 - memory_profile6_log - INFO - deleting df_dt...
2018-04-29 17:38:52,301 - memory_profile6_log - INFO - deleting df_input...
2018-04-29 17:38:52,309 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-29 17:38:52,311 - memory_profile6_log - INFO - deleting df_current...
2018-04-29 17:38:52,312 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-29 17:38:52,322 - memory_profile6_log - INFO - deleting model_fit...
2018-04-29 17:38:52,323 - memory_profile6_log - INFO - deleting result...
2018-04-29 17:38:52,342 - memory_profile6_log - INFO - deleting model_transform...
2018-04-29 17:38:52,344 - memory_profile6_log - INFO - memory left after cleaning: 76.000 percent memory...
2018-04-29 17:38:52,345 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-29 17:38:52,346 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-29 17:38:52,526 - memory_profile6_log - INFO - deleting BR...
2018-04-29 17:38:52,532 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 17:38:52,532 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 17:38:52,533 - memory_profile6_log - INFO - ================================================

2018-04-29 17:38:52,533 - memory_profile6_log - INFO -    113    418.8 MiB    418.8 MiB   @profile

2018-04-29 17:38:52,536 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-29 17:38:52,536 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-29 17:38:52,538 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-29 17:38:52,538 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-29 17:38:52,538 - memory_profile6_log - INFO -    118                                      saveto="datastore", fitted_models_hist=None):

2018-04-29 17:38:52,539 - memory_profile6_log - INFO -    119                                 """

2018-04-29 17:38:52,539 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-29 17:38:52,539 - memory_profile6_log - INFO -    121                                 """

2018-04-29 17:38:52,540 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-29 17:38:52,543 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-29 17:38:52,545 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-29 17:38:52,546 - memory_profile6_log - INFO -    125    418.8 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-29 17:38:52,546 - memory_profile6_log - INFO -    126    426.8 MiB      8.0 MiB       df_dut = df_input.copy(deep=True)

2018-04-29 17:38:52,546 - memory_profile6_log - INFO -    127    426.8 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 17:38:52,546 - memory_profile6_log - INFO -    128                             

2018-04-29 17:38:52,548 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-29 17:38:52,548 - memory_profile6_log - INFO -    130    434.0 MiB      7.3 MiB       df_dt = df_current.copy(deep=True)

2018-04-29 17:38:52,549 - memory_profile6_log - INFO -    131    434.0 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 17:38:52,549 - memory_profile6_log - INFO -    132                             

2018-04-29 17:38:52,555 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-29 17:38:52,555 - memory_profile6_log - INFO -    134    434.0 MiB      0.0 MiB       t0 = time.time()

2018-04-29 17:38:52,559 - memory_profile6_log - INFO -    135    434.0 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-29 17:38:52,559 - memory_profile6_log - INFO -    136    434.0 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-29 17:38:52,559 - memory_profile6_log - INFO -    137    434.0 MiB      0.0 MiB       logger.info("apply on: %d total history...)", len(df_hist))

2018-04-29 17:38:52,561 - memory_profile6_log - INFO -    138                             

2018-04-29 17:38:52,561 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-29 17:38:52,561 - memory_profile6_log - INFO -    140    434.0 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-29 17:38:52,562 - memory_profile6_log - INFO -    141                             

2018-04-29 17:38:52,562 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-29 17:38:52,562 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-29 17:38:52,562 - memory_profile6_log - INFO -    144    436.6 MiB      2.5 MiB       NB = BR.processX(df_dut)

2018-04-29 17:38:52,562 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-29 17:38:52,565 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-29 17:38:52,566 - memory_profile6_log - INFO -    147    446.4 MiB      9.8 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-29 17:38:52,568 - memory_profile6_log - INFO -    148                                 """

2018-04-29 17:38:52,569 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-29 17:38:52,569 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-29 17:38:52,569 - memory_profile6_log - INFO -    151                                 """

2018-04-29 17:38:52,569 - memory_profile6_log - INFO -    152    446.4 MiB      0.0 MiB       fitby_sigmant = True

2018-04-29 17:38:52,569 - memory_profile6_log - INFO -    153    446.4 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-29 17:38:52,571 - memory_profile6_log - INFO -    154    446.4 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-29 17:38:52,571 - memory_profile6_log - INFO -    155    456.1 MiB      9.7 MiB                            'is_general']]

2018-04-29 17:38:52,571 - memory_profile6_log - INFO -    156                                 # get sigma_Nt from fitted_models_hist

2018-04-29 17:38:52,572 - memory_profile6_log - INFO -    157    456.4 MiB      0.3 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-04-29 17:38:52,572 - memory_profile6_log - INFO -    158    456.4 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-04-29 17:38:52,572 - memory_profile6_log - INFO -    159    456.5 MiB      0.1 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-04-29 17:38:52,572 - memory_profile6_log - INFO -    160    456.5 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-04-29 17:38:52,578 - memory_profile6_log - INFO -    161                             

2018-04-29 17:38:52,578 - memory_profile6_log - INFO -    162                                 # begin fit

2018-04-29 17:38:52,579 - memory_profile6_log - INFO -    163    456.5 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-29 17:38:52,581 - memory_profile6_log - INFO -    164    456.5 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-29 17:38:52,581 - memory_profile6_log - INFO -    165    485.4 MiB     28.9 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-04-29 17:38:52,582 - memory_profile6_log - INFO -    166    485.4 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-29 17:38:52,584 - memory_profile6_log - INFO -    167    485.4 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-29 17:38:52,584 - memory_profile6_log - INFO -    168                             

2018-04-29 17:38:52,584 - memory_profile6_log - INFO -    169                                 # ~~ and Transform ~~

2018-04-29 17:38:52,585 - memory_profile6_log - INFO -    170                                 #   handling current news interest == current date

2018-04-29 17:38:52,585 - memory_profile6_log - INFO -    171    485.4 MiB      0.0 MiB       if df_dt.empty:

2018-04-29 17:38:52,585 - memory_profile6_log - INFO -    172                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-29 17:38:52,585 - memory_profile6_log - INFO -    173                                     return None

2018-04-29 17:38:52,585 - memory_profile6_log - INFO -    174    485.8 MiB      0.3 MiB       NB = BR.processX(df_dt)

2018-04-29 17:38:52,586 - memory_profile6_log - INFO -    175    496.4 MiB     10.6 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-29 17:38:52,586 - memory_profile6_log - INFO -    176                             

2018-04-29 17:38:52,586 - memory_profile6_log - INFO -    177    496.4 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-29 17:38:52,591 - memory_profile6_log - INFO -    178    495.5 MiB     -0.9 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-29 17:38:52,591 - memory_profile6_log - INFO -    179                             

2018-04-29 17:38:52,592 - memory_profile6_log - INFO -    180    495.5 MiB      0.0 MiB       print "model_fit dtypes:\n", model_fit.dtypes

2018-04-29 17:38:52,592 - memory_profile6_log - INFO -    181    495.5 MiB      0.0 MiB       print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-04-29 17:38:52,594 - memory_profile6_log - INFO -    182    495.5 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-29 17:38:52,594 - memory_profile6_log - INFO -    183    495.5 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-29 17:38:52,595 - memory_profile6_log - INFO -    184    495.5 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-04-29 17:38:52,595 - memory_profile6_log - INFO -    185    495.5 MiB      0.0 MiB                                                                                           "topic_id", "user_id"]],

2018-04-29 17:38:52,595 - memory_profile6_log - INFO -    186    513.9 MiB     18.4 MiB                                                     verbose=False)

2018-04-29 17:38:52,595 - memory_profile6_log - INFO -    187                                 # ~~~ filter is general and specific topic ~~~

2018-04-29 17:38:52,595 - memory_profile6_log - INFO -    188                                 # the idea is just we need to rerank every topic according

2018-04-29 17:38:52,596 - memory_profile6_log - INFO -    189                                 # user_id and and is_general by p0_posterior

2018-04-29 17:38:52,596 - memory_profile6_log - INFO -    190    513.9 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-29 17:38:52,596 - memory_profile6_log - INFO -    191    515.8 MiB      1.9 MiB                                     'is_general']].groupby(['topic_id',

2018-04-29 17:38:52,598 - memory_profile6_log - INFO -    192    513.9 MiB     -1.9 MiB                                                             'is_general']

2018-04-29 17:38:52,598 - memory_profile6_log - INFO -    193                                                                                      ).size().to_frame().reset_index()

2018-04-29 17:38:52,602 - memory_profile6_log - INFO -    194                             

2018-04-29 17:38:52,604 - memory_profile6_log - INFO -    195                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-29 17:38:52,605 - memory_profile6_log - INFO -    196                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-29 17:38:52,605 - memory_profile6_log - INFO -    197    513.9 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-29 17:38:52,605 - memory_profile6_log - INFO -    198                             

2018-04-29 17:38:52,607 - memory_profile6_log - INFO -    199                                 # ~ start by provide rank for each topic type ~

2018-04-29 17:38:52,608 - memory_profile6_log - INFO -    200    536.2 MiB     22.3 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-29 17:38:52,608 - memory_profile6_log - INFO -    201    539.8 MiB      3.6 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-29 17:38:52,609 - memory_profile6_log - INFO -    202                             

2018-04-29 17:38:52,609 - memory_profile6_log - INFO -    203                                 # ~ set threshold to filter output

2018-04-29 17:38:52,609 - memory_profile6_log - INFO -    204    539.8 MiB      0.0 MiB       if threshold > 0:

2018-04-29 17:38:52,611 - memory_profile6_log - INFO -    205    539.8 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-29 17:38:52,611 - memory_profile6_log - INFO -    206    539.8 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-29 17:38:52,614 - memory_profile6_log - INFO -    207    539.1 MiB     -0.7 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-29 17:38:52,615 - memory_profile6_log - INFO -    208                             

2018-04-29 17:38:52,618 - memory_profile6_log - INFO -    209    539.1 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 17:38:52,618 - memory_profile6_log - INFO -    210    539.1 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-29 17:38:52,618 - memory_profile6_log - INFO -    211                             

2018-04-29 17:38:52,618 - memory_profile6_log - INFO -    212    539.1 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 17:38:52,618 - memory_profile6_log - INFO -    213                             

2018-04-29 17:38:52,619 - memory_profile6_log - INFO -    214    539.1 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-29 17:38:52,619 - memory_profile6_log - INFO -    215    539.1 MiB      0.0 MiB       del df_dut

2018-04-29 17:38:52,621 - memory_profile6_log - INFO -    216    539.1 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-29 17:38:52,621 - memory_profile6_log - INFO -    217    539.1 MiB      0.0 MiB       del df_dt

2018-04-29 17:38:52,621 - memory_profile6_log - INFO -    218    539.1 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-29 17:38:52,621 - memory_profile6_log - INFO -    219    539.1 MiB      0.0 MiB       del df_input

2018-04-29 17:38:52,621 - memory_profile6_log - INFO -    220    539.1 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-29 17:38:52,625 - memory_profile6_log - INFO -    221    530.3 MiB     -8.8 MiB       del df_input_X

2018-04-29 17:38:52,627 - memory_profile6_log - INFO -    222    530.3 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-29 17:38:52,627 - memory_profile6_log - INFO -    223    530.3 MiB      0.0 MiB       del df_current

2018-04-29 17:38:52,628 - memory_profile6_log - INFO -    224    530.3 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-29 17:38:52,628 - memory_profile6_log - INFO -    225    530.3 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-29 17:38:52,628 - memory_profile6_log - INFO -    226    530.3 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-29 17:38:52,628 - memory_profile6_log - INFO -    227    507.0 MiB    -23.3 MiB       del model_fit

2018-04-29 17:38:52,628 - memory_profile6_log - INFO -    228    507.0 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-29 17:38:52,630 - memory_profile6_log - INFO -    229    507.0 MiB      0.0 MiB       del result

2018-04-29 17:38:52,630 - memory_profile6_log - INFO -    230    507.0 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-29 17:38:52,631 - memory_profile6_log - INFO -    231                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 17:38:52,631 - memory_profile6_log - INFO -    232                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 17:38:52,631 - memory_profile6_log - INFO -    233                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 17:38:52,634 - memory_profile6_log - INFO -    234    507.0 MiB      0.0 MiB       if savetrain:

2018-04-29 17:38:52,635 - memory_profile6_log - INFO -    235    512.5 MiB      5.4 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-29 17:38:52,637 - memory_profile6_log - INFO -    236    512.5 MiB      0.0 MiB           del model_transform

2018-04-29 17:38:52,638 - memory_profile6_log - INFO -    237    512.5 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-29 17:38:52,638 - memory_profile6_log - INFO -    238    512.5 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 17:38:52,638 - memory_profile6_log - INFO -    239                             

2018-04-29 17:38:52,638 - memory_profile6_log - INFO -    240    512.5 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-29 17:38:52,638 - memory_profile6_log - INFO -    241                                     # ~ Place your code to save the training model here ~

2018-04-29 17:38:52,640 - memory_profile6_log - INFO -    242    512.5 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-29 17:38:52,641 - memory_profile6_log - INFO -    243    512.5 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-29 17:38:52,641 - memory_profile6_log - INFO -    244    512.5 MiB      0.0 MiB               if multproc:

2018-04-29 17:38:52,642 - memory_profile6_log - INFO -    245                                             # ~ save transform models ~

2018-04-29 17:38:52,642 - memory_profile6_log - INFO -    246    490.3 MiB    -22.1 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-29 17:38:52,642 - memory_profile6_log - INFO -    247                             

2018-04-29 17:38:52,644 - memory_profile6_log - INFO -    248                                             """

2018-04-29 17:38:52,644 - memory_profile6_log - INFO -    249                                             # ~ save fitted models ~

2018-04-29 17:38:52,644 - memory_profile6_log - INFO -    250                                             logger.info("Saving fitted_models as history...")

2018-04-29 17:38:52,648 - memory_profile6_log - INFO -    251                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 17:38:52,648 - memory_profile6_log - INFO -    252                                             fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-29 17:38:52,648 - memory_profile6_log - INFO -    253                             

2018-04-29 17:38:52,651 - memory_profile6_log - INFO -    254                                             X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-29 17:38:52,651 - memory_profile6_log - INFO -    255                                             logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-29 17:38:52,651 - memory_profile6_log - INFO -    256                                             for ix in range(len(X_split)):

2018-04-29 17:38:52,653 - memory_profile6_log - INFO -    257                                                 logger.info("processing batch-%d", ix)

2018-04-29 17:38:52,653 - memory_profile6_log - INFO -    258                                                 mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-29 17:38:52,654 - memory_profile6_log - INFO -    259                             

2018-04-29 17:38:52,654 - memory_profile6_log - INFO -    260                                             del X_split

2018-04-29 17:38:52,654 - memory_profile6_log - INFO -    261                                             logger.info("deleting X_split...")

2018-04-29 17:38:52,654 - memory_profile6_log - INFO -    262                                             del save_sigma_nt

2018-04-29 17:38:52,655 - memory_profile6_log - INFO -    263                                             logger.info("deleting save_sigma_nt...")

2018-04-29 17:38:52,658 - memory_profile6_log - INFO -    264                                             """

2018-04-29 17:38:52,661 - memory_profile6_log - INFO -    265                             

2018-04-29 17:38:52,661 - memory_profile6_log - INFO -    266    490.3 MiB      0.0 MiB                   del BR

2018-04-29 17:38:52,661 - memory_profile6_log - INFO -    267    490.3 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-29 17:38:52,661 - memory_profile6_log - INFO -    268                             

2018-04-29 17:38:52,663 - memory_profile6_log - INFO -    269                                     elif str(saveto).lower() == "elastic":

2018-04-29 17:38:52,663 - memory_profile6_log - INFO -    270                                         logger.info("Using ElasticSearch as storage...")

2018-04-29 17:38:52,664 - memory_profile6_log - INFO -    271                                         mh.saveElasticS(model_transformsv)

2018-04-29 17:38:52,664 - memory_profile6_log - INFO -    272                             

2018-04-29 17:38:52,664 - memory_profile6_log - INFO -    273                                     # need save sigma_nt for daily train

2018-04-29 17:38:52,664 - memory_profile6_log - INFO -    274                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-29 17:38:52,667 - memory_profile6_log - INFO -    275                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-29 17:38:52,670 - memory_profile6_log - INFO -    276    490.3 MiB      0.0 MiB           if start_date and end_date:

2018-04-29 17:38:52,671 - memory_profile6_log - INFO -    277                                         if not fitby_sigmant:

2018-04-29 17:38:52,671 - memory_profile6_log - INFO -    278                                             logging.info("Saving sigma Nt...")

2018-04-29 17:38:52,671 - memory_profile6_log - INFO -    279                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 17:38:52,673 - memory_profile6_log - INFO -    280                                             save_sigma_nt['start_date'] = start_date

2018-04-29 17:38:52,676 - memory_profile6_log - INFO -    281                                             save_sigma_nt['end_date'] = end_date

2018-04-29 17:38:52,676 - memory_profile6_log - INFO -    282                                             print save_sigma_nt.head(5)

2018-04-29 17:38:52,677 - memory_profile6_log - INFO -    283                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-29 17:38:52,677 - memory_profile6_log - INFO -    284                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-29 17:38:52,677 - memory_profile6_log - INFO -    285    490.3 MiB      0.0 MiB       return

2018-04-29 17:38:52,677 - memory_profile6_log - INFO - 


2018-04-29 17:38:52,677 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-29 18:28:23,822 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 18:28:23,825 - memory_profile6_log - INFO - date_generated: 
2018-04-29 18:28:23,825 - memory_profile6_log - INFO -  
2018-04-29 18:28:23,825 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 18:28:23,826 - memory_profile6_log - INFO - 

2018-04-29 18:28:23,826 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 18:28:23,828 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 18:28:23,828 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 18:28:23,953 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 18:28:23,957 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 18:29:34,921 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 18:29:34,921 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 18:29:34,963 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 18:29:34,963 - memory_profile6_log - INFO - Appending history data...
2018-04-29 18:29:34,964 - memory_profile6_log - INFO - processing batch-0
2018-04-29 18:29:34,966 - memory_profile6_log - INFO - creating list history data...
2018-04-29 18:29:35,007 - memory_profile6_log - INFO - call history data...
2018-04-29 18:30:07,163 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 18:30:07,826 - memory_profile6_log - INFO - processing batch-1
2018-04-29 18:30:07,828 - memory_profile6_log - INFO - creating list history data...
2018-04-29 18:30:07,836 - memory_profile6_log - INFO - call history data...
2018-04-29 18:30:37,776 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 18:30:38,444 - memory_profile6_log - INFO - processing batch-2
2018-04-29 18:30:38,447 - memory_profile6_log - INFO - creating list history data...
2018-04-29 18:30:38,454 - memory_profile6_log - INFO - call history data...
2018-04-29 18:31:07,184 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 18:31:07,842 - memory_profile6_log - INFO - processing batch-3
2018-04-29 18:31:07,842 - memory_profile6_log - INFO - creating list history data...
2018-04-29 18:31:07,851 - memory_profile6_log - INFO - call history data...
2018-04-29 18:31:35,867 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 18:31:36,525 - memory_profile6_log - INFO - processing batch-4
2018-04-29 18:31:36,526 - memory_profile6_log - INFO - creating list history data...
2018-04-29 18:31:36,536 - memory_profile6_log - INFO - call history data...
2018-04-29 18:32:04,362 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 18:32:05,019 - memory_profile6_log - INFO - Appending training data...
2018-04-29 18:32:05,020 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 18:32:05,022 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 18:32:05,023 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 18:32:05,023 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 18:32:05,025 - memory_profile6_log - INFO - ================================================

2018-04-29 18:32:05,026 - memory_profile6_log - INFO -    311     87.0 MiB     87.0 MiB   @profile

2018-04-29 18:32:05,026 - memory_profile6_log - INFO -    312                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 18:32:05,029 - memory_profile6_log - INFO -    313     87.0 MiB      0.0 MiB       bq_client = client

2018-04-29 18:32:05,030 - memory_profile6_log - INFO -    314     87.0 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 18:32:05,030 - memory_profile6_log - INFO -    315                             

2018-04-29 18:32:05,032 - memory_profile6_log - INFO -    316     87.0 MiB      0.0 MiB       datalist = []

2018-04-29 18:32:05,032 - memory_profile6_log - INFO -    317     87.0 MiB      0.0 MiB       datalist_hist = []

2018-04-29 18:32:05,033 - memory_profile6_log - INFO -    318                             

2018-04-29 18:32:05,035 - memory_profile6_log - INFO -    319     87.0 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 18:32:05,036 - memory_profile6_log - INFO -    320    352.0 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 18:32:05,036 - memory_profile6_log - INFO -    321    340.1 MiB    253.0 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 18:32:05,038 - memory_profile6_log - INFO -    322    340.1 MiB      0.0 MiB           if tframe is not None:

2018-04-29 18:32:05,040 - memory_profile6_log - INFO -    323    340.1 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 18:32:05,042 - memory_profile6_log - INFO -    324    347.7 MiB      7.6 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 18:32:05,042 - memory_profile6_log - INFO -    325    347.7 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 18:32:05,043 - memory_profile6_log - INFO -    326    347.7 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 18:32:05,045 - memory_profile6_log - INFO -    327    352.0 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 18:32:05,046 - memory_profile6_log - INFO -    328                                                 # ~ loading history

2018-04-29 18:32:05,046 - memory_profile6_log - INFO -    329                                                 """

2018-04-29 18:32:05,048 - memory_profile6_log - INFO -    330                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 18:32:05,051 - memory_profile6_log - INFO -    331                                                 """

2018-04-29 18:32:05,052 - memory_profile6_log - INFO -    332    351.9 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 18:32:05,052 - memory_profile6_log - INFO -    333                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 18:32:05,055 - memory_profile6_log - INFO -    334    351.9 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 18:32:05,055 - memory_profile6_log - INFO -    335    351.9 MiB      0.6 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 18:32:05,056 - memory_profile6_log - INFO -    336                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 18:32:05,058 - memory_profile6_log - INFO -    337                             

2018-04-29 18:32:05,061 - memory_profile6_log - INFO -    338    351.9 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 18:32:05,062 - memory_profile6_log - INFO -    339    352.0 MiB      3.0 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 18:32:05,063 - memory_profile6_log - INFO -    340                             

2018-04-29 18:32:05,065 - memory_profile6_log - INFO -    341                                                 # me = os.getpid()

2018-04-29 18:32:05,065 - memory_profile6_log - INFO -    342                                                 # kill_proc_tree(me)

2018-04-29 18:32:05,066 - memory_profile6_log - INFO -    343                             

2018-04-29 18:32:05,066 - memory_profile6_log - INFO -    344    352.0 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 18:32:05,069 - memory_profile6_log - INFO -    345    352.0 MiB      0.0 MiB                       for m in h_frame:

2018-04-29 18:32:05,072 - memory_profile6_log - INFO -    346    352.0 MiB      0.0 MiB                           if m is not None:

2018-04-29 18:32:05,075 - memory_profile6_log - INFO -    347    352.0 MiB      0.0 MiB                               if len(m) > 0:

2018-04-29 18:32:05,075 - memory_profile6_log - INFO -    348    352.0 MiB      0.8 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 18:32:05,076 - memory_profile6_log - INFO -    349    352.0 MiB      0.0 MiB                       del h_frame

2018-04-29 18:32:05,078 - memory_profile6_log - INFO -    350    352.0 MiB      0.0 MiB                       del lhistory

2018-04-29 18:32:05,078 - memory_profile6_log - INFO -    351                             

2018-04-29 18:32:05,079 - memory_profile6_log - INFO -    352    352.0 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 18:32:05,082 - memory_profile6_log - INFO -    353    352.0 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 18:32:05,085 - memory_profile6_log - INFO -    354                                     else: 

2018-04-29 18:32:05,085 - memory_profile6_log - INFO -    355                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 18:32:05,088 - memory_profile6_log - INFO -    356    352.0 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 18:32:05,088 - memory_profile6_log - INFO -    357    352.0 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 18:32:05,091 - memory_profile6_log - INFO -    358                             

2018-04-29 18:32:05,092 - memory_profile6_log - INFO -    359    352.0 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 18:32:05,095 - memory_profile6_log - INFO - 


2018-04-29 18:32:06,213 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 18:32:06,282 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0   0.000646        1844.325967        32            1854.325967  10960288  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
1   0.000646        1844.325967        16            1854.325967  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
2   0.000646        1844.325967        16            1854.325967  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
3   0.000646        1844.325967        16            1854.325967  10960288  1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...
4   0.000646        1844.325967        32            1854.325967  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
5        NaN      333823.000000       256          333833.000000  10959239  161129f951435-0d660a18dbcd87-173a7640-100200-1...
6   0.000646        1844.325967        32            1854.325967  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
0   0.000646        1844.325967         8            1854.325967  10960288  16169731ce618-00742c5e307396-5b4d6a36-38400-16...
1   0.000646        1844.325967        16            1854.325967  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
2   0.000646        1844.325967        16            1854.325967  10960288  1626fd160d9114-055d801ae6b39d-66265f05-38400-1...
3   0.000646        1844.325967         8            1854.325967  10960288  161e219cd891c8-0e882943705df4-15290f5f-38400-1...
4   0.000646        1844.325967         8            1854.325967  10960288  1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...
5   0.000646        1844.325967        16            1854.325967  10960288  161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...
0   0.000646        1844.325967         4            1854.325967  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
1   0.000646        1844.325967         4            1854.325967  10960288  1612d88e55ab8-09af9658f30a46-65117b23-38400-16...
2   0.000646        1844.325967         8            1854.325967  10960288  16149f0a1611-00a034775-63446032-38400-16149f0a...
3   0.000646        1844.325967         8            1854.325967  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
4   0.000646        1844.325967         4            1854.325967  10960288  16132f1045f30b-0613b72b33445e-5d640103-38400-1...
5   0.000646        1844.325967         4            1854.325967  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
6   0.000646        1844.325967         4            1854.325967  10960288  16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...
2018-04-29 18:32:06,286 - memory_profile6_log - INFO - 

2018-04-29 18:32:06,295 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 18:32:06,362 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 18:32:06,375 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-29 18:32:40,907 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 18:32:40,910 - memory_profile6_log - INFO - date_generated: 
2018-04-29 18:32:40,910 - memory_profile6_log - INFO -  
2018-04-29 18:32:40,911 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-29 18:32:40,911 - memory_profile6_log - INFO - 

2018-04-29 18:32:40,911 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-29 18:32:40,913 - memory_profile6_log - INFO - using start date: 2018-04-14 00:00:00
2018-04-29 18:32:40,913 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-29 18:32:41,040 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 18:32:41,045 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-29 18:33:51,947 - memory_profile6_log - INFO - size of df: 64.30 MB
2018-04-29 18:33:51,950 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-29 18:33:51,993 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 18:33:51,994 - memory_profile6_log - INFO - Appending history data...
2018-04-29 18:33:51,996 - memory_profile6_log - INFO - processing batch-0
2018-04-29 18:33:51,997 - memory_profile6_log - INFO - creating list history data...
2018-04-29 18:33:52,039 - memory_profile6_log - INFO - call history data...
2018-04-29 18:34:21,138 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 18:34:21,793 - memory_profile6_log - INFO - processing batch-1
2018-04-29 18:34:21,795 - memory_profile6_log - INFO - creating list history data...
2018-04-29 18:34:21,802 - memory_profile6_log - INFO - call history data...
2018-04-29 18:34:49,115 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 18:34:49,828 - memory_profile6_log - INFO - processing batch-2
2018-04-29 18:34:49,829 - memory_profile6_log - INFO - creating list history data...
2018-04-29 18:34:49,836 - memory_profile6_log - INFO - call history data...
2018-04-29 18:35:17,161 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 18:35:17,829 - memory_profile6_log - INFO - processing batch-3
2018-04-29 18:35:17,831 - memory_profile6_log - INFO - creating list history data...
2018-04-29 18:35:17,838 - memory_profile6_log - INFO - call history data...
2018-04-29 18:35:45,269 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 18:35:45,961 - memory_profile6_log - INFO - processing batch-4
2018-04-29 18:35:45,963 - memory_profile6_log - INFO - creating list history data...
2018-04-29 18:35:45,970 - memory_profile6_log - INFO - call history data...
2018-04-29 18:36:13,907 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 18:36:14,581 - memory_profile6_log - INFO - Appending training data...
2018-04-29 18:36:14,582 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 18:36:14,585 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 18:36:14,586 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 18:36:14,588 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 18:36:14,588 - memory_profile6_log - INFO - ================================================

2018-04-29 18:36:14,588 - memory_profile6_log - INFO -    311     86.6 MiB     86.6 MiB   @profile

2018-04-29 18:36:14,589 - memory_profile6_log - INFO -    312                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 18:36:14,592 - memory_profile6_log - INFO -    313     86.6 MiB      0.0 MiB       bq_client = client

2018-04-29 18:36:14,592 - memory_profile6_log - INFO -    314     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 18:36:14,594 - memory_profile6_log - INFO -    315                             

2018-04-29 18:36:14,594 - memory_profile6_log - INFO -    316     86.6 MiB      0.0 MiB       datalist = []

2018-04-29 18:36:14,595 - memory_profile6_log - INFO -    317     86.6 MiB      0.0 MiB       datalist_hist = []

2018-04-29 18:36:14,595 - memory_profile6_log - INFO -    318                             

2018-04-29 18:36:14,596 - memory_profile6_log - INFO -    319     86.6 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 18:36:14,596 - memory_profile6_log - INFO -    320    350.3 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 18:36:14,598 - memory_profile6_log - INFO -    321    340.5 MiB    253.9 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 18:36:14,598 - memory_profile6_log - INFO -    322    340.5 MiB      0.0 MiB           if tframe is not None:

2018-04-29 18:36:14,599 - memory_profile6_log - INFO -    323    340.5 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 18:36:14,599 - memory_profile6_log - INFO -    324    347.0 MiB      6.5 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 18:36:14,602 - memory_profile6_log - INFO -    325    347.0 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 18:36:14,604 - memory_profile6_log - INFO -    326    347.0 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 18:36:14,605 - memory_profile6_log - INFO -    327    350.3 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 18:36:14,605 - memory_profile6_log - INFO -    328                                                 # ~ loading history

2018-04-29 18:36:14,605 - memory_profile6_log - INFO -    329                                                 """

2018-04-29 18:36:14,607 - memory_profile6_log - INFO -    330                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 18:36:14,608 - memory_profile6_log - INFO -    331                                                 """

2018-04-29 18:36:14,608 - memory_profile6_log - INFO -    332    350.2 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 18:36:14,608 - memory_profile6_log - INFO -    333                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 18:36:14,609 - memory_profile6_log - INFO -    334    350.2 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 18:36:14,609 - memory_profile6_log - INFO -    335    350.2 MiB      0.6 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 18:36:14,611 - memory_profile6_log - INFO -    336                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 18:36:14,614 - memory_profile6_log - INFO -    337                             

2018-04-29 18:36:14,615 - memory_profile6_log - INFO -    338    350.2 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 18:36:14,615 - memory_profile6_log - INFO -    339    350.3 MiB      1.9 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 18:36:14,617 - memory_profile6_log - INFO -    340                             

2018-04-29 18:36:14,617 - memory_profile6_log - INFO -    341                                                 # me = os.getpid()

2018-04-29 18:36:14,618 - memory_profile6_log - INFO -    342                                                 # kill_proc_tree(me)

2018-04-29 18:36:14,618 - memory_profile6_log - INFO -    343                             

2018-04-29 18:36:14,619 - memory_profile6_log - INFO -    344    350.3 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 18:36:14,619 - memory_profile6_log - INFO -    345    350.3 MiB      0.0 MiB                       for m in h_frame:

2018-04-29 18:36:14,621 - memory_profile6_log - INFO -    346    350.3 MiB      0.0 MiB                           if m is not None:

2018-04-29 18:36:14,621 - memory_profile6_log - INFO -    347    350.3 MiB      0.0 MiB                               if len(m) > 0:

2018-04-29 18:36:14,621 - memory_profile6_log - INFO -    348    350.3 MiB      0.7 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 18:36:14,625 - memory_profile6_log - INFO -    349    350.3 MiB      0.0 MiB                       del h_frame

2018-04-29 18:36:14,625 - memory_profile6_log - INFO -    350    350.3 MiB      0.0 MiB                       del lhistory

2018-04-29 18:36:14,625 - memory_profile6_log - INFO -    351                             

2018-04-29 18:36:14,627 - memory_profile6_log - INFO -    352    350.3 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 18:36:14,628 - memory_profile6_log - INFO -    353    350.3 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 18:36:14,628 - memory_profile6_log - INFO -    354                                     else: 

2018-04-29 18:36:14,628 - memory_profile6_log - INFO -    355                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 18:36:14,630 - memory_profile6_log - INFO -    356    350.3 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 18:36:14,631 - memory_profile6_log - INFO -    357    350.3 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 18:36:14,631 - memory_profile6_log - INFO -    358                             

2018-04-29 18:36:14,632 - memory_profile6_log - INFO -    359    350.3 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 18:36:14,635 - memory_profile6_log - INFO - 


2018-04-29 18:36:15,734 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 18:36:15,805 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0   0.000646        1844.325967        16            1854.325967  10960288  1613abe2993109-0f9dea18faf5f4-7f3a3529-3d10d-1...
1   0.000646        1844.325967        16            1854.325967  10960288  1612d89d26e160-06bda131a544f7-5c1d251c-38400-1...
2   0.000646        1844.325967        32            1854.325967  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
3        NaN      333823.000000       256          333833.000000  10959239  161129f951435-0d660a18dbcd87-173a7640-100200-1...
4   0.000646        1844.325967        32            1854.325967  10960288  16140595aef60-0f425b909e43ea-5768397b-100200-1...
5   0.000646        1844.325967        32            1854.325967  10960288  161890d29c1d5-0e69b9c4731723-2406423c-38400-16...
6   0.000646        1844.325967        16            1854.325967  10960288  16290be16f412b-0b174766972ba1-78227b65-38400-1...
0   0.000646        1844.325967         8            1854.325967  10960288  1610e2b5ce23f5-0c8171c37c4a46-32607402-13c680-...
1   0.000646        1844.325967         8            1854.325967  10960288  161e219cd891c8-0e882943705df4-15290f5f-38400-1...
2   0.000646        1844.325967        16            1854.325967  10960288  1626fd160d9114-055d801ae6b39d-66265f05-38400-1...
3   0.000646        1844.325967        16            1854.325967  10960288  161618b9b3a35-08578a4b7b44a2-50157622-2c880-16...
4   0.000646        1844.325967         8            1854.325967  10960288  16169731ce618-00742c5e307396-5b4d6a36-38400-16...
5   0.000646        1844.325967        16            1854.325967  10960288  1612d37eb2c191-086fe83c132786-7b5d495a-38400-1...
0   0.000646        1844.325967         4            1854.325967  10960288  1612f9c94a0d3-06b13440462307-6a6c6934-38400-16...
1   0.000646        1844.325967         4            1854.325967  10960288  16132f1045f30b-0613b72b33445e-5d640103-38400-1...
2   0.000646        1844.325967         8            1854.325967  10960288  16149f0a1611-00a034775-63446032-38400-16149f0a...
3   0.000646        1844.325967         4            1854.325967  10960288  16235c3c6cb6-07b114d72-27544e12-38400-16235c3c...
4   0.000646        1844.325967         4            1854.325967  10960288  16152b41f7c8f-042e6e322-114d0b42-38400-16152b4...
5   0.000646        1844.325967         8            1854.325967  10960288  1613f4d321b10-00841d4071241b-42686c23-38400-16...
6   0.000646        1844.325967         4            1854.325967  10960288  1612d88e55ab8-09af9658f30a46-65117b23-38400-16...
2018-04-29 18:36:15,806 - memory_profile6_log - INFO - 

2018-04-29 18:36:15,816 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 18:36:15,887 - memory_profile6_log - INFO - size of big_frame: 64.30 MB
2018-04-29 18:36:15,898 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-04-29 18:36:57,322 - memory_profile6_log - INFO - size of df: 36.25 MB
2018-04-29 18:36:57,325 - memory_profile6_log - INFO - getting total: 148182 training data(current date interest)
2018-04-29 18:36:57,381 - memory_profile6_log - INFO - size of current_frame: 37.38 MB
2018-04-29 18:36:57,381 - memory_profile6_log - INFO - loading time of: 402177 total genuine-current interest data ~ take 256.363s
2018-04-29 18:36:57,388 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 18:36:57,388 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 18:36:57,390 - memory_profile6_log - INFO - ================================================

2018-04-29 18:36:57,391 - memory_profile6_log - INFO -    361     86.5 MiB     86.5 MiB   @profile

2018-04-29 18:36:57,392 - memory_profile6_log - INFO -    362                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 18:36:57,394 - memory_profile6_log - INFO -    363     86.6 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 18:36:57,394 - memory_profile6_log - INFO -    364     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 18:36:57,394 - memory_profile6_log - INFO -    365                             

2018-04-29 18:36:57,395 - memory_profile6_log - INFO -    366                                 # ~~~ Begin collecting data ~~~

2018-04-29 18:36:57,397 - memory_profile6_log - INFO -    367     86.6 MiB      0.0 MiB       t0 = time.time()

2018-04-29 18:36:57,397 - memory_profile6_log - INFO -    368    350.3 MiB    263.6 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 18:36:57,398 - memory_profile6_log - INFO -    369    350.3 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 18:36:57,398 - memory_profile6_log - INFO -    370                                     logger.info("Training cannot be empty..")

2018-04-29 18:36:57,398 - memory_profile6_log - INFO -    371                                     return False

2018-04-29 18:36:57,398 - memory_profile6_log - INFO -    372    348.5 MiB     -1.8 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 18:36:57,400 - memory_profile6_log - INFO -    373    348.6 MiB      0.1 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 18:36:57,403 - memory_profile6_log - INFO -    374    348.6 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 18:36:57,404 - memory_profile6_log - INFO -    375                             

2018-04-29 18:36:57,404 - memory_profile6_log - INFO -    376    354.4 MiB      5.8 MiB       big_frame = pd.concat(datalist)

2018-04-29 18:36:57,404 - memory_profile6_log - INFO -    377    354.4 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 18:36:57,404 - memory_profile6_log - INFO -    378    348.6 MiB     -5.8 MiB       del datalist

2018-04-29 18:36:57,405 - memory_profile6_log - INFO -    379                             

2018-04-29 18:36:57,405 - memory_profile6_log - INFO -    380    348.6 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 18:36:57,407 - memory_profile6_log - INFO -    381                             

2018-04-29 18:36:57,407 - memory_profile6_log - INFO -    382                                 # ~ get current news interest ~

2018-04-29 18:36:57,407 - memory_profile6_log - INFO -    383    348.6 MiB      0.0 MiB       if not cd:

2018-04-29 18:36:57,407 - memory_profile6_log - INFO -    384    348.6 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-04-29 18:36:57,407 - memory_profile6_log - INFO -    385    355.2 MiB      6.6 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 18:36:57,408 - memory_profile6_log - INFO -    386    355.2 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 18:36:57,408 - memory_profile6_log - INFO -    387                                 else:

2018-04-29 18:36:57,410 - memory_profile6_log - INFO -    388                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 18:36:57,410 - memory_profile6_log - INFO -    389                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 18:36:57,410 - memory_profile6_log - INFO -    390                             

2018-04-29 18:36:57,411 - memory_profile6_log - INFO -    391                                     # safe handling of query parameter

2018-04-29 18:36:57,411 - memory_profile6_log - INFO -    392                                     query_params = [

2018-04-29 18:36:57,414 - memory_profile6_log - INFO -    393                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 18:36:57,415 - memory_profile6_log - INFO -    394                                     ]

2018-04-29 18:36:57,417 - memory_profile6_log - INFO -    395                             

2018-04-29 18:36:57,417 - memory_profile6_log - INFO -    396                                     job_config.query_parameters = query_params

2018-04-29 18:36:57,417 - memory_profile6_log - INFO -    397                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 18:36:57,417 - memory_profile6_log - INFO -    398                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 18:36:57,418 - memory_profile6_log - INFO -    399                             

2018-04-29 18:36:57,420 - memory_profile6_log - INFO -    400    356.3 MiB      1.1 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 18:36:57,421 - memory_profile6_log - INFO -    401    356.3 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 18:36:57,421 - memory_profile6_log - INFO -    402    356.4 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 18:36:57,421 - memory_profile6_log - INFO -    403    356.4 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 18:36:57,421 - memory_profile6_log - INFO -    404    356.4 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 18:36:57,423 - memory_profile6_log - INFO -    405    356.4 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 18:36:57,423 - memory_profile6_log - INFO -    406                             

2018-04-29 18:36:57,426 - memory_profile6_log - INFO -    407    356.4 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 18:36:57,427 - memory_profile6_log - INFO - 


2018-04-29 18:36:57,430 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 18:36:57,463 - memory_profile6_log - INFO - train on: 253995 total genuine interest data(D(u, t))
2018-04-29 18:36:57,463 - memory_profile6_log - INFO - transform on: 148182 total current data(D(t))
2018-04-29 18:36:57,464 - memory_profile6_log - INFO - apply on: 5000 total history...)
2018-04-29 18:36:57,684 - memory_profile6_log - INFO - len of uniques_fit_hist:5000
2018-04-29 18:36:57,690 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:3753
2018-04-29 18:36:58,499 - memory_profile6_log - INFO - Len of model_fit: 253995
2018-04-29 18:36:58,500 - memory_profile6_log - INFO - Len of df_dut: 253995
2018-04-29 18:36:58,651 - memory_profile6_log - INFO - model_fit dtypes:

2018-04-29 18:36:58,655 - memory_profile6_log - INFO - date              datetime64[ns, UTC]
user_id                        object
topic_id                       object
num_x                           int64
num_y                           int64
is_general                       bool
date_all_click                  int64
Ntotal                          int64
sigma_Nt                        int64
joinprob_ci                   float64
p_cat_ci                      float64
posterior                     float64
dtype: object
2018-04-29 18:36:58,657 - memory_profile6_log - INFO - 

2018-04-29 18:36:58,658 - memory_profile6_log - INFO - fitted_models_hist dtypes:

2018-04-29 18:36:58,664 - memory_profile6_log - INFO - p0_cat_ci                float64
pt_posterior_x_Nt        float64
sigma_Nt                   int64
smoothed_pt_posterior    float64
topic_id                  object
user_id                   object
dtype: object
2018-04-29 18:36:58,665 - memory_profile6_log - INFO - 

2018-04-29 18:36:59,122 - memory_profile6_log - INFO - Len of fitted_models on main class: 253995
2018-04-29 18:36:59,124 - memory_profile6_log - INFO - 

2018-04-29 18:36:59,125 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-29 18:36:59,125 - memory_profile6_log - INFO - 

2018-04-29 18:36:59,127 - memory_profile6_log - INFO - Fitted models before concat:

2018-04-29 18:36:59,148 - memory_profile6_log - INFO -                                 user_id    topic_id  pt_posterior_x_Nt  smoothed_pt_posterior
0  107e02cd-54af-40fd-a602-e6105250ae8c  1046474724         951.062678             961.062678
1  107e02cd-54af-40fd-a602-e6105250ae8c  1060896301         673.030242             683.030242
2  107e02cd-54af-40fd-a602-e6105250ae8c   188400148          72.193555              82.193555
3  107e02cd-54af-40fd-a602-e6105250ae8c    22553543          35.980922              45.980922
4  107e02cd-54af-40fd-a602-e6105250ae8c    22596701          61.042850              71.042850
2018-04-29 18:36:59,148 - memory_profile6_log - INFO - 

2018-04-29 18:36:59,150 - memory_profile6_log - INFO - len of current fitted models: 253995
2018-04-29 18:36:59,151 - memory_profile6_log - INFO - 

2018-04-29 18:36:59,151 - memory_profile6_log - INFO - len of history fitted models: 5000
2018-04-29 18:36:59,153 - memory_profile6_log - INFO - 

2018-04-29 18:36:59,187 - memory_profile6_log - INFO - len of fitted models after concat: 258995
2018-04-29 18:36:59,188 - memory_profile6_log - INFO - 

2018-04-29 18:36:59,190 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-29 18:36:59,190 - memory_profile6_log - INFO - 

2018-04-29 18:36:59,361 - memory_profile6_log - INFO - Fitted models after concat:

2018-04-29 18:36:59,380 - memory_profile6_log - INFO -                                 user_id    topic_id  pt_posterior_x_Nt
0  107e02cd-54af-40fd-a602-e6105250ae8c  1046474724         951.062678
1  107e02cd-54af-40fd-a602-e6105250ae8c  1060896301         673.030242
2  107e02cd-54af-40fd-a602-e6105250ae8c   188400148          72.193555
3  107e02cd-54af-40fd-a602-e6105250ae8c    22553543          35.980922
4  107e02cd-54af-40fd-a602-e6105250ae8c    22596701          61.042850
2018-04-29 18:36:59,381 - memory_profile6_log - INFO - 

2018-04-29 18:36:59,400 - memory_profile6_log - INFO - Fitted models after concat-simplify:

2018-04-29 18:36:59,421 - memory_profile6_log - INFO -                                 user_id    topic_id  pt_posterior_x_Nt  smoothed_pt_posterior
0  107e02cd-54af-40fd-a602-e6105250ae8c  1046474724         951.062678             961.062678
1  107e02cd-54af-40fd-a602-e6105250ae8c  1060896301         673.030242             683.030242
2  107e02cd-54af-40fd-a602-e6105250ae8c   188400148          72.193555              82.193555
3  107e02cd-54af-40fd-a602-e6105250ae8c    22553543          35.980922              45.980922
4  107e02cd-54af-40fd-a602-e6105250ae8c    22596701          61.042850              71.042850
2018-04-29 18:36:59,424 - memory_profile6_log - INFO - 

2018-04-29 18:36:59,424 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 253995
2018-04-29 18:36:59,427 - memory_profile6_log - INFO - 

2018-04-29 18:36:59,516 - memory_profile6_log - INFO -                                 user_id    topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci
0  107e02cd-54af-40fd-a602-e6105250ae8c  1046474724         951.062678             961.062678        NaN
1  107e02cd-54af-40fd-a602-e6105250ae8c  1060896301         673.030242             683.030242        NaN
2  107e02cd-54af-40fd-a602-e6105250ae8c   188400148          72.193555              82.193555   0.006656
3  107e02cd-54af-40fd-a602-e6105250ae8c    22553543          35.980922              45.980922   0.121529
4  107e02cd-54af-40fd-a602-e6105250ae8c    22596701          61.042850              71.042850   0.022806
5  107e02cd-54af-40fd-a602-e6105250ae8c    22601470          16.743893              26.743893   0.054982
6  107e02cd-54af-40fd-a602-e6105250ae8c    22617325         569.663823             579.663823   0.000579
7  107e02cd-54af-40fd-a602-e6105250ae8c    22661796          78.768995              88.768995   0.022598
8  107e02cd-54af-40fd-a602-e6105250ae8c    27312625         287.778448             297.778448   0.001685
9  107e02cd-54af-40fd-a602-e6105250ae8c    27313228        1290.552835            1300.552835   0.000500
2018-04-29 18:36:59,517 - memory_profile6_log - INFO - 

2018-04-29 18:37:41,017 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-29 18:37:41,046 - memory_profile6_log - INFO - Total train time: 43.584s
2018-04-29 18:37:41,048 - memory_profile6_log - INFO - memory left before cleaning: 76.700 percent memory...
2018-04-29 18:37:41,049 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-29 18:37:41,051 - memory_profile6_log - INFO - deleting df_dut...
2018-04-29 18:37:41,052 - memory_profile6_log - INFO - deleting df_dt...
2018-04-29 18:37:41,052 - memory_profile6_log - INFO - deleting df_input...
2018-04-29 18:37:41,059 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-29 18:37:41,059 - memory_profile6_log - INFO - deleting df_current...
2018-04-29 18:37:41,061 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-29 18:37:41,072 - memory_profile6_log - INFO - deleting model_fit...
2018-04-29 18:37:41,072 - memory_profile6_log - INFO - deleting result...
2018-04-29 18:37:41,088 - memory_profile6_log - INFO - deleting model_transform...
2018-04-29 18:37:41,088 - memory_profile6_log - INFO - memory left after cleaning: 76.500 percent memory...
2018-04-29 18:37:41,091 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-29 18:37:41,092 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-29 18:37:41,263 - memory_profile6_log - INFO - deleting BR...
2018-04-29 18:37:41,267 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 18:37:41,269 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 18:37:41,269 - memory_profile6_log - INFO - ================================================

2018-04-29 18:37:41,269 - memory_profile6_log - INFO -    113    356.3 MiB    356.3 MiB   @profile

2018-04-29 18:37:41,269 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-29 18:37:41,270 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-29 18:37:41,270 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-29 18:37:41,273 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-29 18:37:41,275 - memory_profile6_log - INFO -    118                                      saveto="datastore", fitted_models_hist=None):

2018-04-29 18:37:41,276 - memory_profile6_log - INFO -    119                                 """

2018-04-29 18:37:41,276 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-29 18:37:41,276 - memory_profile6_log - INFO -    121                                 """

2018-04-29 18:37:41,276 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-29 18:37:41,278 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-29 18:37:41,278 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-29 18:37:41,279 - memory_profile6_log - INFO -    125    356.3 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-29 18:37:41,279 - memory_profile6_log - INFO -    126    364.1 MiB      7.9 MiB       df_dut = df_input.copy(deep=True)

2018-04-29 18:37:41,279 - memory_profile6_log - INFO -    127    364.1 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 18:37:41,279 - memory_profile6_log - INFO -    128                             

2018-04-29 18:37:41,280 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-29 18:37:41,280 - memory_profile6_log - INFO -    130    368.8 MiB      4.7 MiB       df_dt = df_current.copy(deep=True)

2018-04-29 18:37:41,280 - memory_profile6_log - INFO -    131    368.8 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 18:37:41,285 - memory_profile6_log - INFO -    132                             

2018-04-29 18:37:41,285 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-29 18:37:41,286 - memory_profile6_log - INFO -    134    368.8 MiB      0.0 MiB       t0 = time.time()

2018-04-29 18:37:41,286 - memory_profile6_log - INFO -    135    368.8 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-29 18:37:41,286 - memory_profile6_log - INFO -    136    368.8 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-29 18:37:41,288 - memory_profile6_log - INFO -    137    368.8 MiB      0.0 MiB       logger.info("apply on: %d total history...)", len(df_hist))

2018-04-29 18:37:41,288 - memory_profile6_log - INFO -    138                             

2018-04-29 18:37:41,289 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-29 18:37:41,289 - memory_profile6_log - INFO -    140    368.8 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-29 18:37:41,289 - memory_profile6_log - INFO -    141                             

2018-04-29 18:37:41,289 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-29 18:37:41,289 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-29 18:37:41,289 - memory_profile6_log - INFO -    144    371.5 MiB      2.8 MiB       NB = BR.processX(df_dut)

2018-04-29 18:37:41,290 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-29 18:37:41,290 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-29 18:37:41,290 - memory_profile6_log - INFO -    147    381.4 MiB      9.8 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-29 18:37:41,292 - memory_profile6_log - INFO -    148                                 """

2018-04-29 18:37:41,292 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-29 18:37:41,292 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-29 18:37:41,292 - memory_profile6_log - INFO -    151                                 """

2018-04-29 18:37:41,292 - memory_profile6_log - INFO -    152    381.4 MiB      0.0 MiB       fitby_sigmant = True

2018-04-29 18:37:41,296 - memory_profile6_log - INFO -    153    381.4 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-29 18:37:41,296 - memory_profile6_log - INFO -    154    381.4 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-29 18:37:41,298 - memory_profile6_log - INFO -    155    391.1 MiB      9.7 MiB                            'is_general']]

2018-04-29 18:37:41,299 - memory_profile6_log - INFO -    156                                 # get sigma_Nt from fitted_models_hist

2018-04-29 18:37:41,299 - memory_profile6_log - INFO -    157    391.3 MiB      0.2 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-04-29 18:37:41,299 - memory_profile6_log - INFO -    158    391.3 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-04-29 18:37:41,301 - memory_profile6_log - INFO -    159    391.4 MiB      0.2 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-04-29 18:37:41,301 - memory_profile6_log - INFO -    160    391.4 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-04-29 18:37:41,301 - memory_profile6_log - INFO -    161                             

2018-04-29 18:37:41,302 - memory_profile6_log - INFO -    162                                 # begin fit

2018-04-29 18:37:41,302 - memory_profile6_log - INFO -    163    391.4 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-29 18:37:41,302 - memory_profile6_log - INFO -    164    391.4 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-29 18:37:41,302 - memory_profile6_log - INFO -    165    419.4 MiB     27.9 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-04-29 18:37:41,302 - memory_profile6_log - INFO -    166    419.4 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-29 18:37:41,303 - memory_profile6_log - INFO -    167    419.4 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-29 18:37:41,303 - memory_profile6_log - INFO -    168                             

2018-04-29 18:37:41,303 - memory_profile6_log - INFO -    169                                 # ~~ and Transform ~~

2018-04-29 18:37:41,305 - memory_profile6_log - INFO -    170                                 #   handling current news interest == current date

2018-04-29 18:37:41,305 - memory_profile6_log - INFO -    171    419.4 MiB      0.0 MiB       if df_dt.empty:

2018-04-29 18:37:41,305 - memory_profile6_log - INFO -    172                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-29 18:37:41,309 - memory_profile6_log - INFO -    173                                     return None

2018-04-29 18:37:41,309 - memory_profile6_log - INFO -    174    420.0 MiB      0.6 MiB       NB = BR.processX(df_dt)

2018-04-29 18:37:41,311 - memory_profile6_log - INFO -    175    426.8 MiB      6.8 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-29 18:37:41,311 - memory_profile6_log - INFO -    176                             

2018-04-29 18:37:41,311 - memory_profile6_log - INFO -    177    426.8 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-29 18:37:41,312 - memory_profile6_log - INFO -    178    422.8 MiB     -4.0 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-29 18:37:41,312 - memory_profile6_log - INFO -    179                             

2018-04-29 18:37:41,312 - memory_profile6_log - INFO -    180    422.8 MiB      0.0 MiB       print "model_fit dtypes:\n", model_fit.dtypes

2018-04-29 18:37:41,312 - memory_profile6_log - INFO -    181    422.8 MiB      0.0 MiB       print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-04-29 18:37:41,312 - memory_profile6_log - INFO -    182    422.8 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-29 18:37:41,313 - memory_profile6_log - INFO -    183    422.8 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-29 18:37:41,313 - memory_profile6_log - INFO -    184    422.8 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-04-29 18:37:41,313 - memory_profile6_log - INFO -    185    422.8 MiB      0.0 MiB                                                                                           "topic_id", "user_id"]],

2018-04-29 18:37:41,315 - memory_profile6_log - INFO -    186    441.1 MiB     18.3 MiB                                                     verbose=False)

2018-04-29 18:37:41,315 - memory_profile6_log - INFO -    187                                 # ~~~ filter is general and specific topic ~~~

2018-04-29 18:37:41,315 - memory_profile6_log - INFO -    188                                 # the idea is just we need to rerank every topic according

2018-04-29 18:37:41,315 - memory_profile6_log - INFO -    189                                 # user_id and and is_general by p0_posterior

2018-04-29 18:37:41,315 - memory_profile6_log - INFO -    190    441.1 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-29 18:37:41,315 - memory_profile6_log - INFO -    191    443.0 MiB      1.9 MiB                                     'is_general']].groupby(['topic_id',

2018-04-29 18:37:41,316 - memory_profile6_log - INFO -    192    441.1 MiB     -1.9 MiB                                                             'is_general']

2018-04-29 18:37:41,316 - memory_profile6_log - INFO -    193                                                                                      ).size().to_frame().reset_index()

2018-04-29 18:37:41,318 - memory_profile6_log - INFO -    194                             

2018-04-29 18:37:41,321 - memory_profile6_log - INFO -    195                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-29 18:37:41,322 - memory_profile6_log - INFO -    196                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-29 18:37:41,322 - memory_profile6_log - INFO -    197    441.1 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-29 18:37:41,323 - memory_profile6_log - INFO -    198                             

2018-04-29 18:37:41,323 - memory_profile6_log - INFO -    199                                 # ~ start by provide rank for each topic type ~

2018-04-29 18:37:41,325 - memory_profile6_log - INFO -    200    463.3 MiB     22.1 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-29 18:37:41,325 - memory_profile6_log - INFO -    201    465.7 MiB      2.5 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-29 18:37:41,325 - memory_profile6_log - INFO -    202                             

2018-04-29 18:37:41,325 - memory_profile6_log - INFO -    203                                 # ~ set threshold to filter output

2018-04-29 18:37:41,326 - memory_profile6_log - INFO -    204    465.7 MiB      0.0 MiB       if threshold > 0:

2018-04-29 18:37:41,326 - memory_profile6_log - INFO -    205    465.7 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-29 18:37:41,326 - memory_profile6_log - INFO -    206    465.7 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-29 18:37:41,328 - memory_profile6_log - INFO -    207    464.2 MiB     -1.6 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-29 18:37:41,328 - memory_profile6_log - INFO -    208                             

2018-04-29 18:37:41,328 - memory_profile6_log - INFO -    209    464.2 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 18:37:41,328 - memory_profile6_log - INFO -    210    464.2 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-29 18:37:41,328 - memory_profile6_log - INFO -    211                             

2018-04-29 18:37:41,332 - memory_profile6_log - INFO -    212    464.2 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 18:37:41,332 - memory_profile6_log - INFO -    213                             

2018-04-29 18:37:41,334 - memory_profile6_log - INFO -    214    464.2 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-29 18:37:41,334 - memory_profile6_log - INFO -    215    464.2 MiB      0.0 MiB       del df_dut

2018-04-29 18:37:41,335 - memory_profile6_log - INFO -    216    464.2 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-29 18:37:41,335 - memory_profile6_log - INFO -    217    464.2 MiB      0.0 MiB       del df_dt

2018-04-29 18:37:41,335 - memory_profile6_log - INFO -    218    464.2 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-29 18:37:41,335 - memory_profile6_log - INFO -    219    464.2 MiB      0.0 MiB       del df_input

2018-04-29 18:37:41,335 - memory_profile6_log - INFO -    220    464.2 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-29 18:37:41,336 - memory_profile6_log - INFO -    221    458.5 MiB     -5.7 MiB       del df_input_X

2018-04-29 18:37:41,336 - memory_profile6_log - INFO -    222    458.5 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-29 18:37:41,336 - memory_profile6_log - INFO -    223    458.5 MiB      0.0 MiB       del df_current

2018-04-29 18:37:41,338 - memory_profile6_log - INFO -    224    458.5 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-29 18:37:41,338 - memory_profile6_log - INFO -    225    458.5 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-29 18:37:41,338 - memory_profile6_log - INFO -    226    458.5 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-29 18:37:41,338 - memory_profile6_log - INFO -    227    435.0 MiB    -23.5 MiB       del model_fit

2018-04-29 18:37:41,338 - memory_profile6_log - INFO -    228    435.0 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-29 18:37:41,338 - memory_profile6_log - INFO -    229    435.0 MiB      0.0 MiB       del result

2018-04-29 18:37:41,339 - memory_profile6_log - INFO -    230    435.0 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-29 18:37:41,339 - memory_profile6_log - INFO -    231                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 18:37:41,341 - memory_profile6_log - INFO -    232                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 18:37:41,344 - memory_profile6_log - INFO -    233                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 18:37:41,345 - memory_profile6_log - INFO -    234    435.0 MiB      0.0 MiB       if savetrain:

2018-04-29 18:37:41,345 - memory_profile6_log - INFO -    235    440.2 MiB      5.2 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-29 18:37:41,345 - memory_profile6_log - INFO -    236    440.2 MiB      0.0 MiB           del model_transform

2018-04-29 18:37:41,346 - memory_profile6_log - INFO -    237    440.2 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-29 18:37:41,346 - memory_profile6_log - INFO -    238    440.2 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 18:37:41,346 - memory_profile6_log - INFO -    239                             

2018-04-29 18:37:41,348 - memory_profile6_log - INFO -    240    440.2 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-29 18:37:41,348 - memory_profile6_log - INFO -    241                                     # ~ Place your code to save the training model here ~

2018-04-29 18:37:41,348 - memory_profile6_log - INFO -    242    440.2 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-29 18:37:41,348 - memory_profile6_log - INFO -    243    440.2 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-29 18:37:41,348 - memory_profile6_log - INFO -    244    440.2 MiB      0.0 MiB               if multproc:

2018-04-29 18:37:41,349 - memory_profile6_log - INFO -    245                                             # ~ save transform models ~

2018-04-29 18:37:41,349 - memory_profile6_log - INFO -    246    419.1 MiB    -21.0 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-29 18:37:41,349 - memory_profile6_log - INFO -    247                             

2018-04-29 18:37:41,351 - memory_profile6_log - INFO -    248                                             """

2018-04-29 18:37:41,351 - memory_profile6_log - INFO -    249                                             # ~ save fitted models ~

2018-04-29 18:37:41,351 - memory_profile6_log - INFO -    250                                             logger.info("Saving fitted_models as history...")

2018-04-29 18:37:41,354 - memory_profile6_log - INFO -    251                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 18:37:41,355 - memory_profile6_log - INFO -    252                                             fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-29 18:37:41,355 - memory_profile6_log - INFO -    253                             

2018-04-29 18:37:41,357 - memory_profile6_log - INFO -    254                                             X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-29 18:37:41,358 - memory_profile6_log - INFO -    255                                             logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-29 18:37:41,358 - memory_profile6_log - INFO -    256                                             for ix in range(len(X_split)):

2018-04-29 18:37:41,359 - memory_profile6_log - INFO -    257                                                 logger.info("processing batch-%d", ix)

2018-04-29 18:37:41,361 - memory_profile6_log - INFO -    258                                                 mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-29 18:37:41,361 - memory_profile6_log - INFO -    259                             

2018-04-29 18:37:41,362 - memory_profile6_log - INFO -    260                                             del X_split

2018-04-29 18:37:41,365 - memory_profile6_log - INFO -    261                                             logger.info("deleting X_split...")

2018-04-29 18:37:41,365 - memory_profile6_log - INFO -    262                                             del save_sigma_nt

2018-04-29 18:37:41,367 - memory_profile6_log - INFO -    263                                             logger.info("deleting save_sigma_nt...")

2018-04-29 18:37:41,367 - memory_profile6_log - INFO -    264                                             """

2018-04-29 18:37:41,367 - memory_profile6_log - INFO -    265                             

2018-04-29 18:37:41,368 - memory_profile6_log - INFO -    266    419.1 MiB      0.0 MiB                   del BR

2018-04-29 18:37:41,368 - memory_profile6_log - INFO -    267    419.1 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-29 18:37:41,368 - memory_profile6_log - INFO -    268                             

2018-04-29 18:37:41,368 - memory_profile6_log - INFO -    269                                     elif str(saveto).lower() == "elastic":

2018-04-29 18:37:41,368 - memory_profile6_log - INFO -    270                                         logger.info("Using ElasticSearch as storage...")

2018-04-29 18:37:41,368 - memory_profile6_log - INFO -    271                                         mh.saveElasticS(model_transformsv)

2018-04-29 18:37:41,369 - memory_profile6_log - INFO -    272                             

2018-04-29 18:37:41,369 - memory_profile6_log - INFO -    273                                     # need save sigma_nt for daily train

2018-04-29 18:37:41,369 - memory_profile6_log - INFO -    274                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-29 18:37:41,371 - memory_profile6_log - INFO -    275                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-29 18:37:41,371 - memory_profile6_log - INFO -    276    419.1 MiB      0.0 MiB           if start_date and end_date:

2018-04-29 18:37:41,371 - memory_profile6_log - INFO -    277                                         if not fitby_sigmant:

2018-04-29 18:37:41,371 - memory_profile6_log - INFO -    278                                             logging.info("Saving sigma Nt...")

2018-04-29 18:37:41,371 - memory_profile6_log - INFO -    279                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 18:37:41,371 - memory_profile6_log - INFO -    280                                             save_sigma_nt['start_date'] = start_date

2018-04-29 18:37:41,375 - memory_profile6_log - INFO -    281                                             save_sigma_nt['end_date'] = end_date

2018-04-29 18:37:41,375 - memory_profile6_log - INFO -    282                                             print save_sigma_nt.head(5)

2018-04-29 18:37:41,377 - memory_profile6_log - INFO -    283                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-29 18:37:41,377 - memory_profile6_log - INFO -    284                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-29 18:37:41,378 - memory_profile6_log - INFO -    285    419.1 MiB      0.0 MiB       return

2018-04-29 18:37:41,378 - memory_profile6_log - INFO - 


2018-04-29 18:37:41,378 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-29 19:12:24,809 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 19:12:24,812 - memory_profile6_log - INFO - date_generated: 
2018-04-29 19:12:24,812 - memory_profile6_log - INFO -  
2018-04-29 19:12:24,812 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 9, 0, 0)]
2018-04-29 19:12:24,813 - memory_profile6_log - INFO - 

2018-04-29 19:12:24,813 - memory_profile6_log - INFO - using current date: 2018-04-10
2018-04-29 19:12:24,815 - memory_profile6_log - INFO - using start date: 2018-04-09 00:00:00
2018-04-29 19:12:24,815 - memory_profile6_log - INFO - using end date: 2018-04-09
2018-04-29 19:12:24,950 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 19:12:24,953 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-09
2018-04-29 19:14:22,890 - memory_profile6_log - INFO - size of df: 102.25 MB
2018-04-29 19:14:22,891 - memory_profile6_log - INFO - getting total: 403532 training data(genuine interest) for date: 2018-04-09
2018-04-29 19:14:22,947 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 19:14:22,947 - memory_profile6_log - INFO - Appending history data...
2018-04-29 19:14:22,948 - memory_profile6_log - INFO - processing batch-0
2018-04-29 19:14:22,950 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:14:22,990 - memory_profile6_log - INFO - call history data...
2018-04-29 19:14:55,359 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 19:14:56,073 - memory_profile6_log - INFO - processing batch-1
2018-04-29 19:14:56,075 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:14:56,084 - memory_profile6_log - INFO - call history data...
2018-04-29 19:15:29,852 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 19:15:30,546 - memory_profile6_log - INFO - processing batch-2
2018-04-29 19:15:30,549 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:15:30,556 - memory_profile6_log - INFO - call history data...
2018-04-29 19:16:00,101 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 19:16:00,851 - memory_profile6_log - INFO - processing batch-3
2018-04-29 19:16:00,852 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:16:00,861 - memory_profile6_log - INFO - call history data...
2018-04-29 19:16:30,316 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 19:16:31,029 - memory_profile6_log - INFO - processing batch-4
2018-04-29 19:16:31,030 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:16:31,039 - memory_profile6_log - INFO - call history data...
2018-04-29 19:16:59,282 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 19:16:59,940 - memory_profile6_log - INFO - Appending training data...
2018-04-29 19:16:59,940 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 19:16:59,944 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 19:16:59,944 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 19:16:59,946 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 19:16:59,947 - memory_profile6_log - INFO - ================================================

2018-04-29 19:16:59,947 - memory_profile6_log - INFO -    311     86.6 MiB     86.6 MiB   @profile

2018-04-29 19:16:59,948 - memory_profile6_log - INFO -    312                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 19:16:59,950 - memory_profile6_log - INFO -    313     86.6 MiB      0.0 MiB       bq_client = client

2018-04-29 19:16:59,951 - memory_profile6_log - INFO -    314     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 19:16:59,953 - memory_profile6_log - INFO -    315                             

2018-04-29 19:16:59,953 - memory_profile6_log - INFO -    316     86.6 MiB      0.0 MiB       datalist = []

2018-04-29 19:16:59,954 - memory_profile6_log - INFO -    317     86.6 MiB      0.0 MiB       datalist_hist = []

2018-04-29 19:16:59,956 - memory_profile6_log - INFO -    318                             

2018-04-29 19:16:59,957 - memory_profile6_log - INFO -    319     86.6 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 19:16:59,957 - memory_profile6_log - INFO -    320    407.3 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 19:16:59,959 - memory_profile6_log - INFO -    321    392.5 MiB    305.9 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 19:16:59,960 - memory_profile6_log - INFO -    322    392.5 MiB      0.0 MiB           if tframe is not None:

2018-04-29 19:16:59,961 - memory_profile6_log - INFO -    323    392.5 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 19:16:59,963 - memory_profile6_log - INFO -    324    405.6 MiB     13.1 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 19:16:59,963 - memory_profile6_log - INFO -    325    405.6 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 19:16:59,964 - memory_profile6_log - INFO -    326    405.6 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 19:16:59,966 - memory_profile6_log - INFO -    327    408.1 MiB     -0.8 MiB                   for ix in range(len(X_split)):

2018-04-29 19:16:59,967 - memory_profile6_log - INFO -    328                                                 # ~ loading history

2018-04-29 19:16:59,967 - memory_profile6_log - INFO -    329                                                 """

2018-04-29 19:16:59,969 - memory_profile6_log - INFO -    330                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 19:16:59,969 - memory_profile6_log - INFO -    331                                                 """

2018-04-29 19:16:59,970 - memory_profile6_log - INFO -    332    408.1 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 19:16:59,973 - memory_profile6_log - INFO -    333                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 19:16:59,973 - memory_profile6_log - INFO -    334    408.1 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 19:16:59,976 - memory_profile6_log - INFO -    335    408.1 MiB      0.5 MiB                       lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 19:16:59,976 - memory_profile6_log - INFO -    336                                                 # lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 19:16:59,977 - memory_profile6_log - INFO -    337                             

2018-04-29 19:16:59,979 - memory_profile6_log - INFO -    338    408.1 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 19:16:59,979 - memory_profile6_log - INFO -    339    407.9 MiB      0.8 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 19:16:59,980 - memory_profile6_log - INFO -    340                             

2018-04-29 19:16:59,980 - memory_profile6_log - INFO -    341                                                 # me = os.getpid()

2018-04-29 19:16:59,983 - memory_profile6_log - INFO -    342                                                 # kill_proc_tree(me)

2018-04-29 19:16:59,983 - memory_profile6_log - INFO -    343                             

2018-04-29 19:16:59,984 - memory_profile6_log - INFO -    344    407.9 MiB     -0.7 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 19:16:59,986 - memory_profile6_log - INFO -    345    408.1 MiB   -129.0 MiB                       for m in h_frame:

2018-04-29 19:16:59,986 - memory_profile6_log - INFO -    346    408.1 MiB   -128.4 MiB                           if m is not None:

2018-04-29 19:16:59,986 - memory_profile6_log - INFO -    347    408.1 MiB   -128.4 MiB                               if len(m) > 0:

2018-04-29 19:16:59,987 - memory_profile6_log - INFO -    348    408.1 MiB   -127.9 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 19:16:59,989 - memory_profile6_log - INFO -    349    408.1 MiB     -0.8 MiB                       del h_frame

2018-04-29 19:16:59,990 - memory_profile6_log - INFO -    350    408.1 MiB     -0.8 MiB                       del lhistory

2018-04-29 19:16:59,990 - memory_profile6_log - INFO -    351                             

2018-04-29 19:16:59,993 - memory_profile6_log - INFO -    352    407.3 MiB     -0.8 MiB                   logger.info("Appending training data...")

2018-04-29 19:16:59,993 - memory_profile6_log - INFO -    353    407.3 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 19:16:59,994 - memory_profile6_log - INFO -    354                                     else: 

2018-04-29 19:16:59,996 - memory_profile6_log - INFO -    355                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 19:16:59,996 - memory_profile6_log - INFO -    356    407.3 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 19:16:59,996 - memory_profile6_log - INFO -    357    407.3 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 19:16:59,997 - memory_profile6_log - INFO -    358                             

2018-04-29 19:16:59,999 - memory_profile6_log - INFO -    359    407.3 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 19:17:00,000 - memory_profile6_log - INFO - 


2018-04-29 19:17:01,226 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 19:17:01,305 - memory_profile6_log - INFO -    p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0   0.000023       32511.100478        38           32521.100478  10959239  162198fdaba19d-0a7184d8c8672-592a1d75-38400-16...
1   0.000023        9899.213287        26            9909.213287  10959239  1626b3bae3f39d-0af051a01ecd42-33697b04-fa000-1...
2   0.000023       12089.039256       132           12099.039256  10959239  161ea30efe66-02cbbff5d443c5-97a5d17-38400-161e...
3   0.000023       12299.553500       113           12309.553500  10959239  16159190170d8-04cae6409daab9-24403d3b-38400-16...
4   0.000023        3297.010706       968            3307.010706  10959239  1610ca7cec566b-0ee3a59b262c89-4323461-100200-1...
5   0.000023       22520.710227        48           22530.710227  10959239  162496a89202ff-0684c1912b6e5b-7a6a1535-c0000-1...
6   0.000023        7479.405594       117            7489.405594  10959239  1628496d76732-018452219f5435-586b6e3c-38400-16...
0   0.001543         454.567622       129             464.567622  10960288  16298f93916aed-0539950d283a53-3f3c5501-100200-...
1   0.001543        1832.475728        32            1842.475728  10960288  1627f9280c59-0dde331730a581-1452633d-38400-162...
2   0.000023        4030.241263      1852            4040.241263  10959239  16199bdf440a-0d2220577b1b0e-247b140f-2c880-161...
3   0.000023       34552.322540       292           34562.322540  10959239  162aaa233289-0000c74b29fe93-17347840-100200-16...
4   0.000023        6702.354859      1129            6712.354859  10959239  1610cb3a8c1348-0c7cec1541e7ac-4323461-100200-1...
5   0.000646        1844.325967        32            1854.325967  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
0   0.001543         122.165049        60             132.165049  10960288  162234fe6afa0-0ef93e11855267-50683974-ff000-16...
1   0.001543         190.387089       154             200.387089  10960288  1610cb6edd9629-070a17912465ef-32637402-13c680-...
2   0.001543         318.691431        23             328.691431  10960288  1622c6334e1a0-0fa1a987448a9f8-77313729-4a640-1...
3   0.001543         385.170058       157             395.170058  10960288  16270ed081e195-0acf605a4cedf8-42584954-38400-1...
4   0.001543        1610.180397        59            1620.180397  10960288  161f36169fce02-03becb8d4c95b1-b353461-15f900-1...
5   0.001543         135.738943        54             145.738943  10960288  1613102d53112-0a021dd0e-6a1e1337-38400-1613102...
6   0.001543         203.069540       134             213.069540  10960288  1612d61b91d59-05f2c354a451f3-3d254822-64140-16...
2018-04-29 19:17:01,306 - memory_profile6_log - INFO - 

2018-04-29 19:17:01,316 - memory_profile6_log - INFO - size of big_frame_hist: 1.37 MB
2018-04-29 19:17:01,420 - memory_profile6_log - INFO - size of big_frame: 102.25 MB
2018-04-29 19:17:01,440 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-04-29 19:18:39,648 - memory_profile6_log - INFO - size of df: 96.11 MB
2018-04-29 19:18:39,650 - memory_profile6_log - INFO - getting total: 393007 training data(current date interest)
2018-04-29 19:18:39,779 - memory_profile6_log - INFO - size of current_frame: 99.10 MB
2018-04-29 19:18:39,779 - memory_profile6_log - INFO - loading time of: 796539 total genuine-current interest data ~ take 374.852s
2018-04-29 19:18:39,786 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 19:18:39,786 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 19:18:39,788 - memory_profile6_log - INFO - ================================================

2018-04-29 19:18:39,789 - memory_profile6_log - INFO -    361     86.5 MiB     86.5 MiB   @profile

2018-04-29 19:18:39,790 - memory_profile6_log - INFO -    362                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 19:18:39,792 - memory_profile6_log - INFO -    363     86.6 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 19:18:39,792 - memory_profile6_log - INFO -    364     86.6 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 19:18:39,793 - memory_profile6_log - INFO -    365                             

2018-04-29 19:18:39,793 - memory_profile6_log - INFO -    366                                 # ~~~ Begin collecting data ~~~

2018-04-29 19:18:39,795 - memory_profile6_log - INFO -    367     86.6 MiB      0.0 MiB       t0 = time.time()

2018-04-29 19:18:39,795 - memory_profile6_log - INFO -    368    404.5 MiB    317.9 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 19:18:39,796 - memory_profile6_log - INFO -    369    404.5 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 19:18:39,798 - memory_profile6_log - INFO -    370                                     logger.info("Training cannot be empty..")

2018-04-29 19:18:39,798 - memory_profile6_log - INFO -    371                                     return False

2018-04-29 19:18:39,798 - memory_profile6_log - INFO -    372    399.3 MiB     -5.2 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 19:18:39,801 - memory_profile6_log - INFO -    373    399.5 MiB      0.2 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 19:18:39,801 - memory_profile6_log - INFO -    374    399.5 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 19:18:39,802 - memory_profile6_log - INFO -    375                             

2018-04-29 19:18:39,802 - memory_profile6_log - INFO -    376    409.3 MiB      9.7 MiB       big_frame = pd.concat(datalist)

2018-04-29 19:18:39,803 - memory_profile6_log - INFO -    377    409.3 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 19:18:39,805 - memory_profile6_log - INFO -    378    400.0 MiB     -9.2 MiB       del datalist

2018-04-29 19:18:39,805 - memory_profile6_log - INFO -    379                             

2018-04-29 19:18:39,806 - memory_profile6_log - INFO -    380    400.0 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 19:18:39,808 - memory_profile6_log - INFO -    381                             

2018-04-29 19:18:39,808 - memory_profile6_log - INFO -    382                                 # ~ get current news interest ~

2018-04-29 19:18:39,808 - memory_profile6_log - INFO -    383    400.0 MiB      0.0 MiB       if not cd:

2018-04-29 19:18:39,811 - memory_profile6_log - INFO -    384    400.0 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-04-29 19:18:39,815 - memory_profile6_log - INFO -    385    477.6 MiB     77.6 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 19:18:39,815 - memory_profile6_log - INFO -    386    477.6 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 19:18:39,815 - memory_profile6_log - INFO -    387                                 else:

2018-04-29 19:18:39,816 - memory_profile6_log - INFO -    388                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 19:18:39,816 - memory_profile6_log - INFO -    389                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 19:18:39,816 - memory_profile6_log - INFO -    390                             

2018-04-29 19:18:39,818 - memory_profile6_log - INFO -    391                                     # safe handling of query parameter

2018-04-29 19:18:39,818 - memory_profile6_log - INFO -    392                                     query_params = [

2018-04-29 19:18:39,819 - memory_profile6_log - INFO -    393                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 19:18:39,819 - memory_profile6_log - INFO -    394                                     ]

2018-04-29 19:18:39,822 - memory_profile6_log - INFO -    395                             

2018-04-29 19:18:39,823 - memory_profile6_log - INFO -    396                                     job_config.query_parameters = query_params

2018-04-29 19:18:39,825 - memory_profile6_log - INFO -    397                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 19:18:39,825 - memory_profile6_log - INFO -    398                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 19:18:39,826 - memory_profile6_log - INFO -    399                             

2018-04-29 19:18:39,826 - memory_profile6_log - INFO -    400    480.6 MiB      3.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 19:18:39,828 - memory_profile6_log - INFO -    401    480.6 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 19:18:39,828 - memory_profile6_log - INFO -    402    480.6 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 19:18:39,828 - memory_profile6_log - INFO -    403    480.6 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 19:18:39,831 - memory_profile6_log - INFO -    404    480.6 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 19:18:39,832 - memory_profile6_log - INFO -    405    480.6 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 19:18:39,834 - memory_profile6_log - INFO -    406                             

2018-04-29 19:18:39,835 - memory_profile6_log - INFO -    407    480.6 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 19:18:39,835 - memory_profile6_log - INFO - 


2018-04-29 19:18:39,839 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 19:18:39,885 - memory_profile6_log - INFO - train on: 403532 total genuine interest data(D(u, t))
2018-04-29 19:18:39,887 - memory_profile6_log - INFO - transform on: 393007 total current data(D(t))
2018-04-29 19:18:39,890 - memory_profile6_log - INFO - apply on: 5000 total history...)
2018-04-29 19:18:40,227 - memory_profile6_log - INFO - len of uniques_fit_hist:5000
2018-04-29 19:18:40,234 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:4481
2018-04-29 19:18:41,533 - memory_profile6_log - INFO - Len of model_fit: 403532
2018-04-29 19:18:41,536 - memory_profile6_log - INFO - Len of df_dut: 403532
2018-04-29 19:18:41,894 - memory_profile6_log - INFO - model_fit dtypes:

2018-04-29 19:18:41,900 - memory_profile6_log - INFO - date              datetime64[ns, UTC]
user_id                        object
topic_id                       object
num_x                           int64
num_y                           int64
is_general                       bool
date_all_click                  int64
Ntotal                          int64
sigma_Nt                        int64
joinprob_ci                   float64
p_cat_ci                      float64
posterior                     float64
dtype: object
2018-04-29 19:18:41,901 - memory_profile6_log - INFO - 

2018-04-29 19:18:41,903 - memory_profile6_log - INFO - fitted_models_hist dtypes:

2018-04-29 19:18:41,910 - memory_profile6_log - INFO - p0_cat_ci                float64
pt_posterior_x_Nt        float64
sigma_Nt                   int64
smoothed_pt_posterior    float64
topic_id                  object
user_id                   object
dtype: object
2018-04-29 19:18:41,911 - memory_profile6_log - INFO - 

2018-04-29 19:18:42,661 - memory_profile6_log - INFO - Len of fitted_models on main class: 403532
2018-04-29 19:18:42,663 - memory_profile6_log - INFO - 

2018-04-29 19:18:42,664 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-29 19:18:42,664 - memory_profile6_log - INFO - 

2018-04-29 19:18:42,667 - memory_profile6_log - INFO - Fitted models before concat:

2018-04-29 19:18:42,688 - memory_profile6_log - INFO -                                              user_id    topic_id  pt_posterior_x_Nt  smoothed_pt_posterior
0  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1064047503       13168.255814           13178.255814
1  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1065124711         209.639023             219.639023
2  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1443761312         829.040996             839.040996
3  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...   152104836         133.420123             143.420123
4  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...    22553543          28.208788              38.208788
2018-04-29 19:18:42,690 - memory_profile6_log - INFO - 

2018-04-29 19:18:42,691 - memory_profile6_log - INFO - len of current fitted models: 403532
2018-04-29 19:18:42,693 - memory_profile6_log - INFO - 

2018-04-29 19:18:42,694 - memory_profile6_log - INFO - len of history fitted models: 5000
2018-04-29 19:18:42,694 - memory_profile6_log - INFO - 

2018-04-29 19:18:42,743 - memory_profile6_log - INFO - len of fitted models after concat: 408532
2018-04-29 19:18:42,744 - memory_profile6_log - INFO - 

2018-04-29 19:18:42,746 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-29 19:18:42,747 - memory_profile6_log - INFO - 

2018-04-29 19:18:42,984 - memory_profile6_log - INFO - Fitted models after concat:

2018-04-29 19:18:43,006 - memory_profile6_log - INFO -                                              user_id    topic_id  pt_posterior_x_Nt
0  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1064047503       13168.255814
1  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1065124711         209.639023
2  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1443761312         829.040996
3  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...   152104836         133.420123
4  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...    22553543          28.208788
2018-04-29 19:18:43,006 - memory_profile6_log - INFO - 

2018-04-29 19:18:43,028 - memory_profile6_log - INFO - Fitted models after concat-simplify:

2018-04-29 19:18:43,051 - memory_profile6_log - INFO -                                              user_id    topic_id  pt_posterior_x_Nt  smoothed_pt_posterior
0  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1064047503       13168.255814           13178.255814
1  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1065124711         209.639023             219.639023
2  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1443761312         829.040996             839.040996
3  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...   152104836         133.420123             143.420123
4  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...    22553543          28.208788              38.208788
2018-04-29 19:18:43,052 - memory_profile6_log - INFO - 

2018-04-29 19:18:43,053 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 403532
2018-04-29 19:18:43,055 - memory_profile6_log - INFO - 

2018-04-29 19:18:43,147 - memory_profile6_log - INFO -                                              user_id           topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci
0  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...         1064047503       13168.255814           13178.255814   0.002257
1  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...         1065124711         209.639023             219.639023   0.000569
2  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...         1443761312         829.040996             839.040996   0.000754
3  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...          152104836         133.420123             143.420123   0.004796
4  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...           22553543          28.208788              38.208788   0.109945
5  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...           22662160         103.478618             113.478618   0.012649
6  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...           27311712         773.545082             783.545082   0.003327
7  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  27431110790312902        7167.531646            7177.531646   0.000012
8  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  27431110790312909        1959.290657            1969.290657   0.000069
9  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  27431110790313245        4225.634328            4235.634328   0.000208
2018-04-29 19:18:43,148 - memory_profile6_log - INFO - 

2018-04-29 19:19:39,857 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-29 19:19:39,905 - memory_profile6_log - INFO - Total train time: 60.020s
2018-04-29 19:19:39,907 - memory_profile6_log - INFO - memory left before cleaning: 79.800 percent memory...
2018-04-29 19:19:39,907 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-29 19:19:39,911 - memory_profile6_log - INFO - deleting df_dut...
2018-04-29 19:19:39,911 - memory_profile6_log - INFO - deleting df_dt...
2018-04-29 19:19:39,913 - memory_profile6_log - INFO - deleting df_input...
2018-04-29 19:19:39,926 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-29 19:19:39,927 - memory_profile6_log - INFO - deleting df_current...
2018-04-29 19:19:39,927 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-29 19:19:39,947 - memory_profile6_log - INFO - deleting model_fit...
2018-04-29 19:19:39,948 - memory_profile6_log - INFO - deleting result...
2018-04-29 19:19:39,974 - memory_profile6_log - INFO - deleting model_transform...
2018-04-29 19:19:39,976 - memory_profile6_log - INFO - memory left after cleaning: 79.500 percent memory...
2018-04-29 19:19:39,977 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-29 19:19:39,979 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-29 19:19:40,148 - memory_profile6_log - INFO - deleting BR...
2018-04-29 19:19:40,157 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 19:19:40,157 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 19:19:40,157 - memory_profile6_log - INFO - ================================================

2018-04-29 19:19:40,160 - memory_profile6_log - INFO -    113    480.6 MiB    480.6 MiB   @profile

2018-04-29 19:19:40,161 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-29 19:19:40,161 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-29 19:19:40,161 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-29 19:19:40,163 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-29 19:19:40,163 - memory_profile6_log - INFO -    118                                      saveto="datastore", fitted_models_hist=None):

2018-04-29 19:19:40,167 - memory_profile6_log - INFO -    119                                 """

2018-04-29 19:19:40,167 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-29 19:19:40,168 - memory_profile6_log - INFO -    121                                 """

2018-04-29 19:19:40,168 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-29 19:19:40,168 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-29 19:19:40,170 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-29 19:19:40,170 - memory_profile6_log - INFO -    125    480.6 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-29 19:19:40,171 - memory_profile6_log - INFO -    126    493.1 MiB     12.4 MiB       df_dut = df_input.copy(deep=True)

2018-04-29 19:19:40,171 - memory_profile6_log - INFO -    127    493.1 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 19:19:40,174 - memory_profile6_log - INFO -    128                             

2018-04-29 19:19:40,174 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-29 19:19:40,174 - memory_profile6_log - INFO -    130    505.5 MiB     12.4 MiB       df_dt = df_current.copy(deep=True)

2018-04-29 19:19:40,174 - memory_profile6_log - INFO -    131    505.5 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 19:19:40,174 - memory_profile6_log - INFO -    132                             

2018-04-29 19:19:40,178 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-29 19:19:40,180 - memory_profile6_log - INFO -    134    505.5 MiB      0.0 MiB       t0 = time.time()

2018-04-29 19:19:40,183 - memory_profile6_log - INFO -    135    505.5 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-29 19:19:40,183 - memory_profile6_log - INFO -    136    505.5 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-29 19:19:40,184 - memory_profile6_log - INFO -    137    505.5 MiB      0.0 MiB       logger.info("apply on: %d total history...)", len(df_hist))

2018-04-29 19:19:40,184 - memory_profile6_log - INFO -    138                             

2018-04-29 19:19:40,184 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-29 19:19:40,184 - memory_profile6_log - INFO -    140    505.5 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-29 19:19:40,186 - memory_profile6_log - INFO -    141                             

2018-04-29 19:19:40,186 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-29 19:19:40,187 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-29 19:19:40,187 - memory_profile6_log - INFO -    144    506.7 MiB      1.2 MiB       NB = BR.processX(df_dut)

2018-04-29 19:19:40,187 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-29 19:19:40,190 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-29 19:19:40,191 - memory_profile6_log - INFO -    147    522.6 MiB     15.9 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-29 19:19:40,191 - memory_profile6_log - INFO -    148                                 """

2018-04-29 19:19:40,194 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-29 19:19:40,194 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-29 19:19:40,194 - memory_profile6_log - INFO -    151                                 """

2018-04-29 19:19:40,194 - memory_profile6_log - INFO -    152    522.6 MiB      0.0 MiB       fitby_sigmant = True

2018-04-29 19:19:40,194 - memory_profile6_log - INFO -    153    522.6 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-29 19:19:40,196 - memory_profile6_log - INFO -    154    522.6 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-29 19:19:40,196 - memory_profile6_log - INFO -    155    538.0 MiB     15.4 MiB                            'is_general']]

2018-04-29 19:19:40,196 - memory_profile6_log - INFO -    156                                 # get sigma_Nt from fitted_models_hist

2018-04-29 19:19:40,200 - memory_profile6_log - INFO -    157    538.3 MiB      0.3 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-04-29 19:19:40,200 - memory_profile6_log - INFO -    158    538.3 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-04-29 19:19:40,203 - memory_profile6_log - INFO -    159    538.5 MiB      0.3 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-04-29 19:19:40,203 - memory_profile6_log - INFO -    160    538.5 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-04-29 19:19:40,203 - memory_profile6_log - INFO -    161                             

2018-04-29 19:19:40,203 - memory_profile6_log - INFO -    162                                 # begin fit

2018-04-29 19:19:40,204 - memory_profile6_log - INFO -    163    538.5 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-29 19:19:40,207 - memory_profile6_log - INFO -    164    538.5 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-29 19:19:40,207 - memory_profile6_log - INFO -    165    579.2 MiB     40.7 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-04-29 19:19:40,209 - memory_profile6_log - INFO -    166    579.2 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-29 19:19:40,209 - memory_profile6_log - INFO -    167    579.2 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-29 19:19:40,209 - memory_profile6_log - INFO -    168                             

2018-04-29 19:19:40,213 - memory_profile6_log - INFO -    169                                 # ~~ and Transform ~~

2018-04-29 19:19:40,213 - memory_profile6_log - INFO -    170                                 #   handling current news interest == current date

2018-04-29 19:19:40,216 - memory_profile6_log - INFO -    171    579.2 MiB      0.0 MiB       if df_dt.empty:

2018-04-29 19:19:40,217 - memory_profile6_log - INFO -    172                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-29 19:19:40,217 - memory_profile6_log - INFO -    173                                     return None

2018-04-29 19:19:40,217 - memory_profile6_log - INFO -    174    579.3 MiB      0.2 MiB       NB = BR.processX(df_dt)

2018-04-29 19:19:40,217 - memory_profile6_log - INFO -    175    597.3 MiB     18.0 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-29 19:19:40,219 - memory_profile6_log - INFO -    176                             

2018-04-29 19:19:40,219 - memory_profile6_log - INFO -    177    597.3 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-29 19:19:40,220 - memory_profile6_log - INFO -    178    596.9 MiB     -0.4 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-29 19:19:40,220 - memory_profile6_log - INFO -    179                             

2018-04-29 19:19:40,220 - memory_profile6_log - INFO -    180    596.9 MiB      0.0 MiB       print "model_fit dtypes:\n", model_fit.dtypes

2018-04-29 19:19:40,221 - memory_profile6_log - INFO -    181    596.9 MiB      0.0 MiB       print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-04-29 19:19:40,226 - memory_profile6_log - INFO -    182    596.9 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-29 19:19:40,226 - memory_profile6_log - INFO -    183    596.9 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-29 19:19:40,229 - memory_profile6_log - INFO -    184    596.9 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-04-29 19:19:40,230 - memory_profile6_log - INFO -    185    596.9 MiB      0.0 MiB                                                                                           "topic_id", "user_id"]],

2018-04-29 19:19:40,230 - memory_profile6_log - INFO -    186    625.2 MiB     28.2 MiB                                                     verbose=False)

2018-04-29 19:19:40,232 - memory_profile6_log - INFO -    187                                 # ~~~ filter is general and specific topic ~~~

2018-04-29 19:19:40,233 - memory_profile6_log - INFO -    188                                 # the idea is just we need to rerank every topic according

2018-04-29 19:19:40,233 - memory_profile6_log - INFO -    189                                 # user_id and and is_general by p0_posterior

2018-04-29 19:19:40,233 - memory_profile6_log - INFO -    190    625.2 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-29 19:19:40,233 - memory_profile6_log - INFO -    191    628.3 MiB      3.1 MiB                                     'is_general']].groupby(['topic_id',

2018-04-29 19:19:40,233 - memory_profile6_log - INFO -    192    625.2 MiB     -3.1 MiB                                                             'is_general']

2018-04-29 19:19:40,236 - memory_profile6_log - INFO -    193                                                                                      ).size().to_frame().reset_index()

2018-04-29 19:19:40,236 - memory_profile6_log - INFO -    194                             

2018-04-29 19:19:40,240 - memory_profile6_log - INFO -    195                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-29 19:19:40,240 - memory_profile6_log - INFO -    196                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-29 19:19:40,240 - memory_profile6_log - INFO -    197    625.2 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-29 19:19:40,242 - memory_profile6_log - INFO -    198                             

2018-04-29 19:19:40,242 - memory_profile6_log - INFO -    199                                 # ~ start by provide rank for each topic type ~

2018-04-29 19:19:40,243 - memory_profile6_log - INFO -    200    645.8 MiB     20.6 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-29 19:19:40,243 - memory_profile6_log - INFO -    201    651.1 MiB      5.3 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-29 19:19:40,243 - memory_profile6_log - INFO -    202                             

2018-04-29 19:19:40,243 - memory_profile6_log - INFO -    203                                 # ~ set threshold to filter output

2018-04-29 19:19:40,243 - memory_profile6_log - INFO -    204    651.1 MiB      0.0 MiB       if threshold > 0:

2018-04-29 19:19:40,246 - memory_profile6_log - INFO -    205    651.1 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-29 19:19:40,246 - memory_profile6_log - INFO -    206    651.1 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-29 19:19:40,249 - memory_profile6_log - INFO -    207    648.0 MiB     -3.1 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-29 19:19:40,250 - memory_profile6_log - INFO -    208                             

2018-04-29 19:19:40,250 - memory_profile6_log - INFO -    209    648.0 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 19:19:40,250 - memory_profile6_log - INFO -    210    648.0 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-29 19:19:40,250 - memory_profile6_log - INFO -    211                             

2018-04-29 19:19:40,250 - memory_profile6_log - INFO -    212    648.0 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 19:19:40,252 - memory_profile6_log - INFO -    213                             

2018-04-29 19:19:40,252 - memory_profile6_log - INFO -    214    648.0 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-29 19:19:40,253 - memory_profile6_log - INFO -    215    648.0 MiB      0.0 MiB       del df_dut

2018-04-29 19:19:40,253 - memory_profile6_log - INFO -    216    648.0 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-29 19:19:40,253 - memory_profile6_log - INFO -    217    648.0 MiB      0.0 MiB       del df_dt

2018-04-29 19:19:40,253 - memory_profile6_log - INFO -    218    648.0 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-29 19:19:40,253 - memory_profile6_log - INFO -    219    648.0 MiB      0.0 MiB       del df_input

2018-04-29 19:19:40,255 - memory_profile6_log - INFO -    220    648.0 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-29 19:19:40,259 - memory_profile6_log - INFO -    221    633.0 MiB    -15.0 MiB       del df_input_X

2018-04-29 19:19:40,260 - memory_profile6_log - INFO -    222    633.0 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-29 19:19:40,260 - memory_profile6_log - INFO -    223    633.0 MiB      0.0 MiB       del df_current

2018-04-29 19:19:40,262 - memory_profile6_log - INFO -    224    633.0 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-29 19:19:40,262 - memory_profile6_log - INFO -    225    633.0 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-29 19:19:40,263 - memory_profile6_log - INFO -    226    633.0 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-29 19:19:40,263 - memory_profile6_log - INFO -    227    596.1 MiB    -37.0 MiB       del model_fit

2018-04-29 19:19:40,265 - memory_profile6_log - INFO -    228    596.1 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-29 19:19:40,265 - memory_profile6_log - INFO -    229    596.1 MiB      0.0 MiB       del result

2018-04-29 19:19:40,266 - memory_profile6_log - INFO -    230    596.1 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-29 19:19:40,266 - memory_profile6_log - INFO -    231                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 19:19:40,266 - memory_profile6_log - INFO -    232                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 19:19:40,279 - memory_profile6_log - INFO -    233                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 19:19:40,282 - memory_profile6_log - INFO -    234    596.1 MiB      0.0 MiB       if savetrain:

2018-04-29 19:19:40,282 - memory_profile6_log - INFO -    235    604.2 MiB      8.1 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-29 19:19:40,283 - memory_profile6_log - INFO -    236    604.2 MiB      0.0 MiB           del model_transform

2018-04-29 19:19:40,283 - memory_profile6_log - INFO -    237    604.2 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-29 19:19:40,285 - memory_profile6_log - INFO -    238    604.2 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 19:19:40,286 - memory_profile6_log - INFO -    239                             

2018-04-29 19:19:40,286 - memory_profile6_log - INFO -    240    604.2 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-29 19:19:40,286 - memory_profile6_log - INFO -    241                                     # ~ Place your code to save the training model here ~

2018-04-29 19:19:40,286 - memory_profile6_log - INFO -    242    604.2 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-29 19:19:40,288 - memory_profile6_log - INFO -    243    604.2 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-29 19:19:40,288 - memory_profile6_log - INFO -    244    604.2 MiB      0.0 MiB               if multproc:

2018-04-29 19:19:40,292 - memory_profile6_log - INFO -    245                                             # ~ save transform models ~

2018-04-29 19:19:40,292 - memory_profile6_log - INFO -    246    574.5 MiB    -29.6 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-29 19:19:40,293 - memory_profile6_log - INFO -    247                             

2018-04-29 19:19:40,293 - memory_profile6_log - INFO -    248                                             """

2018-04-29 19:19:40,295 - memory_profile6_log - INFO -    249                                             # ~ save fitted models ~

2018-04-29 19:19:40,295 - memory_profile6_log - INFO -    250                                             logger.info("Saving fitted_models as history...")

2018-04-29 19:19:40,295 - memory_profile6_log - INFO -    251                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 19:19:40,296 - memory_profile6_log - INFO -    252                                             fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-29 19:19:40,296 - memory_profile6_log - INFO -    253                             

2018-04-29 19:19:40,298 - memory_profile6_log - INFO -    254                                             X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-29 19:19:40,299 - memory_profile6_log - INFO -    255                                             logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-29 19:19:40,299 - memory_profile6_log - INFO -    256                                             for ix in range(len(X_split)):

2018-04-29 19:19:40,299 - memory_profile6_log - INFO -    257                                                 logger.info("processing batch-%d", ix)

2018-04-29 19:19:40,299 - memory_profile6_log - INFO -    258                                                 mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-29 19:19:40,299 - memory_profile6_log - INFO -    259                             

2018-04-29 19:19:40,301 - memory_profile6_log - INFO -    260                                             del X_split

2018-04-29 19:19:40,302 - memory_profile6_log - INFO -    261                                             logger.info("deleting X_split...")

2018-04-29 19:19:40,305 - memory_profile6_log - INFO -    262                                             del save_sigma_nt

2018-04-29 19:19:40,305 - memory_profile6_log - INFO -    263                                             logger.info("deleting save_sigma_nt...")

2018-04-29 19:19:40,306 - memory_profile6_log - INFO -    264                                             """

2018-04-29 19:19:40,306 - memory_profile6_log - INFO -    265                             

2018-04-29 19:19:40,306 - memory_profile6_log - INFO -    266    574.5 MiB      0.0 MiB                   del BR

2018-04-29 19:19:40,308 - memory_profile6_log - INFO -    267    574.5 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-29 19:19:40,308 - memory_profile6_log - INFO -    268                             

2018-04-29 19:19:40,309 - memory_profile6_log - INFO -    269                                     elif str(saveto).lower() == "elastic":

2018-04-29 19:19:40,309 - memory_profile6_log - INFO -    270                                         logger.info("Using ElasticSearch as storage...")

2018-04-29 19:19:40,309 - memory_profile6_log - INFO -    271                                         mh.saveElasticS(model_transformsv)

2018-04-29 19:19:40,309 - memory_profile6_log - INFO -    272                             

2018-04-29 19:19:40,309 - memory_profile6_log - INFO -    273                                     # need save sigma_nt for daily train

2018-04-29 19:19:40,309 - memory_profile6_log - INFO -    274                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-29 19:19:40,311 - memory_profile6_log - INFO -    275                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-29 19:19:40,311 - memory_profile6_log - INFO -    276    574.5 MiB      0.0 MiB           if start_date and end_date:

2018-04-29 19:19:40,312 - memory_profile6_log - INFO -    277                                         if not fitby_sigmant:

2018-04-29 19:19:40,313 - memory_profile6_log - INFO -    278                                             logging.info("Saving sigma Nt...")

2018-04-29 19:19:40,315 - memory_profile6_log - INFO -    279                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 19:19:40,315 - memory_profile6_log - INFO -    280                                             save_sigma_nt['start_date'] = start_date

2018-04-29 19:19:40,316 - memory_profile6_log - INFO -    281                                             save_sigma_nt['end_date'] = end_date

2018-04-29 19:19:40,316 - memory_profile6_log - INFO -    282                                             print save_sigma_nt.head(5)

2018-04-29 19:19:40,316 - memory_profile6_log - INFO -    283                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-29 19:19:40,318 - memory_profile6_log - INFO -    284                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-29 19:19:40,318 - memory_profile6_log - INFO -    285    574.5 MiB      0.0 MiB       return

2018-04-29 19:19:40,318 - memory_profile6_log - INFO - 


2018-04-29 19:19:40,319 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-29 19:24:57,645 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 19:24:57,648 - memory_profile6_log - INFO - date_generated: 
2018-04-29 19:24:57,648 - memory_profile6_log - INFO -  
2018-04-29 19:24:57,648 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 9, 0, 0)]
2018-04-29 19:24:57,648 - memory_profile6_log - INFO - 

2018-04-29 19:24:57,650 - memory_profile6_log - INFO - using current date: 2018-04-10
2018-04-29 19:24:57,650 - memory_profile6_log - INFO - using start date: 2018-04-09 00:00:00
2018-04-29 19:24:57,650 - memory_profile6_log - INFO - using end date: 2018-04-09
2018-04-29 19:24:57,767 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 19:24:57,772 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-09
2018-04-29 19:26:50,930 - memory_profile6_log - INFO - size of df: 102.25 MB
2018-04-29 19:26:50,931 - memory_profile6_log - INFO - getting total: 403532 training data(genuine interest) for date: 2018-04-09
2018-04-29 19:26:50,986 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 19:26:50,986 - memory_profile6_log - INFO - Appending history data...
2018-04-29 19:26:50,987 - memory_profile6_log - INFO - processing batch-0
2018-04-29 19:26:50,990 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:26:51,092 - memory_profile6_log - INFO - call history data...
2018-04-29 19:28:08,778 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 19:28:10,358 - memory_profile6_log - INFO - processing batch-1
2018-04-29 19:28:10,358 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:28:10,434 - memory_profile6_log - INFO - call history data...
2018-04-29 19:29:48,668 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 19:29:50,296 - memory_profile6_log - INFO - processing batch-2
2018-04-29 19:29:50,298 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:29:50,374 - memory_profile6_log - INFO - call history data...
2018-04-29 19:31:14,719 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 19:31:16,334 - memory_profile6_log - INFO - processing batch-3
2018-04-29 19:31:16,335 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:31:16,410 - memory_profile6_log - INFO - call history data...
2018-04-29 19:34:44,229 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 19:34:44,232 - memory_profile6_log - INFO - date_generated: 
2018-04-29 19:34:44,232 - memory_profile6_log - INFO -  
2018-04-29 19:34:44,232 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 9, 0, 0)]
2018-04-29 19:34:44,232 - memory_profile6_log - INFO - 

2018-04-29 19:34:44,233 - memory_profile6_log - INFO - using current date: 2018-04-10
2018-04-29 19:34:44,233 - memory_profile6_log - INFO - using start date: 2018-04-09 00:00:00
2018-04-29 19:34:44,233 - memory_profile6_log - INFO - using end date: 2018-04-09
2018-04-29 19:34:44,351 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 19:34:44,355 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-09
2018-04-29 19:36:35,736 - memory_profile6_log - INFO - size of df: 102.25 MB
2018-04-29 19:36:35,737 - memory_profile6_log - INFO - getting total: 403532 training data(genuine interest) for date: 2018-04-09
2018-04-29 19:36:35,799 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 19:36:35,801 - memory_profile6_log - INFO - Appending history data...
2018-04-29 19:36:35,802 - memory_profile6_log - INFO - processing batch-0
2018-04-29 19:36:35,802 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:36:35,928 - memory_profile6_log - INFO - call history data...
2018-04-29 19:37:47,816 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 19:37:49,484 - memory_profile6_log - INFO - processing batch-1
2018-04-29 19:37:49,486 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:37:49,559 - memory_profile6_log - INFO - call history data...
2018-04-29 19:39:00,859 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 19:39:02,565 - memory_profile6_log - INFO - processing batch-2
2018-04-29 19:39:02,566 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:39:02,638 - memory_profile6_log - INFO - call history data...
2018-04-29 19:40:56,267 - memory_profile6_log - INFO - Generating date range with N: 1
2018-04-29 19:40:56,272 - memory_profile6_log - INFO - date_generated: 
2018-04-29 19:40:56,272 - memory_profile6_log - INFO -  
2018-04-29 19:40:56,273 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 9, 0, 0)]
2018-04-29 19:40:56,273 - memory_profile6_log - INFO - 

2018-04-29 19:40:56,273 - memory_profile6_log - INFO - using current date: 2018-04-10
2018-04-29 19:40:56,275 - memory_profile6_log - INFO - using start date: 2018-04-09 00:00:00
2018-04-29 19:40:56,275 - memory_profile6_log - INFO - using end date: 2018-04-09
2018-04-29 19:40:56,395 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-29 19:40:56,400 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-09
2018-04-29 19:42:47,125 - memory_profile6_log - INFO - size of df: 102.25 MB
2018-04-29 19:42:47,128 - memory_profile6_log - INFO - getting total: 403532 training data(genuine interest) for date: 2018-04-09
2018-04-29 19:42:47,190 - memory_profile6_log - INFO - Len of X_split for batch load: 5
2018-04-29 19:42:47,193 - memory_profile6_log - INFO - Appending history data...
2018-04-29 19:42:47,194 - memory_profile6_log - INFO - processing batch-0
2018-04-29 19:42:47,196 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:42:47,305 - memory_profile6_log - INFO - call history data...
2018-04-29 19:43:38,927 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 19:43:40,500 - memory_profile6_log - INFO - processing batch-1
2018-04-29 19:43:40,502 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:43:40,572 - memory_profile6_log - INFO - call history data...
2018-04-29 19:44:32,694 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 19:44:34,489 - memory_profile6_log - INFO - processing batch-2
2018-04-29 19:44:34,490 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:44:34,569 - memory_profile6_log - INFO - call history data...
2018-04-29 19:45:26,542 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 19:45:28,401 - memory_profile6_log - INFO - processing batch-3
2018-04-29 19:45:28,403 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:45:28,490 - memory_profile6_log - INFO - call history data...
2018-04-29 19:46:20,619 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 19:46:22,243 - memory_profile6_log - INFO - processing batch-4
2018-04-29 19:46:22,243 - memory_profile6_log - INFO - creating list history data...
2018-04-29 19:46:22,313 - memory_profile6_log - INFO - call history data...
2018-04-29 19:47:17,898 - memory_profile6_log - INFO - done collecting history data, appending now...
2018-04-29 19:47:19,575 - memory_profile6_log - INFO - Appending training data...
2018-04-29 19:47:19,576 - memory_profile6_log - INFO - len datalist: 1
2018-04-29 19:47:19,578 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-29 19:47:19,596 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 19:47:19,598 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 19:47:19,599 - memory_profile6_log - INFO - ================================================

2018-04-29 19:47:19,599 - memory_profile6_log - INFO -    311     86.8 MiB     86.8 MiB   @profile

2018-04-29 19:47:19,601 - memory_profile6_log - INFO -    312                             def BQPreprocess(cpu, date_generated, client, query_fit):

2018-04-29 19:47:19,604 - memory_profile6_log - INFO -    313     86.8 MiB      0.0 MiB       bq_client = client

2018-04-29 19:47:19,604 - memory_profile6_log - INFO -    314     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 19:47:19,605 - memory_profile6_log - INFO -    315                             

2018-04-29 19:47:19,605 - memory_profile6_log - INFO -    316     86.8 MiB      0.0 MiB       datalist = []

2018-04-29 19:47:19,607 - memory_profile6_log - INFO -    317     86.8 MiB      0.0 MiB       datalist_hist = []

2018-04-29 19:47:19,607 - memory_profile6_log - INFO -    318                             

2018-04-29 19:47:19,608 - memory_profile6_log - INFO -    319     86.8 MiB      0.0 MiB       logger.info("Starting data fetch iterative...")

2018-04-29 19:47:19,611 - memory_profile6_log - INFO -    320    655.4 MiB      0.0 MiB       for ndate in date_generated:

2018-04-29 19:47:19,611 - memory_profile6_log - INFO -    321    393.1 MiB    306.4 MiB           tframe = getBig(ndate.strftime("%Y-%m-%d"), query_fit)

2018-04-29 19:47:19,612 - memory_profile6_log - INFO -    322    393.1 MiB      0.0 MiB           if tframe is not None:

2018-04-29 19:47:19,614 - memory_profile6_log - INFO -    323    393.1 MiB      0.0 MiB               if not tframe.empty:

2018-04-29 19:47:19,614 - memory_profile6_log - INFO -    324    406.3 MiB     13.1 MiB                   X_split = np.array_split(tframe, 5)

2018-04-29 19:47:19,615 - memory_profile6_log - INFO -    325    406.3 MiB      0.0 MiB                   logger.info("Len of X_split for batch load: %d", len(X_split))

2018-04-29 19:47:19,615 - memory_profile6_log - INFO -    326    406.3 MiB      0.0 MiB                   logger.info("Appending history data...")

2018-04-29 19:47:19,615 - memory_profile6_log - INFO -    327    655.4 MiB      0.0 MiB                   for ix in range(len(X_split)):

2018-04-29 19:47:19,617 - memory_profile6_log - INFO -    328                                                 # ~ loading history

2018-04-29 19:47:19,618 - memory_profile6_log - INFO -    329                                                 """

2018-04-29 19:47:19,619 - memory_profile6_log - INFO -    330                                                     disini antara kita gabungkan dengan tframe, atau buat df sendiri

2018-04-29 19:47:19,621 - memory_profile6_log - INFO -    331                                                 """

2018-04-29 19:47:19,621 - memory_profile6_log - INFO -    332    628.0 MiB      0.0 MiB                       logger.info("processing batch-%d", ix)

2018-04-29 19:47:19,622 - memory_profile6_log - INFO -    333                                                 # https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas'

2018-04-29 19:47:19,624 - memory_profile6_log - INFO -    334    628.0 MiB      0.0 MiB                       logger.info("creating list history data...")

2018-04-29 19:47:19,624 - memory_profile6_log - INFO -    335                                                 # lhistory = list(X_split[ix]["user_id"].head(1000).map(str) + "_" + X_split[ix]["topic_id"].head(1000).map(str))

2018-04-29 19:47:19,625 - memory_profile6_log - INFO -    336    629.8 MiB      7.7 MiB                       lhistory = list(X_split[ix]["user_id"].map(str) + "_" + X_split[ix]["topic_id"].map(str))

2018-04-29 19:47:19,625 - memory_profile6_log - INFO -    337                             

2018-04-29 19:47:19,625 - memory_profile6_log - INFO -    338    629.8 MiB      0.0 MiB                       logger.info("call history data...")

2018-04-29 19:47:19,627 - memory_profile6_log - INFO -    339    704.7 MiB    525.5 MiB                       h_frame = mh.loadDSHistory(lhistory)

2018-04-29 19:47:19,628 - memory_profile6_log - INFO -    340                             

2018-04-29 19:47:19,631 - memory_profile6_log - INFO -    341                                                 # me = os.getpid()

2018-04-29 19:47:19,631 - memory_profile6_log - INFO -    342                                                 # kill_proc_tree(me)

2018-04-29 19:47:19,632 - memory_profile6_log - INFO -    343                             

2018-04-29 19:47:19,632 - memory_profile6_log - INFO -    344    704.7 MiB      0.0 MiB                       logger.info("done collecting history data, appending now...")

2018-04-29 19:47:19,634 - memory_profile6_log - INFO -    345    706.9 MiB     -0.7 MiB                       for m in h_frame:

2018-04-29 19:47:19,634 - memory_profile6_log - INFO -    346    706.9 MiB     -0.7 MiB                           if m is not None:

2018-04-29 19:47:19,635 - memory_profile6_log - INFO -    347    706.9 MiB     -0.7 MiB                               if len(m) > 0:

2018-04-29 19:47:19,635 - memory_profile6_log - INFO -    348    706.9 MiB      8.8 MiB                                   datalist_hist.append(pd.DataFrame(m))

2018-04-29 19:47:19,637 - memory_profile6_log - INFO -    349    655.4 MiB   -292.5 MiB                       del h_frame

2018-04-29 19:47:19,638 - memory_profile6_log - INFO -    350    655.4 MiB     -1.1 MiB                       del lhistory

2018-04-29 19:47:19,638 - memory_profile6_log - INFO -    351                             

2018-04-29 19:47:19,638 - memory_profile6_log - INFO -    352    655.4 MiB      0.0 MiB                   logger.info("Appending training data...")

2018-04-29 19:47:19,641 - memory_profile6_log - INFO -    353    655.4 MiB      0.0 MiB                   datalist.append(tframe)

2018-04-29 19:47:19,642 - memory_profile6_log - INFO -    354                                     else: 

2018-04-29 19:47:19,645 - memory_profile6_log - INFO -    355                                         logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-29 19:47:19,647 - memory_profile6_log - INFO -    356    655.4 MiB      0.0 MiB       logger.info("len datalist: %d", len(datalist))

2018-04-29 19:47:19,648 - memory_profile6_log - INFO -    357    655.4 MiB      0.0 MiB       logger.info("All data fetch iterative done!!")

2018-04-29 19:47:19,648 - memory_profile6_log - INFO -    358                             

2018-04-29 19:47:19,648 - memory_profile6_log - INFO -    359    655.4 MiB      0.0 MiB       return datalist, datalist_hist

2018-04-29 19:47:19,650 - memory_profile6_log - INFO - 


2018-04-29 19:47:20,783 - memory_profile6_log - INFO - big_frame_hist:

2018-04-29 19:47:20,858 - memory_profile6_log - INFO -     p0_cat_ci  pt_posterior_x_Nt  sigma_Nt  smoothed_pt_posterior  topic_id                                            user_id
0    0.001543         445.737339       185             455.737339  10960288  16137e7988e400-0e4f9d96053ce-2c5f3268-4a640-16...
1    0.001543         791.412368        48             801.412368  10960288  16131c8488cad-00d85aed867452-39626377-55188-16...
2    0.001543         467.043053       143             477.043053  10960288  1616ab2aea51bb-095e9f7c6f74b3-514c673d-38400-1...
3    0.000023       12089.039256       132           12099.039256  10959239  161ea30efe66-02cbbff5d443c5-97a5d17-38400-161e...
4    0.000088         910.931467        37             920.931467  11911567  1614c2d888521-0362a12a9300a8-57516539-38400-16...
5    0.001473         269.647011        66             279.647011  22291119  1621f5b8e9b1-0128731ade18b5-2e483333-38400-162...
6    0.001543         285.850187      1230             295.850187  10960288  1622eb63ec39-0961b0bad640fe-582d1346-8d272-162...
7    0.001543        1105.960925       149            1115.960925  10960288  161b32ec49815-09b38099f85a43-a4f5b5e-38400-161...
8    0.000088        6740.892857        10            6750.892857  11911567  161409a03c93e-0823b8d96d566d-69575974-38400-16...
9    0.001543         233.550828       102             243.550828  10960288  1613a0f880013c-008204f8136e34-435b4850-38400-1...
10   0.001543         381.996451       331             391.996451  10960288  16130047ada52-0ada44117-14d422a-29b80-16130047...
11   0.000088        2202.469943       101            2212.469943  11911567  161ae06c69f28-099979cacde8b8-75194b59-38400-16...
12   0.001543         138.215931       822             148.215931  10960288  1612e0dc6af1c-06e6aa483502b9-b2f5b2f-38400-161...
13   0.001543        1086.583319      1401            1096.583319  10960288  1616ac78a3e5b5-09b19c5de4820b-4c534c69-100200-...
14   0.000646        1844.325967        32            1854.325967  10960288  1612da17a01324-0b4ae4e577dca7-58596970-38400-1...
15   0.001543         502.346467       311             512.346467  10960288  16275ef9b5ad-0d25ed18f50a1c-3b670e20-4df28-162...
16   0.001543         860.466864       115             870.466864  10960288  162a8c642a71e4-0e9ff4c73ef7f1-3f3c5501-100200-...
17   0.000023        9899.213287        26            9909.213287  10959239  1626b3bae3f39d-0af051a01ecd42-33697b04-fa000-1...
18   0.001543         454.567622       129             464.567622  10960288  16298f93916aed-0539950d283a53-3f3c5501-100200-...
19   0.004470         100.216367        61             110.216367  22291119  16199f44e7753-064fe64d8c6899-73261514-3f480-16...
2018-04-29 19:47:20,858 - memory_profile6_log - INFO - 

2018-04-29 19:47:20,990 - memory_profile6_log - INFO - size of big_frame_hist: 111.10 MB
2018-04-29 19:47:21,095 - memory_profile6_log - INFO - size of big_frame: 102.25 MB
2018-04-29 19:47:21,117 - memory_profile6_log - INFO - Collecting training data(current date interest)..
2018-04-29 19:48:28,717 - memory_profile6_log - INFO - size of df: 96.11 MB
2018-04-29 19:48:28,719 - memory_profile6_log - INFO - getting total: 393007 training data(current date interest)
2018-04-29 19:48:28,855 - memory_profile6_log - INFO - size of current_frame: 99.10 MB
2018-04-29 19:48:28,857 - memory_profile6_log - INFO - loading time of: 796539 total genuine-current interest data ~ take 452.480s
2018-04-29 19:48:28,885 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 19:48:28,887 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 19:48:28,888 - memory_profile6_log - INFO - ================================================

2018-04-29 19:48:28,888 - memory_profile6_log - INFO -    361     86.6 MiB     86.6 MiB   @profile

2018-04-29 19:48:28,890 - memory_profile6_log - INFO -    362                             def preprocess(cpu, cd, query_fit, date_generated):

2018-04-29 19:48:28,891 - memory_profile6_log - INFO -    363     86.8 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-29 19:48:28,891 - memory_profile6_log - INFO -    364     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-29 19:48:28,892 - memory_profile6_log - INFO -    365                             

2018-04-29 19:48:28,892 - memory_profile6_log - INFO -    366                                 # ~~~ Begin collecting data ~~~

2018-04-29 19:48:28,894 - memory_profile6_log - INFO -    367     86.8 MiB      0.0 MiB       t0 = time.time()

2018-04-29 19:48:28,894 - memory_profile6_log - INFO -    368    645.6 MiB    558.8 MiB       datalist, datalist_hist = BQPreprocess(cpu, date_generated, bq_client, query_fit)

2018-04-29 19:48:28,897 - memory_profile6_log - INFO -    369    645.6 MiB      0.0 MiB       if len(datalist_hist) <= 0:

2018-04-29 19:48:28,897 - memory_profile6_log - INFO -    370                                     logger.info("Training cannot be empty..")

2018-04-29 19:48:28,898 - memory_profile6_log - INFO -    371                                     return False

2018-04-29 19:48:28,900 - memory_profile6_log - INFO -    372    669.7 MiB     24.1 MiB       big_frame_hist = pd.concat(datalist_hist)

2018-04-29 19:48:28,900 - memory_profile6_log - INFO -    373    669.8 MiB      0.1 MiB       print "big_frame_hist:\n", big_frame_hist.head(20)

2018-04-29 19:48:28,901 - memory_profile6_log - INFO -    374    669.8 MiB      0.0 MiB       logger.info("size of big_frame_hist: %s", humanbytes(sys.getsizeof(big_frame_hist)))

2018-04-29 19:48:28,901 - memory_profile6_log - INFO -    375                             

2018-04-29 19:48:28,901 - memory_profile6_log - INFO -    376    679.1 MiB      9.3 MiB       big_frame = pd.concat(datalist)

2018-04-29 19:48:28,903 - memory_profile6_log - INFO -    377    679.1 MiB      0.0 MiB       logger.info("size of big_frame: %s", humanbytes(sys.getsizeof(big_frame)))

2018-04-29 19:48:28,903 - memory_profile6_log - INFO -    378    669.9 MiB     -9.2 MiB       del datalist

2018-04-29 19:48:28,904 - memory_profile6_log - INFO -    379                             

2018-04-29 19:48:28,907 - memory_profile6_log - INFO -    380    669.9 MiB      0.0 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 19:48:28,907 - memory_profile6_log - INFO -    381                             

2018-04-29 19:48:28,910 - memory_profile6_log - INFO -    382                                 # ~ get current news interest ~

2018-04-29 19:48:28,911 - memory_profile6_log - INFO -    383    669.9 MiB      0.0 MiB       if not cd:

2018-04-29 19:48:28,911 - memory_profile6_log - INFO -    384    669.9 MiB      0.0 MiB           logger.info("Collecting training data(current date interest)..")

2018-04-29 19:48:28,911 - memory_profile6_log - INFO -    385    655.8 MiB    -14.0 MiB           current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-29 19:48:28,911 - memory_profile6_log - INFO -    386    655.8 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-29 19:48:28,914 - memory_profile6_log - INFO -    387                                 else:

2018-04-29 19:48:28,914 - memory_profile6_log - INFO -    388                                     logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-29 19:48:28,914 - memory_profile6_log - INFO -    389                                     query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-29 19:48:28,915 - memory_profile6_log - INFO -    390                             

2018-04-29 19:48:28,915 - memory_profile6_log - INFO -    391                                     # safe handling of query parameter

2018-04-29 19:48:28,918 - memory_profile6_log - INFO -    392                                     query_params = [

2018-04-29 19:48:28,920 - memory_profile6_log - INFO -    393                                         bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-29 19:48:28,924 - memory_profile6_log - INFO -    394                                     ]

2018-04-29 19:48:28,924 - memory_profile6_log - INFO -    395                             

2018-04-29 19:48:28,924 - memory_profile6_log - INFO -    396                                     job_config.query_parameters = query_params

2018-04-29 19:48:28,924 - memory_profile6_log - INFO -    397                                     current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-29 19:48:28,926 - memory_profile6_log - INFO -    398                                     logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-29 19:48:28,926 - memory_profile6_log - INFO -    399                             

2018-04-29 19:48:28,928 - memory_profile6_log - INFO -    400    658.9 MiB      3.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-29 19:48:28,928 - memory_profile6_log - INFO -    401    658.9 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-29 19:48:28,930 - memory_profile6_log - INFO -    402    658.9 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-29 19:48:28,931 - memory_profile6_log - INFO -    403    658.9 MiB      0.0 MiB       logger.info("size of current_frame: %s", humanbytes(sys.getsizeof(current_frame)))

2018-04-29 19:48:28,931 - memory_profile6_log - INFO -    404    658.9 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 19:48:28,933 - memory_profile6_log - INFO -    405    658.9 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-29 19:48:28,933 - memory_profile6_log - INFO -    406                             

2018-04-29 19:48:28,934 - memory_profile6_log - INFO -    407    658.9 MiB      0.0 MiB       return big_frame, current_frame, big_frame_hist

2018-04-29 19:48:28,934 - memory_profile6_log - INFO - 


2018-04-29 19:48:28,937 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-29 19:48:28,982 - memory_profile6_log - INFO - train on: 403532 total genuine interest data(D(u, t))
2018-04-29 19:48:28,983 - memory_profile6_log - INFO - transform on: 393007 total current data(D(t))
2018-04-29 19:48:28,984 - memory_profile6_log - INFO - apply on: 403532 total history...)
2018-04-29 19:48:29,326 - memory_profile6_log - INFO - len of uniques_fit_hist:403532
2018-04-29 19:48:29,556 - memory_profile6_log - INFO - len of uniques_fit_hist after drop duplicate:50399
2018-04-29 19:48:30,897 - memory_profile6_log - INFO - Len of model_fit: 403532
2018-04-29 19:48:30,898 - memory_profile6_log - INFO - Len of df_dut: 403532
2018-04-29 19:48:31,259 - memory_profile6_log - INFO - model_fit dtypes:

2018-04-29 19:48:31,263 - memory_profile6_log - INFO - date              datetime64[ns, UTC]
user_id                        object
topic_id                       object
num_x                           int64
num_y                           int64
is_general                       bool
date_all_click                  int64
Ntotal                          int64
sigma_Nt                        int64
joinprob_ci                   float64
p_cat_ci                      float64
posterior                     float64
dtype: object
2018-04-29 19:48:31,265 - memory_profile6_log - INFO - 

2018-04-29 19:48:31,266 - memory_profile6_log - INFO - fitted_models_hist dtypes:

2018-04-29 19:48:31,269 - memory_profile6_log - INFO - p0_cat_ci                float64
pt_posterior_x_Nt        float64
sigma_Nt                   int64
smoothed_pt_posterior    float64
topic_id                  object
user_id                   object
dtype: object
2018-04-29 19:48:31,270 - memory_profile6_log - INFO - 

2018-04-29 19:48:32,096 - memory_profile6_log - INFO - Len of fitted_models on main class: 403532
2018-04-29 19:48:32,098 - memory_profile6_log - INFO - 

2018-04-29 19:48:32,098 - memory_profile6_log - INFO - Combining current fitted_models with history...
2018-04-29 19:48:32,099 - memory_profile6_log - INFO - 

2018-04-29 19:48:32,101 - memory_profile6_log - INFO - Fitted models before concat:

2018-04-29 19:48:32,128 - memory_profile6_log - INFO -                                              user_id    topic_id  pt_posterior_x_Nt  smoothed_pt_posterior
0  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1064047503        4802.540356            4812.540356
1  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1065124711          76.456585              86.456585
2  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1443761312         302.356128             312.356128
3  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...   152104836          48.659104              58.659104
4  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...    22553543          10.287911              20.287911
2018-04-29 19:48:32,130 - memory_profile6_log - INFO - 

2018-04-29 19:48:32,131 - memory_profile6_log - INFO - len of current fitted models: 403532
2018-04-29 19:48:32,131 - memory_profile6_log - INFO - 

2018-04-29 19:48:32,132 - memory_profile6_log - INFO - len of history fitted models: 403532
2018-04-29 19:48:32,134 - memory_profile6_log - INFO - 

2018-04-29 19:48:32,207 - memory_profile6_log - INFO - len of fitted models after concat: 807064
2018-04-29 19:48:32,207 - memory_profile6_log - INFO - 

2018-04-29 19:48:32,210 - memory_profile6_log - INFO - Recalculating fitted models...
2018-04-29 19:48:32,210 - memory_profile6_log - INFO - 

2018-04-29 19:48:32,733 - memory_profile6_log - INFO - Fitted models after concat:

2018-04-29 19:48:32,753 - memory_profile6_log - INFO -                                              user_id    topic_id  pt_posterior_x_Nt
0  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1064047503       12362.094619
1  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1065124711         196.804912
2  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1443761312         778.287070
3  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...   152104836         125.252137
4  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...    22553543          28.114433
2018-04-29 19:48:32,753 - memory_profile6_log - INFO - 

2018-04-29 19:48:32,775 - memory_profile6_log - INFO - Fitted models after concat-simplify:

2018-04-29 19:48:32,799 - memory_profile6_log - INFO -                                              user_id    topic_id  pt_posterior_x_Nt  smoothed_pt_posterior
0  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1064047503       12362.094619           12372.094619
1  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1065124711         196.804912             206.804912
2  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  1443761312         778.287070             788.287070
3  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...   152104836         125.252137             135.252137
4  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...    22553543          28.114433              38.114433
2018-04-29 19:48:32,799 - memory_profile6_log - INFO - 

2018-04-29 19:48:32,802 - memory_profile6_log - INFO - Len of fitted_models after all concat process on main class: 403532
2018-04-29 19:48:32,802 - memory_profile6_log - INFO - 

2018-04-29 19:48:32,900 - memory_profile6_log - INFO -                                              user_id           topic_id  pt_posterior_x_Nt  smoothed_pt_posterior  p0_cat_ci
0  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...         1064047503       12362.094619           12372.094619   0.002257
1  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...         1065124711         196.804912             206.804912   0.000569
2  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...         1443761312         778.287070             788.287070   0.000754
3  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...          152104836         125.252137             135.252137   0.004796
4  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...           22553543          28.114433              38.114433   0.109945
5  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...           22662160          97.143653             107.143653   0.012649
6  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...           27311712         772.295711             782.295711   0.003327
7  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  27431110790312902        6728.735046            6738.735046   0.000012
8  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  27431110790312909        1839.342798            1849.342798   0.000069
9  160f9bc51b8a9-074d4b6fcbbcb9-16386656-fa000-16...  27431110790313245        3966.940811            3976.940811   0.000208
2018-04-29 19:48:32,901 - memory_profile6_log - INFO - 

2018-04-29 19:49:29,867 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-29 19:49:29,911 - memory_profile6_log - INFO - Total train time: 60.930s
2018-04-29 19:49:29,914 - memory_profile6_log - INFO - memory left before cleaning: 80.600 percent memory...
2018-04-29 19:49:29,914 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-29 19:49:29,915 - memory_profile6_log - INFO - deleting df_dut...
2018-04-29 19:49:29,915 - memory_profile6_log - INFO - deleting df_dt...
2018-04-29 19:49:29,917 - memory_profile6_log - INFO - deleting df_input...
2018-04-29 19:49:29,931 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-29 19:49:29,933 - memory_profile6_log - INFO - deleting df_current...
2018-04-29 19:49:29,934 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-29 19:49:29,953 - memory_profile6_log - INFO - deleting model_fit...
2018-04-29 19:49:29,956 - memory_profile6_log - INFO - deleting result...
2018-04-29 19:49:29,986 - memory_profile6_log - INFO - deleting model_transform...
2018-04-29 19:49:29,986 - memory_profile6_log - INFO - memory left after cleaning: 80.300 percent memory...
2018-04-29 19:49:29,989 - memory_profile6_log - INFO - Begin saving trained data...
2018-04-29 19:49:29,990 - memory_profile6_log - INFO - Using google datastore as storage...
2018-04-29 19:49:30,174 - memory_profile6_log - INFO - deleting BR...
2018-04-29 19:49:30,183 - memory_profile6_log - INFO - Filename: .\daily.py


2018-04-29 19:49:30,183 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-29 19:49:30,183 - memory_profile6_log - INFO - ================================================

2018-04-29 19:49:30,184 - memory_profile6_log - INFO -    113    650.1 MiB    650.1 MiB   @profile

2018-04-29 19:49:30,184 - memory_profile6_log - INFO -    114                             def main(df_input, df_current, df_hist,

2018-04-29 19:49:30,186 - memory_profile6_log - INFO -    115                                      current_date, G, project_id,

2018-04-29 19:49:30,186 - memory_profile6_log - INFO -    116                                      savetrain=False, multproc=True,

2018-04-29 19:49:30,186 - memory_profile6_log - INFO -    117                                      threshold=0, start_date=None, end_date=None,

2018-04-29 19:49:30,187 - memory_profile6_log - INFO -    118                                      saveto="datastore", fitted_models_hist=None):

2018-04-29 19:49:30,187 - memory_profile6_log - INFO -    119                                 """

2018-04-29 19:49:30,188 - memory_profile6_log - INFO -    120                                     Main Process

2018-04-29 19:49:30,191 - memory_profile6_log - INFO -    121                                 """

2018-04-29 19:49:30,191 - memory_profile6_log - INFO -    122                                 # ~ Data Preprocessing ~

2018-04-29 19:49:30,193 - memory_profile6_log - INFO -    123                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-29 19:49:30,194 - memory_profile6_log - INFO -    124                                 # D(u, t)

2018-04-29 19:49:30,194 - memory_profile6_log - INFO -    125    650.1 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-29 19:49:30,194 - memory_profile6_log - INFO -    126    662.8 MiB     12.6 MiB       df_dut = df_input.copy(deep=True)

2018-04-29 19:49:30,194 - memory_profile6_log - INFO -    127    662.8 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 19:49:30,194 - memory_profile6_log - INFO -    128                             

2018-04-29 19:49:30,196 - memory_profile6_log - INFO -    129                                 # D(t)

2018-04-29 19:49:30,197 - memory_profile6_log - INFO -    130    675.1 MiB     12.4 MiB       df_dt = df_current.copy(deep=True)

2018-04-29 19:49:30,197 - memory_profile6_log - INFO -    131    675.1 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-29 19:49:30,198 - memory_profile6_log - INFO -    132                             

2018-04-29 19:49:30,198 - memory_profile6_log - INFO -    133                                 # ~~~~~~ Begin train ~~~~~~

2018-04-29 19:49:30,200 - memory_profile6_log - INFO -    134    675.1 MiB      0.0 MiB       t0 = time.time()

2018-04-29 19:49:30,200 - memory_profile6_log - INFO -    135    675.1 MiB      0.0 MiB       logger.info("train on: %d total genuine interest data(D(u, t))", len(df_dut))

2018-04-29 19:49:30,200 - memory_profile6_log - INFO -    136    675.1 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-29 19:49:30,203 - memory_profile6_log - INFO -    137    675.1 MiB      0.0 MiB       logger.info("apply on: %d total history...)", len(df_hist))

2018-04-29 19:49:30,204 - memory_profile6_log - INFO -    138                             

2018-04-29 19:49:30,209 - memory_profile6_log - INFO -    139                                 # instantiace class

2018-04-29 19:49:30,210 - memory_profile6_log - INFO -    140    675.1 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-29 19:49:30,210 - memory_profile6_log - INFO -    141                             

2018-04-29 19:49:30,210 - memory_profile6_log - INFO -    142                                 # ~~ Fit ~~

2018-04-29 19:49:30,210 - memory_profile6_log - INFO -    143                                 #   handling genuine news interest < current date

2018-04-29 19:49:30,210 - memory_profile6_log - INFO -    144    676.9 MiB      1.8 MiB       NB = BR.processX(df_dut)

2018-04-29 19:49:30,211 - memory_profile6_log - INFO -    145                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-29 19:49:30,211 - memory_profile6_log - INFO -    146                                 #   nanti dipindah ke class train utama

2018-04-29 19:49:30,217 - memory_profile6_log - INFO -    147    692.4 MiB     15.5 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-29 19:49:30,217 - memory_profile6_log - INFO -    148                                 """

2018-04-29 19:49:30,219 - memory_profile6_log - INFO -    149                                     num_y = total global click for category=ci on periode t

2018-04-29 19:49:30,219 - memory_profile6_log - INFO -    150                                     num_x = total click from user_U for category=ci on periode t

2018-04-29 19:49:30,220 - memory_profile6_log - INFO -    151                                 """

2018-04-29 19:49:30,220 - memory_profile6_log - INFO -    152    692.4 MiB      0.0 MiB       fitby_sigmant = True

2018-04-29 19:49:30,220 - memory_profile6_log - INFO -    153    692.4 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-29 19:49:30,223 - memory_profile6_log - INFO -    154    692.4 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-29 19:49:30,223 - memory_profile6_log - INFO -    155    707.8 MiB     15.4 MiB                            'is_general']]

2018-04-29 19:49:30,223 - memory_profile6_log - INFO -    156                                 # get sigma_Nt from fitted_models_hist

2018-04-29 19:49:30,223 - memory_profile6_log - INFO -    157    714.0 MiB      6.2 MiB       uniques_fit_hist = fitted_models_hist[['user_id', 'sigma_Nt']]

2018-04-29 19:49:30,224 - memory_profile6_log - INFO -    158    714.0 MiB      0.0 MiB       logger.info("len of uniques_fit_hist:%d", len(uniques_fit_hist))

2018-04-29 19:49:30,224 - memory_profile6_log - INFO -    159    710.6 MiB     -3.4 MiB       uniques_fit_hist = uniques_fit_hist.drop_duplicates(subset=['user_id','sigma_Nt'])

2018-04-29 19:49:30,227 - memory_profile6_log - INFO -    160    710.6 MiB      0.0 MiB       logger.info("len of uniques_fit_hist after drop duplicate:%d", len(uniques_fit_hist))

2018-04-29 19:49:30,229 - memory_profile6_log - INFO -    161                             

2018-04-29 19:49:30,230 - memory_profile6_log - INFO -    162                                 # begin fit

2018-04-29 19:49:30,230 - memory_profile6_log - INFO -    163    710.6 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-29 19:49:30,232 - memory_profile6_log - INFO -    164    710.6 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-29 19:49:30,232 - memory_profile6_log - INFO -    165    753.1 MiB     42.5 MiB                          sigma_nt_hist=uniques_fit_hist, verbose=False)

2018-04-29 19:49:30,232 - memory_profile6_log - INFO -    166    753.1 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-29 19:49:30,233 - memory_profile6_log - INFO -    167    753.1 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-29 19:49:30,233 - memory_profile6_log - INFO -    168                             

2018-04-29 19:49:30,233 - memory_profile6_log - INFO -    169                                 # ~~ and Transform ~~

2018-04-29 19:49:30,233 - memory_profile6_log - INFO -    170                                 #   handling current news interest == current date

2018-04-29 19:49:30,233 - memory_profile6_log - INFO -    171    753.1 MiB      0.0 MiB       if df_dt.empty:

2018-04-29 19:49:30,234 - memory_profile6_log - INFO -    172                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-29 19:49:30,234 - memory_profile6_log - INFO -    173                                     return None

2018-04-29 19:49:30,234 - memory_profile6_log - INFO -    174    753.2 MiB      0.1 MiB       NB = BR.processX(df_dt)

2018-04-29 19:49:30,236 - memory_profile6_log - INFO -    175    771.2 MiB     18.0 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-29 19:49:30,236 - memory_profile6_log - INFO -    176                             

2018-04-29 19:49:30,236 - memory_profile6_log - INFO -    177    771.2 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-29 19:49:30,240 - memory_profile6_log - INFO -    178    770.8 MiB     -0.4 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-29 19:49:30,242 - memory_profile6_log - INFO -    179                             

2018-04-29 19:49:30,243 - memory_profile6_log - INFO -    180    770.8 MiB      0.0 MiB       print "model_fit dtypes:\n", model_fit.dtypes

2018-04-29 19:49:30,243 - memory_profile6_log - INFO -    181    770.8 MiB      0.0 MiB       print "fitted_models_hist dtypes:\n", fitted_models_hist.dtypes

2018-04-29 19:49:30,243 - memory_profile6_log - INFO -    182    770.8 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-29 19:49:30,243 - memory_profile6_log - INFO -    183    770.8 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-29 19:49:30,243 - memory_profile6_log - INFO -    184    770.8 MiB      0.0 MiB                                                     fitted_model_hist=fitted_models_hist[["pt_posterior_x_Nt",

2018-04-29 19:49:30,243 - memory_profile6_log - INFO -    185    780.0 MiB      9.2 MiB                                                                                           "topic_id", "user_id"]],

2018-04-29 19:49:30,244 - memory_profile6_log - INFO -    186    801.3 MiB     21.3 MiB                                                     verbose=False)

2018-04-29 19:49:30,244 - memory_profile6_log - INFO -    187                                 # ~~~ filter is general and specific topic ~~~

2018-04-29 19:49:30,246 - memory_profile6_log - INFO -    188                                 # the idea is just we need to rerank every topic according

2018-04-29 19:49:30,247 - memory_profile6_log - INFO -    189                                 # user_id and and is_general by p0_posterior

2018-04-29 19:49:30,247 - memory_profile6_log - INFO -    190    801.3 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-29 19:49:30,247 - memory_profile6_log - INFO -    191    804.4 MiB      3.1 MiB                                     'is_general']].groupby(['topic_id',

2018-04-29 19:49:30,252 - memory_profile6_log - INFO -    192    801.4 MiB     -3.0 MiB                                                             'is_general']

2018-04-29 19:49:30,253 - memory_profile6_log - INFO -    193                                                                                      ).size().to_frame().reset_index()

2018-04-29 19:49:30,253 - memory_profile6_log - INFO -    194                             

2018-04-29 19:49:30,253 - memory_profile6_log - INFO -    195                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-29 19:49:30,253 - memory_profile6_log - INFO -    196                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-29 19:49:30,256 - memory_profile6_log - INFO -    197    801.4 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-29 19:49:30,256 - memory_profile6_log - INFO -    198                             

2018-04-29 19:49:30,256 - memory_profile6_log - INFO -    199                                 # ~ start by provide rank for each topic type ~

2018-04-29 19:49:30,256 - memory_profile6_log - INFO -    200    808.2 MiB      6.7 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-29 19:49:30,257 - memory_profile6_log - INFO -    201    815.9 MiB      7.8 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-29 19:49:30,257 - memory_profile6_log - INFO -    202                             

2018-04-29 19:49:30,259 - memory_profile6_log - INFO -    203                                 # ~ set threshold to filter output

2018-04-29 19:49:30,259 - memory_profile6_log - INFO -    204    815.9 MiB      0.0 MiB       if threshold > 0:

2018-04-29 19:49:30,259 - memory_profile6_log - INFO -    205    815.9 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-29 19:49:30,259 - memory_profile6_log - INFO -    206    815.9 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-29 19:49:30,259 - memory_profile6_log - INFO -    207    812.8 MiB     -3.1 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-29 19:49:30,263 - memory_profile6_log - INFO -    208                             

2018-04-29 19:49:30,265 - memory_profile6_log - INFO -    209    812.8 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-29 19:49:30,266 - memory_profile6_log - INFO -    210    812.8 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-29 19:49:30,266 - memory_profile6_log - INFO -    211                             

2018-04-29 19:49:30,266 - memory_profile6_log - INFO -    212    812.8 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 19:49:30,266 - memory_profile6_log - INFO -    213                             

2018-04-29 19:49:30,267 - memory_profile6_log - INFO -    214    812.8 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-29 19:49:30,267 - memory_profile6_log - INFO -    215    812.8 MiB      0.0 MiB       del df_dut

2018-04-29 19:49:30,267 - memory_profile6_log - INFO -    216    812.8 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-29 19:49:30,269 - memory_profile6_log - INFO -    217    812.8 MiB      0.0 MiB       del df_dt

2018-04-29 19:49:30,269 - memory_profile6_log - INFO -    218    812.8 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-29 19:49:30,269 - memory_profile6_log - INFO -    219    812.8 MiB      0.0 MiB       del df_input

2018-04-29 19:49:30,269 - memory_profile6_log - INFO -    220    812.8 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-29 19:49:30,269 - memory_profile6_log - INFO -    221    797.8 MiB    -15.0 MiB       del df_input_X

2018-04-29 19:49:30,270 - memory_profile6_log - INFO -    222    797.8 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-29 19:49:30,275 - memory_profile6_log - INFO -    223    797.8 MiB      0.0 MiB       del df_current

2018-04-29 19:49:30,275 - memory_profile6_log - INFO -    224    797.8 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-29 19:49:30,276 - memory_profile6_log - INFO -    225    797.8 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-29 19:49:30,276 - memory_profile6_log - INFO -    226    797.8 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-29 19:49:30,276 - memory_profile6_log - INFO -    227    760.9 MiB    -37.0 MiB       del model_fit

2018-04-29 19:49:30,276 - memory_profile6_log - INFO -    228    760.9 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-29 19:49:30,278 - memory_profile6_log - INFO -    229    760.9 MiB      0.0 MiB       del result

2018-04-29 19:49:30,278 - memory_profile6_log - INFO -    230    760.9 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-29 19:49:30,278 - memory_profile6_log - INFO -    231                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 19:49:30,279 - memory_profile6_log - INFO -    232                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 19:49:30,280 - memory_profile6_log - INFO -    233                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-29 19:49:30,282 - memory_profile6_log - INFO -    234    760.9 MiB      0.0 MiB       if savetrain:

2018-04-29 19:49:30,282 - memory_profile6_log - INFO -    235    769.0 MiB      8.1 MiB           model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-29 19:49:30,282 - memory_profile6_log - INFO -    236    769.0 MiB      0.0 MiB           del model_transform

2018-04-29 19:49:30,282 - memory_profile6_log - INFO -    237    769.0 MiB      0.0 MiB           logger.info("deleting model_transform...")

2018-04-29 19:49:30,288 - memory_profile6_log - INFO -    238    769.0 MiB      0.0 MiB           logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-29 19:49:30,288 - memory_profile6_log - INFO -    239                             

2018-04-29 19:49:30,289 - memory_profile6_log - INFO -    240    769.0 MiB      0.0 MiB           logger.info("Begin saving trained data...")

2018-04-29 19:49:30,290 - memory_profile6_log - INFO -    241                                     # ~ Place your code to save the training model here ~

2018-04-29 19:49:30,290 - memory_profile6_log - INFO -    242    769.0 MiB      0.0 MiB           if str(saveto).lower() == "datastore":

2018-04-29 19:49:30,292 - memory_profile6_log - INFO -    243    769.0 MiB      0.0 MiB               logger.info("Using google datastore as storage...")

2018-04-29 19:49:30,292 - memory_profile6_log - INFO -    244    769.0 MiB      0.0 MiB               if multproc:

2018-04-29 19:49:30,292 - memory_profile6_log - INFO -    245                                             # ~ save transform models ~

2018-04-29 19:49:30,292 - memory_profile6_log - INFO -    246    740.8 MiB    -28.1 MiB                   mh.saveDataStorePutMulti(model_transformsv)

2018-04-29 19:49:30,292 - memory_profile6_log - INFO -    247                             

2018-04-29 19:49:30,292 - memory_profile6_log - INFO -    248                                             """

2018-04-29 19:49:30,293 - memory_profile6_log - INFO -    249                                             # ~ save fitted models ~

2018-04-29 19:49:30,293 - memory_profile6_log - INFO -    250                                             logger.info("Saving fitted_models as history...")

2018-04-29 19:49:30,295 - memory_profile6_log - INFO -    251                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 19:49:30,295 - memory_profile6_log - INFO -    252                                             fitted_models_sigmant = pd.merge(fitted_models, save_sigma_nt, on=['user_id'])

2018-04-29 19:49:30,295 - memory_profile6_log - INFO -    253                             

2018-04-29 19:49:30,299 - memory_profile6_log - INFO -    254                                             X_split = np.array_split(fitted_models_sigmant, 10)

2018-04-29 19:49:30,299 - memory_profile6_log - INFO -    255                                             logger.info("Len of X_split for batch save fitted_models: %d", len(X_split))

2018-04-29 19:49:30,299 - memory_profile6_log - INFO -    256                                             for ix in range(len(X_split)):

2018-04-29 19:49:30,301 - memory_profile6_log - INFO -    257                                                 logger.info("processing batch-%d", ix)

2018-04-29 19:49:30,301 - memory_profile6_log - INFO -    258                                                 mh.saveDataStorePutMulti(X_split[ix], kinds='topic_recomendation_history')

2018-04-29 19:49:30,301 - memory_profile6_log - INFO -    259                             

2018-04-29 19:49:30,302 - memory_profile6_log - INFO -    260                                             del X_split

2018-04-29 19:49:30,302 - memory_profile6_log - INFO -    261                                             logger.info("deleting X_split...")

2018-04-29 19:49:30,302 - memory_profile6_log - INFO -    262                                             del save_sigma_nt

2018-04-29 19:49:30,302 - memory_profile6_log - INFO -    263                                             logger.info("deleting save_sigma_nt...")

2018-04-29 19:49:30,302 - memory_profile6_log - INFO -    264                                             """

2018-04-29 19:49:30,303 - memory_profile6_log - INFO -    265                             

2018-04-29 19:49:30,305 - memory_profile6_log - INFO -    266    740.8 MiB      0.0 MiB                   del BR

2018-04-29 19:49:30,305 - memory_profile6_log - INFO -    267    740.8 MiB      0.0 MiB                   logger.info("deleting BR...")

2018-04-29 19:49:30,306 - memory_profile6_log - INFO -    268                             

2018-04-29 19:49:30,306 - memory_profile6_log - INFO -    269                                     elif str(saveto).lower() == "elastic":

2018-04-29 19:49:30,308 - memory_profile6_log - INFO -    270                                         logger.info("Using ElasticSearch as storage...")

2018-04-29 19:49:30,312 - memory_profile6_log - INFO -    271                                         mh.saveElasticS(model_transformsv)

2018-04-29 19:49:30,312 - memory_profile6_log - INFO -    272                             

2018-04-29 19:49:30,313 - memory_profile6_log - INFO -    273                                     # need save sigma_nt for daily train

2018-04-29 19:49:30,315 - memory_profile6_log - INFO -    274                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-29 19:49:30,315 - memory_profile6_log - INFO -    275                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-29 19:49:30,315 - memory_profile6_log - INFO -    276    740.8 MiB      0.0 MiB           if start_date and end_date:

2018-04-29 19:49:30,315 - memory_profile6_log - INFO -    277                                         if not fitby_sigmant:

2018-04-29 19:49:30,316 - memory_profile6_log - INFO -    278                                             logging.info("Saving sigma Nt...")

2018-04-29 19:49:30,316 - memory_profile6_log - INFO -    279                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-29 19:49:30,318 - memory_profile6_log - INFO -    280                                             save_sigma_nt['start_date'] = start_date

2018-04-29 19:49:30,318 - memory_profile6_log - INFO -    281                                             save_sigma_nt['end_date'] = end_date

2018-04-29 19:49:30,319 - memory_profile6_log - INFO -    282                                             print save_sigma_nt.head(5)

2018-04-29 19:49:30,319 - memory_profile6_log - INFO -    283                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-29 19:49:30,322 - memory_profile6_log - INFO -    284                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-29 19:49:30,322 - memory_profile6_log - INFO -    285    740.8 MiB      0.0 MiB       return

2018-04-29 19:49:30,323 - memory_profile6_log - INFO - 


2018-04-29 19:49:30,325 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~

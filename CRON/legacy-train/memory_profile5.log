2018-04-27 09:26:30,618 - memory_profile5_log - INFO - Generating date range with N: 3
2018-04-27 09:26:30,622 - memory_profile5_log - INFO - date_generated: 
2018-04-27 09:26:30,622 - memory_profile5_log - INFO -  
2018-04-27 09:26:30,624 - memory_profile5_log - INFO - [datetime.datetime(2018, 3, 12, 0, 0), datetime.datetime(2018, 3, 13, 0, 0), datetime.datetime(2018, 3, 14, 0, 0)]
2018-04-27 09:26:30,625 - memory_profile5_log - INFO - 

2018-04-27 09:26:30,625 - memory_profile5_log - INFO - using current date: 2018-03-15
2018-04-27 09:26:30,625 - memory_profile5_log - INFO - using start date: 2018-03-12 00:00:00
2018-04-27 09:26:30,627 - memory_profile5_log - INFO - using end date: 2018-03-14
2018-04-27 09:26:30,743 - memory_profile5_log - INFO - Starting data fetch iterative...
2018-04-27 09:26:30,743 - memory_profile5_log - INFO - Collecting training data for date: 2018-03-12
2018-04-27 09:27:24,450 - memory_profile5_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 09:27:24,453 - memory_profile5_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 09:27:24,454 - memory_profile5_log - INFO - ================================================

2018-04-27 09:27:24,456 - memory_profile5_log - INFO -     66     86.7 MiB     86.7 MiB   @profile

2018-04-27 09:27:24,457 - memory_profile5_log - INFO -     67                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-27 09:27:24,457 - memory_profile5_log - INFO -     68                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-27 09:27:24,459 - memory_profile5_log - INFO -     69     86.7 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 09:27:24,460 - memory_profile5_log - INFO -     70     86.7 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 09:27:24,463 - memory_profile5_log - INFO -     71                             

2018-04-27 09:27:24,466 - memory_profile5_log - INFO -     72     90.0 MiB      3.2 MiB       result  = client.query(query, job_config=job_config)

2018-04-27 09:27:24,467 - memory_profile5_log - INFO -     73                             

2018-04-27 09:27:24,467 - memory_profile5_log - INFO -     74    254.7 MiB    164.7 MiB       rows = list(result)

2018-04-27 09:27:24,469 - memory_profile5_log - INFO -     75                             

2018-04-27 09:27:24,470 - memory_profile5_log - INFO -     76    254.7 MiB      0.0 MiB       col_name = []

2018-04-27 09:27:24,471 - memory_profile5_log - INFO -     77    254.7 MiB      0.0 MiB       for val in rows[:1]:

2018-04-27 09:27:24,474 - memory_profile5_log - INFO -     78    254.7 MiB      0.0 MiB           for k in list(val.items()):

2018-04-27 09:27:24,476 - memory_profile5_log - INFO -     79    254.7 MiB      0.0 MiB               col_name.append(k[0])

2018-04-27 09:27:24,480 - memory_profile5_log - INFO -     80                                 

2018-04-27 09:27:24,480 - memory_profile5_log - INFO -     81    254.7 MiB      0.0 MiB       data = []

2018-04-27 09:27:24,483 - memory_profile5_log - INFO -     82    255.2 MiB  -1421.4 MiB       for row in rows:

2018-04-27 09:27:24,487 - memory_profile5_log - INFO -     83    255.2 MiB  -1420.9 MiB           data.append(dict(list(row.items())).values())

2018-04-27 09:27:24,489 - memory_profile5_log - INFO -     84                             

2018-04-27 09:27:24,490 - memory_profile5_log - INFO -     85    259.0 MiB      3.8 MiB       df = pd.DataFrame(data, columns=col_name)

2018-04-27 09:27:24,490 - memory_profile5_log - INFO -     86    254.3 MiB     -4.7 MiB       del rows

2018-04-27 09:27:24,492 - memory_profile5_log - INFO -     87    254.3 MiB      0.0 MiB       del result

2018-04-27 09:27:24,494 - memory_profile5_log - INFO -     88    239.8 MiB    -14.5 MiB       del data

2018-04-27 09:27:24,496 - memory_profile5_log - INFO -     89    239.8 MiB      0.0 MiB       return df

2018-04-27 09:27:24,496 - memory_profile5_log - INFO - 


2018-04-27 09:29:27,913 - memory_profile5_log - INFO - Generating date range with N: 3
2018-04-27 09:29:27,915 - memory_profile5_log - INFO - date_generated: 
2018-04-27 09:29:27,917 - memory_profile5_log - INFO -  
2018-04-27 09:29:27,918 - memory_profile5_log - INFO - [datetime.datetime(2018, 4, 12, 0, 0), datetime.datetime(2018, 4, 13, 0, 0), datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-27 09:29:27,920 - memory_profile5_log - INFO - 

2018-04-27 09:29:27,920 - memory_profile5_log - INFO - using current date: 2018-04-15
2018-04-27 09:29:27,921 - memory_profile5_log - INFO - using start date: 2018-04-12 00:00:00
2018-04-27 09:29:27,921 - memory_profile5_log - INFO - using end date: 2018-04-14
2018-04-27 09:29:28,035 - memory_profile5_log - INFO - Starting data fetch iterative...
2018-04-27 09:29:28,036 - memory_profile5_log - INFO - Collecting training data for date: 2018-04-12
2018-04-27 09:35:33,470 - memory_profile5_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 09:35:33,471 - memory_profile5_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 09:35:33,473 - memory_profile5_log - INFO - ================================================

2018-04-27 09:35:33,474 - memory_profile5_log - INFO -     66     86.9 MiB     86.9 MiB   @profile

2018-04-27 09:35:33,476 - memory_profile5_log - INFO -     67                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-27 09:35:33,477 - memory_profile5_log - INFO -     68                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-27 09:35:33,477 - memory_profile5_log - INFO -     69     86.9 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 09:35:33,479 - memory_profile5_log - INFO -     70     86.9 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 09:35:33,480 - memory_profile5_log - INFO -     71                             

2018-04-27 09:35:33,483 - memory_profile5_log - INFO -     72     89.9 MiB      3.0 MiB       result  = client.query(query, job_config=job_config)

2018-04-27 09:35:33,483 - memory_profile5_log - INFO -     73                             

2018-04-27 09:35:33,486 - memory_profile5_log - INFO -     74    474.5 MiB    384.6 MiB       rows = list(result)

2018-04-27 09:35:33,486 - memory_profile5_log - INFO -     75                             

2018-04-27 09:35:33,487 - memory_profile5_log - INFO -     76    474.5 MiB      0.0 MiB       col_name = []

2018-04-27 09:35:33,489 - memory_profile5_log - INFO -     77    474.5 MiB      0.0 MiB       for val in rows[:1]:

2018-04-27 09:35:33,490 - memory_profile5_log - INFO -     78    474.5 MiB      0.0 MiB           for k in list(val.items()):

2018-04-27 09:35:33,490 - memory_profile5_log - INFO -     79    474.5 MiB      0.0 MiB               col_name.append(k[0])

2018-04-27 09:35:33,490 - memory_profile5_log - INFO -     80                                 

2018-04-27 09:35:33,493 - memory_profile5_log - INFO -     81    474.5 MiB      0.0 MiB       data = []

2018-04-27 09:35:33,494 - memory_profile5_log - INFO -     82    480.3 MiB  -9093.9 MiB       for row in rows:

2018-04-27 09:35:33,496 - memory_profile5_log - INFO -     83    480.3 MiB  -9088.2 MiB           data.append(dict(list(row.items())).values())

2018-04-27 09:35:33,496 - memory_profile5_log - INFO -     84                             

2018-04-27 09:35:33,497 - memory_profile5_log - INFO -     85    495.5 MiB     15.3 MiB       df = pd.DataFrame(data, columns=col_name)

2018-04-27 09:35:33,499 - memory_profile5_log - INFO -     86    419.8 MiB    -75.7 MiB       del rows

2018-04-27 09:35:33,500 - memory_profile5_log - INFO -     87    419.8 MiB      0.0 MiB       del result

2018-04-27 09:35:33,500 - memory_profile5_log - INFO -     88    404.3 MiB    -15.5 MiB       del data

2018-04-27 09:35:33,502 - memory_profile5_log - INFO -     89    404.3 MiB      0.0 MiB       return df

2018-04-27 09:35:33,505 - memory_profile5_log - INFO - 


2018-04-27 09:35:33,506 - memory_profile5_log - INFO - getting total: 462579 training data(genuine interest) for date: 2018-04-12
2018-04-27 09:35:33,507 - memory_profile5_log - INFO - Collecting training data for date: 2018-04-13
2018-04-27 09:38:22,253 - memory_profile5_log - INFO - Generating date range with N: 3
2018-04-27 09:38:22,256 - memory_profile5_log - INFO - date_generated: 
2018-04-27 09:38:22,256 - memory_profile5_log - INFO -  
2018-04-27 09:38:22,257 - memory_profile5_log - INFO - [datetime.datetime(2018, 4, 12, 0, 0), datetime.datetime(2018, 4, 13, 0, 0), datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-27 09:38:22,259 - memory_profile5_log - INFO - 

2018-04-27 09:38:22,259 - memory_profile5_log - INFO - using current date: 2018-04-15
2018-04-27 09:38:22,259 - memory_profile5_log - INFO - using start date: 2018-04-12 00:00:00
2018-04-27 09:38:22,259 - memory_profile5_log - INFO - using end date: 2018-04-14
2018-04-27 09:38:22,371 - memory_profile5_log - INFO - Starting data fetch iterative...
2018-04-27 09:38:22,372 - memory_profile5_log - INFO - Collecting training data for date: 2018-04-12
2018-04-27 09:41:49,108 - memory_profile5_log - INFO - Generating date range with N: 3
2018-04-27 09:41:49,111 - memory_profile5_log - INFO - date_generated: 
2018-04-27 09:41:49,111 - memory_profile5_log - INFO -  
2018-04-27 09:41:49,112 - memory_profile5_log - INFO - [datetime.datetime(2018, 4, 12, 0, 0), datetime.datetime(2018, 4, 13, 0, 0), datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-27 09:41:49,114 - memory_profile5_log - INFO - 

2018-04-27 09:41:49,114 - memory_profile5_log - INFO - using current date: 2018-04-15
2018-04-27 09:41:49,115 - memory_profile5_log - INFO - using start date: 2018-04-12 00:00:00
2018-04-27 09:41:49,115 - memory_profile5_log - INFO - using end date: 2018-04-14
2018-04-27 09:41:49,236 - memory_profile5_log - INFO - Starting data fetch iterative...
2018-04-27 09:41:49,237 - memory_profile5_log - INFO - Collecting training data for date: 2018-04-12
2018-04-27 09:48:29,737 - memory_profile5_log - INFO - Generating date range with N: 3
2018-04-27 09:48:29,740 - memory_profile5_log - INFO - date_generated: 
2018-04-27 09:48:29,742 - memory_profile5_log - INFO -  
2018-04-27 09:48:29,743 - memory_profile5_log - INFO - [datetime.datetime(2018, 4, 12, 0, 0), datetime.datetime(2018, 4, 13, 0, 0), datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-27 09:48:29,743 - memory_profile5_log - INFO - 

2018-04-27 09:48:29,743 - memory_profile5_log - INFO - using current date: 2018-04-15
2018-04-27 09:48:29,743 - memory_profile5_log - INFO - using start date: 2018-04-12 00:00:00
2018-04-27 09:48:29,743 - memory_profile5_log - INFO - using end date: 2018-04-14
2018-04-27 09:48:29,888 - memory_profile5_log - INFO - Starting data fetch iterative...
2018-04-27 09:48:29,888 - memory_profile5_log - INFO - Collecting training data for date: 2018-04-12
2018-04-27 09:50:49,180 - memory_profile5_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 09:50:49,180 - memory_profile5_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 09:50:49,181 - memory_profile5_log - INFO - ================================================

2018-04-27 09:50:49,184 - memory_profile5_log - INFO -     66     87.0 MiB     87.0 MiB   @profile

2018-04-27 09:50:49,184 - memory_profile5_log - INFO -     67                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-27 09:50:49,186 - memory_profile5_log - INFO -     68                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-27 09:50:49,187 - memory_profile5_log - INFO -     69     87.0 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 09:50:49,187 - memory_profile5_log - INFO -     70     87.0 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 09:50:49,188 - memory_profile5_log - INFO -     71                             

2018-04-27 09:50:49,190 - memory_profile5_log - INFO -     72     90.1 MiB      3.1 MiB       result  = client.query(query, job_config=job_config)

2018-04-27 09:50:49,193 - memory_profile5_log - INFO -     73     90.2 MiB      0.1 MiB       rows = result.result()

2018-04-27 09:50:49,194 - memory_profile5_log - INFO -     74                             

2018-04-27 09:50:49,196 - memory_profile5_log - INFO -     75     90.2 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-27 09:50:49,197 - memory_profile5_log - INFO -     76                                 

2018-04-27 09:50:49,198 - memory_profile5_log - INFO -     77     90.2 MiB      0.0 MiB       data = []

2018-04-27 09:50:49,200 - memory_profile5_log - INFO -     78    499.7 MiB   -580.1 MiB       for row in list(rows):

2018-04-27 09:50:49,203 - memory_profile5_log - INFO -     79    499.7 MiB   -934.8 MiB           data.append(list(row))

2018-04-27 09:50:49,204 - memory_profile5_log - INFO -     80                             

2018-04-27 09:50:49,206 - memory_profile5_log - INFO -     81    511.5 MiB     11.7 MiB       df = pd.DataFrame(data, columns=col_name)

2018-04-27 09:50:49,207 - memory_profile5_log - INFO -     82    511.5 MiB      0.0 MiB       del rows

2018-04-27 09:50:49,209 - memory_profile5_log - INFO -     83    511.5 MiB      0.0 MiB       del result

2018-04-27 09:50:49,210 - memory_profile5_log - INFO -     84    393.9 MiB   -117.6 MiB       del data

2018-04-27 09:50:49,210 - memory_profile5_log - INFO -     85    393.9 MiB      0.0 MiB       return df

2018-04-27 09:50:49,211 - memory_profile5_log - INFO - 


2018-04-27 09:50:49,214 - memory_profile5_log - INFO - getting total: 462579 training data(genuine interest) for date: 2018-04-12
2018-04-27 09:50:49,216 - memory_profile5_log - INFO - Collecting training data for date: 2018-04-13
2018-04-27 09:52:34,476 - memory_profile5_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 09:52:34,477 - memory_profile5_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 09:52:34,479 - memory_profile5_log - INFO - ================================================

2018-04-27 09:52:34,480 - memory_profile5_log - INFO -     66    393.9 MiB    393.9 MiB   @profile

2018-04-27 09:52:34,480 - memory_profile5_log - INFO -     67                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-27 09:52:34,482 - memory_profile5_log - INFO -     68                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-27 09:52:34,483 - memory_profile5_log - INFO -     69    393.9 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 09:52:34,483 - memory_profile5_log - INFO -     70    393.9 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 09:52:34,484 - memory_profile5_log - INFO -     71                             

2018-04-27 09:52:34,487 - memory_profile5_log - INFO -     72    393.9 MiB      0.0 MiB       result  = client.query(query, job_config=job_config)

2018-04-27 09:52:34,490 - memory_profile5_log - INFO -     73    393.9 MiB      0.0 MiB       rows = result.result()

2018-04-27 09:52:34,490 - memory_profile5_log - INFO -     74                             

2018-04-27 09:52:34,492 - memory_profile5_log - INFO -     75    393.9 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-27 09:52:34,493 - memory_profile5_log - INFO -     76                                 

2018-04-27 09:52:34,494 - memory_profile5_log - INFO -     77    393.9 MiB      0.0 MiB       data = []

2018-04-27 09:52:34,496 - memory_profile5_log - INFO -     78    576.8 MiB  -2067.8 MiB       for row in list(rows):

2018-04-27 09:52:34,496 - memory_profile5_log - INFO -     79    576.8 MiB  -2209.7 MiB           data.append(list(row))

2018-04-27 09:52:34,499 - memory_profile5_log - INFO -     80                             

2018-04-27 09:52:34,500 - memory_profile5_log - INFO -     81    585.8 MiB      9.0 MiB       df = pd.DataFrame(data, columns=col_name)

2018-04-27 09:52:34,502 - memory_profile5_log - INFO -     82    585.8 MiB      0.0 MiB       del rows

2018-04-27 09:52:34,503 - memory_profile5_log - INFO -     83    585.8 MiB      0.0 MiB       del result

2018-04-27 09:52:34,503 - memory_profile5_log - INFO -     84    498.0 MiB    -87.8 MiB       del data

2018-04-27 09:52:34,505 - memory_profile5_log - INFO -     85    498.0 MiB      0.0 MiB       return df

2018-04-27 09:52:34,506 - memory_profile5_log - INFO - 


2018-04-27 09:52:34,509 - memory_profile5_log - INFO - getting total: 361145 training data(genuine interest) for date: 2018-04-13
2018-04-27 09:52:34,509 - memory_profile5_log - INFO - Collecting training data for date: 2018-04-14
2018-04-27 09:53:52,196 - memory_profile5_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 09:53:52,200 - memory_profile5_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 09:53:52,201 - memory_profile5_log - INFO - ================================================

2018-04-27 09:53:52,204 - memory_profile5_log - INFO -     66    498.0 MiB    498.0 MiB   @profile

2018-04-27 09:53:52,206 - memory_profile5_log - INFO -     67                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-27 09:53:52,207 - memory_profile5_log - INFO -     68                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-27 09:53:52,210 - memory_profile5_log - INFO -     69    498.0 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 09:53:52,213 - memory_profile5_log - INFO -     70    498.0 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 09:53:52,213 - memory_profile5_log - INFO -     71                             

2018-04-27 09:53:52,214 - memory_profile5_log - INFO -     72    498.0 MiB      0.0 MiB       result  = client.query(query, job_config=job_config)

2018-04-27 09:53:52,216 - memory_profile5_log - INFO -     73    498.0 MiB      0.0 MiB       rows = result.result()

2018-04-27 09:53:52,217 - memory_profile5_log - INFO -     74                             

2018-04-27 09:53:52,219 - memory_profile5_log - INFO -     75    498.0 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-27 09:53:52,220 - memory_profile5_log - INFO -     76                                 

2018-04-27 09:53:52,221 - memory_profile5_log - INFO -     77    498.0 MiB      0.0 MiB       data = []

2018-04-27 09:53:52,223 - memory_profile5_log - INFO -     78    627.9 MiB  -7965.1 MiB       for row in list(rows):

2018-04-27 09:53:52,223 - memory_profile5_log - INFO -     79    627.9 MiB  -8080.7 MiB           data.append(list(row))

2018-04-27 09:53:52,224 - memory_profile5_log - INFO -     80                             

2018-04-27 09:53:52,224 - memory_profile5_log - INFO -     81    634.2 MiB      6.3 MiB       df = pd.DataFrame(data, columns=col_name)

2018-04-27 09:53:52,226 - memory_profile5_log - INFO -     82    634.2 MiB      0.0 MiB       del rows

2018-04-27 09:53:52,226 - memory_profile5_log - INFO -     83    634.2 MiB      0.0 MiB       del result

2018-04-27 09:53:52,230 - memory_profile5_log - INFO -     84    569.7 MiB    -64.5 MiB       del data

2018-04-27 09:53:52,230 - memory_profile5_log - INFO -     85    569.7 MiB      0.0 MiB       return df

2018-04-27 09:53:52,232 - memory_profile5_log - INFO - 


2018-04-27 09:53:52,233 - memory_profile5_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-27 09:53:52,233 - memory_profile5_log - INFO - len datalist: 3
2018-04-27 09:53:52,234 - memory_profile5_log - INFO - All data fetch iterative done!!
2018-04-27 09:53:52,342 - memory_profile5_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-27 09:55:02,733 - memory_profile5_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 09:55:02,734 - memory_profile5_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 09:55:02,736 - memory_profile5_log - INFO - ================================================

2018-04-27 09:55:02,736 - memory_profile5_log - INFO -     66    588.8 MiB    588.8 MiB   @profile

2018-04-27 09:55:02,739 - memory_profile5_log - INFO -     67                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-27 09:55:02,739 - memory_profile5_log - INFO -     68                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-27 09:55:02,740 - memory_profile5_log - INFO -     69    588.8 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 09:55:02,740 - memory_profile5_log - INFO -     70    588.8 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 09:55:02,742 - memory_profile5_log - INFO -     71                             

2018-04-27 09:55:02,746 - memory_profile5_log - INFO -     72    588.8 MiB      0.0 MiB       result  = client.query(query, job_config=job_config)

2018-04-27 09:55:02,746 - memory_profile5_log - INFO -     73    588.8 MiB      0.0 MiB       rows = result.result()

2018-04-27 09:55:02,749 - memory_profile5_log - INFO -     74                             

2018-04-27 09:55:02,750 - memory_profile5_log - INFO -     75    588.8 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-27 09:55:02,750 - memory_profile5_log - INFO -     76                                 

2018-04-27 09:55:02,752 - memory_profile5_log - INFO -     77    588.8 MiB      0.0 MiB       data = []

2018-04-27 09:55:02,753 - memory_profile5_log - INFO -     78    713.2 MiB  -1665.9 MiB       for row in list(rows):

2018-04-27 09:55:02,755 - memory_profile5_log - INFO -     79    713.2 MiB  -1781.5 MiB           data.append(list(row))

2018-04-27 09:55:02,756 - memory_profile5_log - INFO -     80                             

2018-04-27 09:55:02,757 - memory_profile5_log - INFO -     81    718.9 MiB      5.7 MiB       df = pd.DataFrame(data, columns=col_name)

2018-04-27 09:55:02,759 - memory_profile5_log - INFO -     82    718.9 MiB      0.0 MiB       del rows

2018-04-27 09:55:02,759 - memory_profile5_log - INFO -     83    718.9 MiB      0.0 MiB       del result

2018-04-27 09:55:02,760 - memory_profile5_log - INFO -     84    655.0 MiB    -63.9 MiB       del data

2018-04-27 09:55:02,762 - memory_profile5_log - INFO -     85    655.0 MiB      0.0 MiB       return df

2018-04-27 09:55:02,763 - memory_profile5_log - INFO - 


2018-04-27 09:55:02,763 - memory_profile5_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-27 09:55:02,772 - memory_profile5_log - INFO - loading time of: 1308144 total genuine-current interest data ~ take 392.885s
2018-04-27 09:55:02,780 - memory_profile5_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 09:55:02,782 - memory_profile5_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 09:55:02,783 - memory_profile5_log - INFO - ================================================

2018-04-27 09:55:02,789 - memory_profile5_log - INFO -    243     86.9 MiB     86.9 MiB   @profile

2018-04-27 09:55:02,790 - memory_profile5_log - INFO -    244                             def preprocess(loadmp, cpu, cd, query_fit, date_generated):

2018-04-27 09:55:02,792 - memory_profile5_log - INFO -    245     87.0 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-27 09:55:02,792 - memory_profile5_log - INFO -    246     87.0 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-27 09:55:02,795 - memory_profile5_log - INFO -    247                             

2018-04-27 09:55:02,796 - memory_profile5_log - INFO -    248                                 # ~~~ Begin collecting data ~~~

2018-04-27 09:55:02,796 - memory_profile5_log - INFO -    249     87.0 MiB      0.0 MiB       t0 = time.time()

2018-04-27 09:55:02,796 - memory_profile5_log - INFO -    250     87.0 MiB      0.0 MiB       datalist = []

2018-04-27 09:55:02,799 - memory_profile5_log - INFO -    251                             

2018-04-27 09:55:02,802 - memory_profile5_log - INFO -    252    498.0 MiB      0.0 MiB       def _getBig(procdate, client):

2018-04-27 09:55:02,802 - memory_profile5_log - INFO -    253    498.0 MiB      0.0 MiB           logger.info("Collecting training data for date: %s", procdate)

2018-04-27 09:55:02,805 - memory_profile5_log - INFO -    254                                     # ~ get genuine news interest ~

2018-04-27 09:55:02,805 - memory_profile5_log - INFO -    255    498.0 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-27 09:55:02,805 - memory_profile5_log - INFO -    256                             

2018-04-27 09:55:02,806 - memory_profile5_log - INFO -    257                                     # safe handling of query parameter

2018-04-27 09:55:02,806 - memory_profile5_log - INFO -    258                                     query_params = [

2018-04-27 09:55:02,808 - memory_profile5_log - INFO -    259    498.0 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', procdate)

2018-04-27 09:55:02,809 - memory_profile5_log - INFO -    260                                     ]

2018-04-27 09:55:02,812 - memory_profile5_log - INFO -    261                             

2018-04-27 09:55:02,813 - memory_profile5_log - INFO -    262    498.0 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-27 09:55:02,815 - memory_profile5_log - INFO -    263                                     # temp_df = load_bigquery(client, query_fit + query_fit_where, job_config)

2018-04-27 09:55:02,815 - memory_profile5_log - INFO -    264    569.7 MiB    482.6 MiB           temp_df = loadBQ(client, query_fit + query_fit_where, job_config)

2018-04-27 09:55:02,815 - memory_profile5_log - INFO -    265                             

2018-04-27 09:55:02,816 - memory_profile5_log - INFO -    266    569.7 MiB      0.0 MiB           if temp_df.empty:

2018-04-27 09:55:02,818 - memory_profile5_log - INFO -    267                                         logger.info("%s data is empty!", procdate)

2018-04-27 09:55:02,818 - memory_profile5_log - INFO -    268                                         return None

2018-04-27 09:55:02,822 - memory_profile5_log - INFO -    269                                     else:

2018-04-27 09:55:02,822 - memory_profile5_log - INFO -    270    569.7 MiB      0.0 MiB               logger.info("getting total: %d training data(genuine interest) for date: %s" % (len(temp_df), procdate))

2018-04-27 09:55:02,825 - memory_profile5_log - INFO -    271    569.7 MiB      0.0 MiB               if loadmp:

2018-04-27 09:55:02,825 - memory_profile5_log - INFO -    272                                             logger.info("Exiting: %s", mp.current_process().name)

2018-04-27 09:55:02,825 - memory_profile5_log - INFO -    273    569.7 MiB      0.0 MiB               return temp_df

2018-04-27 09:55:02,826 - memory_profile5_log - INFO -    274                             

2018-04-27 09:55:02,828 - memory_profile5_log - INFO -    275     87.0 MiB      0.0 MiB       if loadmp:

2018-04-27 09:55:02,828 - memory_profile5_log - INFO -    276                                     logger.info("Starting data fetch multiprocess..")

2018-04-27 09:55:02,829 - memory_profile5_log - INFO -    277                                     logger.info("number of process: %d", len(date_generated))

2018-04-27 09:55:02,831 - memory_profile5_log - INFO -    278                             

2018-04-27 09:55:02,835 - memory_profile5_log - INFO -    279                                     pool = mp.Pool(processes=cpu)

2018-04-27 09:55:02,835 - memory_profile5_log - INFO -    280                                     # multprocessA = [pool.apply(_getBig, args=(ndate.strftime("%Y-%m-%d"), )) for ndate in date_generated]

2018-04-27 09:55:02,838 - memory_profile5_log - INFO -    281                                     multprocessA = [pool.apply_async(_getBig, args=(ndate.strftime("%Y-%m-%d"), bq_client, )) for ndate in date_generated]

2018-04-27 09:55:02,838 - memory_profile5_log - INFO -    282                                     output_multprocessA = [p.get() for p in multprocessA]

2018-04-27 09:55:02,838 - memory_profile5_log - INFO -    283                             

2018-04-27 09:55:02,838 - memory_profile5_log - INFO -    284                                     for m in output_multprocessA:

2018-04-27 09:55:02,839 - memory_profile5_log - INFO -    285                                         if m is not None:

2018-04-27 09:55:02,841 - memory_profile5_log - INFO -    286                                             if not m.empty:

2018-04-27 09:55:02,841 - memory_profile5_log - INFO -    287                                                 datalist.append(m)

2018-04-27 09:55:02,842 - memory_profile5_log - INFO -    288                             

2018-04-27 09:55:02,842 - memory_profile5_log - INFO -    289                                     logger.info("len datalist: %d", len(datalist))

2018-04-27 09:55:02,842 - memory_profile5_log - INFO -    290                                     logger.info("All data fetch multiprocess done!!")

2018-04-27 09:55:02,846 - memory_profile5_log - INFO -    291                                 else:

2018-04-27 09:55:02,848 - memory_profile5_log - INFO -    292     87.0 MiB      0.0 MiB           logger.info("Starting data fetch iterative...")

2018-04-27 09:55:02,848 - memory_profile5_log - INFO -    293    569.7 MiB      0.0 MiB           for ndate in date_generated:

2018-04-27 09:55:02,849 - memory_profile5_log - INFO -    294    569.7 MiB      0.0 MiB               tframe = _getBig(ndate.strftime("%Y-%m-%d"), bq_client)

2018-04-27 09:55:02,851 - memory_profile5_log - INFO -    295    569.7 MiB      0.0 MiB               if tframe is not None:

2018-04-27 09:55:02,851 - memory_profile5_log - INFO -    296    569.7 MiB      0.0 MiB                   if not tframe.empty:

2018-04-27 09:55:02,854 - memory_profile5_log - INFO -    297    569.7 MiB      0.0 MiB                       datalist.append(tframe)

2018-04-27 09:55:02,854 - memory_profile5_log - INFO -    298                                         else: 

2018-04-27 09:55:02,855 - memory_profile5_log - INFO -    299                                             logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-27 09:55:02,855 - memory_profile5_log - INFO -    300    569.7 MiB      0.0 MiB           logger.info("len datalist: %d", len(datalist))

2018-04-27 09:55:02,855 - memory_profile5_log - INFO -    301    569.7 MiB      0.0 MiB           logger.info("All data fetch iterative done!!")

2018-04-27 09:55:02,858 - memory_profile5_log - INFO -    302                             

2018-04-27 09:55:02,861 - memory_profile5_log - INFO -    303    569.7 MiB      0.0 MiB       if len(datalist) > 1:

2018-04-27 09:55:02,862 - memory_profile5_log - INFO -    304    620.1 MiB     50.5 MiB           big_frame = pd.concat(datalist)

2018-04-27 09:55:02,864 - memory_profile5_log - INFO -    305    588.7 MiB    -31.4 MiB           del datalist

2018-04-27 09:55:02,864 - memory_profile5_log - INFO -    306                                 else:

2018-04-27 09:55:02,865 - memory_profile5_log - INFO -    307                                     big_frame = datalist

2018-04-27 09:55:02,865 - memory_profile5_log - INFO -    308    588.8 MiB      0.1 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-27 09:55:02,865 - memory_profile5_log - INFO -    309                             

2018-04-27 09:55:02,865 - memory_profile5_log - INFO -    310                                 # ~ get current news interest ~

2018-04-27 09:55:02,868 - memory_profile5_log - INFO -    311    588.8 MiB      0.0 MiB       if not cd:

2018-04-27 09:55:02,869 - memory_profile5_log - INFO -    312                                     logger.info("Collecting training data(current date interest)..")

2018-04-27 09:55:02,871 - memory_profile5_log - INFO -    313                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-27 09:55:02,871 - memory_profile5_log - INFO -    314                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-27 09:55:02,871 - memory_profile5_log - INFO -    315                                 else:

2018-04-27 09:55:02,872 - memory_profile5_log - INFO -    316    588.8 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-27 09:55:02,872 - memory_profile5_log - INFO -    317    588.8 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-27 09:55:02,874 - memory_profile5_log - INFO -    318                             

2018-04-27 09:55:02,874 - memory_profile5_log - INFO -    319                                     # safe handling of query parameter

2018-04-27 09:55:02,875 - memory_profile5_log - INFO -    320                                     query_params = [

2018-04-27 09:55:02,875 - memory_profile5_log - INFO -    321    588.8 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-27 09:55:02,875 - memory_profile5_log - INFO -    322                                     ]

2018-04-27 09:55:02,881 - memory_profile5_log - INFO -    323                             

2018-04-27 09:55:02,882 - memory_profile5_log - INFO -    324    588.8 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-27 09:55:02,885 - memory_profile5_log - INFO -    325    655.0 MiB     66.2 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-27 09:55:02,885 - memory_profile5_log - INFO -    326    655.0 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-27 09:55:02,887 - memory_profile5_log - INFO -    327                             

2018-04-27 09:55:02,888 - memory_profile5_log - INFO -    328    655.1 MiB      0.1 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-27 09:55:02,888 - memory_profile5_log - INFO -    329    655.1 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-27 09:55:02,888 - memory_profile5_log - INFO -    330    655.1 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-27 09:55:02,891 - memory_profile5_log - INFO -    331                             

2018-04-27 09:55:02,891 - memory_profile5_log - INFO -    332    655.1 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-27 09:55:02,895 - memory_profile5_log - INFO -    333    655.1 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-27 09:55:02,898 - memory_profile5_log - INFO -    334                             

2018-04-27 09:55:02,898 - memory_profile5_log - INFO -    335    655.1 MiB      0.0 MiB       return big_frame, current_frame

2018-04-27 09:55:02,900 - memory_profile5_log - INFO - 


2018-04-27 09:55:02,905 - memory_profile5_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-27 09:55:02,982 - memory_profile5_log - INFO - train on: 1077719 total history data(D(u, t))
2018-04-27 09:55:02,983 - memory_profile5_log - INFO - transform on: 230425 total current data(D(t))
2018-04-27 09:55:07,789 - memory_profile5_log - INFO - Len of model_fit: 1077719
2018-04-27 09:55:07,790 - memory_profile5_log - INFO - Len of df_dut: 1077719
2018-04-27 09:56:38,546 - memory_profile5_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-27 09:56:38,651 - memory_profile5_log - INFO - Total train time: 95.669s
2018-04-27 09:56:38,653 - memory_profile5_log - INFO - memory left before cleaning: 76.700 percent memory...
2018-04-27 09:56:38,654 - memory_profile5_log - INFO - cleaning up some objects...
2018-04-27 09:56:38,654 - memory_profile5_log - INFO - deleting df_dut...
2018-04-27 09:56:38,655 - memory_profile5_log - INFO - deleting df_dt...
2018-04-27 09:56:38,657 - memory_profile5_log - INFO - deleting df_input...
2018-04-27 09:56:38,665 - memory_profile5_log - INFO - deleting df_input_X...
2018-04-27 09:56:38,667 - memory_profile5_log - INFO - deleting df_current...
2018-04-27 09:56:38,671 - memory_profile5_log - INFO - deleting map_topic_isgeneral...
2018-04-27 09:56:38,743 - memory_profile5_log - INFO - deleting model_fit...
2018-04-27 09:56:38,744 - memory_profile5_log - INFO - deleting result...
2018-04-27 09:56:38,770 - memory_profile5_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 09:56:38,772 - memory_profile5_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 09:56:38,772 - memory_profile5_log - INFO - ================================================

2018-04-27 09:56:38,773 - memory_profile5_log - INFO -     97    645.4 MiB    645.4 MiB   @profile

2018-04-27 09:56:38,773 - memory_profile5_log - INFO -     98                             def main(df_input, df_current, current_date, G,

2018-04-27 09:56:38,775 - memory_profile5_log - INFO -     99                                      project_id, savetrain=False, multproc=True,

2018-04-27 09:56:38,776 - memory_profile5_log - INFO -    100                                      threshold=0, start_date=None, end_date=None,

2018-04-27 09:56:38,776 - memory_profile5_log - INFO -    101                                      saveto="datastore"):

2018-04-27 09:56:38,776 - memory_profile5_log - INFO -    102                                 """

2018-04-27 09:56:38,778 - memory_profile5_log - INFO -    103                                     Main cron method

2018-04-27 09:56:38,779 - memory_profile5_log - INFO -    104                                 """

2018-04-27 09:56:38,779 - memory_profile5_log - INFO -    105                                 # ~ Data Preprocessing ~

2018-04-27 09:56:38,782 - memory_profile5_log - INFO -    106                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-27 09:56:38,785 - memory_profile5_log - INFO -    107                                 # D(u, t)

2018-04-27 09:56:38,786 - memory_profile5_log - INFO -    108    645.4 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-27 09:56:38,786 - memory_profile5_log - INFO -    109    679.3 MiB     33.9 MiB       df_dut = df_input.copy(deep=True)

2018-04-27 09:56:38,788 - memory_profile5_log - INFO -    110    679.3 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-27 09:56:38,788 - memory_profile5_log - INFO -    111                             

2018-04-27 09:56:38,789 - memory_profile5_log - INFO -    112                                 # D(t)

2018-04-27 09:56:38,789 - memory_profile5_log - INFO -    113    686.6 MiB      7.3 MiB       df_dt = df_current.copy(deep=True)

2018-04-27 09:56:38,789 - memory_profile5_log - INFO -    114    686.6 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-27 09:56:38,790 - memory_profile5_log - INFO -    115                             

2018-04-27 09:56:38,790 - memory_profile5_log - INFO -    116                                 # ~~~~~~ Begin train ~~~~~~

2018-04-27 09:56:38,795 - memory_profile5_log - INFO -    117    686.6 MiB      0.0 MiB       t0 = time.time()

2018-04-27 09:56:38,795 - memory_profile5_log - INFO -    118    686.6 MiB      0.0 MiB       logger.info("train on: %d total history data(D(u, t))", len(df_dut))

2018-04-27 09:56:38,796 - memory_profile5_log - INFO -    119    686.6 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-27 09:56:38,798 - memory_profile5_log - INFO -    120                             

2018-04-27 09:56:38,799 - memory_profile5_log - INFO -    121                                 # instantiace class

2018-04-27 09:56:38,799 - memory_profile5_log - INFO -    122    686.6 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-27 09:56:38,799 - memory_profile5_log - INFO -    123                             

2018-04-27 09:56:38,799 - memory_profile5_log - INFO -    124                                 # ~~ Fit ~~

2018-04-27 09:56:38,801 - memory_profile5_log - INFO -    125                                 #   handling genuine news interest < current date

2018-04-27 09:56:38,801 - memory_profile5_log - INFO -    126    690.6 MiB      4.0 MiB       NB = BR.processX(df_dut)

2018-04-27 09:56:38,802 - memory_profile5_log - INFO -    127                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-27 09:56:38,802 - memory_profile5_log - INFO -    128                                 #   nanti dipindah ke class train utama

2018-04-27 09:56:38,802 - memory_profile5_log - INFO -    129    733.0 MiB     42.4 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-27 09:56:38,806 - memory_profile5_log - INFO -    130                                 """

2018-04-27 09:56:38,808 - memory_profile5_log - INFO -    131                                     num_y = total global click for category=ci on periode t

2018-04-27 09:56:38,809 - memory_profile5_log - INFO -    132                                     num_x = total click from user_U for category=ci on periode t

2018-04-27 09:56:38,809 - memory_profile5_log - INFO -    133                                 """

2018-04-27 09:56:38,809 - memory_profile5_log - INFO -    134    733.0 MiB      0.0 MiB       fitby_sigmant = True

2018-04-27 09:56:38,811 - memory_profile5_log - INFO -    135    733.0 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-27 09:56:38,812 - memory_profile5_log - INFO -    136    733.0 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-27 09:56:38,812 - memory_profile5_log - INFO -    137    775.1 MiB     42.1 MiB                            'is_general']]

2018-04-27 09:56:38,812 - memory_profile5_log - INFO -    138    775.1 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-27 09:56:38,812 - memory_profile5_log - INFO -    139    775.1 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-27 09:56:38,813 - memory_profile5_log - INFO -    140    877.0 MiB    101.9 MiB                          verbose=False)

2018-04-27 09:56:38,813 - memory_profile5_log - INFO -    141    877.0 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-27 09:56:38,815 - memory_profile5_log - INFO -    142    877.0 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-27 09:56:38,815 - memory_profile5_log - INFO -    143                             

2018-04-27 09:56:38,815 - memory_profile5_log - INFO -    144                                 # ~~ and Transform ~~

2018-04-27 09:56:38,819 - memory_profile5_log - INFO -    145                                 #   handling current news interest == current date

2018-04-27 09:56:38,819 - memory_profile5_log - INFO -    146    877.0 MiB      0.0 MiB       if df_dt.empty:

2018-04-27 09:56:38,821 - memory_profile5_log - INFO -    147                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-27 09:56:38,821 - memory_profile5_log - INFO -    148                                     return None

2018-04-27 09:56:38,822 - memory_profile5_log - INFO -    149    877.6 MiB      0.6 MiB       NB = BR.processX(df_dt)

2018-04-27 09:56:38,822 - memory_profile5_log - INFO -    150    888.2 MiB     10.6 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-27 09:56:38,822 - memory_profile5_log - INFO -    151                             

2018-04-27 09:56:38,822 - memory_profile5_log - INFO -    152    888.2 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-27 09:56:38,823 - memory_profile5_log - INFO -    153    854.8 MiB    -33.3 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-27 09:56:38,823 - memory_profile5_log - INFO -    154    854.8 MiB      0.0 MiB       model_transform = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-27 09:56:38,825 - memory_profile5_log - INFO -    155    854.8 MiB      0.0 MiB                                      fitted_model=model_fit,

2018-04-27 09:56:38,825 - memory_profile5_log - INFO -    156    916.7 MiB     61.8 MiB                                      verbose=False)

2018-04-27 09:56:38,825 - memory_profile5_log - INFO -    157                                 # ~~~ filter is general and specific topic ~~~

2018-04-27 09:56:38,826 - memory_profile5_log - INFO -    158                                 # the idea is just we need to rerank every topic according

2018-04-27 09:56:38,826 - memory_profile5_log - INFO -    159                                 # user_id and and is_general by p0_posterior

2018-04-27 09:56:38,831 - memory_profile5_log - INFO -    160    916.7 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-27 09:56:38,832 - memory_profile5_log - INFO -    161    925.9 MiB      9.3 MiB                                     'is_general']].groupby(['topic_id',

2018-04-27 09:56:38,832 - memory_profile5_log - INFO -    162    916.7 MiB     -9.2 MiB                                                             'is_general']

2018-04-27 09:56:38,832 - memory_profile5_log - INFO -    163                                                                                      ).size().to_frame().reset_index()

2018-04-27 09:56:38,834 - memory_profile5_log - INFO -    164                             

2018-04-27 09:56:38,835 - memory_profile5_log - INFO -    165                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-27 09:56:38,835 - memory_profile5_log - INFO -    166                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-27 09:56:38,835 - memory_profile5_log - INFO -    167    916.9 MiB      0.1 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-27 09:56:38,838 - memory_profile5_log - INFO -    168                             

2018-04-27 09:56:38,838 - memory_profile5_log - INFO -    169                                 # ~ start by provide rank for each topic type ~

2018-04-27 09:56:38,841 - memory_profile5_log - INFO -    170    962.8 MiB     45.9 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-27 09:56:38,841 - memory_profile5_log - INFO -    171    970.8 MiB      8.0 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-27 09:56:38,842 - memory_profile5_log - INFO -    172                             

2018-04-27 09:56:38,842 - memory_profile5_log - INFO -    173                                 # ~ set threshold to filter output

2018-04-27 09:56:38,845 - memory_profile5_log - INFO -    174    970.8 MiB      0.0 MiB       if threshold > 0:

2018-04-27 09:56:38,845 - memory_profile5_log - INFO -    175    970.8 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-27 09:56:38,845 - memory_profile5_log - INFO -    176    970.8 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-27 09:56:38,846 - memory_profile5_log - INFO -    177    962.8 MiB     -8.0 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-27 09:56:38,846 - memory_profile5_log - INFO -    178                             

2018-04-27 09:56:38,848 - memory_profile5_log - INFO -    179                                 # print "\n", model_transform[['user_id', 'topic_id', 'p0_posterior',

2018-04-27 09:56:38,852 - memory_profile5_log - INFO -    180                                 #                             'is_general', 'rank']].loc[model_transform['user_id'].str.contains('1610c8e6b6466b-05a6ac1ba8c1b8-4323461-100200-1',

2018-04-27 09:56:38,855 - memory_profile5_log - INFO -    181                                 #                                                                                                 case=False)].head(45)

2018-04-27 09:56:38,855 - memory_profile5_log - INFO -    182                                 # print "len(model_transform): %d" % len(model_transform)

2018-04-27 09:56:38,857 - memory_profile5_log - INFO -    183                             

2018-04-27 09:56:38,858 - memory_profile5_log - INFO -    184    962.8 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-27 09:56:38,858 - memory_profile5_log - INFO -    185    962.8 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-27 09:56:38,858 - memory_profile5_log - INFO -    186                             

2018-04-27 09:56:38,859 - memory_profile5_log - INFO -    187    962.8 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-27 09:56:38,861 - memory_profile5_log - INFO -    188                             

2018-04-27 09:56:38,864 - memory_profile5_log - INFO -    189    962.8 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-27 09:56:38,864 - memory_profile5_log - INFO -    190    962.8 MiB      0.0 MiB       del df_dut

2018-04-27 09:56:38,865 - memory_profile5_log - INFO -    191    962.8 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-27 09:56:38,867 - memory_profile5_log - INFO -    192    962.8 MiB      0.0 MiB       del df_dt

2018-04-27 09:56:38,868 - memory_profile5_log - INFO -    193    962.8 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-27 09:56:38,868 - memory_profile5_log - INFO -    194    962.8 MiB      0.0 MiB       del df_input

2018-04-27 09:56:38,869 - memory_profile5_log - INFO -    195    962.8 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-27 09:56:38,869 - memory_profile5_log - INFO -    196    954.0 MiB     -8.8 MiB       del df_input_X

2018-04-27 09:56:38,871 - memory_profile5_log - INFO -    197    954.0 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-27 09:56:38,871 - memory_profile5_log - INFO -    198    954.0 MiB      0.0 MiB       del df_current

2018-04-27 09:56:38,874 - memory_profile5_log - INFO -    199    954.0 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-27 09:56:38,875 - memory_profile5_log - INFO -    200    954.0 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-27 09:56:38,875 - memory_profile5_log - INFO -    201    954.0 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-27 09:56:38,877 - memory_profile5_log - INFO -    202    854.3 MiB    -99.7 MiB       del model_fit

2018-04-27 09:56:38,878 - memory_profile5_log - INFO -    203    854.3 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-27 09:56:38,878 - memory_profile5_log - INFO -    204    854.3 MiB      0.0 MiB       del result

2018-04-27 09:56:38,880 - memory_profile5_log - INFO -    205    854.3 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-27 09:56:38,881 - memory_profile5_log - INFO -    206                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-27 09:56:38,881 - memory_profile5_log - INFO -    207                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-27 09:56:38,881 - memory_profile5_log - INFO -    208    854.3 MiB      0.0 MiB       if savetrain:

2018-04-27 09:56:38,884 - memory_profile5_log - INFO -    209                                     model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-27 09:56:38,888 - memory_profile5_log - INFO -    210                                     del model_transform

2018-04-27 09:56:38,888 - memory_profile5_log - INFO -    211                                     logger.info("deleting model_transform...")

2018-04-27 09:56:38,888 - memory_profile5_log - INFO -    212                                     logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-27 09:56:38,890 - memory_profile5_log - INFO -    213                             

2018-04-27 09:56:38,890 - memory_profile5_log - INFO -    214                                     logger.info("Begin saving trained data...")

2018-04-27 09:56:38,891 - memory_profile5_log - INFO -    215                                     # print "\n", model_transform.head(5)

2018-04-27 09:56:38,891 - memory_profile5_log - INFO -    216                                     # ~ Place your code to save the training model here ~

2018-04-27 09:56:38,891 - memory_profile5_log - INFO -    217                                     if str(saveto).lower() == "datastore":

2018-04-27 09:56:38,894 - memory_profile5_log - INFO -    218                                         logger.info("Using google datastore as storage...")

2018-04-27 09:56:38,895 - memory_profile5_log - INFO -    219                                         if multproc:

2018-04-27 09:56:38,898 - memory_profile5_log - INFO -    220                                             mh.saveDatastoreMP(model_transformsv)

2018-04-27 09:56:38,898 - memory_profile5_log - INFO -    221                                             # mh.saveDataStorePutMulti(model_transform[['user_id', 'topic_id', 'is_general', 'rank']])

2018-04-27 09:56:38,900 - memory_profile5_log - INFO -    222                                         else:

2018-04-27 09:56:38,901 - memory_profile5_log - INFO -    223                                             mh.saveDatastore(model_transformsv)

2018-04-27 09:56:38,903 - memory_profile5_log - INFO -    224                                             

2018-04-27 09:56:38,903 - memory_profile5_log - INFO -    225                                     elif str(saveto).lower() == "elastic":

2018-04-27 09:56:38,905 - memory_profile5_log - INFO -    226                                         logger.info("Using ElasticSearch as storage...")

2018-04-27 09:56:38,907 - memory_profile5_log - INFO -    227                                         mh.saveElasticS(model_transformsv)

2018-04-27 09:56:38,908 - memory_profile5_log - INFO -    228                             

2018-04-27 09:56:38,908 - memory_profile5_log - INFO -    229                                     # need save sigma_nt for daily train

2018-04-27 09:56:38,910 - memory_profile5_log - INFO -    230                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-27 09:56:38,911 - memory_profile5_log - INFO -    231                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-27 09:56:38,911 - memory_profile5_log - INFO -    232                                     if start_date and end_date:

2018-04-27 09:56:38,913 - memory_profile5_log - INFO -    233                                         if not fitby_sigmant:

2018-04-27 09:56:38,913 - memory_profile5_log - INFO -    234                                             logging.info("Saving sigma Nt...")

2018-04-27 09:56:38,914 - memory_profile5_log - INFO -    235                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-27 09:56:38,915 - memory_profile5_log - INFO -    236                                             save_sigma_nt['start_date'] = start_date

2018-04-27 09:56:38,915 - memory_profile5_log - INFO -    237                                             save_sigma_nt['end_date'] = end_date

2018-04-27 09:56:38,917 - memory_profile5_log - INFO -    238                                             print save_sigma_nt.head(5)

2018-04-27 09:56:38,917 - memory_profile5_log - INFO -    239                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-27 09:56:38,918 - memory_profile5_log - INFO -    240                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-27 09:56:38,920 - memory_profile5_log - INFO -    241    854.3 MiB      0.0 MiB       return

2018-04-27 09:56:38,920 - memory_profile5_log - INFO - 


2018-04-27 09:56:38,921 - memory_profile5_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~

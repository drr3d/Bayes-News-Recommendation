2018-04-27 16:55:56,153 - memory_profile7_log - INFO - Generating date range with N: 3
2018-04-27 16:55:56,157 - memory_profile7_log - INFO - date_generated: 
2018-04-27 16:55:56,157 - memory_profile7_log - INFO -  
2018-04-27 16:55:56,158 - memory_profile7_log - INFO - [datetime.datetime(2018, 4, 12, 0, 0), datetime.datetime(2018, 4, 13, 0, 0), datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-27 16:55:56,158 - memory_profile7_log - INFO - 

2018-04-27 16:55:56,158 - memory_profile7_log - INFO - using current date: 2018-04-15
2018-04-27 16:55:56,160 - memory_profile7_log - INFO - using start date: 2018-04-12 00:00:00
2018-04-27 16:55:56,160 - memory_profile7_log - INFO - using end date: 2018-04-14
2018-04-27 16:55:56,302 - memory_profile7_log - INFO - Starting data fetch iterative...
2018-04-27 16:55:56,305 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-12
2018-04-27 16:58:59,938 - memory_profile7_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 16:58:59,940 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 16:58:59,943 - memory_profile7_log - INFO - ================================================

2018-04-27 16:58:59,944 - memory_profile7_log - INFO -     67     87.1 MiB     87.1 MiB   @profile

2018-04-27 16:58:59,944 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-27 16:58:59,946 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-27 16:58:59,947 - memory_profile7_log - INFO -     70     87.1 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 16:58:59,947 - memory_profile7_log - INFO -     71     87.1 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 16:58:59,950 - memory_profile7_log - INFO -     72                             

2018-04-27 16:58:59,953 - memory_profile7_log - INFO -     73     90.3 MiB      3.2 MiB       result  = client.query(query, job_config=job_config)

2018-04-27 16:58:59,953 - memory_profile7_log - INFO -     74     90.3 MiB      0.0 MiB       rows = result.result()

2018-04-27 16:58:59,954 - memory_profile7_log - INFO -     75                             

2018-04-27 16:58:59,956 - memory_profile7_log - INFO -     76     90.3 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-27 16:58:59,957 - memory_profile7_log - INFO -     77                                 

2018-04-27 16:58:59,957 - memory_profile7_log - INFO -     78     90.3 MiB      0.0 MiB       def _q_iterator():

2018-04-27 16:58:59,960 - memory_profile7_log - INFO -     79                                     # data = []

2018-04-27 16:58:59,961 - memory_profile7_log - INFO -     80    474.2 MiB    285.9 MiB           for row in rows:

2018-04-27 16:58:59,963 - memory_profile7_log - INFO -     81                                         # data.append(list(row))

2018-04-27 16:58:59,963 - memory_profile7_log - INFO -     82    474.2 MiB    -90.5 MiB               yield list(row)

2018-04-27 16:58:59,964 - memory_profile7_log - INFO -     83                             

2018-04-27 16:58:59,966 - memory_profile7_log - INFO -     84                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-27 16:58:59,967 - memory_profile7_log - INFO -     85    393.4 MiB    -80.9 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-27 16:58:59,967 - memory_profile7_log - INFO -     86                                 # del rows

2018-04-27 16:58:59,969 - memory_profile7_log - INFO -     87    393.4 MiB      0.0 MiB       del result

2018-04-27 16:58:59,971 - memory_profile7_log - INFO -     88                                 # del data

2018-04-27 16:58:59,973 - memory_profile7_log - INFO -     89                              

2018-04-27 16:58:59,974 - memory_profile7_log - INFO -     90    393.4 MiB      0.0 MiB       return df

2018-04-27 16:58:59,976 - memory_profile7_log - INFO - 


2018-04-27 16:58:59,977 - memory_profile7_log - INFO - getting total: 462579 training data(genuine interest) for date: 2018-04-12
2018-04-27 16:58:59,979 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-13
2018-04-27 17:01:22,766 - memory_profile7_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 17:01:22,769 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 17:01:22,769 - memory_profile7_log - INFO - ================================================

2018-04-27 17:01:22,770 - memory_profile7_log - INFO -     67    393.4 MiB    393.4 MiB   @profile

2018-04-27 17:01:22,772 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-27 17:01:22,773 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-27 17:01:22,775 - memory_profile7_log - INFO -     70    393.4 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 17:01:22,776 - memory_profile7_log - INFO -     71    393.4 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 17:01:22,776 - memory_profile7_log - INFO -     72                             

2018-04-27 17:01:22,778 - memory_profile7_log - INFO -     73    393.4 MiB      0.0 MiB       result  = client.query(query, job_config=job_config)

2018-04-27 17:01:22,779 - memory_profile7_log - INFO -     74    393.4 MiB      0.0 MiB       rows = result.result()

2018-04-27 17:01:22,780 - memory_profile7_log - INFO -     75                             

2018-04-27 17:01:22,782 - memory_profile7_log - INFO -     76    393.4 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-27 17:01:22,782 - memory_profile7_log - INFO -     77                                 

2018-04-27 17:01:22,785 - memory_profile7_log - INFO -     78    393.4 MiB      0.0 MiB       def _q_iterator():

2018-04-27 17:01:22,786 - memory_profile7_log - INFO -     79                                     # data = []

2018-04-27 17:01:22,786 - memory_profile7_log - INFO -     80    564.7 MiB   -385.2 MiB           for row in rows:

2018-04-27 17:01:22,788 - memory_profile7_log - INFO -     81                                         # data.append(list(row))

2018-04-27 17:01:22,789 - memory_profile7_log - INFO -     82    564.7 MiB  -1028.4 MiB               yield list(row)

2018-04-27 17:01:22,790 - memory_profile7_log - INFO -     83                             

2018-04-27 17:01:22,792 - memory_profile7_log - INFO -     84                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-27 17:01:22,792 - memory_profile7_log - INFO -     85    493.2 MiB    -71.5 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-27 17:01:22,793 - memory_profile7_log - INFO -     86                                 # del rows

2018-04-27 17:01:22,795 - memory_profile7_log - INFO -     87    493.2 MiB      0.0 MiB       del result

2018-04-27 17:01:22,796 - memory_profile7_log - INFO -     88                                 # del data

2018-04-27 17:01:22,796 - memory_profile7_log - INFO -     89                              

2018-04-27 17:01:22,796 - memory_profile7_log - INFO -     90    493.2 MiB      0.0 MiB       return df

2018-04-27 17:01:22,799 - memory_profile7_log - INFO - 


2018-04-27 17:01:22,801 - memory_profile7_log - INFO - getting total: 361145 training data(genuine interest) for date: 2018-04-13
2018-04-27 17:01:22,802 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-14
2018-04-27 17:03:14,203 - memory_profile7_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 17:03:14,203 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 17:03:14,206 - memory_profile7_log - INFO - ================================================

2018-04-27 17:03:14,207 - memory_profile7_log - INFO -     67    493.2 MiB    493.2 MiB   @profile

2018-04-27 17:03:14,207 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-27 17:03:14,210 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-27 17:03:14,211 - memory_profile7_log - INFO -     70    493.2 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 17:03:14,213 - memory_profile7_log - INFO -     71    493.2 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 17:03:14,214 - memory_profile7_log - INFO -     72                             

2018-04-27 17:03:14,216 - memory_profile7_log - INFO -     73    493.2 MiB     -0.0 MiB       result  = client.query(query, job_config=job_config)

2018-04-27 17:03:14,217 - memory_profile7_log - INFO -     74    493.2 MiB      0.0 MiB       rows = result.result()

2018-04-27 17:03:14,219 - memory_profile7_log - INFO -     75                             

2018-04-27 17:03:14,220 - memory_profile7_log - INFO -     76    493.2 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-27 17:03:14,220 - memory_profile7_log - INFO -     77                                 

2018-04-27 17:03:14,223 - memory_profile7_log - INFO -     78    493.2 MiB      0.0 MiB       def _q_iterator():

2018-04-27 17:03:14,226 - memory_profile7_log - INFO -     79                                     # data = []

2018-04-27 17:03:14,226 - memory_profile7_log - INFO -     80    622.0 MiB     84.9 MiB           for row in rows:

2018-04-27 17:03:14,227 - memory_profile7_log - INFO -     81                                         # data.append(list(row))

2018-04-27 17:03:14,230 - memory_profile7_log - INFO -     82    622.0 MiB    -28.4 MiB               yield list(row)

2018-04-27 17:03:14,230 - memory_profile7_log - INFO -     83                             

2018-04-27 17:03:14,233 - memory_profile7_log - INFO -     84                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-27 17:03:14,236 - memory_profile7_log - INFO -     85    571.0 MiB    -50.9 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-27 17:03:14,237 - memory_profile7_log - INFO -     86                                 # del rows

2018-04-27 17:03:14,240 - memory_profile7_log - INFO -     87    571.0 MiB      0.0 MiB       del result

2018-04-27 17:03:14,240 - memory_profile7_log - INFO -     88                                 # del data

2018-04-27 17:03:14,242 - memory_profile7_log - INFO -     89                              

2018-04-27 17:03:14,243 - memory_profile7_log - INFO -     90    571.0 MiB      0.0 MiB       return df

2018-04-27 17:03:14,246 - memory_profile7_log - INFO - 


2018-04-27 17:03:14,247 - memory_profile7_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-27 17:03:14,250 - memory_profile7_log - INFO - len datalist: 3
2018-04-27 17:03:14,250 - memory_profile7_log - INFO - All data fetch iterative done!!
2018-04-27 17:03:14,252 - memory_profile7_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 17:03:14,253 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 17:03:14,256 - memory_profile7_log - INFO - ================================================

2018-04-27 17:03:14,257 - memory_profile7_log - INFO -    236     87.1 MiB     87.1 MiB   @profile

2018-04-27 17:03:14,257 - memory_profile7_log - INFO -    237                             def BQPreprocess(loadmp, cpu, date_generated):

2018-04-27 17:03:14,259 - memory_profile7_log - INFO -    238     87.1 MiB      0.0 MiB       bq_client = bigquery.Client()

2018-04-27 17:03:14,259 - memory_profile7_log - INFO -    239     87.1 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-27 17:03:14,260 - memory_profile7_log - INFO -    240                             

2018-04-27 17:03:14,262 - memory_profile7_log - INFO -    241     87.1 MiB      0.0 MiB       datalist = []

2018-04-27 17:03:14,263 - memory_profile7_log - INFO -    242    493.2 MiB      0.0 MiB       def _getBig(procdate, client):

2018-04-27 17:03:14,263 - memory_profile7_log - INFO -    243    493.2 MiB      0.0 MiB           logger.info("Collecting training data for date: %s", procdate)

2018-04-27 17:03:14,263 - memory_profile7_log - INFO -    244                                     # ~ get genuine news interest ~

2018-04-27 17:03:14,266 - memory_profile7_log - INFO -    245    493.2 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-27 17:03:14,267 - memory_profile7_log - INFO -    246                             

2018-04-27 17:03:14,269 - memory_profile7_log - INFO -    247                                     # safe handling of query parameter

2018-04-27 17:03:14,269 - memory_profile7_log - INFO -    248                                     query_params = [

2018-04-27 17:03:14,270 - memory_profile7_log - INFO -    249    493.2 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', procdate)

2018-04-27 17:03:14,270 - memory_profile7_log - INFO -    250                                     ]

2018-04-27 17:03:14,272 - memory_profile7_log - INFO -    251                             

2018-04-27 17:03:14,272 - memory_profile7_log - INFO -    252    493.2 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-27 17:03:14,273 - memory_profile7_log - INFO -    253                                     # temp_df = load_bigquery(client, query_fit + query_fit_where, job_config)

2018-04-27 17:03:14,273 - memory_profile7_log - INFO -    254    571.0 MiB    483.9 MiB           temp_df = loadBQ(client, query_fit + query_fit_where, job_config)

2018-04-27 17:03:14,275 - memory_profile7_log - INFO -    255                             

2018-04-27 17:03:14,275 - memory_profile7_log - INFO -    256    571.0 MiB      0.0 MiB           if temp_df.empty:

2018-04-27 17:03:14,276 - memory_profile7_log - INFO -    257                                         logger.info("%s data is empty!", procdate)

2018-04-27 17:03:14,279 - memory_profile7_log - INFO -    258                                         return None

2018-04-27 17:03:14,279 - memory_profile7_log - INFO -    259                                     else:

2018-04-27 17:03:14,279 - memory_profile7_log - INFO -    260    571.0 MiB      0.0 MiB               logger.info("getting total: %d training data(genuine interest) for date: %s" % (len(temp_df), procdate))

2018-04-27 17:03:14,279 - memory_profile7_log - INFO -    261    571.0 MiB      0.0 MiB               if loadmp:

2018-04-27 17:03:14,280 - memory_profile7_log - INFO -    262                                             logger.info("Exiting: %s", mp.current_process().name)

2018-04-27 17:03:14,282 - memory_profile7_log - INFO -    263    571.0 MiB      0.0 MiB               return temp_df

2018-04-27 17:03:14,282 - memory_profile7_log - INFO -    264                             

2018-04-27 17:03:14,282 - memory_profile7_log - INFO -    265     87.1 MiB      0.0 MiB       if loadmp:

2018-04-27 17:03:14,282 - memory_profile7_log - INFO -    266                                     logger.info("Starting data fetch multiprocess..")

2018-04-27 17:03:14,285 - memory_profile7_log - INFO -    267                                     logger.info("number of process: %d", len(date_generated))

2018-04-27 17:03:14,286 - memory_profile7_log - INFO -    268                             

2018-04-27 17:03:14,286 - memory_profile7_log - INFO -    269                                     pool = mp.Pool(processes=cpu)

2018-04-27 17:03:14,289 - memory_profile7_log - INFO -    270                                     # multprocessA = [pool.apply(_getBig, args=(ndate.strftime("%Y-%m-%d"), )) for ndate in date_generated]

2018-04-27 17:03:14,289 - memory_profile7_log - INFO -    271                                     multprocessA = [pool.apply_async(_getBig, args=(ndate.strftime("%Y-%m-%d"), bq_client, )) for ndate in date_generated]

2018-04-27 17:03:14,290 - memory_profile7_log - INFO -    272                                     output_multprocessA = [p.get() for p in multprocessA]

2018-04-27 17:03:14,292 - memory_profile7_log - INFO -    273                             

2018-04-27 17:03:14,293 - memory_profile7_log - INFO -    274                                     for m in output_multprocessA:

2018-04-27 17:03:14,295 - memory_profile7_log - INFO -    275                                         if m is not None:

2018-04-27 17:03:14,295 - memory_profile7_log - INFO -    276                                             if not m.empty:

2018-04-27 17:03:14,296 - memory_profile7_log - INFO -    277                                                 datalist.append(m)

2018-04-27 17:03:14,296 - memory_profile7_log - INFO -    278                             

2018-04-27 17:03:14,301 - memory_profile7_log - INFO -    279                                     logger.info("len datalist: %d", len(datalist))

2018-04-27 17:03:14,302 - memory_profile7_log - INFO -    280                                     logger.info("All data fetch multiprocess done!!")

2018-04-27 17:03:14,303 - memory_profile7_log - INFO -    281                                 else:

2018-04-27 17:03:14,305 - memory_profile7_log - INFO -    282     87.1 MiB      0.0 MiB           logger.info("Starting data fetch iterative...")

2018-04-27 17:03:14,305 - memory_profile7_log - INFO -    283    571.0 MiB      0.0 MiB           for ndate in date_generated:

2018-04-27 17:03:14,306 - memory_profile7_log - INFO -    284    571.0 MiB      0.0 MiB               tframe = _getBig(ndate.strftime("%Y-%m-%d"), bq_client)

2018-04-27 17:03:14,308 - memory_profile7_log - INFO -    285    571.0 MiB      0.0 MiB               if tframe is not None:

2018-04-27 17:03:14,312 - memory_profile7_log - INFO -    286    571.0 MiB      0.0 MiB                   if not tframe.empty:

2018-04-27 17:03:14,312 - memory_profile7_log - INFO -    287    571.0 MiB      0.0 MiB                       datalist.append(tframe)

2018-04-27 17:03:14,313 - memory_profile7_log - INFO -    288                                         else: 

2018-04-27 17:03:14,315 - memory_profile7_log - INFO -    289                                             logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-27 17:03:14,315 - memory_profile7_log - INFO -    290    571.0 MiB      0.0 MiB           logger.info("len datalist: %d", len(datalist))

2018-04-27 17:03:14,316 - memory_profile7_log - INFO -    291    571.0 MiB      0.0 MiB           logger.info("All data fetch iterative done!!")

2018-04-27 17:03:14,318 - memory_profile7_log - INFO -    292                             

2018-04-27 17:03:14,319 - memory_profile7_log - INFO -    293    571.0 MiB      0.0 MiB       return datalist

2018-04-27 17:03:14,322 - memory_profile7_log - INFO - 


2018-04-27 17:03:14,431 - memory_profile7_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-27 17:04:41,589 - memory_profile7_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 17:04:41,591 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 17:04:41,591 - memory_profile7_log - INFO - ================================================

2018-04-27 17:04:41,592 - memory_profile7_log - INFO -     67    580.4 MiB    580.4 MiB   @profile

2018-04-27 17:04:41,592 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-27 17:04:41,594 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-27 17:04:41,595 - memory_profile7_log - INFO -     70    580.4 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 17:04:41,595 - memory_profile7_log - INFO -     71    580.4 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 17:04:41,595 - memory_profile7_log - INFO -     72                             

2018-04-27 17:04:41,596 - memory_profile7_log - INFO -     73    581.8 MiB      1.4 MiB       result  = client.query(query, job_config=job_config)

2018-04-27 17:04:41,598 - memory_profile7_log - INFO -     74    581.8 MiB      0.0 MiB       rows = result.result()

2018-04-27 17:04:41,598 - memory_profile7_log - INFO -     75                             

2018-04-27 17:04:41,601 - memory_profile7_log - INFO -     76    581.8 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-27 17:04:41,602 - memory_profile7_log - INFO -     77                                 

2018-04-27 17:04:41,602 - memory_profile7_log - INFO -     78    581.8 MiB      0.0 MiB       def _q_iterator():

2018-04-27 17:04:41,604 - memory_profile7_log - INFO -     79                                     # data = []

2018-04-27 17:04:41,605 - memory_profile7_log - INFO -     80    699.1 MiB     84.5 MiB           for row in rows:

2018-04-27 17:04:41,605 - memory_profile7_log - INFO -     81                                         # data.append(list(row))

2018-04-27 17:04:41,605 - memory_profile7_log - INFO -     82    699.1 MiB     -5.9 MiB               yield list(row)

2018-04-27 17:04:41,607 - memory_profile7_log - INFO -     83                             

2018-04-27 17:04:41,608 - memory_profile7_log - INFO -     84                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-27 17:04:41,608 - memory_profile7_log - INFO -     85    645.7 MiB    -53.4 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-27 17:04:41,608 - memory_profile7_log - INFO -     86                                 # del rows

2018-04-27 17:04:41,608 - memory_profile7_log - INFO -     87    645.7 MiB      0.0 MiB       del result

2018-04-27 17:04:41,611 - memory_profile7_log - INFO -     88                                 # del data

2018-04-27 17:04:41,612 - memory_profile7_log - INFO -     89                              

2018-04-27 17:04:41,614 - memory_profile7_log - INFO -     90    645.7 MiB      0.0 MiB       return df

2018-04-27 17:04:41,614 - memory_profile7_log - INFO - 


2018-04-27 17:04:41,615 - memory_profile7_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-27 17:04:41,625 - memory_profile7_log - INFO - loading time of: 1308144 total genuine-current interest data ~ take 525.350s
2018-04-27 17:04:41,625 - memory_profile7_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 17:04:41,625 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 17:04:41,627 - memory_profile7_log - INFO - ================================================

2018-04-27 17:04:41,627 - memory_profile7_log - INFO -    295     86.9 MiB     86.9 MiB   @profile

2018-04-27 17:04:41,628 - memory_profile7_log - INFO -    296                             def preprocess(loadmp, cpu, cd, query_fit, date_generated):

2018-04-27 17:04:41,628 - memory_profile7_log - INFO -    297     87.1 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-27 17:04:41,628 - memory_profile7_log - INFO -    298     87.1 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-27 17:04:41,628 - memory_profile7_log - INFO -    299                             

2018-04-27 17:04:41,628 - memory_profile7_log - INFO -    300                                 # ~~~ Begin collecting data ~~~

2018-04-27 17:04:41,628 - memory_profile7_log - INFO -    301     87.1 MiB      0.0 MiB       t0 = time.time()

2018-04-27 17:04:41,628 - memory_profile7_log - INFO -    302                                 

2018-04-27 17:04:41,630 - memory_profile7_log - INFO -    303    571.0 MiB    483.9 MiB       datalist = BQPreprocess(loadmp, cpu, date_generated)

2018-04-27 17:04:41,630 - memory_profile7_log - INFO -    304                             

2018-04-27 17:04:41,630 - memory_profile7_log - INFO -    305    571.0 MiB      0.0 MiB       if len(datalist) > 1:

2018-04-27 17:04:41,630 - memory_profile7_log - INFO -    306    621.8 MiB     50.8 MiB           big_frame = pd.concat(datalist)

2018-04-27 17:04:41,631 - memory_profile7_log - INFO -    307    580.3 MiB    -41.5 MiB           del datalist

2018-04-27 17:04:41,631 - memory_profile7_log - INFO -    308                                 else:

2018-04-27 17:04:41,631 - memory_profile7_log - INFO -    309                                     big_frame = datalist

2018-04-27 17:04:41,637 - memory_profile7_log - INFO -    310    580.4 MiB      0.1 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-27 17:04:41,638 - memory_profile7_log - INFO -    311                             

2018-04-27 17:04:41,638 - memory_profile7_log - INFO -    312                                 # ~ get current news interest ~

2018-04-27 17:04:41,638 - memory_profile7_log - INFO -    313    580.4 MiB      0.0 MiB       if not cd:

2018-04-27 17:04:41,638 - memory_profile7_log - INFO -    314                                     logger.info("Collecting training data(current date interest)..")

2018-04-27 17:04:41,638 - memory_profile7_log - INFO -    315                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-27 17:04:41,640 - memory_profile7_log - INFO -    316                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-27 17:04:41,641 - memory_profile7_log - INFO -    317                                 else:

2018-04-27 17:04:41,641 - memory_profile7_log - INFO -    318    580.4 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-27 17:04:41,641 - memory_profile7_log - INFO -    319    580.4 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-27 17:04:41,642 - memory_profile7_log - INFO -    320                             

2018-04-27 17:04:41,644 - memory_profile7_log - INFO -    321                                     # safe handling of query parameter

2018-04-27 17:04:41,644 - memory_profile7_log - INFO -    322                                     query_params = [

2018-04-27 17:04:41,648 - memory_profile7_log - INFO -    323    580.4 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-27 17:04:41,648 - memory_profile7_log - INFO -    324                                     ]

2018-04-27 17:04:41,648 - memory_profile7_log - INFO -    325                             

2018-04-27 17:04:41,648 - memory_profile7_log - INFO -    326    580.4 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-27 17:04:41,650 - memory_profile7_log - INFO -    327    645.7 MiB     65.3 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-27 17:04:41,650 - memory_profile7_log - INFO -    328    645.7 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-27 17:04:41,650 - memory_profile7_log - INFO -    329                             

2018-04-27 17:04:41,651 - memory_profile7_log - INFO -    330    645.7 MiB      0.1 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-27 17:04:41,651 - memory_profile7_log - INFO -    331    645.7 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-27 17:04:41,651 - memory_profile7_log - INFO -    332    645.7 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-27 17:04:41,651 - memory_profile7_log - INFO -    333                             

2018-04-27 17:04:41,651 - memory_profile7_log - INFO -    334    645.7 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-27 17:04:41,651 - memory_profile7_log - INFO -    335    645.7 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-27 17:04:41,653 - memory_profile7_log - INFO -    336                             

2018-04-27 17:04:41,653 - memory_profile7_log - INFO -    337    645.7 MiB      0.0 MiB       return big_frame, current_frame

2018-04-27 17:04:41,653 - memory_profile7_log - INFO - 


2018-04-27 17:04:41,655 - memory_profile7_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-27 17:04:41,727 - memory_profile7_log - INFO - train on: 1077719 total history data(D(u, t))
2018-04-27 17:04:41,729 - memory_profile7_log - INFO - transform on: 230425 total current data(D(t))
2018-04-27 17:04:46,596 - memory_profile7_log - INFO - Len of model_fit: 1077719
2018-04-27 17:04:46,598 - memory_profile7_log - INFO - Len of df_dut: 1077719
2018-04-27 17:06:19,632 - memory_profile7_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-27 17:06:19,733 - memory_profile7_log - INFO - Total train time: 98.006s
2018-04-27 17:06:19,736 - memory_profile7_log - INFO - memory left before cleaning: 86.400 percent memory...
2018-04-27 17:06:19,736 - memory_profile7_log - INFO - cleaning up some objects...
2018-04-27 17:06:19,737 - memory_profile7_log - INFO - deleting df_dut...
2018-04-27 17:06:19,739 - memory_profile7_log - INFO - deleting df_dt...
2018-04-27 17:06:19,739 - memory_profile7_log - INFO - deleting df_input...
2018-04-27 17:06:19,749 - memory_profile7_log - INFO - deleting df_input_X...
2018-04-27 17:06:19,750 - memory_profile7_log - INFO - deleting df_current...
2018-04-27 17:06:19,752 - memory_profile7_log - INFO - deleting map_topic_isgeneral...
2018-04-27 17:06:19,813 - memory_profile7_log - INFO - deleting model_fit...
2018-04-27 17:06:19,815 - memory_profile7_log - INFO - deleting result...
2018-04-27 17:06:19,854 - memory_profile7_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 17:06:19,855 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 17:06:19,855 - memory_profile7_log - INFO - ================================================

2018-04-27 17:06:19,855 - memory_profile7_log - INFO -    102    645.7 MiB    645.7 MiB   @profile

2018-04-27 17:06:19,855 - memory_profile7_log - INFO -    103                             def main(df_input, df_current, current_date, G,

2018-04-27 17:06:19,855 - memory_profile7_log - INFO -    104                                      project_id, savetrain=False, multproc=True,

2018-04-27 17:06:19,855 - memory_profile7_log - INFO -    105                                      threshold=0, start_date=None, end_date=None,

2018-04-27 17:06:19,855 - memory_profile7_log - INFO -    106                                      saveto="datastore"):

2018-04-27 17:06:19,857 - memory_profile7_log - INFO -    107                                 """

2018-04-27 17:06:19,857 - memory_profile7_log - INFO -    108                                     Main cron method

2018-04-27 17:06:19,857 - memory_profile7_log - INFO -    109                                 """

2018-04-27 17:06:19,857 - memory_profile7_log - INFO -    110                                 # ~ Data Preprocessing ~

2018-04-27 17:06:19,857 - memory_profile7_log - INFO -    111                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-27 17:06:19,858 - memory_profile7_log - INFO -    112                                 # D(u, t)

2018-04-27 17:06:19,858 - memory_profile7_log - INFO -    113    645.7 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-27 17:06:19,858 - memory_profile7_log - INFO -    114    679.7 MiB     33.9 MiB       df_dut = df_input.copy(deep=True)

2018-04-27 17:06:19,858 - memory_profile7_log - INFO -    115    679.7 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-27 17:06:19,858 - memory_profile7_log - INFO -    116                             

2018-04-27 17:06:19,858 - memory_profile7_log - INFO -    117                                 # D(t)

2018-04-27 17:06:19,858 - memory_profile7_log - INFO -    118    686.8 MiB      7.1 MiB       df_dt = df_current.copy(deep=True)

2018-04-27 17:06:19,859 - memory_profile7_log - INFO -    119    686.8 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-27 17:06:19,859 - memory_profile7_log - INFO -    120                             

2018-04-27 17:06:19,861 - memory_profile7_log - INFO -    121                                 # ~~~~~~ Begin train ~~~~~~

2018-04-27 17:06:19,861 - memory_profile7_log - INFO -    122    686.8 MiB      0.0 MiB       t0 = time.time()

2018-04-27 17:06:19,861 - memory_profile7_log - INFO -    123    686.8 MiB      0.0 MiB       logger.info("train on: %d total history data(D(u, t))", len(df_dut))

2018-04-27 17:06:19,861 - memory_profile7_log - INFO -    124    686.8 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-27 17:06:19,861 - memory_profile7_log - INFO -    125                             

2018-04-27 17:06:19,861 - memory_profile7_log - INFO -    126                                 # instantiace class

2018-04-27 17:06:19,862 - memory_profile7_log - INFO -    127    686.8 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-27 17:06:19,862 - memory_profile7_log - INFO -    128                             

2018-04-27 17:06:19,862 - memory_profile7_log - INFO -    129                                 # ~~ Fit ~~

2018-04-27 17:06:19,862 - memory_profile7_log - INFO -    130                                 #   handling genuine news interest < current date

2018-04-27 17:06:19,862 - memory_profile7_log - INFO -    131    690.8 MiB      4.0 MiB       NB = BR.processX(df_dut)

2018-04-27 17:06:19,868 - memory_profile7_log - INFO -    132                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-27 17:06:19,868 - memory_profile7_log - INFO -    133                                 #   nanti dipindah ke class train utama

2018-04-27 17:06:19,868 - memory_profile7_log - INFO -    134    733.4 MiB     42.7 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-27 17:06:19,869 - memory_profile7_log - INFO -    135                                 """

2018-04-27 17:06:19,869 - memory_profile7_log - INFO -    136                                     num_y = total global click for category=ci on periode t

2018-04-27 17:06:19,869 - memory_profile7_log - INFO -    137                                     num_x = total click from user_U for category=ci on periode t

2018-04-27 17:06:19,869 - memory_profile7_log - INFO -    138                                 """

2018-04-27 17:06:19,871 - memory_profile7_log - INFO -    139    733.4 MiB      0.0 MiB       fitby_sigmant = True

2018-04-27 17:06:19,871 - memory_profile7_log - INFO -    140    733.4 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-27 17:06:19,871 - memory_profile7_log - INFO -    141    733.4 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-27 17:06:19,871 - memory_profile7_log - INFO -    142    775.6 MiB     42.1 MiB                            'is_general']]

2018-04-27 17:06:19,871 - memory_profile7_log - INFO -    143    775.6 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-27 17:06:19,871 - memory_profile7_log - INFO -    144    775.6 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-27 17:06:19,872 - memory_profile7_log - INFO -    145    879.0 MiB    103.4 MiB                          verbose=False)

2018-04-27 17:06:19,872 - memory_profile7_log - INFO -    146    879.0 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-27 17:06:19,872 - memory_profile7_log - INFO -    147    879.0 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-27 17:06:19,874 - memory_profile7_log - INFO -    148                             

2018-04-27 17:06:19,874 - memory_profile7_log - INFO -    149                                 # ~~ and Transform ~~

2018-04-27 17:06:19,874 - memory_profile7_log - INFO -    150                                 #   handling current news interest == current date

2018-04-27 17:06:19,874 - memory_profile7_log - INFO -    151    879.0 MiB      0.0 MiB       if df_dt.empty:

2018-04-27 17:06:19,875 - memory_profile7_log - INFO -    152                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-27 17:06:19,875 - memory_profile7_log - INFO -    153                                     return None

2018-04-27 17:06:19,875 - memory_profile7_log - INFO -    154    879.4 MiB      0.4 MiB       NB = BR.processX(df_dt)

2018-04-27 17:06:19,875 - memory_profile7_log - INFO -    155    890.0 MiB     10.6 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-27 17:06:19,875 - memory_profile7_log - INFO -    156                             

2018-04-27 17:06:19,875 - memory_profile7_log - INFO -    157    890.0 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-27 17:06:19,875 - memory_profile7_log - INFO -    158    856.7 MiB    -33.3 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-27 17:06:19,881 - memory_profile7_log - INFO -    159    856.7 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-27 17:06:19,881 - memory_profile7_log - INFO -    160    856.7 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-27 17:06:19,881 - memory_profile7_log - INFO -    161    919.9 MiB     63.3 MiB                                                     verbose=False)

2018-04-27 17:06:19,882 - memory_profile7_log - INFO -    162                                 # ~~~ filter is general and specific topic ~~~

2018-04-27 17:06:19,882 - memory_profile7_log - INFO -    163                                 # the idea is just we need to rerank every topic according

2018-04-27 17:06:19,882 - memory_profile7_log - INFO -    164                                 # user_id and and is_general by p0_posterior

2018-04-27 17:06:19,884 - memory_profile7_log - INFO -    165    919.9 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-27 17:06:19,884 - memory_profile7_log - INFO -    166    929.2 MiB      9.3 MiB                                     'is_general']].groupby(['topic_id',

2018-04-27 17:06:19,884 - memory_profile7_log - INFO -    167    920.0 MiB     -9.2 MiB                                                             'is_general']

2018-04-27 17:06:19,884 - memory_profile7_log - INFO -    168                                                                                      ).size().to_frame().reset_index()

2018-04-27 17:06:19,885 - memory_profile7_log - INFO -    169                             

2018-04-27 17:06:19,885 - memory_profile7_log - INFO -    170                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-27 17:06:19,885 - memory_profile7_log - INFO -    171                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-27 17:06:19,887 - memory_profile7_log - INFO -    172    920.0 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-27 17:06:19,887 - memory_profile7_log - INFO -    173                             

2018-04-27 17:06:19,888 - memory_profile7_log - INFO -    174                                 # ~ start by provide rank for each topic type ~

2018-04-27 17:06:19,888 - memory_profile7_log - INFO -    175   1005.3 MiB     85.3 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-27 17:06:19,888 - memory_profile7_log - INFO -    176   1015.3 MiB     10.0 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-27 17:06:19,888 - memory_profile7_log - INFO -    177                             

2018-04-27 17:06:19,888 - memory_profile7_log - INFO -    178                                 # ~ set threshold to filter output

2018-04-27 17:06:19,888 - memory_profile7_log - INFO -    179   1015.3 MiB      0.0 MiB       if threshold > 0:

2018-04-27 17:06:19,904 - memory_profile7_log - INFO -    180   1015.3 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-27 17:06:19,904 - memory_profile7_log - INFO -    181   1015.3 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-27 17:06:19,904 - memory_profile7_log - INFO -    182   1004.2 MiB    -11.1 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-27 17:06:19,904 - memory_profile7_log - INFO -    183                             

2018-04-27 17:06:19,905 - memory_profile7_log - INFO -    184                                 # print "\n", model_transform[['user_id', 'topic_id', 'p0_posterior',

2018-04-27 17:06:19,907 - memory_profile7_log - INFO -    185                                 #                             'is_general', 'rank']].loc[model_transform['user_id'].str.contains('1610c8e6b6466b-05a6ac1ba8c1b8-4323461-100200-1',

2018-04-27 17:06:19,907 - memory_profile7_log - INFO -    186                                 #                                                                                                 case=False)].head(45)

2018-04-27 17:06:19,907 - memory_profile7_log - INFO -    187                                 # print "len(model_transform): %d" % len(model_transform)

2018-04-27 17:06:19,908 - memory_profile7_log - INFO -    188                             

2018-04-27 17:06:19,908 - memory_profile7_log - INFO -    189   1004.2 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-27 17:06:19,910 - memory_profile7_log - INFO -    190   1004.2 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-27 17:06:19,911 - memory_profile7_log - INFO -    191                             

2018-04-27 17:06:19,911 - memory_profile7_log - INFO -    192   1004.2 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-27 17:06:19,911 - memory_profile7_log - INFO -    193                             

2018-04-27 17:06:19,911 - memory_profile7_log - INFO -    194   1004.2 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-27 17:06:19,914 - memory_profile7_log - INFO -    195   1004.2 MiB      0.0 MiB       del df_dut

2018-04-27 17:06:19,914 - memory_profile7_log - INFO -    196   1004.2 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-27 17:06:19,914 - memory_profile7_log - INFO -    197   1004.2 MiB      0.0 MiB       del df_dt

2018-04-27 17:06:19,915 - memory_profile7_log - INFO -    198   1004.2 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-27 17:06:19,915 - memory_profile7_log - INFO -    199   1004.2 MiB      0.0 MiB       del df_input

2018-04-27 17:06:19,915 - memory_profile7_log - INFO -    200   1004.2 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-27 17:06:19,917 - memory_profile7_log - INFO -    201    995.4 MiB     -8.8 MiB       del df_input_X

2018-04-27 17:06:19,917 - memory_profile7_log - INFO -    202    995.4 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-27 17:06:19,917 - memory_profile7_log - INFO -    203    995.4 MiB      0.0 MiB       del df_current

2018-04-27 17:06:19,917 - memory_profile7_log - INFO -    204    995.4 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-27 17:06:19,917 - memory_profile7_log - INFO -    205    995.4 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-27 17:06:19,918 - memory_profile7_log - INFO -    206    995.4 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-27 17:06:19,918 - memory_profile7_log - INFO -    207    895.7 MiB    -99.7 MiB       del model_fit

2018-04-27 17:06:19,920 - memory_profile7_log - INFO -    208    895.7 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-27 17:06:19,920 - memory_profile7_log - INFO -    209    895.7 MiB      0.0 MiB       del result

2018-04-27 17:06:19,920 - memory_profile7_log - INFO -    210    895.7 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-27 17:06:19,921 - memory_profile7_log - INFO -    211                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-27 17:06:19,921 - memory_profile7_log - INFO -    212                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-27 17:06:19,921 - memory_profile7_log - INFO -    213    895.7 MiB      0.0 MiB       if savetrain:

2018-04-27 17:06:19,921 - memory_profile7_log - INFO -    214                                     model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-27 17:06:19,921 - memory_profile7_log - INFO -    215                                     del model_transform

2018-04-27 17:06:19,921 - memory_profile7_log - INFO -    216                                     logger.info("deleting model_transform...")

2018-04-27 17:06:19,921 - memory_profile7_log - INFO -    217                                     logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-27 17:06:19,921 - memory_profile7_log - INFO -    218                             

2018-04-27 17:06:19,921 - memory_profile7_log - INFO -    219                                     logger.info("Begin saving trained data...")

2018-04-27 17:06:19,923 - memory_profile7_log - INFO -    220                                     # print "\n", model_transform.head(5)

2018-04-27 17:06:19,923 - memory_profile7_log - INFO -    221                                     # ~ Place your code to save the training model here ~

2018-04-27 17:06:19,923 - memory_profile7_log - INFO -    222                                     if str(saveto).lower() == "datastore":

2018-04-27 17:06:19,923 - memory_profile7_log - INFO -    223                                         logger.info("Using google datastore as storage...")

2018-04-27 17:06:19,923 - memory_profile7_log - INFO -    224                                         if multproc:

2018-04-27 17:06:19,923 - memory_profile7_log - INFO -    225                                             mh.saveDatastoreMP(model_transformsv)

2018-04-27 17:06:19,928 - memory_profile7_log - INFO -    226                                             # mh.saveDataStorePutMulti(model_transform[['user_id', 'topic_id', 'is_general', 'rank']])

2018-04-27 17:06:19,928 - memory_profile7_log - INFO -    227                                         else:

2018-04-27 17:06:19,930 - memory_profile7_log - INFO -    228                                             mh.saveDatastore(model_transformsv)

2018-04-27 17:06:19,930 - memory_profile7_log - INFO -    229                                             

2018-04-27 17:06:19,930 - memory_profile7_log - INFO -    230                                     elif str(saveto).lower() == "elastic":

2018-04-27 17:06:19,930 - memory_profile7_log - INFO -    231                                         logger.info("Using ElasticSearch as storage...")

2018-04-27 17:06:19,930 - memory_profile7_log - INFO -    232                                         mh.saveElasticS(model_transformsv)

2018-04-27 17:06:19,930 - memory_profile7_log - INFO -    233                             

2018-04-27 17:06:19,930 - memory_profile7_log - INFO -    234    895.7 MiB      0.0 MiB       return

2018-04-27 17:06:19,931 - memory_profile7_log - INFO - 


2018-04-27 17:06:19,931 - memory_profile7_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-27 17:12:50,653 - memory_profile7_log - INFO - Generating date range with N: 3
2018-04-27 17:12:50,655 - memory_profile7_log - INFO - date_generated: 
2018-04-27 17:12:50,655 - memory_profile7_log - INFO -  
2018-04-27 17:12:50,657 - memory_profile7_log - INFO - [datetime.datetime(2018, 4, 12, 0, 0), datetime.datetime(2018, 4, 13, 0, 0), datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-27 17:12:50,657 - memory_profile7_log - INFO - 

2018-04-27 17:12:50,657 - memory_profile7_log - INFO - using current date: 2018-04-15
2018-04-27 17:12:50,657 - memory_profile7_log - INFO - using start date: 2018-04-12 00:00:00
2018-04-27 17:12:50,658 - memory_profile7_log - INFO - using end date: 2018-04-14
2018-04-27 17:12:50,816 - memory_profile7_log - INFO - Starting data fetch iterative...
2018-04-27 17:12:50,819 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-12
2018-04-27 17:15:52,865 - memory_profile7_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 17:15:52,868 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 17:15:52,869 - memory_profile7_log - INFO - ================================================

2018-04-27 17:15:52,871 - memory_profile7_log - INFO -     67     86.8 MiB     86.8 MiB   @profile

2018-04-27 17:15:52,871 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-27 17:15:52,872 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-27 17:15:52,874 - memory_profile7_log - INFO -     70     86.8 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 17:15:52,875 - memory_profile7_log - INFO -     71     86.8 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 17:15:52,878 - memory_profile7_log - INFO -     72                             

2018-04-27 17:15:52,878 - memory_profile7_log - INFO -     73     89.9 MiB      3.1 MiB       result  = client.query(query, job_config=job_config)

2018-04-27 17:15:52,880 - memory_profile7_log - INFO -     74     90.0 MiB      0.1 MiB       rows = result.result()

2018-04-27 17:15:52,881 - memory_profile7_log - INFO -     75                             

2018-04-27 17:15:52,881 - memory_profile7_log - INFO -     76     90.0 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-27 17:15:52,882 - memory_profile7_log - INFO -     77                                 

2018-04-27 17:15:52,884 - memory_profile7_log - INFO -     78     90.0 MiB      0.0 MiB       def _q_iterator():

2018-04-27 17:15:52,885 - memory_profile7_log - INFO -     79                                     # data = []

2018-04-27 17:15:52,887 - memory_profile7_log - INFO -     80    472.2 MiB    286.5 MiB           for row in rows:

2018-04-27 17:15:52,888 - memory_profile7_log - INFO -     81                                         # data.append(list(row))

2018-04-27 17:15:52,890 - memory_profile7_log - INFO -     82    472.2 MiB    -86.7 MiB               yield list(row)

2018-04-27 17:15:52,890 - memory_profile7_log - INFO -     83                             

2018-04-27 17:15:52,891 - memory_profile7_log - INFO -     84                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-27 17:15:52,892 - memory_profile7_log - INFO -     85    394.2 MiB    -78.1 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-27 17:15:52,894 - memory_profile7_log - INFO -     86                                 # del rows

2018-04-27 17:15:52,894 - memory_profile7_log - INFO -     87    394.2 MiB      0.0 MiB       del result

2018-04-27 17:15:52,897 - memory_profile7_log - INFO -     88                                 # del data

2018-04-27 17:15:52,898 - memory_profile7_log - INFO -     89                              

2018-04-27 17:15:52,901 - memory_profile7_log - INFO -     90    394.2 MiB      0.0 MiB       return df

2018-04-27 17:15:52,901 - memory_profile7_log - INFO - 


2018-04-27 17:15:52,904 - memory_profile7_log - INFO - getting total: 462579 training data(genuine interest) for date: 2018-04-12
2018-04-27 17:15:52,904 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-13
2018-04-27 17:18:12,516 - memory_profile7_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 17:18:12,516 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 17:18:12,519 - memory_profile7_log - INFO - ================================================

2018-04-27 17:18:12,520 - memory_profile7_log - INFO -     67    394.2 MiB    394.2 MiB   @profile

2018-04-27 17:18:12,522 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-27 17:18:12,523 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-27 17:18:12,523 - memory_profile7_log - INFO -     70    394.2 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 17:18:12,525 - memory_profile7_log - INFO -     71    394.2 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 17:18:12,526 - memory_profile7_log - INFO -     72                             

2018-04-27 17:18:12,528 - memory_profile7_log - INFO -     73    394.2 MiB      0.0 MiB       result  = client.query(query, job_config=job_config)

2018-04-27 17:18:12,529 - memory_profile7_log - INFO -     74    394.2 MiB      0.0 MiB       rows = result.result()

2018-04-27 17:18:12,530 - memory_profile7_log - INFO -     75                             

2018-04-27 17:18:12,532 - memory_profile7_log - INFO -     76    394.2 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-27 17:18:12,532 - memory_profile7_log - INFO -     77                                 

2018-04-27 17:18:12,533 - memory_profile7_log - INFO -     78    394.2 MiB      0.0 MiB       def _q_iterator():

2018-04-27 17:18:12,535 - memory_profile7_log - INFO -     79                                     # data = []

2018-04-27 17:18:12,536 - memory_profile7_log - INFO -     80    557.4 MiB   -235.1 MiB           for row in rows:

2018-04-27 17:18:12,539 - memory_profile7_log - INFO -     81                                         # data.append(list(row))

2018-04-27 17:18:12,540 - memory_profile7_log - INFO -     82    557.4 MiB   -714.5 MiB               yield list(row)

2018-04-27 17:18:12,542 - memory_profile7_log - INFO -     83                             

2018-04-27 17:18:12,542 - memory_profile7_log - INFO -     84                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-27 17:18:12,543 - memory_profile7_log - INFO -     85    493.1 MiB    -64.4 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-27 17:18:12,546 - memory_profile7_log - INFO -     86                                 # del rows

2018-04-27 17:18:12,546 - memory_profile7_log - INFO -     87    493.1 MiB      0.0 MiB       del result

2018-04-27 17:18:12,549 - memory_profile7_log - INFO -     88                                 # del data

2018-04-27 17:18:12,551 - memory_profile7_log - INFO -     89                              

2018-04-27 17:18:12,552 - memory_profile7_log - INFO -     90    493.1 MiB      0.0 MiB       return df

2018-04-27 17:18:12,552 - memory_profile7_log - INFO - 


2018-04-27 17:18:12,553 - memory_profile7_log - INFO - getting total: 361145 training data(genuine interest) for date: 2018-04-13
2018-04-27 17:18:12,555 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-14
2018-04-27 17:19:54,296 - memory_profile7_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 17:19:54,298 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 17:19:54,299 - memory_profile7_log - INFO - ================================================

2018-04-27 17:19:54,301 - memory_profile7_log - INFO -     67    493.1 MiB    493.1 MiB   @profile

2018-04-27 17:19:54,302 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-27 17:19:54,303 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-27 17:19:54,305 - memory_profile7_log - INFO -     70    493.1 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 17:19:54,305 - memory_profile7_log - INFO -     71    493.1 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 17:19:54,309 - memory_profile7_log - INFO -     72                             

2018-04-27 17:19:54,311 - memory_profile7_log - INFO -     73    493.1 MiB      0.0 MiB       result  = client.query(query, job_config=job_config)

2018-04-27 17:19:54,312 - memory_profile7_log - INFO -     74    493.1 MiB      0.0 MiB       rows = result.result()

2018-04-27 17:19:54,312 - memory_profile7_log - INFO -     75                             

2018-04-27 17:19:54,313 - memory_profile7_log - INFO -     76    493.1 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-27 17:19:54,315 - memory_profile7_log - INFO -     77                                 

2018-04-27 17:19:54,316 - memory_profile7_log - INFO -     78    493.1 MiB      0.0 MiB       def _q_iterator():

2018-04-27 17:19:54,318 - memory_profile7_log - INFO -     79                                     # data = []

2018-04-27 17:19:54,321 - memory_profile7_log - INFO -     80    616.9 MiB     70.2 MiB           for row in rows:

2018-04-27 17:19:54,322 - memory_profile7_log - INFO -     81                                         # data.append(list(row))

2018-04-27 17:19:54,322 - memory_profile7_log - INFO -     82    616.9 MiB    -45.0 MiB               yield list(row)

2018-04-27 17:19:54,323 - memory_profile7_log - INFO -     83                             

2018-04-27 17:19:54,325 - memory_profile7_log - INFO -     84                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-27 17:19:54,326 - memory_profile7_log - INFO -     85    565.0 MiB    -51.9 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-27 17:19:54,328 - memory_profile7_log - INFO -     86                                 # del rows

2018-04-27 17:19:54,328 - memory_profile7_log - INFO -     87    565.0 MiB      0.0 MiB       del result

2018-04-27 17:19:54,329 - memory_profile7_log - INFO -     88                                 # del data

2018-04-27 17:19:54,332 - memory_profile7_log - INFO -     89                              

2018-04-27 17:19:54,334 - memory_profile7_log - INFO -     90    565.0 MiB      0.0 MiB       return df

2018-04-27 17:19:54,335 - memory_profile7_log - INFO - 


2018-04-27 17:19:54,336 - memory_profile7_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-27 17:19:54,338 - memory_profile7_log - INFO - len datalist: 3
2018-04-27 17:19:54,338 - memory_profile7_log - INFO - All data fetch iterative done!!
2018-04-27 17:19:54,341 - memory_profile7_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 17:19:54,342 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 17:19:54,344 - memory_profile7_log - INFO - ================================================

2018-04-27 17:19:54,344 - memory_profile7_log - INFO -    236     86.8 MiB     86.8 MiB   @profile

2018-04-27 17:19:54,345 - memory_profile7_log - INFO -    237                             def BQPreprocess(loadmp, cpu, date_generated, client):

2018-04-27 17:19:54,345 - memory_profile7_log - INFO -    238     86.8 MiB      0.0 MiB       bq_client = client

2018-04-27 17:19:54,346 - memory_profile7_log - INFO -    239     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-27 17:19:54,348 - memory_profile7_log - INFO -    240                             

2018-04-27 17:19:54,348 - memory_profile7_log - INFO -    241     86.8 MiB      0.0 MiB       datalist = []

2018-04-27 17:19:54,348 - memory_profile7_log - INFO -    242    493.1 MiB      0.0 MiB       def _getBig(procdate, client):

2018-04-27 17:19:54,349 - memory_profile7_log - INFO -    243    493.1 MiB      0.0 MiB           logger.info("Collecting training data for date: %s", procdate)

2018-04-27 17:19:54,351 - memory_profile7_log - INFO -    244                                     # ~ get genuine news interest ~

2018-04-27 17:19:54,354 - memory_profile7_log - INFO -    245    493.1 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-27 17:19:54,355 - memory_profile7_log - INFO -    246                             

2018-04-27 17:19:54,355 - memory_profile7_log - INFO -    247                                     # safe handling of query parameter

2018-04-27 17:19:54,355 - memory_profile7_log - INFO -    248                                     query_params = [

2018-04-27 17:19:54,357 - memory_profile7_log - INFO -    249    493.1 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', procdate)

2018-04-27 17:19:54,358 - memory_profile7_log - INFO -    250                                     ]

2018-04-27 17:19:54,358 - memory_profile7_log - INFO -    251                             

2018-04-27 17:19:54,358 - memory_profile7_log - INFO -    252    493.1 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-27 17:19:54,359 - memory_profile7_log - INFO -    253                                     # temp_df = load_bigquery(client, query_fit + query_fit_where, job_config)

2018-04-27 17:19:54,359 - memory_profile7_log - INFO -    254    565.0 MiB    478.2 MiB           temp_df = loadBQ(client, query_fit + query_fit_where, job_config)

2018-04-27 17:19:54,361 - memory_profile7_log - INFO -    255                             

2018-04-27 17:19:54,361 - memory_profile7_log - INFO -    256    565.0 MiB      0.0 MiB           if temp_df.empty:

2018-04-27 17:19:54,361 - memory_profile7_log - INFO -    257                                         logger.info("%s data is empty!", procdate)

2018-04-27 17:19:54,365 - memory_profile7_log - INFO -    258                                         return None

2018-04-27 17:19:54,367 - memory_profile7_log - INFO -    259                                     else:

2018-04-27 17:19:54,367 - memory_profile7_log - INFO -    260    565.0 MiB      0.0 MiB               logger.info("getting total: %d training data(genuine interest) for date: %s" % (len(temp_df), procdate))

2018-04-27 17:19:54,368 - memory_profile7_log - INFO -    261    565.0 MiB      0.0 MiB               if loadmp:

2018-04-27 17:19:54,368 - memory_profile7_log - INFO -    262                                             logger.info("Exiting: %s", mp.current_process().name)

2018-04-27 17:19:54,368 - memory_profile7_log - INFO -    263    565.0 MiB      0.0 MiB               return temp_df

2018-04-27 17:19:54,369 - memory_profile7_log - INFO -    264                             

2018-04-27 17:19:54,369 - memory_profile7_log - INFO -    265     86.8 MiB      0.0 MiB       if loadmp:

2018-04-27 17:19:54,371 - memory_profile7_log - INFO -    266                                     logger.info("Starting data fetch multiprocess..")

2018-04-27 17:19:54,371 - memory_profile7_log - INFO -    267                                     logger.info("number of process: %d", len(date_generated))

2018-04-27 17:19:54,372 - memory_profile7_log - INFO -    268                             

2018-04-27 17:19:54,372 - memory_profile7_log - INFO -    269                                     pool = mp.Pool(processes=cpu)

2018-04-27 17:19:54,377 - memory_profile7_log - INFO -    270                                     # multprocessA = [pool.apply(_getBig, args=(ndate.strftime("%Y-%m-%d"), )) for ndate in date_generated]

2018-04-27 17:19:54,377 - memory_profile7_log - INFO -    271                                     multprocessA = [pool.apply_async(_getBig, args=(ndate.strftime("%Y-%m-%d"), bq_client, )) for ndate in date_generated]

2018-04-27 17:19:54,378 - memory_profile7_log - INFO -    272                                     output_multprocessA = [p.get() for p in multprocessA]

2018-04-27 17:19:54,378 - memory_profile7_log - INFO -    273                             

2018-04-27 17:19:54,378 - memory_profile7_log - INFO -    274                                     for m in output_multprocessA:

2018-04-27 17:19:54,380 - memory_profile7_log - INFO -    275                                         if m is not None:

2018-04-27 17:19:54,381 - memory_profile7_log - INFO -    276                                             if not m.empty:

2018-04-27 17:19:54,381 - memory_profile7_log - INFO -    277                                                 datalist.append(m)

2018-04-27 17:19:54,382 - memory_profile7_log - INFO -    278                             

2018-04-27 17:19:54,382 - memory_profile7_log - INFO -    279                                     logger.info("len datalist: %d", len(datalist))

2018-04-27 17:19:54,384 - memory_profile7_log - INFO -    280                                     logger.info("All data fetch multiprocess done!!")

2018-04-27 17:19:54,388 - memory_profile7_log - INFO -    281                                 else:

2018-04-27 17:19:54,388 - memory_profile7_log - INFO -    282     86.8 MiB      0.0 MiB           logger.info("Starting data fetch iterative...")

2018-04-27 17:19:54,390 - memory_profile7_log - INFO -    283    565.0 MiB      0.0 MiB           for ndate in date_generated:

2018-04-27 17:19:54,390 - memory_profile7_log - INFO -    284    565.0 MiB      0.0 MiB               tframe = _getBig(ndate.strftime("%Y-%m-%d"), bq_client)

2018-04-27 17:19:54,391 - memory_profile7_log - INFO -    285    565.0 MiB      0.0 MiB               if tframe is not None:

2018-04-27 17:19:54,391 - memory_profile7_log - INFO -    286    565.0 MiB      0.0 MiB                   if not tframe.empty:

2018-04-27 17:19:54,392 - memory_profile7_log - INFO -    287    565.0 MiB      0.0 MiB                       datalist.append(tframe)

2018-04-27 17:19:54,392 - memory_profile7_log - INFO -    288                                         else: 

2018-04-27 17:19:54,394 - memory_profile7_log - INFO -    289                                             logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-27 17:19:54,395 - memory_profile7_log - INFO -    290    565.0 MiB      0.0 MiB           logger.info("len datalist: %d", len(datalist))

2018-04-27 17:19:54,395 - memory_profile7_log - INFO -    291    565.0 MiB      0.0 MiB           logger.info("All data fetch iterative done!!")

2018-04-27 17:19:54,398 - memory_profile7_log - INFO -    292                             

2018-04-27 17:19:54,400 - memory_profile7_log - INFO -    293    565.0 MiB      0.0 MiB       return datalist

2018-04-27 17:19:54,401 - memory_profile7_log - INFO - 


2018-04-27 17:19:54,503 - memory_profile7_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-27 17:21:10,292 - memory_profile7_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 17:21:10,293 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 17:21:10,293 - memory_profile7_log - INFO - ================================================

2018-04-27 17:21:10,296 - memory_profile7_log - INFO -     67    574.5 MiB    574.5 MiB   @profile

2018-04-27 17:21:10,296 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-27 17:21:10,298 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-27 17:21:10,299 - memory_profile7_log - INFO -     70    574.5 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 17:21:10,301 - memory_profile7_log - INFO -     71    574.5 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 17:21:10,303 - memory_profile7_log - INFO -     72                             

2018-04-27 17:21:10,305 - memory_profile7_log - INFO -     73    574.5 MiB      0.0 MiB       result  = client.query(query, job_config=job_config)

2018-04-27 17:21:10,306 - memory_profile7_log - INFO -     74    574.3 MiB     -0.3 MiB       rows = result.result()

2018-04-27 17:21:10,308 - memory_profile7_log - INFO -     75                             

2018-04-27 17:21:10,309 - memory_profile7_log - INFO -     76    574.3 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-27 17:21:10,309 - memory_profile7_log - INFO -     77                                 

2018-04-27 17:21:10,311 - memory_profile7_log - INFO -     78    574.3 MiB      0.0 MiB       def _q_iterator():

2018-04-27 17:21:10,312 - memory_profile7_log - INFO -     79                                     # data = []

2018-04-27 17:21:10,312 - memory_profile7_log - INFO -     80    698.9 MiB     86.7 MiB           for row in rows:

2018-04-27 17:21:10,315 - memory_profile7_log - INFO -     81                                         # data.append(list(row))

2018-04-27 17:21:10,316 - memory_profile7_log - INFO -     82    698.9 MiB    -15.0 MiB               yield list(row)

2018-04-27 17:21:10,318 - memory_profile7_log - INFO -     83                             

2018-04-27 17:21:10,318 - memory_profile7_log - INFO -     84                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-27 17:21:10,319 - memory_profile7_log - INFO -     85    643.8 MiB    -55.0 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-27 17:21:10,319 - memory_profile7_log - INFO -     86                                 # del rows

2018-04-27 17:21:10,321 - memory_profile7_log - INFO -     87    643.8 MiB      0.0 MiB       del result

2018-04-27 17:21:10,322 - memory_profile7_log - INFO -     88                                 # del data

2018-04-27 17:21:10,322 - memory_profile7_log - INFO -     89                              

2018-04-27 17:21:10,322 - memory_profile7_log - INFO -     90    643.8 MiB      0.0 MiB       return df

2018-04-27 17:21:10,322 - memory_profile7_log - INFO - 


2018-04-27 17:21:10,326 - memory_profile7_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-27 17:21:10,336 - memory_profile7_log - INFO - loading time of: 1308144 total genuine-current interest data ~ take 499.548s
2018-04-27 17:21:10,338 - memory_profile7_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 17:21:10,338 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 17:21:10,338 - memory_profile7_log - INFO - ================================================

2018-04-27 17:21:10,339 - memory_profile7_log - INFO -    295     86.7 MiB     86.7 MiB   @profile

2018-04-27 17:21:10,339 - memory_profile7_log - INFO -    296                             def preprocess(loadmp, cpu, cd, query_fit, date_generated):

2018-04-27 17:21:10,342 - memory_profile7_log - INFO -    297     86.8 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-27 17:21:10,342 - memory_profile7_log - INFO -    298     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-27 17:21:10,342 - memory_profile7_log - INFO -    299                             

2018-04-27 17:21:10,342 - memory_profile7_log - INFO -    300                                 # ~~~ Begin collecting data ~~~

2018-04-27 17:21:10,342 - memory_profile7_log - INFO -    301     86.8 MiB      0.0 MiB       t0 = time.time()

2018-04-27 17:21:10,342 - memory_profile7_log - INFO -    302                                 

2018-04-27 17:21:10,342 - memory_profile7_log - INFO -    303    565.0 MiB    478.2 MiB       datalist = BQPreprocess(loadmp, cpu, date_generated, bq_client)

2018-04-27 17:21:10,342 - memory_profile7_log - INFO -    304                             

2018-04-27 17:21:10,344 - memory_profile7_log - INFO -    305    565.0 MiB      0.0 MiB       if len(datalist) > 1:

2018-04-27 17:21:10,344 - memory_profile7_log - INFO -    306    615.8 MiB     50.8 MiB           big_frame = pd.concat(datalist)

2018-04-27 17:21:10,344 - memory_profile7_log - INFO -    307    574.4 MiB    -41.4 MiB           del datalist

2018-04-27 17:21:10,344 - memory_profile7_log - INFO -    308                                 else:

2018-04-27 17:21:10,344 - memory_profile7_log - INFO -    309                                     big_frame = datalist

2018-04-27 17:21:10,348 - memory_profile7_log - INFO -    310    574.5 MiB      0.1 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-27 17:21:10,348 - memory_profile7_log - INFO -    311                             

2018-04-27 17:21:10,348 - memory_profile7_log - INFO -    312                                 # ~ get current news interest ~

2018-04-27 17:21:10,348 - memory_profile7_log - INFO -    313    574.5 MiB      0.0 MiB       if not cd:

2018-04-27 17:21:10,349 - memory_profile7_log - INFO -    314                                     logger.info("Collecting training data(current date interest)..")

2018-04-27 17:21:10,349 - memory_profile7_log - INFO -    315                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-27 17:21:10,351 - memory_profile7_log - INFO -    316                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-27 17:21:10,351 - memory_profile7_log - INFO -    317                                 else:

2018-04-27 17:21:10,351 - memory_profile7_log - INFO -    318    574.5 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-27 17:21:10,351 - memory_profile7_log - INFO -    319    574.5 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-27 17:21:10,351 - memory_profile7_log - INFO -    320                             

2018-04-27 17:21:10,352 - memory_profile7_log - INFO -    321                                     # safe handling of query parameter

2018-04-27 17:21:10,352 - memory_profile7_log - INFO -    322                                     query_params = [

2018-04-27 17:21:10,352 - memory_profile7_log - INFO -    323    574.5 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-27 17:21:10,352 - memory_profile7_log - INFO -    324                                     ]

2018-04-27 17:21:10,352 - memory_profile7_log - INFO -    325                             

2018-04-27 17:21:10,354 - memory_profile7_log - INFO -    326    574.5 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-27 17:21:10,354 - memory_profile7_log - INFO -    327    643.8 MiB     69.3 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-27 17:21:10,354 - memory_profile7_log - INFO -    328    643.8 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-27 17:21:10,354 - memory_profile7_log - INFO -    329                             

2018-04-27 17:21:10,354 - memory_profile7_log - INFO -    330    643.9 MiB      0.1 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-27 17:21:10,355 - memory_profile7_log - INFO -    331    643.9 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-27 17:21:10,355 - memory_profile7_log - INFO -    332    643.9 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-27 17:21:10,355 - memory_profile7_log - INFO -    333                             

2018-04-27 17:21:10,355 - memory_profile7_log - INFO -    334    643.9 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-27 17:21:10,355 - memory_profile7_log - INFO -    335    643.9 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-27 17:21:10,355 - memory_profile7_log - INFO -    336                             

2018-04-27 17:21:10,355 - memory_profile7_log - INFO -    337    643.9 MiB      0.0 MiB       return big_frame, current_frame

2018-04-27 17:21:10,355 - memory_profile7_log - INFO - 


2018-04-27 17:21:10,358 - memory_profile7_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-27 17:21:10,434 - memory_profile7_log - INFO - train on: 1077719 total history data(D(u, t))
2018-04-27 17:21:10,434 - memory_profile7_log - INFO - transform on: 230425 total current data(D(t))
2018-04-27 17:21:14,924 - memory_profile7_log - INFO - Len of model_fit: 1077719
2018-04-27 17:21:14,924 - memory_profile7_log - INFO - Len of df_dut: 1077719
2018-04-27 17:22:48,318 - memory_profile7_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-27 17:22:48,433 - memory_profile7_log - INFO - Total train time: 97.998s
2018-04-27 17:22:48,434 - memory_profile7_log - INFO - memory left before cleaning: 83.600 percent memory...
2018-04-27 17:22:48,434 - memory_profile7_log - INFO - cleaning up some objects...
2018-04-27 17:22:48,437 - memory_profile7_log - INFO - deleting df_dut...
2018-04-27 17:22:48,437 - memory_profile7_log - INFO - deleting df_dt...
2018-04-27 17:22:48,438 - memory_profile7_log - INFO - deleting df_input...
2018-04-27 17:22:48,447 - memory_profile7_log - INFO - deleting df_input_X...
2018-04-27 17:22:48,448 - memory_profile7_log - INFO - deleting df_current...
2018-04-27 17:22:48,450 - memory_profile7_log - INFO - deleting map_topic_isgeneral...
2018-04-27 17:22:48,515 - memory_profile7_log - INFO - deleting model_fit...
2018-04-27 17:22:48,516 - memory_profile7_log - INFO - deleting result...
2018-04-27 17:22:48,552 - memory_profile7_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 17:22:48,553 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 17:22:48,553 - memory_profile7_log - INFO - ================================================

2018-04-27 17:22:48,555 - memory_profile7_log - INFO -    102    643.9 MiB    643.9 MiB   @profile

2018-04-27 17:22:48,555 - memory_profile7_log - INFO -    103                             def main(df_input, df_current, current_date, G,

2018-04-27 17:22:48,555 - memory_profile7_log - INFO -    104                                      project_id, savetrain=False, multproc=True,

2018-04-27 17:22:48,555 - memory_profile7_log - INFO -    105                                      threshold=0, start_date=None, end_date=None,

2018-04-27 17:22:48,555 - memory_profile7_log - INFO -    106                                      saveto="datastore"):

2018-04-27 17:22:48,555 - memory_profile7_log - INFO -    107                                 """

2018-04-27 17:22:48,556 - memory_profile7_log - INFO -    108                                     Main cron method

2018-04-27 17:22:48,556 - memory_profile7_log - INFO -    109                                 """

2018-04-27 17:22:48,556 - memory_profile7_log - INFO -    110                                 # ~ Data Preprocessing ~

2018-04-27 17:22:48,556 - memory_profile7_log - INFO -    111                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-27 17:22:48,556 - memory_profile7_log - INFO -    112                                 # D(u, t)

2018-04-27 17:22:48,558 - memory_profile7_log - INFO -    113    643.9 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-27 17:22:48,558 - memory_profile7_log - INFO -    114    677.9 MiB     33.9 MiB       df_dut = df_input.copy(deep=True)

2018-04-27 17:22:48,559 - memory_profile7_log - INFO -    115    677.9 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-27 17:22:48,559 - memory_profile7_log - INFO -    116                             

2018-04-27 17:22:48,559 - memory_profile7_log - INFO -    117                                 # D(t)

2018-04-27 17:22:48,559 - memory_profile7_log - INFO -    118    685.1 MiB      7.3 MiB       df_dt = df_current.copy(deep=True)

2018-04-27 17:22:48,559 - memory_profile7_log - INFO -    119    685.1 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-27 17:22:48,559 - memory_profile7_log - INFO -    120                             

2018-04-27 17:22:48,561 - memory_profile7_log - INFO -    121                                 # ~~~~~~ Begin train ~~~~~~

2018-04-27 17:22:48,561 - memory_profile7_log - INFO -    122    685.1 MiB      0.0 MiB       t0 = time.time()

2018-04-27 17:22:48,561 - memory_profile7_log - INFO -    123    685.1 MiB      0.0 MiB       logger.info("train on: %d total history data(D(u, t))", len(df_dut))

2018-04-27 17:22:48,562 - memory_profile7_log - INFO -    124    685.1 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-27 17:22:48,562 - memory_profile7_log - INFO -    125                             

2018-04-27 17:22:48,568 - memory_profile7_log - INFO -    126                                 # instantiace class

2018-04-27 17:22:48,569 - memory_profile7_log - INFO -    127    685.1 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-27 17:22:48,569 - memory_profile7_log - INFO -    128                             

2018-04-27 17:22:48,569 - memory_profile7_log - INFO -    129                                 # ~~ Fit ~~

2018-04-27 17:22:48,569 - memory_profile7_log - INFO -    130                                 #   handling genuine news interest < current date

2018-04-27 17:22:48,569 - memory_profile7_log - INFO -    131    689.2 MiB      4.1 MiB       NB = BR.processX(df_dut)

2018-04-27 17:22:48,569 - memory_profile7_log - INFO -    132                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-27 17:22:48,571 - memory_profile7_log - INFO -    133                                 #   nanti dipindah ke class train utama

2018-04-27 17:22:48,571 - memory_profile7_log - INFO -    134    731.8 MiB     42.6 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-27 17:22:48,572 - memory_profile7_log - INFO -    135                                 """

2018-04-27 17:22:48,572 - memory_profile7_log - INFO -    136                                     num_y = total global click for category=ci on periode t

2018-04-27 17:22:48,572 - memory_profile7_log - INFO -    137                                     num_x = total click from user_U for category=ci on periode t

2018-04-27 17:22:48,572 - memory_profile7_log - INFO -    138                                 """

2018-04-27 17:22:48,572 - memory_profile7_log - INFO -    139    731.8 MiB      0.0 MiB       fitby_sigmant = True

2018-04-27 17:22:48,572 - memory_profile7_log - INFO -    140    731.8 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-27 17:22:48,572 - memory_profile7_log - INFO -    141    731.8 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-27 17:22:48,572 - memory_profile7_log - INFO -    142    774.0 MiB     42.1 MiB                            'is_general']]

2018-04-27 17:22:48,573 - memory_profile7_log - INFO -    143    774.0 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-27 17:22:48,573 - memory_profile7_log - INFO -    144    774.0 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-27 17:22:48,573 - memory_profile7_log - INFO -    145    877.2 MiB    103.2 MiB                          verbose=False)

2018-04-27 17:22:48,573 - memory_profile7_log - INFO -    146    877.2 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-27 17:22:48,575 - memory_profile7_log - INFO -    147    877.2 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-27 17:22:48,575 - memory_profile7_log - INFO -    148                             

2018-04-27 17:22:48,575 - memory_profile7_log - INFO -    149                                 # ~~ and Transform ~~

2018-04-27 17:22:48,575 - memory_profile7_log - INFO -    150                                 #   handling current news interest == current date

2018-04-27 17:22:48,575 - memory_profile7_log - INFO -    151    877.2 MiB      0.0 MiB       if df_dt.empty:

2018-04-27 17:22:48,575 - memory_profile7_log - INFO -    152                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-27 17:22:48,575 - memory_profile7_log - INFO -    153                                     return None

2018-04-27 17:22:48,575 - memory_profile7_log - INFO -    154    877.3 MiB      0.1 MiB       NB = BR.processX(df_dt)

2018-04-27 17:22:48,581 - memory_profile7_log - INFO -    155    887.9 MiB     10.6 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-27 17:22:48,582 - memory_profile7_log - INFO -    156                             

2018-04-27 17:22:48,582 - memory_profile7_log - INFO -    157    887.9 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-27 17:22:48,582 - memory_profile7_log - INFO -    158    854.6 MiB    -33.3 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-27 17:22:48,582 - memory_profile7_log - INFO -    159    854.6 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-27 17:22:48,584 - memory_profile7_log - INFO -    160    854.6 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-27 17:22:48,584 - memory_profile7_log - INFO -    161    918.1 MiB     63.5 MiB                                                     verbose=False)

2018-04-27 17:22:48,584 - memory_profile7_log - INFO -    162                                 # ~~~ filter is general and specific topic ~~~

2018-04-27 17:22:48,585 - memory_profile7_log - INFO -    163                                 # the idea is just we need to rerank every topic according

2018-04-27 17:22:48,585 - memory_profile7_log - INFO -    164                                 # user_id and and is_general by p0_posterior

2018-04-27 17:22:48,585 - memory_profile7_log - INFO -    165    918.1 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-27 17:22:48,585 - memory_profile7_log - INFO -    166    927.3 MiB      9.3 MiB                                     'is_general']].groupby(['topic_id',

2018-04-27 17:22:48,585 - memory_profile7_log - INFO -    167    918.1 MiB     -9.2 MiB                                                             'is_general']

2018-04-27 17:22:48,585 - memory_profile7_log - INFO -    168                                                                                      ).size().to_frame().reset_index()

2018-04-27 17:22:48,586 - memory_profile7_log - INFO -    169                             

2018-04-27 17:22:48,586 - memory_profile7_log - INFO -    170                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-27 17:22:48,586 - memory_profile7_log - INFO -    171                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-27 17:22:48,588 - memory_profile7_log - INFO -    172    919.1 MiB      1.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-27 17:22:48,588 - memory_profile7_log - INFO -    173                             

2018-04-27 17:22:48,588 - memory_profile7_log - INFO -    174                                 # ~ start by provide rank for each topic type ~

2018-04-27 17:22:48,588 - memory_profile7_log - INFO -    175   1008.8 MiB     89.7 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-27 17:22:48,588 - memory_profile7_log - INFO -    176   1018.0 MiB      9.2 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-27 17:22:48,588 - memory_profile7_log - INFO -    177                             

2018-04-27 17:22:48,595 - memory_profile7_log - INFO -    178                                 # ~ set threshold to filter output

2018-04-27 17:22:48,595 - memory_profile7_log - INFO -    179   1018.0 MiB      0.0 MiB       if threshold > 0:

2018-04-27 17:22:48,595 - memory_profile7_log - INFO -    180   1018.0 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-27 17:22:48,595 - memory_profile7_log - INFO -    181   1018.0 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-27 17:22:48,596 - memory_profile7_log - INFO -    182   1006.3 MiB    -11.6 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-27 17:22:48,596 - memory_profile7_log - INFO -    183                             

2018-04-27 17:22:48,596 - memory_profile7_log - INFO -    184                                 # print "\n", model_transform[['user_id', 'topic_id', 'p0_posterior',

2018-04-27 17:22:48,598 - memory_profile7_log - INFO -    185                                 #                             'is_general', 'rank']].loc[model_transform['user_id'].str.contains('1610c8e6b6466b-05a6ac1ba8c1b8-4323461-100200-1',

2018-04-27 17:22:48,598 - memory_profile7_log - INFO -    186                                 #                                                                                                 case=False)].head(45)

2018-04-27 17:22:48,598 - memory_profile7_log - INFO -    187                                 # print "len(model_transform): %d" % len(model_transform)

2018-04-27 17:22:48,598 - memory_profile7_log - INFO -    188                             

2018-04-27 17:22:48,598 - memory_profile7_log - INFO -    189   1006.3 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-27 17:22:48,598 - memory_profile7_log - INFO -    190   1006.3 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-27 17:22:48,598 - memory_profile7_log - INFO -    191                             

2018-04-27 17:22:48,598 - memory_profile7_log - INFO -    192   1006.3 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-27 17:22:48,599 - memory_profile7_log - INFO -    193                             

2018-04-27 17:22:48,599 - memory_profile7_log - INFO -    194   1006.3 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-27 17:22:48,599 - memory_profile7_log - INFO -    195   1006.3 MiB      0.0 MiB       del df_dut

2018-04-27 17:22:48,601 - memory_profile7_log - INFO -    196   1006.3 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-27 17:22:48,601 - memory_profile7_log - INFO -    197   1006.3 MiB      0.0 MiB       del df_dt

2018-04-27 17:22:48,601 - memory_profile7_log - INFO -    198   1006.3 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-27 17:22:48,601 - memory_profile7_log - INFO -    199   1006.3 MiB      0.0 MiB       del df_input

2018-04-27 17:22:48,601 - memory_profile7_log - INFO -    200   1006.3 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-27 17:22:48,601 - memory_profile7_log - INFO -    201    997.5 MiB     -8.8 MiB       del df_input_X

2018-04-27 17:22:48,601 - memory_profile7_log - INFO -    202    997.5 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-27 17:22:48,601 - memory_profile7_log - INFO -    203    997.5 MiB      0.0 MiB       del df_current

2018-04-27 17:22:48,601 - memory_profile7_log - INFO -    204    997.5 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-27 17:22:48,607 - memory_profile7_log - INFO -    205    997.5 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-27 17:22:48,607 - memory_profile7_log - INFO -    206    997.5 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-27 17:22:48,608 - memory_profile7_log - INFO -    207    897.8 MiB    -99.7 MiB       del model_fit

2018-04-27 17:22:48,608 - memory_profile7_log - INFO -    208    897.8 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-27 17:22:48,608 - memory_profile7_log - INFO -    209    897.8 MiB      0.0 MiB       del result

2018-04-27 17:22:48,608 - memory_profile7_log - INFO -    210    897.8 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-27 17:22:48,608 - memory_profile7_log - INFO -    211                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-27 17:22:48,609 - memory_profile7_log - INFO -    212                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-27 17:22:48,609 - memory_profile7_log - INFO -    213    897.8 MiB      0.0 MiB       if savetrain:

2018-04-27 17:22:48,609 - memory_profile7_log - INFO -    214                                     model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-27 17:22:48,611 - memory_profile7_log - INFO -    215                                     del model_transform

2018-04-27 17:22:48,611 - memory_profile7_log - INFO -    216                                     logger.info("deleting model_transform...")

2018-04-27 17:22:48,611 - memory_profile7_log - INFO -    217                                     logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-27 17:22:48,611 - memory_profile7_log - INFO -    218                             

2018-04-27 17:22:48,611 - memory_profile7_log - INFO -    219                                     logger.info("Begin saving trained data...")

2018-04-27 17:22:48,611 - memory_profile7_log - INFO -    220                                     # print "\n", model_transform.head(5)

2018-04-27 17:22:48,611 - memory_profile7_log - INFO -    221                                     # ~ Place your code to save the training model here ~

2018-04-27 17:22:48,612 - memory_profile7_log - INFO -    222                                     if str(saveto).lower() == "datastore":

2018-04-27 17:22:48,612 - memory_profile7_log - INFO -    223                                         logger.info("Using google datastore as storage...")

2018-04-27 17:22:48,612 - memory_profile7_log - INFO -    224                                         if multproc:

2018-04-27 17:22:48,612 - memory_profile7_log - INFO -    225                                             mh.saveDatastoreMP(model_transformsv)

2018-04-27 17:22:48,612 - memory_profile7_log - INFO -    226                                             # mh.saveDataStorePutMulti(model_transform[['user_id', 'topic_id', 'is_general', 'rank']])

2018-04-27 17:22:48,614 - memory_profile7_log - INFO -    227                                         else:

2018-04-27 17:22:48,614 - memory_profile7_log - INFO -    228                                             mh.saveDatastore(model_transformsv)

2018-04-27 17:22:48,614 - memory_profile7_log - INFO -    229                                             

2018-04-27 17:22:48,622 - memory_profile7_log - INFO -    230                                     elif str(saveto).lower() == "elastic":

2018-04-27 17:22:48,628 - memory_profile7_log - INFO -    231                                         logger.info("Using ElasticSearch as storage...")

2018-04-27 17:22:48,628 - memory_profile7_log - INFO -    232                                         mh.saveElasticS(model_transformsv)

2018-04-27 17:22:48,630 - memory_profile7_log - INFO -    233                             

2018-04-27 17:22:48,631 - memory_profile7_log - INFO -    234    897.8 MiB      0.0 MiB       return

2018-04-27 17:22:48,631 - memory_profile7_log - INFO - 


2018-04-27 17:22:48,631 - memory_profile7_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-27 18:25:57,171 - memory_profile7_log - INFO - Generating date range with N: 3
2018-04-27 18:25:57,176 - memory_profile7_log - INFO - date_generated: 
2018-04-27 18:25:57,176 - memory_profile7_log - INFO -  
2018-04-27 18:25:57,176 - memory_profile7_log - INFO - [datetime.datetime(2018, 4, 12, 0, 0), datetime.datetime(2018, 4, 13, 0, 0), datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-27 18:25:57,177 - memory_profile7_log - INFO - 

2018-04-27 18:25:57,177 - memory_profile7_log - INFO - using current date: 2018-04-15
2018-04-27 18:25:57,177 - memory_profile7_log - INFO - using start date: 2018-04-12 00:00:00
2018-04-27 18:25:57,177 - memory_profile7_log - INFO - using end date: 2018-04-14
2018-04-27 18:25:57,335 - memory_profile7_log - INFO - Starting data fetch iterative...
2018-04-27 18:25:57,335 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-12
2018-04-27 18:29:07,427 - memory_profile7_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 18:29:07,428 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 18:29:07,430 - memory_profile7_log - INFO - ================================================

2018-04-27 18:29:07,430 - memory_profile7_log - INFO -     67     86.8 MiB     86.8 MiB   @profile

2018-04-27 18:29:07,431 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-27 18:29:07,433 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-27 18:29:07,434 - memory_profile7_log - INFO -     70     86.8 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 18:29:07,434 - memory_profile7_log - INFO -     71     86.8 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 18:29:07,436 - memory_profile7_log - INFO -     72                             

2018-04-27 18:29:07,437 - memory_profile7_log - INFO -     73     89.9 MiB      3.1 MiB       result  = client.query(query, job_config=job_config)

2018-04-27 18:29:07,440 - memory_profile7_log - INFO -     74     90.0 MiB      0.1 MiB       rows = result.result()

2018-04-27 18:29:07,441 - memory_profile7_log - INFO -     75                             

2018-04-27 18:29:07,443 - memory_profile7_log - INFO -     76     90.0 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-27 18:29:07,444 - memory_profile7_log - INFO -     77                                 

2018-04-27 18:29:07,444 - memory_profile7_log - INFO -     78     90.0 MiB      0.0 MiB       def _q_iterator():

2018-04-27 18:29:07,446 - memory_profile7_log - INFO -     79                                     # data = []

2018-04-27 18:29:07,447 - memory_profile7_log - INFO -     80    473.5 MiB    286.2 MiB           for row in rows:

2018-04-27 18:29:07,447 - memory_profile7_log - INFO -     81                                         # data.append(list(row))

2018-04-27 18:29:07,450 - memory_profile7_log - INFO -     82    473.5 MiB    -89.8 MiB               yield list(row)

2018-04-27 18:29:07,450 - memory_profile7_log - INFO -     83                             

2018-04-27 18:29:07,451 - memory_profile7_log - INFO -     84                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-27 18:29:07,453 - memory_profile7_log - INFO -     85    394.1 MiB    -79.4 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-27 18:29:07,453 - memory_profile7_log - INFO -     86                                 # del rows

2018-04-27 18:29:07,454 - memory_profile7_log - INFO -     87    394.1 MiB      0.0 MiB       del result

2018-04-27 18:29:07,456 - memory_profile7_log - INFO -     88                                 # del data

2018-04-27 18:29:07,457 - memory_profile7_log - INFO -     89                              

2018-04-27 18:29:07,459 - memory_profile7_log - INFO -     90    394.1 MiB      0.0 MiB       return df

2018-04-27 18:29:07,460 - memory_profile7_log - INFO - 


2018-04-27 18:29:07,461 - memory_profile7_log - INFO - getting total: 462579 training data(genuine interest) for date: 2018-04-12
2018-04-27 18:29:07,463 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-13
2018-04-27 18:31:47,444 - memory_profile7_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 18:31:47,446 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 18:31:47,447 - memory_profile7_log - INFO - ================================================

2018-04-27 18:31:47,447 - memory_profile7_log - INFO -     67    394.1 MiB    394.1 MiB   @profile

2018-04-27 18:31:47,448 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-27 18:31:47,450 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-27 18:31:47,453 - memory_profile7_log - INFO -     70    394.1 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 18:31:47,457 - memory_profile7_log - INFO -     71    394.1 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 18:31:47,457 - memory_profile7_log - INFO -     72                             

2018-04-27 18:31:47,459 - memory_profile7_log - INFO -     73    394.1 MiB      0.0 MiB       result  = client.query(query, job_config=job_config)

2018-04-27 18:31:47,460 - memory_profile7_log - INFO -     74    394.1 MiB      0.0 MiB       rows = result.result()

2018-04-27 18:31:47,461 - memory_profile7_log - INFO -     75                             

2018-04-27 18:31:47,463 - memory_profile7_log - INFO -     76    394.1 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-27 18:31:47,463 - memory_profile7_log - INFO -     77                                 

2018-04-27 18:31:47,467 - memory_profile7_log - INFO -     78    394.1 MiB      0.0 MiB       def _q_iterator():

2018-04-27 18:31:47,467 - memory_profile7_log - INFO -     79                                     # data = []

2018-04-27 18:31:47,469 - memory_profile7_log - INFO -     80    559.6 MiB     88.8 MiB           for row in rows:

2018-04-27 18:31:47,470 - memory_profile7_log - INFO -     81                                         # data.append(list(row))

2018-04-27 18:31:47,471 - memory_profile7_log - INFO -     82    559.6 MiB    -70.9 MiB               yield list(row)

2018-04-27 18:31:47,473 - memory_profile7_log - INFO -     83                             

2018-04-27 18:31:47,473 - memory_profile7_log - INFO -     84                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-27 18:31:47,474 - memory_profile7_log - INFO -     85    493.7 MiB    -65.9 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-27 18:31:47,477 - memory_profile7_log - INFO -     86                                 # del rows

2018-04-27 18:31:47,479 - memory_profile7_log - INFO -     87    493.7 MiB      0.0 MiB       del result

2018-04-27 18:31:47,480 - memory_profile7_log - INFO -     88                                 # del data

2018-04-27 18:31:47,480 - memory_profile7_log - INFO -     89                              

2018-04-27 18:31:47,482 - memory_profile7_log - INFO -     90    493.7 MiB      0.0 MiB       return df

2018-04-27 18:31:47,483 - memory_profile7_log - INFO - 


2018-04-27 18:31:47,483 - memory_profile7_log - INFO - getting total: 361145 training data(genuine interest) for date: 2018-04-13
2018-04-27 18:31:47,484 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-14
2018-04-27 18:33:36,997 - memory_profile7_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 18:33:37,000 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 18:33:37,002 - memory_profile7_log - INFO - ================================================

2018-04-27 18:33:37,003 - memory_profile7_log - INFO -     67    493.7 MiB    493.7 MiB   @profile

2018-04-27 18:33:37,005 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-27 18:33:37,006 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-27 18:33:37,007 - memory_profile7_log - INFO -     70    493.7 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 18:33:37,010 - memory_profile7_log - INFO -     71    493.7 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 18:33:37,012 - memory_profile7_log - INFO -     72                             

2018-04-27 18:33:37,013 - memory_profile7_log - INFO -     73    493.7 MiB      0.0 MiB       result  = client.query(query, job_config=job_config)

2018-04-27 18:33:37,015 - memory_profile7_log - INFO -     74    493.7 MiB      0.0 MiB       rows = result.result()

2018-04-27 18:33:37,016 - memory_profile7_log - INFO -     75                             

2018-04-27 18:33:37,016 - memory_profile7_log - INFO -     76    493.7 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-27 18:33:37,020 - memory_profile7_log - INFO -     77                                 

2018-04-27 18:33:37,022 - memory_profile7_log - INFO -     78    493.7 MiB      0.0 MiB       def _q_iterator():

2018-04-27 18:33:37,023 - memory_profile7_log - INFO -     79                                     # data = []

2018-04-27 18:33:37,025 - memory_profile7_log - INFO -     80    619.1 MiB     65.6 MiB           for row in rows:

2018-04-27 18:33:37,026 - memory_profile7_log - INFO -     81                                         # data.append(list(row))

2018-04-27 18:33:37,026 - memory_profile7_log - INFO -     82    619.1 MiB    -58.4 MiB               yield list(row)

2018-04-27 18:33:37,030 - memory_profile7_log - INFO -     83                             

2018-04-27 18:33:37,032 - memory_profile7_log - INFO -     84                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-27 18:33:37,032 - memory_profile7_log - INFO -     85    564.6 MiB    -54.5 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-27 18:33:37,035 - memory_profile7_log - INFO -     86                                 # del rows

2018-04-27 18:33:37,036 - memory_profile7_log - INFO -     87    564.6 MiB      0.0 MiB       del result

2018-04-27 18:33:37,038 - memory_profile7_log - INFO -     88                                 # del data

2018-04-27 18:33:37,040 - memory_profile7_log - INFO -     89                              

2018-04-27 18:33:37,042 - memory_profile7_log - INFO -     90    564.6 MiB      0.0 MiB       return df

2018-04-27 18:33:37,043 - memory_profile7_log - INFO - 


2018-04-27 18:33:37,045 - memory_profile7_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-27 18:33:37,046 - memory_profile7_log - INFO - len datalist: 3
2018-04-27 18:33:37,048 - memory_profile7_log - INFO - All data fetch iterative done!!
2018-04-27 18:33:37,051 - memory_profile7_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 18:33:37,052 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 18:33:37,052 - memory_profile7_log - INFO - ================================================

2018-04-27 18:33:37,053 - memory_profile7_log - INFO -    236     86.8 MiB     86.8 MiB   @profile

2018-04-27 18:33:37,055 - memory_profile7_log - INFO -    237                             def BQPreprocess(loadmp, cpu, date_generated, client):

2018-04-27 18:33:37,055 - memory_profile7_log - INFO -    238     86.8 MiB      0.0 MiB       bq_client = client

2018-04-27 18:33:37,056 - memory_profile7_log - INFO -    239     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-27 18:33:37,056 - memory_profile7_log - INFO -    240                             

2018-04-27 18:33:37,058 - memory_profile7_log - INFO -    241     86.8 MiB      0.0 MiB       datalist = []

2018-04-27 18:33:37,061 - memory_profile7_log - INFO -    242    493.7 MiB      0.0 MiB       def _getBig(procdate, client):

2018-04-27 18:33:37,062 - memory_profile7_log - INFO -    243    493.7 MiB      0.0 MiB           logger.info("Collecting training data for date: %s", procdate)

2018-04-27 18:33:37,062 - memory_profile7_log - INFO -    244                                     # ~ get genuine news interest ~

2018-04-27 18:33:37,063 - memory_profile7_log - INFO -    245    493.7 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-27 18:33:37,065 - memory_profile7_log - INFO -    246                             

2018-04-27 18:33:37,065 - memory_profile7_log - INFO -    247                                     # safe handling of query parameter

2018-04-27 18:33:37,066 - memory_profile7_log - INFO -    248                                     query_params = [

2018-04-27 18:33:37,068 - memory_profile7_log - INFO -    249    493.7 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', procdate)

2018-04-27 18:33:37,069 - memory_profile7_log - INFO -    250                                     ]

2018-04-27 18:33:37,072 - memory_profile7_log - INFO -    251                             

2018-04-27 18:33:37,072 - memory_profile7_log - INFO -    252    493.7 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-27 18:33:37,072 - memory_profile7_log - INFO -    253                                     # temp_df = load_bigquery(client, query_fit + query_fit_where, job_config)

2018-04-27 18:33:37,073 - memory_profile7_log - INFO -    254    564.6 MiB    477.8 MiB           temp_df = loadBQ(client, query_fit + query_fit_where, job_config)

2018-04-27 18:33:37,075 - memory_profile7_log - INFO -    255                             

2018-04-27 18:33:37,075 - memory_profile7_log - INFO -    256    564.6 MiB      0.0 MiB           if temp_df.empty:

2018-04-27 18:33:37,076 - memory_profile7_log - INFO -    257                                         logger.info("%s data is empty!", procdate)

2018-04-27 18:33:37,078 - memory_profile7_log - INFO -    258                                         return None

2018-04-27 18:33:37,078 - memory_profile7_log - INFO -    259                                     else:

2018-04-27 18:33:37,079 - memory_profile7_log - INFO -    260    564.6 MiB      0.0 MiB               logger.info("getting total: %d training data(genuine interest) for date: %s" % (len(temp_df), procdate))

2018-04-27 18:33:37,082 - memory_profile7_log - INFO -    261    564.6 MiB      0.0 MiB               if loadmp:

2018-04-27 18:33:37,084 - memory_profile7_log - INFO -    262                                             logger.info("Exiting: %s", mp.current_process().name)

2018-04-27 18:33:37,084 - memory_profile7_log - INFO -    263    564.6 MiB      0.0 MiB               return temp_df

2018-04-27 18:33:37,085 - memory_profile7_log - INFO -    264                             

2018-04-27 18:33:37,086 - memory_profile7_log - INFO -    265     86.8 MiB      0.0 MiB       if loadmp:

2018-04-27 18:33:37,088 - memory_profile7_log - INFO -    266                                     logger.info("Starting data fetch multiprocess..")

2018-04-27 18:33:37,088 - memory_profile7_log - INFO -    267                                     logger.info("number of process: %d", len(date_generated))

2018-04-27 18:33:37,089 - memory_profile7_log - INFO -    268                             

2018-04-27 18:33:37,092 - memory_profile7_log - INFO -    269                                     pool = mp.Pool(processes=cpu)

2018-04-27 18:33:37,092 - memory_profile7_log - INFO -    270                                     # multprocessA = [pool.apply(_getBig, args=(ndate.strftime("%Y-%m-%d"), )) for ndate in date_generated]

2018-04-27 18:33:37,094 - memory_profile7_log - INFO -    271                                     multprocessA = [pool.apply_async(_getBig, args=(ndate.strftime("%Y-%m-%d"), bq_client, )) for ndate in date_generated]

2018-04-27 18:33:37,095 - memory_profile7_log - INFO -    272                                     output_multprocessA = [p.get() for p in multprocessA]

2018-04-27 18:33:37,096 - memory_profile7_log - INFO -    273                             

2018-04-27 18:33:37,098 - memory_profile7_log - INFO -    274                                     for m in output_multprocessA:

2018-04-27 18:33:37,099 - memory_profile7_log - INFO -    275                                         if m is not None:

2018-04-27 18:33:37,101 - memory_profile7_log - INFO -    276                                             if not m.empty:

2018-04-27 18:33:37,104 - memory_profile7_log - INFO -    277                                                 datalist.append(m)

2018-04-27 18:33:37,105 - memory_profile7_log - INFO -    278                             

2018-04-27 18:33:37,105 - memory_profile7_log - INFO -    279                                     logger.info("len datalist: %d", len(datalist))

2018-04-27 18:33:37,107 - memory_profile7_log - INFO -    280                                     logger.info("All data fetch multiprocess done!!")

2018-04-27 18:33:37,108 - memory_profile7_log - INFO -    281                                 else:

2018-04-27 18:33:37,108 - memory_profile7_log - INFO -    282     86.8 MiB      0.0 MiB           logger.info("Starting data fetch iterative...")

2018-04-27 18:33:37,109 - memory_profile7_log - INFO -    283    564.6 MiB      0.0 MiB           for ndate in date_generated:

2018-04-27 18:33:37,111 - memory_profile7_log - INFO -    284    564.6 MiB      0.0 MiB               tframe = _getBig(ndate.strftime("%Y-%m-%d"), bq_client)

2018-04-27 18:33:37,111 - memory_profile7_log - INFO -    285    564.6 MiB      0.0 MiB               if tframe is not None:

2018-04-27 18:33:37,115 - memory_profile7_log - INFO -    286    564.6 MiB      0.0 MiB                   if not tframe.empty:

2018-04-27 18:33:37,115 - memory_profile7_log - INFO -    287    564.6 MiB      0.0 MiB                       datalist.append(tframe)

2018-04-27 18:33:37,117 - memory_profile7_log - INFO -    288                                         else: 

2018-04-27 18:33:37,118 - memory_profile7_log - INFO -    289                                             logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-27 18:33:37,118 - memory_profile7_log - INFO -    290    564.6 MiB      0.0 MiB           logger.info("len datalist: %d", len(datalist))

2018-04-27 18:33:37,119 - memory_profile7_log - INFO -    291    564.6 MiB      0.0 MiB           logger.info("All data fetch iterative done!!")

2018-04-27 18:33:37,121 - memory_profile7_log - INFO -    292                             

2018-04-27 18:33:37,121 - memory_profile7_log - INFO -    293    564.6 MiB      0.0 MiB       return datalist

2018-04-27 18:33:37,124 - memory_profile7_log - INFO - 


2018-04-27 18:33:37,243 - memory_profile7_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-27 18:34:59,010 - memory_profile7_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 18:34:59,013 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 18:34:59,013 - memory_profile7_log - INFO - ================================================

2018-04-27 18:34:59,015 - memory_profile7_log - INFO -     67    573.7 MiB    573.7 MiB   @profile

2018-04-27 18:34:59,016 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-27 18:34:59,016 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-27 18:34:59,016 - memory_profile7_log - INFO -     70    573.7 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 18:34:59,017 - memory_profile7_log - INFO -     71    573.7 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 18:34:59,019 - memory_profile7_log - INFO -     72                             

2018-04-27 18:34:59,019 - memory_profile7_log - INFO -     73    573.5 MiB     -0.1 MiB       result  = client.query(query, job_config=job_config)

2018-04-27 18:34:59,023 - memory_profile7_log - INFO -     74    573.5 MiB      0.0 MiB       rows = result.result()

2018-04-27 18:34:59,025 - memory_profile7_log - INFO -     75                             

2018-04-27 18:34:59,026 - memory_profile7_log - INFO -     76    573.5 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-27 18:34:59,026 - memory_profile7_log - INFO -     77                                 

2018-04-27 18:34:59,026 - memory_profile7_log - INFO -     78    573.5 MiB      0.0 MiB       def _q_iterator():

2018-04-27 18:34:59,028 - memory_profile7_log - INFO -     79                                     # data = []

2018-04-27 18:34:59,029 - memory_profile7_log - INFO -     80    695.8 MiB     80.3 MiB           for row in rows:

2018-04-27 18:34:59,029 - memory_profile7_log - INFO -     81                                         # data.append(list(row))

2018-04-27 18:34:59,029 - memory_profile7_log - INFO -     82    695.8 MiB    -23.1 MiB               yield list(row)

2018-04-27 18:34:59,030 - memory_profile7_log - INFO -     83                             

2018-04-27 18:34:59,033 - memory_profile7_log - INFO -     84                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-27 18:34:59,035 - memory_profile7_log - INFO -     85    640.8 MiB    -55.0 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-27 18:34:59,036 - memory_profile7_log - INFO -     86                                 # del rows

2018-04-27 18:34:59,036 - memory_profile7_log - INFO -     87    640.8 MiB      0.0 MiB       del result

2018-04-27 18:34:59,036 - memory_profile7_log - INFO -     88                                 # del data

2018-04-27 18:34:59,038 - memory_profile7_log - INFO -     89                              

2018-04-27 18:34:59,039 - memory_profile7_log - INFO -     90    640.8 MiB      0.0 MiB       return df

2018-04-27 18:34:59,039 - memory_profile7_log - INFO - 


2018-04-27 18:34:59,039 - memory_profile7_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-27 18:34:59,048 - memory_profile7_log - INFO - loading time of: 1308144 total genuine-current interest data ~ take 541.749s
2018-04-27 18:34:59,049 - memory_profile7_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 18:34:59,049 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 18:34:59,051 - memory_profile7_log - INFO - ================================================

2018-04-27 18:34:59,052 - memory_profile7_log - INFO -    295     86.7 MiB     86.7 MiB   @profile

2018-04-27 18:34:59,055 - memory_profile7_log - INFO -    296                             def preprocess(loadmp, cpu, cd, query_fit, date_generated):

2018-04-27 18:34:59,055 - memory_profile7_log - INFO -    297     86.8 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-27 18:34:59,055 - memory_profile7_log - INFO -    298     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-27 18:34:59,056 - memory_profile7_log - INFO -    299                             

2018-04-27 18:34:59,056 - memory_profile7_log - INFO -    300                                 # ~~~ Begin collecting data ~~~

2018-04-27 18:34:59,058 - memory_profile7_log - INFO -    301     86.8 MiB      0.0 MiB       t0 = time.time()

2018-04-27 18:34:59,059 - memory_profile7_log - INFO -    302                                 

2018-04-27 18:34:59,059 - memory_profile7_log - INFO -    303    564.6 MiB    477.8 MiB       datalist = BQPreprocess(loadmp, cpu, date_generated, bq_client)

2018-04-27 18:34:59,059 - memory_profile7_log - INFO -    304                             

2018-04-27 18:34:59,061 - memory_profile7_log - INFO -    305    564.6 MiB      0.0 MiB       if len(datalist) > 1:

2018-04-27 18:34:59,061 - memory_profile7_log - INFO -    306    615.1 MiB     50.5 MiB           big_frame = pd.concat(datalist)

2018-04-27 18:34:59,062 - memory_profile7_log - INFO -    307    573.6 MiB    -41.5 MiB           del datalist

2018-04-27 18:34:59,062 - memory_profile7_log - INFO -    308                                 else:

2018-04-27 18:34:59,062 - memory_profile7_log - INFO -    309                                     big_frame = datalist

2018-04-27 18:34:59,062 - memory_profile7_log - INFO -    310    573.7 MiB      0.1 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-27 18:34:59,062 - memory_profile7_log - INFO -    311                             

2018-04-27 18:34:59,065 - memory_profile7_log - INFO -    312                                 # ~ get current news interest ~

2018-04-27 18:34:59,065 - memory_profile7_log - INFO -    313    573.7 MiB      0.0 MiB       if not cd:

2018-04-27 18:34:59,066 - memory_profile7_log - INFO -    314                                     logger.info("Collecting training data(current date interest)..")

2018-04-27 18:34:59,068 - memory_profile7_log - INFO -    315                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-27 18:34:59,069 - memory_profile7_log - INFO -    316                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-27 18:34:59,069 - memory_profile7_log - INFO -    317                                 else:

2018-04-27 18:34:59,069 - memory_profile7_log - INFO -    318    573.7 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-27 18:34:59,069 - memory_profile7_log - INFO -    319    573.7 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-27 18:34:59,069 - memory_profile7_log - INFO -    320                             

2018-04-27 18:34:59,071 - memory_profile7_log - INFO -    321                                     # safe handling of query parameter

2018-04-27 18:34:59,071 - memory_profile7_log - INFO -    322                                     query_params = [

2018-04-27 18:34:59,071 - memory_profile7_log - INFO -    323    573.7 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-27 18:34:59,071 - memory_profile7_log - INFO -    324                                     ]

2018-04-27 18:34:59,071 - memory_profile7_log - INFO -    325                             

2018-04-27 18:34:59,072 - memory_profile7_log - INFO -    326    573.7 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-27 18:34:59,072 - memory_profile7_log - INFO -    327    640.8 MiB     67.1 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-27 18:34:59,072 - memory_profile7_log - INFO -    328    640.8 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-27 18:34:59,072 - memory_profile7_log - INFO -    329                             

2018-04-27 18:34:59,072 - memory_profile7_log - INFO -    330    640.9 MiB      0.1 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-27 18:34:59,072 - memory_profile7_log - INFO -    331    640.9 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-27 18:34:59,072 - memory_profile7_log - INFO -    332    640.9 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-27 18:34:59,072 - memory_profile7_log - INFO -    333                             

2018-04-27 18:34:59,072 - memory_profile7_log - INFO -    334    640.9 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-27 18:34:59,073 - memory_profile7_log - INFO -    335    640.9 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-27 18:34:59,073 - memory_profile7_log - INFO -    336                             

2018-04-27 18:34:59,073 - memory_profile7_log - INFO -    337    640.9 MiB      0.0 MiB       return big_frame, current_frame

2018-04-27 18:34:59,078 - memory_profile7_log - INFO - 


2018-04-27 18:34:59,082 - memory_profile7_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-27 18:34:59,160 - memory_profile7_log - INFO - train on: 1077719 total history data(D(u, t))
2018-04-27 18:34:59,161 - memory_profile7_log - INFO - transform on: 230425 total current data(D(t))
2018-04-27 18:35:04,563 - memory_profile7_log - INFO - Len of model_fit: 1077719
2018-04-27 18:35:04,565 - memory_profile7_log - INFO - Len of df_dut: 1077719
2018-04-27 18:36:46,750 - memory_profile7_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-27 18:36:46,884 - memory_profile7_log - INFO - Total train time: 107.725s
2018-04-27 18:36:46,887 - memory_profile7_log - INFO - memory left before cleaning: 86.800 percent memory...
2018-04-27 18:36:46,888 - memory_profile7_log - INFO - cleaning up some objects...
2018-04-27 18:36:46,888 - memory_profile7_log - INFO - deleting df_dut...
2018-04-27 18:36:46,891 - memory_profile7_log - INFO - deleting df_dt...
2018-04-27 18:36:46,891 - memory_profile7_log - INFO - deleting df_input...
2018-04-27 18:36:46,908 - memory_profile7_log - INFO - deleting df_input_X...
2018-04-27 18:36:46,911 - memory_profile7_log - INFO - deleting df_current...
2018-04-27 18:36:46,913 - memory_profile7_log - INFO - deleting map_topic_isgeneral...
2018-04-27 18:36:46,994 - memory_profile7_log - INFO - deleting model_fit...
2018-04-27 18:36:46,996 - memory_profile7_log - INFO - deleting result...
2018-04-27 18:36:47,052 - memory_profile7_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 18:36:47,053 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 18:36:47,055 - memory_profile7_log - INFO - ================================================

2018-04-27 18:36:47,055 - memory_profile7_log - INFO -    102    640.9 MiB    640.9 MiB   @profile

2018-04-27 18:36:47,055 - memory_profile7_log - INFO -    103                             def main(df_input, df_current, current_date, G,

2018-04-27 18:36:47,055 - memory_profile7_log - INFO -    104                                      project_id, savetrain=False, multproc=True,

2018-04-27 18:36:47,056 - memory_profile7_log - INFO -    105                                      threshold=0, start_date=None, end_date=None,

2018-04-27 18:36:47,058 - memory_profile7_log - INFO -    106                                      saveto="datastore"):

2018-04-27 18:36:47,059 - memory_profile7_log - INFO -    107                                 """

2018-04-27 18:36:47,059 - memory_profile7_log - INFO -    108                                     Main cron method

2018-04-27 18:36:47,059 - memory_profile7_log - INFO -    109                                 """

2018-04-27 18:36:47,059 - memory_profile7_log - INFO -    110                                 # ~ Data Preprocessing ~

2018-04-27 18:36:47,061 - memory_profile7_log - INFO -    111                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-27 18:36:47,062 - memory_profile7_log - INFO -    112                                 # D(u, t)

2018-04-27 18:36:47,062 - memory_profile7_log - INFO -    113    640.9 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-27 18:36:47,069 - memory_profile7_log - INFO -    114    674.8 MiB     33.9 MiB       df_dut = df_input.copy(deep=True)

2018-04-27 18:36:47,069 - memory_profile7_log - INFO -    115    674.8 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-27 18:36:47,069 - memory_profile7_log - INFO -    116                             

2018-04-27 18:36:47,069 - memory_profile7_log - INFO -    117                                 # D(t)

2018-04-27 18:36:47,069 - memory_profile7_log - INFO -    118    682.1 MiB      7.3 MiB       df_dt = df_current.copy(deep=True)

2018-04-27 18:36:47,071 - memory_profile7_log - INFO -    119    682.1 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-27 18:36:47,071 - memory_profile7_log - INFO -    120                             

2018-04-27 18:36:47,072 - memory_profile7_log - INFO -    121                                 # ~~~~~~ Begin train ~~~~~~

2018-04-27 18:36:47,072 - memory_profile7_log - INFO -    122    682.1 MiB      0.0 MiB       t0 = time.time()

2018-04-27 18:36:47,072 - memory_profile7_log - INFO -    123    682.1 MiB      0.0 MiB       logger.info("train on: %d total history data(D(u, t))", len(df_dut))

2018-04-27 18:36:47,073 - memory_profile7_log - INFO -    124    682.1 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-27 18:36:47,073 - memory_profile7_log - INFO -    125                             

2018-04-27 18:36:47,076 - memory_profile7_log - INFO -    126                                 # instantiace class

2018-04-27 18:36:47,082 - memory_profile7_log - INFO -    127    682.1 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-27 18:36:47,084 - memory_profile7_log - INFO -    128                             

2018-04-27 18:36:47,084 - memory_profile7_log - INFO -    129                                 # ~~ Fit ~~

2018-04-27 18:36:47,084 - memory_profile7_log - INFO -    130                                 #   handling genuine news interest < current date

2018-04-27 18:36:47,085 - memory_profile7_log - INFO -    131    686.2 MiB      4.2 MiB       NB = BR.processX(df_dut)

2018-04-27 18:36:47,085 - memory_profile7_log - INFO -    132                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-27 18:36:47,085 - memory_profile7_log - INFO -    133                                 #   nanti dipindah ke class train utama

2018-04-27 18:36:47,085 - memory_profile7_log - INFO -    134    729.2 MiB     43.0 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-27 18:36:47,085 - memory_profile7_log - INFO -    135                                 """

2018-04-27 18:36:47,086 - memory_profile7_log - INFO -    136                                     num_y = total global click for category=ci on periode t

2018-04-27 18:36:47,088 - memory_profile7_log - INFO -    137                                     num_x = total click from user_U for category=ci on periode t

2018-04-27 18:36:47,089 - memory_profile7_log - INFO -    138                                 """

2018-04-27 18:36:47,094 - memory_profile7_log - INFO -    139    729.2 MiB      0.0 MiB       fitby_sigmant = True

2018-04-27 18:36:47,095 - memory_profile7_log - INFO -    140    729.2 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-27 18:36:47,095 - memory_profile7_log - INFO -    141    729.2 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-27 18:36:47,095 - memory_profile7_log - INFO -    142    771.4 MiB     42.1 MiB                            'is_general']]

2018-04-27 18:36:47,095 - memory_profile7_log - INFO -    143    771.4 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-27 18:36:47,095 - memory_profile7_log - INFO -    144    771.4 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-27 18:36:47,095 - memory_profile7_log - INFO -    145    877.9 MiB    106.5 MiB                          verbose=False)

2018-04-27 18:36:47,096 - memory_profile7_log - INFO -    146    877.9 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-27 18:36:47,096 - memory_profile7_log - INFO -    147    877.9 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-27 18:36:47,098 - memory_profile7_log - INFO -    148                             

2018-04-27 18:36:47,098 - memory_profile7_log - INFO -    149                                 # ~~ and Transform ~~

2018-04-27 18:36:47,099 - memory_profile7_log - INFO -    150                                 #   handling current news interest == current date

2018-04-27 18:36:47,099 - memory_profile7_log - INFO -    151    877.9 MiB      0.0 MiB       if df_dt.empty:

2018-04-27 18:36:47,101 - memory_profile7_log - INFO -    152                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-27 18:36:47,101 - memory_profile7_log - INFO -    153                                     return None

2018-04-27 18:36:47,107 - memory_profile7_log - INFO -    154    877.2 MiB     -0.8 MiB       NB = BR.processX(df_dt)

2018-04-27 18:36:47,107 - memory_profile7_log - INFO -    155    887.8 MiB     10.6 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-27 18:36:47,108 - memory_profile7_log - INFO -    156                             

2018-04-27 18:36:47,108 - memory_profile7_log - INFO -    157    887.8 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-27 18:36:47,108 - memory_profile7_log - INFO -    158    854.4 MiB    -33.3 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-27 18:36:47,109 - memory_profile7_log - INFO -    159    854.4 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-27 18:36:47,109 - memory_profile7_log - INFO -    160    854.4 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-27 18:36:47,111 - memory_profile7_log - INFO -    161    918.0 MiB     63.5 MiB                                                     verbose=False)

2018-04-27 18:36:47,111 - memory_profile7_log - INFO -    162                                 # ~~~ filter is general and specific topic ~~~

2018-04-27 18:36:47,111 - memory_profile7_log - INFO -    163                                 # the idea is just we need to rerank every topic according

2018-04-27 18:36:47,111 - memory_profile7_log - INFO -    164                                 # user_id and and is_general by p0_posterior

2018-04-27 18:36:47,111 - memory_profile7_log - INFO -    165    918.0 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-27 18:36:47,111 - memory_profile7_log - INFO -    166    927.2 MiB      9.3 MiB                                     'is_general']].groupby(['topic_id',

2018-04-27 18:36:47,112 - memory_profile7_log - INFO -    167    918.0 MiB     -9.2 MiB                                                             'is_general']

2018-04-27 18:36:47,112 - memory_profile7_log - INFO -    168                                                                                      ).size().to_frame().reset_index()

2018-04-27 18:36:47,112 - memory_profile7_log - INFO -    169                             

2018-04-27 18:36:47,112 - memory_profile7_log - INFO -    170                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-27 18:36:47,114 - memory_profile7_log - INFO -    171                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-27 18:36:47,114 - memory_profile7_log - INFO -    172    918.1 MiB      0.2 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-27 18:36:47,114 - memory_profile7_log - INFO -    173                             

2018-04-27 18:36:47,118 - memory_profile7_log - INFO -    174                                 # ~ start by provide rank for each topic type ~

2018-04-27 18:36:47,119 - memory_profile7_log - INFO -    175   1008.8 MiB     90.7 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-27 18:36:47,119 - memory_profile7_log - INFO -    176   1016.3 MiB      7.5 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-27 18:36:47,119 - memory_profile7_log - INFO -    177                             

2018-04-27 18:36:47,119 - memory_profile7_log - INFO -    178                                 # ~ set threshold to filter output

2018-04-27 18:36:47,121 - memory_profile7_log - INFO -    179   1016.3 MiB      0.0 MiB       if threshold > 0:

2018-04-27 18:36:47,121 - memory_profile7_log - INFO -    180   1016.3 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-27 18:36:47,121 - memory_profile7_log - INFO -    181   1016.3 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-27 18:36:47,121 - memory_profile7_log - INFO -    182   1004.3 MiB    -12.0 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-27 18:36:47,121 - memory_profile7_log - INFO -    183                             

2018-04-27 18:36:47,121 - memory_profile7_log - INFO -    184                                 # print "\n", model_transform[['user_id', 'topic_id', 'p0_posterior',

2018-04-27 18:36:47,121 - memory_profile7_log - INFO -    185                                 #                             'is_general', 'rank']].loc[model_transform['user_id'].str.contains('1610c8e6b6466b-05a6ac1ba8c1b8-4323461-100200-1',

2018-04-27 18:36:47,122 - memory_profile7_log - INFO -    186                                 #                                                                                                 case=False)].head(45)

2018-04-27 18:36:47,122 - memory_profile7_log - INFO -    187                                 # print "len(model_transform): %d" % len(model_transform)

2018-04-27 18:36:47,122 - memory_profile7_log - INFO -    188                             

2018-04-27 18:36:47,122 - memory_profile7_log - INFO -    189   1004.3 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-27 18:36:47,124 - memory_profile7_log - INFO -    190   1004.3 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-27 18:36:47,124 - memory_profile7_log - INFO -    191                             

2018-04-27 18:36:47,124 - memory_profile7_log - INFO -    192   1004.3 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-27 18:36:47,125 - memory_profile7_log - INFO -    193                             

2018-04-27 18:36:47,125 - memory_profile7_log - INFO -    194   1004.3 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-27 18:36:47,125 - memory_profile7_log - INFO -    195   1004.3 MiB      0.0 MiB       del df_dut

2018-04-27 18:36:47,125 - memory_profile7_log - INFO -    196   1004.3 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-27 18:36:47,125 - memory_profile7_log - INFO -    197   1004.3 MiB      0.0 MiB       del df_dt

2018-04-27 18:36:47,125 - memory_profile7_log - INFO -    198   1004.3 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-27 18:36:47,125 - memory_profile7_log - INFO -    199   1004.3 MiB      0.0 MiB       del df_input

2018-04-27 18:36:47,125 - memory_profile7_log - INFO -    200   1004.3 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-27 18:36:47,127 - memory_profile7_log - INFO -    201    995.5 MiB     -8.8 MiB       del df_input_X

2018-04-27 18:36:47,127 - memory_profile7_log - INFO -    202    995.5 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-27 18:36:47,127 - memory_profile7_log - INFO -    203    995.5 MiB      0.0 MiB       del df_current

2018-04-27 18:36:47,132 - memory_profile7_log - INFO -    204    995.5 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-27 18:36:47,134 - memory_profile7_log - INFO -    205    995.5 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-27 18:36:47,134 - memory_profile7_log - INFO -    206    995.5 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-27 18:36:47,134 - memory_profile7_log - INFO -    207    895.8 MiB    -99.7 MiB       del model_fit

2018-04-27 18:36:47,134 - memory_profile7_log - INFO -    208    895.8 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-27 18:36:47,134 - memory_profile7_log - INFO -    209    895.8 MiB      0.0 MiB       del result

2018-04-27 18:36:47,134 - memory_profile7_log - INFO -    210    895.8 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-27 18:36:47,134 - memory_profile7_log - INFO -    211                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-27 18:36:47,134 - memory_profile7_log - INFO -    212                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-27 18:36:47,135 - memory_profile7_log - INFO -    213    895.8 MiB      0.0 MiB       if savetrain:

2018-04-27 18:36:47,135 - memory_profile7_log - INFO -    214                                     model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-27 18:36:47,135 - memory_profile7_log - INFO -    215                                     del model_transform

2018-04-27 18:36:47,135 - memory_profile7_log - INFO -    216                                     logger.info("deleting model_transform...")

2018-04-27 18:36:47,137 - memory_profile7_log - INFO -    217                                     logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-27 18:36:47,137 - memory_profile7_log - INFO -    218                             

2018-04-27 18:36:47,137 - memory_profile7_log - INFO -    219                                     logger.info("Begin saving trained data...")

2018-04-27 18:36:47,137 - memory_profile7_log - INFO -    220                                     # print "\n", model_transform.head(5)

2018-04-27 18:36:47,138 - memory_profile7_log - INFO -    221                                     # ~ Place your code to save the training model here ~

2018-04-27 18:36:47,138 - memory_profile7_log - INFO -    222                                     if str(saveto).lower() == "datastore":

2018-04-27 18:36:47,138 - memory_profile7_log - INFO -    223                                         logger.info("Using google datastore as storage...")

2018-04-27 18:36:47,138 - memory_profile7_log - INFO -    224                                         if multproc:

2018-04-27 18:36:47,138 - memory_profile7_log - INFO -    225                                             mh.saveDatastoreMP(model_transformsv)

2018-04-27 18:36:47,138 - memory_profile7_log - INFO -    226                                             # mh.saveDataStorePutMulti(model_transform[['user_id', 'topic_id', 'is_general', 'rank']])

2018-04-27 18:36:47,138 - memory_profile7_log - INFO -    227                                         else:

2018-04-27 18:36:47,138 - memory_profile7_log - INFO -    228                                             mh.saveDatastore(model_transformsv)

2018-04-27 18:36:47,140 - memory_profile7_log - INFO -    229                                             

2018-04-27 18:36:47,140 - memory_profile7_log - INFO -    230                                     elif str(saveto).lower() == "elastic":

2018-04-27 18:36:47,140 - memory_profile7_log - INFO -    231                                         logger.info("Using ElasticSearch as storage...")

2018-04-27 18:36:47,140 - memory_profile7_log - INFO -    232                                         mh.saveElasticS(model_transformsv)

2018-04-27 18:36:47,141 - memory_profile7_log - INFO -    233                             

2018-04-27 18:36:47,141 - memory_profile7_log - INFO -    234    895.8 MiB      0.0 MiB       return

2018-04-27 18:36:47,141 - memory_profile7_log - INFO - 


2018-04-27 18:36:47,141 - memory_profile7_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-27 18:52:47,763 - memory_profile7_log - INFO - Generating date range with N: 3
2018-04-27 18:52:47,765 - memory_profile7_log - INFO - date_generated: 
2018-04-27 18:52:47,765 - memory_profile7_log - INFO -  
2018-04-27 18:52:47,766 - memory_profile7_log - INFO - [datetime.datetime(2018, 4, 12, 0, 0), datetime.datetime(2018, 4, 13, 0, 0), datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-27 18:52:47,766 - memory_profile7_log - INFO - 

2018-04-27 18:52:47,766 - memory_profile7_log - INFO - using current date: 2018-04-15
2018-04-27 18:52:47,767 - memory_profile7_log - INFO - using start date: 2018-04-12 00:00:00
2018-04-27 18:52:47,767 - memory_profile7_log - INFO - using end date: 2018-04-14
2018-04-27 18:52:47,905 - memory_profile7_log - INFO - Starting data fetch iterative...
2018-04-27 18:52:47,907 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-12
2018-04-27 18:55:56,536 - memory_profile7_log - INFO - <class 'pandas.core.frame.DataFrame'>
RangeIndex: 462579 entries, 0 to 462578
Data columns (total 5 columns):
date          462579 non-null datetime64[ns, UTC]
user_id       462579 non-null object
topic_id      462579 non-null object
is_general    462579 non-null bool
num           462579 non-null int64
dtypes: bool(1), datetime64[ns, UTC](1), int64(1), object(2)
memory usage: 117.0 MB

2018-04-27 18:55:56,538 - memory_profile7_log - INFO - None
2018-04-27 18:55:56,539 - memory_profile7_log - INFO - 

2018-04-27 18:55:56,542 - memory_profile7_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 18:55:56,542 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 18:55:56,545 - memory_profile7_log - INFO - ================================================

2018-04-27 18:55:56,546 - memory_profile7_log - INFO -     67     86.8 MiB     86.8 MiB   @profile

2018-04-27 18:55:56,549 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-27 18:55:56,549 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-27 18:55:56,552 - memory_profile7_log - INFO -     70     86.8 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 18:55:56,552 - memory_profile7_log - INFO -     71     86.8 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 18:55:56,555 - memory_profile7_log - INFO -     72                             

2018-04-27 18:55:56,555 - memory_profile7_log - INFO -     73     89.9 MiB      3.1 MiB       result  = client.query(query, job_config=job_config)

2018-04-27 18:55:56,556 - memory_profile7_log - INFO -     74     89.9 MiB      0.1 MiB       rows = result.result()

2018-04-27 18:55:56,559 - memory_profile7_log - INFO -     75                             

2018-04-27 18:55:56,561 - memory_profile7_log - INFO -     76     89.9 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-27 18:55:56,562 - memory_profile7_log - INFO -     77                                 

2018-04-27 18:55:56,563 - memory_profile7_log - INFO -     78     89.9 MiB      0.0 MiB       def _q_iterator():

2018-04-27 18:55:56,565 - memory_profile7_log - INFO -     79                                     # data = []

2018-04-27 18:55:56,566 - memory_profile7_log - INFO -     80    473.2 MiB    295.8 MiB           for row in rows:

2018-04-27 18:55:56,569 - memory_profile7_log - INFO -     81                                         # data.append(list(row))

2018-04-27 18:55:56,571 - memory_profile7_log - INFO -     82    473.2 MiB    -69.8 MiB               yield list(row)

2018-04-27 18:55:56,572 - memory_profile7_log - INFO -     83                             

2018-04-27 18:55:56,573 - memory_profile7_log - INFO -     84                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-27 18:55:56,575 - memory_profile7_log - INFO -     85    393.1 MiB    -80.1 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-27 18:55:56,575 - memory_profile7_log - INFO -     86                                 # del rows

2018-04-27 18:55:56,579 - memory_profile7_log - INFO -     87    393.1 MiB      0.0 MiB       del result

2018-04-27 18:55:56,581 - memory_profile7_log - INFO -     88                                 # del data

2018-04-27 18:55:56,582 - memory_profile7_log - INFO -     89    395.2 MiB      2.1 MiB       print df.info(memory_usage='deep')

2018-04-27 18:55:56,584 - memory_profile7_log - INFO -     90                                 

2018-04-27 18:55:56,585 - memory_profile7_log - INFO -     91    395.2 MiB      0.0 MiB       return df

2018-04-27 18:55:56,585 - memory_profile7_log - INFO - 


2018-04-27 18:55:56,586 - memory_profile7_log - INFO - getting total: 462579 training data(genuine interest) for date: 2018-04-12
2018-04-27 18:55:56,588 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-13
2018-04-27 18:58:22,167 - memory_profile7_log - INFO - <class 'pandas.core.frame.DataFrame'>
RangeIndex: 361145 entries, 0 to 361144
Data columns (total 5 columns):
date          361145 non-null datetime64[ns, UTC]
user_id       361145 non-null object
topic_id      361145 non-null object
is_general    361145 non-null bool
num           361145 non-null int64
dtypes: bool(1), datetime64[ns, UTC](1), int64(1), object(2)
memory usage: 91.4 MB

2018-04-27 20:51:00,266 - memory_profile7_log - INFO - None
2018-04-27 20:51:00,270 - memory_profile7_log - INFO - 

2018-04-27 20:51:00,278 - memory_profile7_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 20:51:00,279 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 20:51:00,282 - memory_profile7_log - INFO - ================================================

2018-04-27 20:51:00,283 - memory_profile7_log - INFO -     67    395.2 MiB    395.2 MiB   @profile

2018-04-27 20:51:00,285 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-27 20:51:00,286 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-27 20:51:00,288 - memory_profile7_log - INFO -     70    395.2 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 20:51:00,289 - memory_profile7_log - INFO -     71    395.2 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 20:51:00,289 - memory_profile7_log - INFO -     72                             

2018-04-27 20:51:00,293 - memory_profile7_log - INFO -     73    395.2 MiB      0.0 MiB       result  = client.query(query, job_config=job_config)

2018-04-27 20:51:00,295 - memory_profile7_log - INFO -     74    395.2 MiB      0.0 MiB       rows = result.result()

2018-04-27 20:51:00,296 - memory_profile7_log - INFO -     75                             

2018-04-27 20:51:00,298 - memory_profile7_log - INFO -     76    395.2 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-27 20:51:00,299 - memory_profile7_log - INFO -     77                                 

2018-04-27 20:51:00,301 - memory_profile7_log - INFO -     78    395.2 MiB      0.0 MiB       def _q_iterator():

2018-04-27 20:51:00,303 - memory_profile7_log - INFO -     79                                     # data = []

2018-04-27 20:51:00,305 - memory_profile7_log - INFO -     80    558.5 MiB     93.1 MiB           for row in rows:

2018-04-27 20:51:00,306 - memory_profile7_log - INFO -     81                                         # data.append(list(row))

2018-04-27 20:51:00,308 - memory_profile7_log - INFO -     82    558.5 MiB    -58.4 MiB               yield list(row)

2018-04-27 20:51:00,309 - memory_profile7_log - INFO -     83                             

2018-04-27 20:51:00,311 - memory_profile7_log - INFO -     84                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-27 20:51:00,313 - memory_profile7_log - INFO -     85    493.2 MiB    -65.3 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-27 20:51:00,315 - memory_profile7_log - INFO -     86                                 # del rows

2018-04-27 20:51:00,316 - memory_profile7_log - INFO -     87    493.2 MiB      0.0 MiB       del result

2018-04-27 20:51:00,318 - memory_profile7_log - INFO -     88                                 # del data

2018-04-27 20:51:00,319 - memory_profile7_log - INFO -     89     22.1 MiB   -471.1 MiB       print df.info(memory_usage='deep')

2018-04-27 20:51:00,319 - memory_profile7_log - INFO -     90                                 

2018-04-27 20:51:00,322 - memory_profile7_log - INFO -     91     22.2 MiB      0.0 MiB       return df

2018-04-27 20:51:00,323 - memory_profile7_log - INFO - 


2018-04-27 20:51:00,328 - memory_profile7_log - INFO - getting total: 361145 training data(genuine interest) for date: 2018-04-13
2018-04-27 20:51:00,329 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-14
2018-04-28 20:28:41,599 - memory_profile7_log - INFO - Generating date range with N: 5
2018-04-28 20:28:41,615 - memory_profile7_log - INFO - date_generated: 
2018-04-28 20:28:41,615 - memory_profile7_log - INFO -  
2018-04-28 20:28:41,615 - memory_profile7_log - INFO - [datetime.datetime(2018, 4, 10, 0, 0), datetime.datetime(2018, 4, 11, 0, 0), datetime.datetime(2018, 4, 12, 0, 0), datetime.datetime(2018, 4, 13, 0, 0), datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-28 20:28:41,615 - memory_profile7_log - INFO - 

2018-04-28 20:28:41,631 - memory_profile7_log - INFO - using current date: 2018-04-15
2018-04-28 20:28:41,631 - memory_profile7_log - INFO - using start date: 2018-04-10 00:00:00
2018-04-28 20:28:41,631 - memory_profile7_log - INFO - using end date: 2018-04-14
2018-04-28 20:28:41,732 - memory_profile7_log - INFO - Starting data fetch multiprocess..
2018-04-28 20:28:41,732 - memory_profile7_log - INFO - number of process: 5
2018-04-28 20:28:43,822 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-10
2018-04-28 20:28:43,825 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-11
2018-04-28 20:28:43,825 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-12
2018-04-28 20:28:43,828 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-13
2018-04-28 20:28:43,828 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-14
2018-04-28 20:30:50,696 - memory_profile7_log - INFO - Generating date range with N: 5
2018-04-28 20:30:50,698 - memory_profile7_log - INFO - date_generated: 
2018-04-28 20:30:50,698 - memory_profile7_log - INFO -  
2018-04-28 20:30:50,700 - memory_profile7_log - INFO - [datetime.datetime(2018, 4, 10, 0, 0), datetime.datetime(2018, 4, 11, 0, 0), datetime.datetime(2018, 4, 12, 0, 0), datetime.datetime(2018, 4, 13, 0, 0), datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-28 20:30:50,700 - memory_profile7_log - INFO - 

2018-04-28 20:30:50,700 - memory_profile7_log - INFO - using current date: 2018-04-15
2018-04-28 20:30:50,701 - memory_profile7_log - INFO - using start date: 2018-04-10 00:00:00
2018-04-28 20:30:50,701 - memory_profile7_log - INFO - using end date: 2018-04-14
2018-04-28 20:30:50,821 - memory_profile7_log - INFO - Starting data fetch multiprocess..
2018-04-28 20:30:50,822 - memory_profile7_log - INFO - number of process: 5
2018-04-28 20:30:53,114 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-10
2018-04-28 20:30:53,114 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-11
2018-04-28 20:30:53,114 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-12
2018-04-28 20:30:53,134 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-13
2018-04-28 20:32:55,951 - memory_profile7_log - INFO - <class 'pandas.core.frame.DataFrame'>
RangeIndex: 361145 entries, 0 to 361144
Data columns (total 5 columns):
date          361145 non-null datetime64[ns, UTC]
user_id       361145 non-null object
topic_id      361145 non-null object
is_general    361145 non-null bool
num           361145 non-null int64
dtypes: bool(1), datetime64[ns, UTC](1), int64(1), object(2)
memory usage: 91.4 MB

2018-04-28 20:32:55,951 - memory_profile7_log - INFO - None
2018-04-28 20:32:55,953 - memory_profile7_log - INFO - 

2018-04-28 20:32:55,953 - memory_profile7_log - INFO - Filename: D:\Kerjaan\python\Recommender\topic-recommender\CRON\legacy-train\legacy_bigquery.py


2018-04-28 20:32:55,953 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-28 20:32:55,953 - memory_profile7_log - INFO - ================================================

2018-04-28 20:32:55,953 - memory_profile7_log - INFO -     67     86.8 MiB     86.8 MiB   @profile

2018-04-28 20:32:55,953 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-28 20:32:55,953 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-28 20:32:55,954 - memory_profile7_log - INFO -     70                                 # https://www.sjoerdlangkemper.nl/2016/06/09/clearing-memory-in-python/

2018-04-28 20:32:55,954 - memory_profile7_log - INFO -     71     86.8 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-28 20:32:55,954 - memory_profile7_log - INFO -     72     86.8 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-28 20:32:55,954 - memory_profile7_log - INFO -     73                             

2018-04-28 20:32:55,954 - memory_profile7_log - INFO -     74     90.0 MiB      3.2 MiB       result  = client.query(query, job_config=job_config)

2018-04-28 20:32:55,956 - memory_profile7_log - INFO -     75     90.1 MiB      0.1 MiB       rows = result.result()

2018-04-28 20:32:55,956 - memory_profile7_log - INFO -     76                             

2018-04-28 20:32:55,956 - memory_profile7_log - INFO -     77     90.1 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-28 20:32:55,956 - memory_profile7_log - INFO -     78                                 

2018-04-28 20:32:55,956 - memory_profile7_log - INFO -     79     90.1 MiB      0.0 MiB       def _q_iterator():

2018-04-28 20:32:55,957 - memory_profile7_log - INFO -     80                                     # data = []

2018-04-28 20:32:55,957 - memory_profile7_log - INFO -     81    430.2 MiB    236.0 MiB           for row in rows:

2018-04-28 20:32:55,957 - memory_profile7_log - INFO -     82                                         # data.append(list(row))

2018-04-28 20:32:55,959 - memory_profile7_log - INFO -     83    430.2 MiB   -125.9 MiB               yield list(row)

2018-04-28 20:32:55,960 - memory_profile7_log - INFO -     84                             

2018-04-28 20:32:55,960 - memory_profile7_log - INFO -     85                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-28 20:32:55,960 - memory_profile7_log - INFO -     86    366.0 MiB    -64.2 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-28 20:32:55,960 - memory_profile7_log - INFO -     87                                 # del rows

2018-04-28 20:32:55,961 - memory_profile7_log - INFO -     88    366.0 MiB      0.0 MiB       del result

2018-04-28 20:32:55,961 - memory_profile7_log - INFO -     89                                 # del data

2018-04-28 20:32:55,961 - memory_profile7_log - INFO -     90    368.8 MiB      2.8 MiB       print df.info(memory_usage='deep')

2018-04-28 20:32:55,963 - memory_profile7_log - INFO -     91                             

2018-04-28 20:32:55,963 - memory_profile7_log - INFO -     92    368.8 MiB      0.0 MiB       return df

2018-04-28 20:32:55,963 - memory_profile7_log - INFO - 


2018-04-28 20:32:55,963 - memory_profile7_log - INFO - getting total: 361145 training data(genuine interest) for date: 2018-04-13
2018-04-28 20:32:55,964 - memory_profile7_log - INFO - Exiting: PoolWorker-3
2018-04-28 20:32:56,819 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-14
2018-04-28 20:33:24,430 - memory_profile7_log - INFO - <class 'pandas.core.frame.DataFrame'>
RangeIndex: 427635 entries, 0 to 427634
Data columns (total 5 columns):
date          427635 non-null datetime64[ns, UTC]
user_id       427635 non-null object
topic_id      427635 non-null object
is_general    427635 non-null bool
num           427635 non-null int64
dtypes: bool(1), datetime64[ns, UTC](1), int64(1), object(2)
memory usage: 108.5 MB

2018-04-28 20:33:24,433 - memory_profile7_log - INFO - None
2018-04-28 20:33:24,433 - memory_profile7_log - INFO - 

2018-04-28 20:33:24,434 - memory_profile7_log - INFO - Filename: D:\Kerjaan\python\Recommender\topic-recommender\CRON\legacy-train\legacy_bigquery.py


2018-04-28 20:33:24,434 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-28 20:33:24,434 - memory_profile7_log - INFO - ================================================

2018-04-28 20:33:24,436 - memory_profile7_log - INFO -     67     86.6 MiB     86.6 MiB   @profile

2018-04-28 20:33:24,436 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-28 20:33:24,437 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-28 20:33:24,437 - memory_profile7_log - INFO -     70                                 # https://www.sjoerdlangkemper.nl/2016/06/09/clearing-memory-in-python/

2018-04-28 20:33:24,437 - memory_profile7_log - INFO -     71     86.6 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-28 20:33:24,437 - memory_profile7_log - INFO -     72     86.6 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-28 20:33:24,438 - memory_profile7_log - INFO -     73                             

2018-04-28 20:33:24,438 - memory_profile7_log - INFO -     74     89.8 MiB      3.2 MiB       result  = client.query(query, job_config=job_config)

2018-04-28 20:33:24,440 - memory_profile7_log - INFO -     75     89.8 MiB      0.1 MiB       rows = result.result()

2018-04-28 20:33:24,440 - memory_profile7_log - INFO -     76                             

2018-04-28 20:33:24,440 - memory_profile7_log - INFO -     77     89.8 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-28 20:33:24,440 - memory_profile7_log - INFO -     78                                 

2018-04-28 20:33:24,440 - memory_profile7_log - INFO -     79     89.8 MiB      0.0 MiB       def _q_iterator():

2018-04-28 20:33:24,440 - memory_profile7_log - INFO -     80                                     # data = []

2018-04-28 20:33:24,446 - memory_profile7_log - INFO -     81    469.1 MiB    333.2 MiB           for row in rows:

2018-04-28 20:33:24,447 - memory_profile7_log - INFO -     82                                         # data.append(list(row))

2018-04-28 20:33:24,448 - memory_profile7_log - INFO -     83    469.1 MiB     12.1 MiB               yield list(row)

2018-04-28 20:33:24,448 - memory_profile7_log - INFO -     84                             

2018-04-28 20:33:24,448 - memory_profile7_log - INFO -     85                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-28 20:33:24,450 - memory_profile7_log - INFO -     86    393.4 MiB    -75.7 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-28 20:33:24,450 - memory_profile7_log - INFO -     87                                 # del rows

2018-04-28 20:33:24,450 - memory_profile7_log - INFO -     88    393.4 MiB      0.0 MiB       del result

2018-04-28 20:33:24,450 - memory_profile7_log - INFO -     89                                 # del data

2018-04-28 20:33:24,450 - memory_profile7_log - INFO -     90    396.7 MiB      3.3 MiB       print df.info(memory_usage='deep')

2018-04-28 20:33:24,451 - memory_profile7_log - INFO -     91                             

2018-04-28 20:33:24,451 - memory_profile7_log - INFO -     92    396.7 MiB      0.0 MiB       return df

2018-04-28 20:33:24,451 - memory_profile7_log - INFO - 


2018-04-28 20:33:24,453 - memory_profile7_log - INFO - getting total: 427635 training data(genuine interest) for date: 2018-04-10
2018-04-28 20:33:24,453 - memory_profile7_log - INFO - Exiting: PoolWorker-2
2018-04-28 20:33:24,989 - memory_profile7_log - INFO - <class 'pandas.core.frame.DataFrame'>
RangeIndex: 462579 entries, 0 to 462578
Data columns (total 5 columns):
date          462579 non-null datetime64[ns, UTC]
user_id       462579 non-null object
topic_id      462579 non-null object
is_general    462579 non-null bool
num           462579 non-null int64
dtypes: bool(1), datetime64[ns, UTC](1), int64(1), object(2)
memory usage: 117.0 MB

2018-04-28 20:33:24,990 - memory_profile7_log - INFO - None
2018-04-28 20:33:24,990 - memory_profile7_log - INFO - 

2018-04-28 20:33:24,992 - memory_profile7_log - INFO - Filename: D:\Kerjaan\python\Recommender\topic-recommender\CRON\legacy-train\legacy_bigquery.py


2018-04-28 20:33:24,992 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-28 20:33:24,992 - memory_profile7_log - INFO - ================================================

2018-04-28 20:33:24,993 - memory_profile7_log - INFO -     67     86.8 MiB     86.8 MiB   @profile

2018-04-28 20:33:24,993 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-28 20:33:24,993 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-28 20:33:24,993 - memory_profile7_log - INFO -     70                                 # https://www.sjoerdlangkemper.nl/2016/06/09/clearing-memory-in-python/

2018-04-28 20:33:24,994 - memory_profile7_log - INFO -     71     86.8 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-28 20:33:24,994 - memory_profile7_log - INFO -     72     86.8 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-28 20:33:24,994 - memory_profile7_log - INFO -     73                             

2018-04-28 20:33:24,996 - memory_profile7_log - INFO -     74     90.0 MiB      3.2 MiB       result  = client.query(query, job_config=job_config)

2018-04-28 20:33:24,996 - memory_profile7_log - INFO -     75     90.1 MiB      0.1 MiB       rows = result.result()

2018-04-28 20:33:24,996 - memory_profile7_log - INFO -     76                             

2018-04-28 20:33:24,997 - memory_profile7_log - INFO -     77     90.1 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-28 20:33:24,997 - memory_profile7_log - INFO -     78                                 

2018-04-28 20:33:24,999 - memory_profile7_log - INFO -     79     90.1 MiB      0.0 MiB       def _q_iterator():

2018-04-28 20:33:24,999 - memory_profile7_log - INFO -     80                                     # data = []

2018-04-28 20:33:24,999 - memory_profile7_log - INFO -     81    474.5 MiB    273.1 MiB           for row in rows:

2018-04-28 20:33:25,000 - memory_profile7_log - INFO -     82                                         # data.append(list(row))

2018-04-28 20:33:25,000 - memory_profile7_log - INFO -     83    474.5 MiB   -118.7 MiB               yield list(row)

2018-04-28 20:33:25,000 - memory_profile7_log - INFO -     84                             

2018-04-28 20:33:25,000 - memory_profile7_log - INFO -     85                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-28 20:33:25,002 - memory_profile7_log - INFO -     86    393.7 MiB    -80.7 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-28 20:33:25,002 - memory_profile7_log - INFO -     87                                 # del rows

2018-04-28 20:33:25,003 - memory_profile7_log - INFO -     88    393.7 MiB      0.0 MiB       del result

2018-04-28 20:33:25,003 - memory_profile7_log - INFO -     89                                 # del data

2018-04-28 20:33:25,003 - memory_profile7_log - INFO -     90    395.0 MiB      1.3 MiB       print df.info(memory_usage='deep')

2018-04-28 20:33:25,003 - memory_profile7_log - INFO -     91                             

2018-04-28 20:33:25,003 - memory_profile7_log - INFO -     92    395.0 MiB      0.0 MiB       return df

2018-04-28 20:33:25,005 - memory_profile7_log - INFO - 


2018-04-28 20:33:25,005 - memory_profile7_log - INFO - getting total: 462579 training data(genuine interest) for date: 2018-04-12
2018-04-28 20:33:25,006 - memory_profile7_log - INFO - Exiting: PoolWorker-4
2018-04-28 20:33:26,369 - memory_profile7_log - INFO - <class 'pandas.core.frame.DataFrame'>
RangeIndex: 428418 entries, 0 to 428417
Data columns (total 5 columns):
date          428418 non-null datetime64[ns, UTC]
user_id       428418 non-null object
topic_id      428418 non-null object
is_general    428418 non-null bool
num           428418 non-null int64
dtypes: bool(1), datetime64[ns, UTC](1), int64(1), object(2)
memory usage: 108.5 MB

2018-04-28 20:33:26,371 - memory_profile7_log - INFO - None
2018-04-28 20:33:26,371 - memory_profile7_log - INFO - 

2018-04-28 20:33:26,372 - memory_profile7_log - INFO - Filename: D:\Kerjaan\python\Recommender\topic-recommender\CRON\legacy-train\legacy_bigquery.py


2018-04-28 20:33:26,372 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-28 20:33:26,374 - memory_profile7_log - INFO - ================================================

2018-04-28 20:33:26,374 - memory_profile7_log - INFO -     67     86.8 MiB     86.8 MiB   @profile

2018-04-28 20:33:26,375 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-28 20:33:26,375 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-28 20:33:26,375 - memory_profile7_log - INFO -     70                                 # https://www.sjoerdlangkemper.nl/2016/06/09/clearing-memory-in-python/

2018-04-28 20:33:26,375 - memory_profile7_log - INFO -     71     86.8 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-28 20:33:26,377 - memory_profile7_log - INFO -     72     86.8 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-28 20:33:26,377 - memory_profile7_log - INFO -     73                             

2018-04-28 20:33:26,378 - memory_profile7_log - INFO -     74     90.1 MiB      3.3 MiB       result  = client.query(query, job_config=job_config)

2018-04-28 20:33:26,378 - memory_profile7_log - INFO -     75     90.2 MiB      0.1 MiB       rows = result.result()

2018-04-28 20:33:26,378 - memory_profile7_log - INFO -     76                             

2018-04-28 20:33:26,378 - memory_profile7_log - INFO -     77     90.2 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-28 20:33:26,380 - memory_profile7_log - INFO -     78                                 

2018-04-28 20:33:26,380 - memory_profile7_log - INFO -     79     90.2 MiB      0.0 MiB       def _q_iterator():

2018-04-28 20:33:26,380 - memory_profile7_log - INFO -     80                                     # data = []

2018-04-28 20:33:26,381 - memory_profile7_log - INFO -     81    470.2 MiB    336.9 MiB           for row in rows:

2018-04-28 20:33:26,381 - memory_profile7_log - INFO -     82                                         # data.append(list(row))

2018-04-28 20:33:26,381 - memory_profile7_log - INFO -     83    470.2 MiB     17.6 MiB               yield list(row)

2018-04-28 20:33:26,387 - memory_profile7_log - INFO -     84                             

2018-04-28 20:33:26,388 - memory_profile7_log - INFO -     85                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-28 20:33:26,390 - memory_profile7_log - INFO -     86    393.0 MiB    -77.2 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-28 20:33:26,391 - memory_profile7_log - INFO -     87                                 # del rows

2018-04-28 20:33:26,391 - memory_profile7_log - INFO -     88    393.0 MiB      0.0 MiB       del result

2018-04-28 20:33:26,391 - memory_profile7_log - INFO -     89                                 # del data

2018-04-28 20:33:26,392 - memory_profile7_log - INFO -     90    395.4 MiB      2.5 MiB       print df.info(memory_usage='deep')

2018-04-28 20:33:26,392 - memory_profile7_log - INFO -     91                             

2018-04-28 20:33:26,392 - memory_profile7_log - INFO -     92    395.4 MiB      0.0 MiB       return df

2018-04-28 20:33:26,394 - memory_profile7_log - INFO - 


2018-04-28 20:33:26,394 - memory_profile7_log - INFO - getting total: 428418 training data(genuine interest) for date: 2018-04-11
2018-04-28 20:33:26,394 - memory_profile7_log - INFO - Exiting: PoolWorker-1
2018-04-28 20:34:07,578 - memory_profile7_log - INFO - <class 'pandas.core.frame.DataFrame'>
RangeIndex: 253995 entries, 0 to 253994
Data columns (total 5 columns):
date          253995 non-null datetime64[ns, UTC]
user_id       253995 non-null object
topic_id      253995 non-null object
is_general    253995 non-null bool
num           253995 non-null int64
dtypes: bool(1), datetime64[ns, UTC](1), int64(1), object(2)
memory usage: 64.3 MB

2018-04-28 20:34:07,579 - memory_profile7_log - INFO - None
2018-04-28 20:34:07,582 - memory_profile7_log - INFO - 

2018-04-28 20:34:07,582 - memory_profile7_log - INFO - Filename: D:\Kerjaan\python\Recommender\topic-recommender\CRON\legacy-train\legacy_bigquery.py


2018-04-28 20:34:07,584 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-28 20:34:07,584 - memory_profile7_log - INFO - ================================================

2018-04-28 20:34:07,585 - memory_profile7_log - INFO -     67    225.7 MiB    225.7 MiB   @profile

2018-04-28 20:34:07,585 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-28 20:34:07,585 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-28 20:34:07,586 - memory_profile7_log - INFO -     70                                 # https://www.sjoerdlangkemper.nl/2016/06/09/clearing-memory-in-python/

2018-04-28 20:34:07,586 - memory_profile7_log - INFO -     71    225.7 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-28 20:34:07,588 - memory_profile7_log - INFO -     72    225.7 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-28 20:34:07,592 - memory_profile7_log - INFO -     73                             

2018-04-28 20:34:07,592 - memory_profile7_log - INFO -     74    227.3 MiB      1.6 MiB       result  = client.query(query, job_config=job_config)

2018-04-28 20:34:07,592 - memory_profile7_log - INFO -     75    227.3 MiB      0.0 MiB       rows = result.result()

2018-04-28 20:34:07,592 - memory_profile7_log - INFO -     76                             

2018-04-28 20:34:07,592 - memory_profile7_log - INFO -     77    227.3 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-28 20:34:07,592 - memory_profile7_log - INFO -     78                                 

2018-04-28 20:34:07,592 - memory_profile7_log - INFO -     79    227.3 MiB      0.0 MiB       def _q_iterator():

2018-04-28 20:34:07,592 - memory_profile7_log - INFO -     80                                     # data = []

2018-04-28 20:34:07,592 - memory_profile7_log - INFO -     81    389.5 MiB    102.5 MiB           for row in rows:

2018-04-28 20:34:07,592 - memory_profile7_log - INFO -     82                                         # data.append(list(row))

2018-04-28 20:34:07,592 - memory_profile7_log - INFO -     83    389.5 MiB    -56.2 MiB               yield list(row)

2018-04-28 20:34:07,592 - memory_profile7_log - INFO -     84                             

2018-04-28 20:34:07,592 - memory_profile7_log - INFO -     85                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-28 20:34:07,592 - memory_profile7_log - INFO -     86    344.5 MiB    -45.0 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-28 20:34:07,592 - memory_profile7_log - INFO -     87                                 # del rows

2018-04-28 20:34:07,592 - memory_profile7_log - INFO -     88    344.5 MiB      0.0 MiB       del result

2018-04-28 20:34:07,592 - memory_profile7_log - INFO -     89                                 # del data

2018-04-28 20:34:07,592 - memory_profile7_log - INFO -     90    346.4 MiB      1.9 MiB       print df.info(memory_usage='deep')

2018-04-28 20:34:07,592 - memory_profile7_log - INFO -     91                             

2018-04-28 20:34:07,592 - memory_profile7_log - INFO -     92    346.4 MiB      0.0 MiB       return df

2018-04-28 20:34:07,592 - memory_profile7_log - INFO - 


2018-04-28 20:34:07,592 - memory_profile7_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-28 20:34:07,592 - memory_profile7_log - INFO - Exiting: PoolWorker-3
2018-04-28 20:34:08,427 - memory_profile7_log - INFO - len datalist: 5
2018-04-28 20:34:08,427 - memory_profile7_log - INFO - All data fetch multiprocess done!!
2018-04-28 20:34:08,427 - memory_profile7_log - INFO - Filename: .\legacy_bigquery.py


2018-04-28 20:34:08,427 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-28 20:34:08,427 - memory_profile7_log - INFO - ================================================

2018-04-28 20:34:08,427 - memory_profile7_log - INFO -    265     87.2 MiB     87.2 MiB   @profile

2018-04-28 20:34:08,427 - memory_profile7_log - INFO -    266                             def BQPreprocess(loadmp, cpu, date_generated, client, query_fit):

2018-04-28 20:34:08,427 - memory_profile7_log - INFO -    267     87.2 MiB      0.0 MiB       bq_client = client

2018-04-28 20:34:08,427 - memory_profile7_log - INFO -    268     87.2 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-28 20:34:08,443 - memory_profile7_log - INFO -    269                             

2018-04-28 20:34:08,443 - memory_profile7_log - INFO -    270     87.2 MiB      0.0 MiB       datalist = []

2018-04-28 20:34:08,443 - memory_profile7_log - INFO -    271                             

2018-04-28 20:34:08,443 - memory_profile7_log - INFO -    272     87.2 MiB      0.0 MiB       if loadmp:

2018-04-28 20:34:08,443 - memory_profile7_log - INFO -    273     87.2 MiB      0.0 MiB           logger.info("Starting data fetch multiprocess..")

2018-04-28 20:34:08,443 - memory_profile7_log - INFO -    274     87.2 MiB      0.0 MiB           logger.info("number of process: %d", len(date_generated))

2018-04-28 20:34:08,443 - memory_profile7_log - INFO -    275                             

2018-04-28 20:34:08,443 - memory_profile7_log - INFO -    276     87.9 MiB      0.6 MiB           pool = mp.Pool(processes=cpu)

2018-04-28 20:34:08,443 - memory_profile7_log - INFO -    277                                     # multprocessA = [pool.apply(_getBig, args=(ndate.strftime("%Y-%m-%d"), )) for ndate in date_generated]

2018-04-28 20:34:08,443 - memory_profile7_log - INFO -    278     87.9 MiB      0.0 MiB           multprocessA = [pool.apply_async(getBig, args=(ndate.strftime("%Y-%m-%d"), loadmp, query_fit, )) for ndate in date_generated]

2018-04-28 20:34:08,443 - memory_profile7_log - INFO -    279    641.8 MiB    553.9 MiB           output_multprocessA = [p.get() for p in multprocessA]

2018-04-28 20:34:08,443 - memory_profile7_log - INFO -    280                             

2018-04-28 20:34:08,443 - memory_profile7_log - INFO -    281    641.8 MiB      0.0 MiB           for m in output_multprocessA:

2018-04-28 20:34:08,443 - memory_profile7_log - INFO -    282    641.8 MiB      0.0 MiB               if m is not None:

2018-04-28 20:34:08,443 - memory_profile7_log - INFO -    283    641.8 MiB      0.0 MiB                   if not m.empty:

2018-04-28 20:34:08,443 - memory_profile7_log - INFO -    284    641.8 MiB      0.0 MiB                       datalist.append(m)

2018-04-28 20:34:08,443 - memory_profile7_log - INFO -    285                             

2018-04-28 20:34:08,443 - memory_profile7_log - INFO -    286    641.8 MiB      0.0 MiB           pool.close()

2018-04-28 20:34:08,443 - memory_profile7_log - INFO -    287    641.7 MiB     -0.0 MiB           pool.terminate()

2018-04-28 20:34:08,443 - memory_profile7_log - INFO -    288                             

2018-04-28 20:34:08,443 - memory_profile7_log - INFO -    289    641.7 MiB      0.0 MiB           logger.info("len datalist: %d", len(datalist))

2018-04-28 20:34:08,443 - memory_profile7_log - INFO -    290    641.7 MiB      0.0 MiB           logger.info("All data fetch multiprocess done!!")

2018-04-28 20:34:08,461 - memory_profile7_log - INFO -    291                                 else:

2018-04-28 20:34:08,461 - memory_profile7_log - INFO -    292                                     logger.info("Starting data fetch iterative...")

2018-04-28 20:34:08,461 - memory_profile7_log - INFO -    293                                     for ndate in date_generated:

2018-04-28 20:34:08,461 - memory_profile7_log - INFO -    294                                         tframe = getBig(ndate.strftime("%Y-%m-%d"), loadmp)

2018-04-28 20:34:08,461 - memory_profile7_log - INFO -    295                                         if tframe is not None:

2018-04-28 20:34:08,461 - memory_profile7_log - INFO -    296                                             if not tframe.empty:

2018-04-28 20:34:08,461 - memory_profile7_log - INFO -    297                                                 datalist.append(tframe)

2018-04-28 20:34:08,461 - memory_profile7_log - INFO -    298                                         else: 

2018-04-28 20:34:08,461 - memory_profile7_log - INFO -    299                                             logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-28 20:34:08,461 - memory_profile7_log - INFO -    300                                     logger.info("len datalist: %d", len(datalist))

2018-04-28 20:34:08,461 - memory_profile7_log - INFO -    301                                     logger.info("All data fetch iterative done!!")

2018-04-28 20:34:08,461 - memory_profile7_log - INFO -    302                             

2018-04-28 20:34:08,461 - memory_profile7_log - INFO -    303    641.7 MiB      0.0 MiB       return datalist

2018-04-28 20:34:08,461 - memory_profile7_log - INFO - 


2018-04-28 20:34:08,595 - memory_profile7_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-28 20:35:35,132 - memory_profile7_log - INFO - <class 'pandas.core.frame.DataFrame'>
RangeIndex: 230425 entries, 0 to 230424
Data columns (total 5 columns):
date          230425 non-null datetime64[ns, UTC]
user_id       230425 non-null object
topic_id      230425 non-null object
is_general    230425 non-null bool
num           230425 non-null int64
dtypes: bool(1), datetime64[ns, UTC](1), int64(1), object(2)
memory usage: 58.3 MB

2018-04-28 20:35:35,134 - memory_profile7_log - INFO - None
2018-04-28 20:35:35,135 - memory_profile7_log - INFO - 

2018-04-28 20:35:35,138 - memory_profile7_log - INFO - Filename: .\legacy_bigquery.py


2018-04-28 20:35:35,140 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-28 20:35:35,141 - memory_profile7_log - INFO - ================================================

2018-04-28 20:35:35,141 - memory_profile7_log - INFO -     67    658.5 MiB    658.5 MiB   @profile

2018-04-28 20:35:35,148 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-28 20:35:35,154 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-28 20:35:35,158 - memory_profile7_log - INFO -     70                                 # https://www.sjoerdlangkemper.nl/2016/06/09/clearing-memory-in-python/

2018-04-28 20:35:35,161 - memory_profile7_log - INFO -     71    658.5 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-28 20:35:35,165 - memory_profile7_log - INFO -     72    658.5 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-28 20:35:35,171 - memory_profile7_log - INFO -     73                             

2018-04-28 20:35:35,173 - memory_profile7_log - INFO -     74    659.6 MiB      1.0 MiB       result  = client.query(query, job_config=job_config)

2018-04-28 20:35:35,174 - memory_profile7_log - INFO -     75    659.6 MiB      0.0 MiB       rows = result.result()

2018-04-28 20:35:35,176 - memory_profile7_log - INFO -     76                             

2018-04-28 20:35:35,178 - memory_profile7_log - INFO -     77    659.6 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-28 20:35:35,181 - memory_profile7_log - INFO -     78                                 

2018-04-28 20:35:35,183 - memory_profile7_log - INFO -     79    659.6 MiB      0.0 MiB       def _q_iterator():

2018-04-28 20:35:35,187 - memory_profile7_log - INFO -     80                                     # data = []

2018-04-28 20:35:35,188 - memory_profile7_log - INFO -     81    932.3 MiB    226.3 MiB           for row in rows:

2018-04-28 20:35:35,194 - memory_profile7_log - INFO -     82                                         # data.append(list(row))

2018-04-28 20:35:35,196 - memory_profile7_log - INFO -     83    932.3 MiB    -36.4 MiB               yield list(row)

2018-04-28 20:35:35,197 - memory_profile7_log - INFO -     84                             

2018-04-28 20:35:35,200 - memory_profile7_log - INFO -     85                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-28 20:35:35,203 - memory_profile7_log - INFO -     86    929.7 MiB     -2.6 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-28 20:35:35,206 - memory_profile7_log - INFO -     87                                 # del rows

2018-04-28 20:35:35,207 - memory_profile7_log - INFO -     88    929.7 MiB      0.0 MiB       del result

2018-04-28 20:35:35,210 - memory_profile7_log - INFO -     89                                 # del data

2018-04-28 20:35:35,210 - memory_profile7_log - INFO -     90    927.2 MiB     -2.5 MiB       print df.info(memory_usage='deep')

2018-04-28 20:35:35,213 - memory_profile7_log - INFO -     91                             

2018-04-28 20:35:35,216 - memory_profile7_log - INFO -     92    927.2 MiB      0.0 MiB       return df

2018-04-28 20:35:35,217 - memory_profile7_log - INFO - 


2018-04-28 20:35:35,219 - memory_profile7_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-28 20:35:35,230 - memory_profile7_log - INFO - loading time of: 2164197 total genuine-current interest data ~ take 284.427s
2018-04-28 20:35:35,232 - memory_profile7_log - INFO - Filename: .\legacy_bigquery.py


2018-04-28 20:35:35,233 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-28 20:35:35,234 - memory_profile7_log - INFO - ================================================

2018-04-28 20:35:35,236 - memory_profile7_log - INFO -    305     87.1 MiB     87.1 MiB   @profile

2018-04-28 20:35:35,236 - memory_profile7_log - INFO -    306                             def preprocess(loadmp, cpu, cd, query_fit, date_generated):

2018-04-28 20:35:35,237 - memory_profile7_log - INFO -    307     87.2 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-28 20:35:35,237 - memory_profile7_log - INFO -    308     87.2 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-28 20:35:35,237 - memory_profile7_log - INFO -    309                             

2018-04-28 20:35:35,239 - memory_profile7_log - INFO -    310                                 # ~~~ Begin collecting data ~~~

2018-04-28 20:35:35,239 - memory_profile7_log - INFO -    311     87.2 MiB      0.0 MiB       t0 = time.time()

2018-04-28 20:35:35,240 - memory_profile7_log - INFO -    312                                 

2018-04-28 20:35:35,240 - memory_profile7_log - INFO -    313    641.8 MiB    554.5 MiB       datalist = BQPreprocess(loadmp, cpu, date_generated, bq_client, query_fit)

2018-04-28 20:35:35,243 - memory_profile7_log - INFO -    314                             

2018-04-28 20:35:35,246 - memory_profile7_log - INFO -    315    641.8 MiB      0.0 MiB       if len(datalist) > 1:

2018-04-28 20:35:35,247 - memory_profile7_log - INFO -    316    732.2 MiB     90.5 MiB           big_frame = pd.concat(datalist)

2018-04-28 20:35:35,247 - memory_profile7_log - INFO -    317    658.4 MiB    -73.8 MiB           del datalist

2018-04-28 20:35:35,250 - memory_profile7_log - INFO -    318                                 else:

2018-04-28 20:35:35,250 - memory_profile7_log - INFO -    319                                     big_frame = datalist

2018-04-28 20:35:35,252 - memory_profile7_log - INFO -    320    658.5 MiB      0.1 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-28 20:35:35,252 - memory_profile7_log - INFO -    321                             

2018-04-28 20:35:35,252 - memory_profile7_log - INFO -    322                                 # ~ get current news interest ~

2018-04-28 20:35:35,253 - memory_profile7_log - INFO -    323    658.5 MiB      0.0 MiB       if not cd:

2018-04-28 20:35:35,253 - memory_profile7_log - INFO -    324                                     logger.info("Collecting training data(current date interest)..")

2018-04-28 20:35:35,253 - memory_profile7_log - INFO -    325                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-28 20:35:35,253 - memory_profile7_log - INFO -    326                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-28 20:35:35,256 - memory_profile7_log - INFO -    327                                 else:

2018-04-28 20:35:35,257 - memory_profile7_log - INFO -    328    658.5 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-28 20:35:35,260 - memory_profile7_log - INFO -    329    658.5 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-28 20:35:35,260 - memory_profile7_log - INFO -    330                             

2018-04-28 20:35:35,262 - memory_profile7_log - INFO -    331                                     # safe handling of query parameter

2018-04-28 20:35:35,262 - memory_profile7_log - INFO -    332                                     query_params = [

2018-04-28 20:35:35,263 - memory_profile7_log - INFO -    333    658.5 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-28 20:35:35,263 - memory_profile7_log - INFO -    334                                     ]

2018-04-28 20:35:35,263 - memory_profile7_log - INFO -    335                             

2018-04-28 20:35:35,263 - memory_profile7_log - INFO -    336    658.5 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-28 20:35:35,265 - memory_profile7_log - INFO -    337    927.2 MiB    268.7 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-28 20:35:35,269 - memory_profile7_log - INFO -    338    927.2 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-28 20:35:35,270 - memory_profile7_log - INFO -    339                             

2018-04-28 20:35:35,270 - memory_profile7_log - INFO -    340    927.3 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-28 20:35:35,272 - memory_profile7_log - INFO -    341    927.3 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-28 20:35:35,272 - memory_profile7_log - INFO -    342    927.3 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-28 20:35:35,273 - memory_profile7_log - INFO -    343                             

2018-04-28 20:35:35,273 - memory_profile7_log - INFO -    344    927.3 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-28 20:35:35,275 - memory_profile7_log - INFO -    345    927.3 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-28 20:35:35,276 - memory_profile7_log - INFO -    346                             

2018-04-28 20:35:35,276 - memory_profile7_log - INFO -    347    927.3 MiB      0.0 MiB       return big_frame, current_frame

2018-04-28 20:35:35,276 - memory_profile7_log - INFO - 


2018-04-28 20:35:35,279 - memory_profile7_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-28 20:35:35,355 - memory_profile7_log - INFO - train on: 1933772 total history data(D(u, t))
2018-04-28 20:35:35,357 - memory_profile7_log - INFO - transform on: 230425 total current data(D(t))
2018-04-28 20:35:42,289 - memory_profile7_log - INFO - Len of model_fit: 1933772
2018-04-28 20:35:42,289 - memory_profile7_log - INFO - Len of df_dut: 1933772
2018-04-28 20:37:38,582 - memory_profile7_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-28 20:37:38,719 - memory_profile7_log - INFO - Total train time: 123.363s
2018-04-28 20:37:38,720 - memory_profile7_log - INFO - memory left before cleaning: 65.700 percent memory...
2018-04-28 20:37:38,720 - memory_profile7_log - INFO - cleaning up some objects...
2018-04-28 20:37:38,723 - memory_profile7_log - INFO - deleting df_dut...
2018-04-28 20:37:38,723 - memory_profile7_log - INFO - deleting df_dt...
2018-04-28 20:37:38,724 - memory_profile7_log - INFO - deleting df_input...
2018-04-28 20:37:38,734 - memory_profile7_log - INFO - deleting df_input_X...
2018-04-28 20:37:38,736 - memory_profile7_log - INFO - deleting df_current...
2018-04-28 20:37:38,739 - memory_profile7_log - INFO - deleting map_topic_isgeneral...
2018-04-28 20:37:38,851 - memory_profile7_log - INFO - deleting model_fit...
2018-04-28 20:37:38,851 - memory_profile7_log - INFO - deleting result...
2018-04-28 20:37:38,882 - memory_profile7_log - INFO - Filename: .\legacy_bigquery.py


2018-04-28 20:37:38,882 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-28 20:37:38,882 - memory_profile7_log - INFO - ================================================

2018-04-28 20:37:38,882 - memory_profile7_log - INFO -    104    927.3 MiB    927.3 MiB   @profile

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    105                             def main(df_input, df_current, current_date, G,

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    106                                      project_id, savetrain=False, multproc=True,

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    107                                      threshold=0, start_date=None, end_date=None,

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    108                                      saveto="datastore"):

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    109                                 """

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    110                                     Main cron method

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    111                                 """

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    112                                 # ~ Data Preprocessing ~

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    113                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    114                                 # D(u, t)

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    115    927.3 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    116    988.1 MiB     60.9 MiB       df_dut = df_input.copy(deep=True)

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    117    988.1 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    118                             

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    119                                 # D(t)

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    120    995.2 MiB      7.0 MiB       df_dt = df_current.copy(deep=True)

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    121    995.2 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    122                             

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    123                                 # ~~~~~~ Begin train ~~~~~~

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    124    995.2 MiB      0.0 MiB       t0 = time.time()

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    125    995.2 MiB      0.0 MiB       logger.info("train on: %d total history data(D(u, t))", len(df_dut))

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    126    995.2 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    127                             

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    128                                 # instantiace class

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    129    995.2 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    130                             

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    131                                 # ~~ Fit ~~

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    132                                 #   handling genuine news interest < current date

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    133   1001.0 MiB      5.9 MiB       NB = BR.processX(df_dut)

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    134                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    135                                 #   nanti dipindah ke class train utama

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    136   1076.8 MiB     75.7 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    137                                 """

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    138                                     num_y = total global click for category=ci on periode t

2018-04-28 20:37:38,898 - memory_profile7_log - INFO -    139                                     num_x = total click from user_U for category=ci on periode t

2018-04-28 20:37:38,918 - memory_profile7_log - INFO -    140                                 """

2018-04-28 20:37:38,920 - memory_profile7_log - INFO -    141   1076.8 MiB      0.0 MiB       fitby_sigmant = True

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    142   1076.8 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    143   1076.8 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    144   1146.8 MiB     70.1 MiB                            'is_general']]

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    145   1146.8 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    146   1146.8 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    147   1326.4 MiB    179.5 MiB                          verbose=False)

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    148   1326.4 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    149   1326.4 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    150                             

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    151                                 # ~~ and Transform ~~

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    152                                 #   handling current news interest == current date

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    153   1326.4 MiB      0.0 MiB       if df_dt.empty:

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    154                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    155                                     return None

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    156   1326.4 MiB      0.0 MiB       NB = BR.processX(df_dt)

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    157   1337.0 MiB     10.6 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    158                             

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    159   1337.0 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    160   1270.1 MiB    -66.8 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    161   1270.1 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    162   1270.1 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    163   1367.9 MiB     97.8 MiB                                                     verbose=False)

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    164                                 # ~~~ filter is general and specific topic ~~~

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    165                                 # the idea is just we need to rerank every topic according

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    166                                 # user_id and and is_general by p0_posterior

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    167   1367.9 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    168   1384.5 MiB     16.6 MiB                                     'is_general']].groupby(['topic_id',

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    169   1368.0 MiB    -16.6 MiB                                                             'is_general']

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    170                                                                                      ).size().to_frame().reset_index()

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    171                             

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    172                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    173                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-28 20:37:38,921 - memory_profile7_log - INFO -    174   1369.5 MiB      1.5 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-28 20:37:38,936 - memory_profile7_log - INFO -    175                             

2018-04-28 20:37:38,936 - memory_profile7_log - INFO -    176                                 # ~ start by provide rank for each topic type ~

2018-04-28 20:37:38,936 - memory_profile7_log - INFO -    177   1448.6 MiB     79.1 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-28 20:37:38,936 - memory_profile7_log - INFO -    178   1461.0 MiB     12.5 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-28 20:37:38,936 - memory_profile7_log - INFO -    179                             

2018-04-28 20:37:38,936 - memory_profile7_log - INFO -    180                                 # ~ set threshold to filter output

2018-04-28 20:37:38,936 - memory_profile7_log - INFO -    181   1461.0 MiB      0.0 MiB       if threshold > 0:

2018-04-28 20:37:38,936 - memory_profile7_log - INFO -    182   1461.0 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-28 20:37:38,936 - memory_profile7_log - INFO -    183   1462.6 MiB      1.5 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-28 20:37:38,936 - memory_profile7_log - INFO -    184   1426.2 MiB    -36.4 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-28 20:37:38,936 - memory_profile7_log - INFO -    185                             

2018-04-28 20:37:38,936 - memory_profile7_log - INFO -    186                                 # print "\n", model_transform[['user_id', 'topic_id', 'p0_posterior',

2018-04-28 20:37:38,936 - memory_profile7_log - INFO -    187                                 #                             'is_general', 'rank']].loc[model_transform['user_id'].str.contains('1610c8e6b6466b-05a6ac1ba8c1b8-4323461-100200-1',

2018-04-28 20:37:38,936 - memory_profile7_log - INFO -    188                                 #                                                                                                 case=False)].head(45)

2018-04-28 20:37:38,936 - memory_profile7_log - INFO -    189                                 # print "len(model_transform): %d" % len(model_transform)

2018-04-28 20:37:38,936 - memory_profile7_log - INFO -    190                             

2018-04-28 20:37:38,951 - memory_profile7_log - INFO -    191   1426.2 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-28 20:37:38,951 - memory_profile7_log - INFO -    192   1426.2 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-28 20:37:38,951 - memory_profile7_log - INFO -    193                             

2018-04-28 20:37:38,951 - memory_profile7_log - INFO -    194   1426.2 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-28 20:37:38,951 - memory_profile7_log - INFO -    195                             

2018-04-28 20:37:38,951 - memory_profile7_log - INFO -    196   1426.2 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-28 20:37:38,951 - memory_profile7_log - INFO -    197   1426.2 MiB      0.0 MiB       del df_dut

2018-04-28 20:37:38,951 - memory_profile7_log - INFO -    198   1426.2 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-28 20:37:38,951 - memory_profile7_log - INFO -    199   1426.2 MiB      0.0 MiB       del df_dt

2018-04-28 20:37:38,951 - memory_profile7_log - INFO -    200   1426.2 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-28 20:37:38,951 - memory_profile7_log - INFO -    201   1426.2 MiB      0.0 MiB       del df_input

2018-04-28 20:37:38,951 - memory_profile7_log - INFO -    202   1426.2 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-28 20:37:38,951 - memory_profile7_log - INFO -    203   1417.4 MiB     -8.8 MiB       del df_input_X

2018-04-28 20:37:38,951 - memory_profile7_log - INFO -    204   1417.4 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-28 20:37:38,951 - memory_profile7_log - INFO -    205   1417.4 MiB      0.0 MiB       del df_current

2018-04-28 20:37:38,951 - memory_profile7_log - INFO -    206   1417.4 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-28 20:37:38,951 - memory_profile7_log - INFO -    207   1417.4 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-28 20:37:38,951 - memory_profile7_log - INFO -    208   1417.4 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-28 20:37:38,951 - memory_profile7_log - INFO -    209   1238.5 MiB   -178.9 MiB       del model_fit

2018-04-28 20:37:38,951 - memory_profile7_log - INFO -    210   1238.5 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-28 20:37:38,951 - memory_profile7_log - INFO -    211   1238.5 MiB      0.0 MiB       del result

2018-04-28 20:37:38,951 - memory_profile7_log - INFO -    212   1238.5 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-28 20:37:38,967 - memory_profile7_log - INFO -    213                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-28 20:37:38,967 - memory_profile7_log - INFO -    214                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-28 20:37:38,967 - memory_profile7_log - INFO -    215   1238.5 MiB      0.0 MiB       if savetrain:

2018-04-28 20:37:38,967 - memory_profile7_log - INFO -    216                                     model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-28 20:37:38,967 - memory_profile7_log - INFO -    217                                     del model_transform

2018-04-28 20:37:38,967 - memory_profile7_log - INFO -    218                                     logger.info("deleting model_transform...")

2018-04-28 20:37:38,967 - memory_profile7_log - INFO -    219                                     logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-28 20:37:38,967 - memory_profile7_log - INFO -    220                             

2018-04-28 20:37:38,967 - memory_profile7_log - INFO -    221                                     logger.info("Begin saving trained data...")

2018-04-28 20:37:38,967 - memory_profile7_log - INFO -    222                                     # print "\n", model_transform.head(5)

2018-04-28 20:37:38,967 - memory_profile7_log - INFO -    223                                     # ~ Place your code to save the training model here ~

2018-04-28 20:37:38,967 - memory_profile7_log - INFO -    224                                     if str(saveto).lower() == "datastore":

2018-04-28 20:37:38,967 - memory_profile7_log - INFO -    225                                         logger.info("Using google datastore as storage...")

2018-04-28 20:37:38,967 - memory_profile7_log - INFO -    226                                         if multproc:

2018-04-28 20:37:38,967 - memory_profile7_log - INFO -    227                                             mh.saveDatastoreMP(model_transformsv)

2018-04-28 20:37:38,967 - memory_profile7_log - INFO -    228                                             # mh.saveDataStorePutMulti(model_transform[['user_id', 'topic_id', 'is_general', 'rank']])

2018-04-28 20:37:38,967 - memory_profile7_log - INFO -    229                                         else:

2018-04-28 20:37:38,967 - memory_profile7_log - INFO -    230                                             mh.saveDatastore(model_transformsv)

2018-04-28 20:37:38,967 - memory_profile7_log - INFO -    231                                             

2018-04-28 20:37:38,967 - memory_profile7_log - INFO -    232                                     elif str(saveto).lower() == "elastic":

2018-04-28 20:37:38,967 - memory_profile7_log - INFO -    233                                         logger.info("Using ElasticSearch as storage...")

2018-04-28 20:37:38,983 - memory_profile7_log - INFO -    234                                         mh.saveElasticS(model_transformsv)

2018-04-28 20:37:38,983 - memory_profile7_log - INFO -    235                             

2018-04-28 20:37:38,983 - memory_profile7_log - INFO -    236   1238.5 MiB      0.0 MiB       return

2018-04-28 20:37:38,983 - memory_profile7_log - INFO - 


2018-04-28 20:37:38,983 - memory_profile7_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-28 20:45:12,309 - memory_profile7_log - INFO - Generating date range with N: 5
2018-04-28 20:45:12,349 - memory_profile7_log - INFO - date_generated: 
2018-04-28 20:45:12,351 - memory_profile7_log - INFO -  
2018-04-28 20:45:12,351 - memory_profile7_log - INFO - [datetime.datetime(2018, 4, 10, 0, 0), datetime.datetime(2018, 4, 11, 0, 0), datetime.datetime(2018, 4, 12, 0, 0), datetime.datetime(2018, 4, 13, 0, 0), datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-28 20:45:12,351 - memory_profile7_log - INFO - 

2018-04-28 20:45:12,351 - memory_profile7_log - INFO - using current date: 2018-04-15
2018-04-28 20:45:12,351 - memory_profile7_log - INFO - using start date: 2018-04-10 00:00:00
2018-04-28 20:45:12,351 - memory_profile7_log - INFO - using end date: 2018-04-14
2018-04-28 20:45:12,486 - memory_profile7_log - INFO - Starting data fetch multiprocess..
2018-04-28 20:45:12,487 - memory_profile7_log - INFO - number of process: 5
2018-04-28 20:45:14,786 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-10
2018-04-28 20:45:14,799 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-11
2018-04-28 20:45:14,802 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-12
2018-04-28 20:45:14,802 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-13
2018-04-28 20:47:14,380 - memory_profile7_log - INFO - <class 'pandas.core.frame.DataFrame'>
RangeIndex: 361145 entries, 0 to 361144
Data columns (total 5 columns):
date          361145 non-null datetime64[ns, UTC]
user_id       361145 non-null object
topic_id      361145 non-null object
is_general    361145 non-null bool
num           361145 non-null int64
dtypes: bool(1), datetime64[ns, UTC](1), int64(1), object(2)
memory usage: 91.4 MB

2018-04-28 20:47:14,381 - memory_profile7_log - INFO - None
2018-04-28 20:47:14,382 - memory_profile7_log - INFO - 

2018-04-28 20:47:14,384 - memory_profile7_log - INFO - Filename: D:\Kerjaan\python\Recommender\topic-recommender\CRON\legacy-train\legacy_bigquery.py


2018-04-28 20:47:14,384 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-28 20:47:14,384 - memory_profile7_log - INFO - ================================================

2018-04-28 20:47:14,384 - memory_profile7_log - INFO -     67     86.8 MiB     86.8 MiB   @profile

2018-04-28 20:47:14,384 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-28 20:47:14,384 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-28 20:47:14,385 - memory_profile7_log - INFO -     70                                 # https://www.sjoerdlangkemper.nl/2016/06/09/clearing-memory-in-python/

2018-04-28 20:47:14,385 - memory_profile7_log - INFO -     71     86.8 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-28 20:47:14,385 - memory_profile7_log - INFO -     72     86.8 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-28 20:47:14,385 - memory_profile7_log - INFO -     73                             

2018-04-28 20:47:14,387 - memory_profile7_log - INFO -     74     89.9 MiB      3.2 MiB       result  = client.query(query, job_config=job_config)

2018-04-28 20:47:14,387 - memory_profile7_log - INFO -     75     90.0 MiB      0.1 MiB       rows = result.result()

2018-04-28 20:47:14,387 - memory_profile7_log - INFO -     76                             

2018-04-28 20:47:14,388 - memory_profile7_log - INFO -     77     90.0 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-28 20:47:14,388 - memory_profile7_log - INFO -     78                                 

2018-04-28 20:47:14,388 - memory_profile7_log - INFO -     79     90.0 MiB      0.0 MiB       def _q_iterator():

2018-04-28 20:47:14,388 - memory_profile7_log - INFO -     80                                     # data = []

2018-04-28 20:47:14,388 - memory_profile7_log - INFO -     81    424.9 MiB -23785.9 MiB           for row in rows:

2018-04-28 20:47:14,390 - memory_profile7_log - INFO -     82                                         # data.append(list(row))

2018-04-28 20:47:14,398 - memory_profile7_log - INFO -     83    424.9 MiB -48164.3 MiB               yield list(row)

2018-04-28 20:47:14,398 - memory_profile7_log - INFO -     84                             

2018-04-28 20:47:14,400 - memory_profile7_log - INFO -     85                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-28 20:47:14,401 - memory_profile7_log - INFO -     86    361.6 MiB    -63.3 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-28 20:47:14,401 - memory_profile7_log - INFO -     87                                 # del rows

2018-04-28 20:47:14,403 - memory_profile7_log - INFO -     88    361.6 MiB      0.0 MiB       del result

2018-04-28 20:47:14,404 - memory_profile7_log - INFO -     89                                 # del data

2018-04-28 20:47:14,404 - memory_profile7_log - INFO -     90    364.6 MiB      2.9 MiB       print df.info(memory_usage='deep')

2018-04-28 20:47:14,404 - memory_profile7_log - INFO -     91                             

2018-04-28 20:47:14,408 - memory_profile7_log - INFO -     92    364.6 MiB      0.0 MiB       return df

2018-04-28 20:47:14,410 - memory_profile7_log - INFO - 


2018-04-28 20:47:14,411 - memory_profile7_log - INFO - getting total: 361145 training data(genuine interest) for date: 2018-04-13
2018-04-28 20:47:14,411 - memory_profile7_log - INFO - Exiting: PoolWorker-2
2018-04-28 20:47:15,381 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-14
2018-04-28 20:47:47,934 - memory_profile7_log - INFO - <class 'pandas.core.frame.DataFrame'>
RangeIndex: 428418 entries, 0 to 428417
Data columns (total 5 columns):
date          428418 non-null datetime64[ns, UTC]
user_id       428418 non-null object
topic_id      428418 non-null object
is_general    428418 non-null bool
num           428418 non-null int64
dtypes: bool(1), datetime64[ns, UTC](1), int64(1), object(2)
memory usage: 108.5 MB

2018-04-28 20:47:47,936 - memory_profile7_log - INFO - None
2018-04-28 20:47:47,937 - memory_profile7_log - INFO - 

2018-04-28 20:47:47,937 - memory_profile7_log - INFO - Filename: D:\Kerjaan\python\Recommender\topic-recommender\CRON\legacy-train\legacy_bigquery.py


2018-04-28 20:47:47,938 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-28 20:47:47,938 - memory_profile7_log - INFO - ================================================

2018-04-28 20:47:47,940 - memory_profile7_log - INFO -     67     86.8 MiB     86.8 MiB   @profile

2018-04-28 20:47:47,940 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-28 20:47:47,941 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-28 20:47:47,943 - memory_profile7_log - INFO -     70                                 # https://www.sjoerdlangkemper.nl/2016/06/09/clearing-memory-in-python/

2018-04-28 20:47:47,943 - memory_profile7_log - INFO -     71     86.8 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-28 20:47:47,944 - memory_profile7_log - INFO -     72     86.8 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-28 20:47:47,947 - memory_profile7_log - INFO -     73                             

2018-04-28 20:47:47,948 - memory_profile7_log - INFO -     74     89.9 MiB      3.2 MiB       result  = client.query(query, job_config=job_config)

2018-04-28 20:47:47,950 - memory_profile7_log - INFO -     75     90.0 MiB      0.1 MiB       rows = result.result()

2018-04-28 20:47:47,950 - memory_profile7_log - INFO -     76                             

2018-04-28 20:47:47,950 - memory_profile7_log - INFO -     77     90.0 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-28 20:47:47,950 - memory_profile7_log - INFO -     78                                 

2018-04-28 20:47:47,951 - memory_profile7_log - INFO -     79     90.0 MiB      0.0 MiB       def _q_iterator():

2018-04-28 20:47:47,951 - memory_profile7_log - INFO -     80                                     # data = []

2018-04-28 20:47:47,953 - memory_profile7_log - INFO -     81    465.3 MiB  -5459.6 MiB           for row in rows:

2018-04-28 20:47:47,953 - memory_profile7_log - INFO -     82                                         # data.append(list(row))

2018-04-28 20:47:47,953 - memory_profile7_log - INFO -     83    465.3 MiB -11569.6 MiB               yield list(row)

2018-04-28 20:47:47,953 - memory_profile7_log - INFO -     84                             

2018-04-28 20:47:47,953 - memory_profile7_log - INFO -     85                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-28 20:47:47,954 - memory_profile7_log - INFO -     86    388.4 MiB    -77.0 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-28 20:47:47,954 - memory_profile7_log - INFO -     87                                 # del rows

2018-04-28 20:47:47,954 - memory_profile7_log - INFO -     88    388.4 MiB      0.0 MiB       del result

2018-04-28 20:47:47,956 - memory_profile7_log - INFO -     89                                 # del data

2018-04-28 20:47:47,960 - memory_profile7_log - INFO -     90    390.6 MiB      2.2 MiB       print df.info(memory_usage='deep')

2018-04-28 20:47:47,961 - memory_profile7_log - INFO -     91                             

2018-04-28 20:47:47,961 - memory_profile7_log - INFO -     92    390.6 MiB      0.0 MiB       return df

2018-04-28 20:47:47,963 - memory_profile7_log - INFO - 


2018-04-28 20:47:47,963 - memory_profile7_log - INFO - getting total: 428418 training data(genuine interest) for date: 2018-04-11
2018-04-28 20:47:47,963 - memory_profile7_log - INFO - Exiting: PoolWorker-3
2018-04-28 20:48:33,765 - memory_profile7_log - INFO - Generating date range with N: 10
2018-04-28 20:48:33,767 - memory_profile7_log - INFO - date_generated: 
2018-04-28 20:48:33,769 - memory_profile7_log - INFO -  
2018-04-28 20:48:33,769 - memory_profile7_log - INFO - [datetime.datetime(2018, 4, 5, 0, 0), datetime.datetime(2018, 4, 6, 0, 0), datetime.datetime(2018, 4, 7, 0, 0), datetime.datetime(2018, 4, 8, 0, 0), datetime.datetime(2018, 4, 9, 0, 0), datetime.datetime(2018, 4, 10, 0, 0), datetime.datetime(2018, 4, 11, 0, 0), datetime.datetime(2018, 4, 12, 0, 0), datetime.datetime(2018, 4, 13, 0, 0), datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-28 20:48:33,769 - memory_profile7_log - INFO - 

2018-04-28 20:48:33,770 - memory_profile7_log - INFO - using current date: 2018-04-15
2018-04-28 20:48:33,770 - memory_profile7_log - INFO - using start date: 2018-04-05 00:00:00
2018-04-28 20:48:33,770 - memory_profile7_log - INFO - using end date: 2018-04-14
2018-04-28 20:48:33,903 - memory_profile7_log - INFO - Starting data fetch multiprocess..
2018-04-28 20:48:33,904 - memory_profile7_log - INFO - number of process: 10
2018-04-28 20:48:36,164 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-05
2018-04-28 20:48:36,171 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-06
2018-04-28 20:48:36,174 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-07
2018-04-28 20:48:36,174 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-08
2018-04-28 20:50:06,845 - memory_profile7_log - INFO - <class 'pandas.core.frame.DataFrame'>
RangeIndex: 297844 entries, 0 to 297843
Data columns (total 5 columns):
date          297844 non-null datetime64[ns, UTC]
user_id       297844 non-null object
topic_id      297844 non-null object
is_general    297844 non-null bool
num           297844 non-null int64
dtypes: bool(1), datetime64[ns, UTC](1), int64(1), object(2)
memory usage: 75.5 MB

2018-04-28 20:50:06,845 - memory_profile7_log - INFO - None
2018-04-28 20:50:06,846 - memory_profile7_log - INFO - 

2018-04-28 20:50:06,846 - memory_profile7_log - INFO - Filename: D:\Kerjaan\python\Recommender\topic-recommender\CRON\legacy-train\legacy_bigquery.py


2018-04-28 20:50:06,848 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-28 20:50:06,848 - memory_profile7_log - INFO - ================================================

2018-04-28 20:50:06,848 - memory_profile7_log - INFO -     67     87.0 MiB     87.0 MiB   @profile

2018-04-28 20:50:06,848 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-28 20:50:06,848 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-28 20:50:06,848 - memory_profile7_log - INFO -     70                                 # https://www.sjoerdlangkemper.nl/2016/06/09/clearing-memory-in-python/

2018-04-28 20:50:06,848 - memory_profile7_log - INFO -     71     87.0 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-28 20:50:06,848 - memory_profile7_log - INFO -     72     87.0 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-28 20:50:06,849 - memory_profile7_log - INFO -     73                             

2018-04-28 20:50:06,849 - memory_profile7_log - INFO -     74     90.1 MiB      3.1 MiB       result  = client.query(query, job_config=job_config)

2018-04-28 20:50:06,849 - memory_profile7_log - INFO -     75     90.2 MiB      0.1 MiB       rows = result.result()

2018-04-28 20:50:06,849 - memory_profile7_log - INFO -     76                             

2018-04-28 20:50:06,851 - memory_profile7_log - INFO -     77     90.2 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-28 20:50:06,851 - memory_profile7_log - INFO -     78                                 

2018-04-28 20:50:06,851 - memory_profile7_log - INFO -     79     90.2 MiB      0.0 MiB       def _q_iterator():

2018-04-28 20:50:06,851 - memory_profile7_log - INFO -     80                                     # data = []

2018-04-28 20:50:06,851 - memory_profile7_log - INFO -     81    417.1 MiB    273.1 MiB           for row in rows:

2018-04-28 20:50:06,851 - memory_profile7_log - INFO -     82                                         # data.append(list(row))

2018-04-28 20:50:06,852 - memory_profile7_log - INFO -     83    417.1 MiB    -27.0 MiB               yield list(row)

2018-04-28 20:50:06,852 - memory_profile7_log - INFO -     84                             

2018-04-28 20:50:06,854 - memory_profile7_log - INFO -     85                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-28 20:50:06,854 - memory_profile7_log - INFO -     86    361.3 MiB    -55.9 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-28 20:50:06,859 - memory_profile7_log - INFO -     87                                 # del rows

2018-04-28 20:50:06,861 - memory_profile7_log - INFO -     88    361.3 MiB      0.0 MiB       del result

2018-04-28 20:50:06,862 - memory_profile7_log - INFO -     89                                 # del data

2018-04-28 20:50:06,862 - memory_profile7_log - INFO -     90    363.2 MiB      1.9 MiB       print df.info(memory_usage='deep')

2018-04-28 20:50:06,864 - memory_profile7_log - INFO -     91                             

2018-04-28 20:50:06,864 - memory_profile7_log - INFO -     92    363.2 MiB      0.0 MiB       return df

2018-04-28 20:50:06,865 - memory_profile7_log - INFO - 


2018-04-28 20:50:06,865 - memory_profile7_log - INFO - getting total: 297844 training data(genuine interest) for date: 2018-04-07
2018-04-28 20:50:06,865 - memory_profile7_log - INFO - Exiting: PoolWorker-3
2018-04-28 20:50:07,571 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-09
2018-04-28 20:50:25,430 - memory_profile7_log - INFO - <class 'pandas.core.frame.DataFrame'>
RangeIndex: 344288 entries, 0 to 344287
Data columns (total 5 columns):
date          344288 non-null datetime64[ns, UTC]
user_id       344288 non-null object
topic_id      344288 non-null object
is_general    344288 non-null bool
num           344288 non-null int64
dtypes: bool(1), datetime64[ns, UTC](1), int64(1), object(2)
memory usage: 87.2 MB

2018-04-28 20:50:25,430 - memory_profile7_log - INFO - None
2018-04-28 20:50:25,431 - memory_profile7_log - INFO - 

2018-04-28 20:50:25,433 - memory_profile7_log - INFO - Filename: D:\Kerjaan\python\Recommender\topic-recommender\CRON\legacy-train\legacy_bigquery.py


2018-04-28 20:50:25,433 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-28 20:50:25,434 - memory_profile7_log - INFO - ================================================

2018-04-28 20:50:25,434 - memory_profile7_log - INFO -     67     86.7 MiB     86.7 MiB   @profile

2018-04-28 20:50:25,434 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-28 20:50:25,434 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-28 20:50:25,434 - memory_profile7_log - INFO -     70                                 # https://www.sjoerdlangkemper.nl/2016/06/09/clearing-memory-in-python/

2018-04-28 20:50:25,436 - memory_profile7_log - INFO -     71     86.7 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-28 20:50:25,436 - memory_profile7_log - INFO -     72     86.7 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-28 20:50:25,437 - memory_profile7_log - INFO -     73                             

2018-04-28 20:50:25,437 - memory_profile7_log - INFO -     74     89.9 MiB      3.2 MiB       result  = client.query(query, job_config=job_config)

2018-04-28 20:50:25,437 - memory_profile7_log - INFO -     75     90.0 MiB      0.1 MiB       rows = result.result()

2018-04-28 20:50:25,437 - memory_profile7_log - INFO -     76                             

2018-04-28 20:50:25,438 - memory_profile7_log - INFO -     77     90.0 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-28 20:50:25,438 - memory_profile7_log - INFO -     78                                 

2018-04-28 20:50:25,444 - memory_profile7_log - INFO -     79     90.0 MiB      0.0 MiB       def _q_iterator():

2018-04-28 20:50:25,444 - memory_profile7_log - INFO -     80                                     # data = []

2018-04-28 20:50:25,446 - memory_profile7_log - INFO -     81    426.3 MiB    261.2 MiB           for row in rows:

2018-04-28 20:50:25,446 - memory_profile7_log - INFO -     82                                         # data.append(list(row))

2018-04-28 20:50:25,447 - memory_profile7_log - INFO -     83    426.3 MiB    -68.2 MiB               yield list(row)

2018-04-28 20:50:25,447 - memory_profile7_log - INFO -     84                             

2018-04-28 20:50:25,447 - memory_profile7_log - INFO -     85                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-28 20:50:25,447 - memory_profile7_log - INFO -     86    366.6 MiB    -59.7 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-28 20:50:25,448 - memory_profile7_log - INFO -     87                                 # del rows

2018-04-28 20:50:25,448 - memory_profile7_log - INFO -     88    366.6 MiB      0.0 MiB       del result

2018-04-28 20:50:25,450 - memory_profile7_log - INFO -     89                                 # del data

2018-04-28 20:50:25,451 - memory_profile7_log - INFO -     90    369.0 MiB      2.5 MiB       print df.info(memory_usage='deep')

2018-04-28 20:50:25,456 - memory_profile7_log - INFO -     91                             

2018-04-28 20:50:25,457 - memory_profile7_log - INFO -     92    369.0 MiB      0.0 MiB       return df

2018-04-28 20:50:25,457 - memory_profile7_log - INFO - 


2018-04-28 20:50:25,459 - memory_profile7_log - INFO - getting total: 344288 training data(genuine interest) for date: 2018-04-08
2018-04-28 20:50:25,460 - memory_profile7_log - INFO - Exiting: PoolWorker-1
2018-04-28 20:50:26,148 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-10
2018-04-28 20:50:35,825 - memory_profile7_log - INFO - <class 'pandas.core.frame.DataFrame'>
RangeIndex: 390831 entries, 0 to 390830
Data columns (total 5 columns):
date          390831 non-null datetime64[ns, UTC]
user_id       390831 non-null object
topic_id      390831 non-null object
is_general    390831 non-null bool
num           390831 non-null int64
dtypes: bool(1), datetime64[ns, UTC](1), int64(1), object(2)
memory usage: 99.2 MB

2018-04-28 20:50:35,826 - memory_profile7_log - INFO - None
2018-04-28 20:50:35,828 - memory_profile7_log - INFO - 

2018-04-28 20:50:35,828 - memory_profile7_log - INFO - Filename: D:\Kerjaan\python\Recommender\topic-recommender\CRON\legacy-train\legacy_bigquery.py


2018-04-28 20:50:35,828 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-28 20:50:35,829 - memory_profile7_log - INFO - ================================================

2018-04-28 20:50:35,831 - memory_profile7_log - INFO -     67     87.0 MiB     87.0 MiB   @profile

2018-04-28 20:50:35,831 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-28 20:50:35,832 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-28 20:50:35,832 - memory_profile7_log - INFO -     70                                 # https://www.sjoerdlangkemper.nl/2016/06/09/clearing-memory-in-python/

2018-04-28 20:50:35,832 - memory_profile7_log - INFO -     71     87.0 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-28 20:50:35,834 - memory_profile7_log - INFO -     72     87.0 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-28 20:50:35,834 - memory_profile7_log - INFO -     73                             

2018-04-28 20:50:35,834 - memory_profile7_log - INFO -     74     90.1 MiB      3.1 MiB       result  = client.query(query, job_config=job_config)

2018-04-28 20:50:35,835 - memory_profile7_log - INFO -     75     90.2 MiB      0.1 MiB       rows = result.result()

2018-04-28 20:50:35,838 - memory_profile7_log - INFO -     76                             

2018-04-28 20:50:35,838 - memory_profile7_log - INFO -     77     90.2 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-28 20:50:35,841 - memory_profile7_log - INFO -     78                                 

2018-04-28 20:50:35,841 - memory_profile7_log - INFO -     79     90.2 MiB      0.0 MiB       def _q_iterator():

2018-04-28 20:50:35,842 - memory_profile7_log - INFO -     80                                     # data = []

2018-04-28 20:50:35,842 - memory_profile7_log - INFO -     81    440.1 MiB    291.2 MiB           for row in rows:

2018-04-28 20:50:35,842 - memory_profile7_log - INFO -     82                                         # data.append(list(row))

2018-04-28 20:50:35,842 - memory_profile7_log - INFO -     83    440.1 MiB    -17.1 MiB               yield list(row)

2018-04-28 20:50:35,842 - memory_profile7_log - INFO -     84                             

2018-04-28 20:50:35,844 - memory_profile7_log - INFO -     85                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-28 20:50:35,844 - memory_profile7_log - INFO -     86    373.5 MiB    -66.6 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-28 20:50:35,845 - memory_profile7_log - INFO -     87                                 # del rows

2018-04-28 20:50:35,845 - memory_profile7_log - INFO -     88    373.5 MiB      0.0 MiB       del result

2018-04-28 20:50:35,845 - memory_profile7_log - INFO -     89                                 # del data

2018-04-28 20:50:35,845 - memory_profile7_log - INFO -     90    376.6 MiB      3.1 MiB       print df.info(memory_usage='deep')

2018-04-28 20:50:35,846 - memory_profile7_log - INFO -     91                             

2018-04-28 20:50:35,851 - memory_profile7_log - INFO -     92    376.6 MiB      0.0 MiB       return df

2018-04-28 20:50:35,852 - memory_profile7_log - INFO - 


2018-04-28 20:50:35,852 - memory_profile7_log - INFO - getting total: 390831 training data(genuine interest) for date: 2018-04-06
2018-04-28 20:50:35,852 - memory_profile7_log - INFO - Exiting: PoolWorker-4
2018-04-28 20:50:36,809 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-11
2018-04-28 20:50:56,466 - memory_profile7_log - INFO - <class 'pandas.core.frame.DataFrame'>
RangeIndex: 413536 entries, 0 to 413535
Data columns (total 5 columns):
date          413536 non-null datetime64[ns, UTC]
user_id       413536 non-null object
topic_id      413536 non-null object
is_general    413536 non-null bool
num           413536 non-null int64
dtypes: bool(1), datetime64[ns, UTC](1), int64(1), object(2)
memory usage: 104.9 MB

2018-04-28 20:50:56,467 - memory_profile7_log - INFO - None
2018-04-28 20:50:56,467 - memory_profile7_log - INFO - 

2018-04-28 20:50:56,469 - memory_profile7_log - INFO - Filename: D:\Kerjaan\python\Recommender\topic-recommender\CRON\legacy-train\legacy_bigquery.py


2018-04-28 20:50:56,469 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-28 20:50:56,470 - memory_profile7_log - INFO - ================================================

2018-04-28 20:50:56,470 - memory_profile7_log - INFO -     67     86.8 MiB     86.8 MiB   @profile

2018-04-28 20:50:56,470 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-28 20:50:56,471 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-28 20:50:56,471 - memory_profile7_log - INFO -     70                                 # https://www.sjoerdlangkemper.nl/2016/06/09/clearing-memory-in-python/

2018-04-28 20:50:56,473 - memory_profile7_log - INFO -     71     86.8 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-28 20:50:56,473 - memory_profile7_log - INFO -     72     86.8 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-28 20:50:56,473 - memory_profile7_log - INFO -     73                             

2018-04-28 20:50:56,473 - memory_profile7_log - INFO -     74     90.0 MiB      3.2 MiB       result  = client.query(query, job_config=job_config)

2018-04-28 20:50:56,473 - memory_profile7_log - INFO -     75     90.0 MiB      0.1 MiB       rows = result.result()

2018-04-28 20:50:56,474 - memory_profile7_log - INFO -     76                             

2018-04-28 20:50:56,474 - memory_profile7_log - INFO -     77     90.0 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-28 20:50:56,479 - memory_profile7_log - INFO -     78                                 

2018-04-28 20:50:56,480 - memory_profile7_log - INFO -     79     90.0 MiB      0.0 MiB       def _q_iterator():

2018-04-28 20:50:56,480 - memory_profile7_log - INFO -     80                                     # data = []

2018-04-28 20:50:56,482 - memory_profile7_log - INFO -     81    468.2 MiB    320.9 MiB           for row in rows:

2018-04-28 20:50:56,482 - memory_profile7_log - INFO -     82                                         # data.append(list(row))

2018-04-28 20:50:56,482 - memory_profile7_log - INFO -     83    468.2 MiB    -11.3 MiB               yield list(row)

2018-04-28 20:50:56,483 - memory_profile7_log - INFO -     84                             

2018-04-28 20:50:56,483 - memory_profile7_log - INFO -     85                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-28 20:50:56,484 - memory_profile7_log - INFO -     86    393.4 MiB    -74.8 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-28 20:50:56,484 - memory_profile7_log - INFO -     87                                 # del rows

2018-04-28 20:50:56,486 - memory_profile7_log - INFO -     88    393.4 MiB      0.0 MiB       del result

2018-04-28 20:50:56,486 - memory_profile7_log - INFO -     89                                 # del data

2018-04-28 20:50:56,486 - memory_profile7_log - INFO -     90    395.5 MiB      2.0 MiB       print df.info(memory_usage='deep')

2018-04-28 20:50:56,492 - memory_profile7_log - INFO -     91                             

2018-04-28 20:50:56,493 - memory_profile7_log - INFO -     92    395.5 MiB      0.0 MiB       return df

2018-04-28 20:50:56,493 - memory_profile7_log - INFO - 


2018-04-28 20:50:56,494 - memory_profile7_log - INFO - getting total: 413536 training data(genuine interest) for date: 2018-04-05
2018-04-28 20:50:56,494 - memory_profile7_log - INFO - Exiting: PoolWorker-2
2018-04-28 20:50:57,496 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-12
2018-04-28 20:52:06,874 - memory_profile7_log - INFO - <class 'pandas.core.frame.DataFrame'>
RangeIndex: 403532 entries, 0 to 403531
Data columns (total 5 columns):
date          403532 non-null datetime64[ns, UTC]
user_id       403532 non-null object
topic_id      403532 non-null object
is_general    403532 non-null bool
num           403532 non-null int64
dtypes: bool(1), datetime64[ns, UTC](1), int64(1), object(2)
memory usage: 102.2 MB

2018-04-28 20:52:06,875 - memory_profile7_log - INFO - None
2018-04-28 20:52:06,875 - memory_profile7_log - INFO - 

2018-04-28 20:52:06,875 - memory_profile7_log - INFO - Filename: D:\Kerjaan\python\Recommender\topic-recommender\CRON\legacy-train\legacy_bigquery.py


2018-04-28 20:52:06,877 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-28 20:52:06,877 - memory_profile7_log - INFO - ================================================

2018-04-28 20:52:06,878 - memory_profile7_log - INFO -     67    196.5 MiB    196.5 MiB   @profile

2018-04-28 20:52:06,878 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-28 20:52:06,878 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-28 20:52:06,880 - memory_profile7_log - INFO -     70                                 # https://www.sjoerdlangkemper.nl/2016/06/09/clearing-memory-in-python/

2018-04-28 20:52:06,880 - memory_profile7_log - INFO -     71    196.5 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-28 20:52:06,881 - memory_profile7_log - INFO -     72    196.5 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-28 20:52:06,881 - memory_profile7_log - INFO -     73                             

2018-04-28 20:52:06,881 - memory_profile7_log - INFO -     74    198.2 MiB      1.7 MiB       result  = client.query(query, job_config=job_config)

2018-04-28 20:52:06,881 - memory_profile7_log - INFO -     75    198.3 MiB      0.1 MiB       rows = result.result()

2018-04-28 20:52:06,881 - memory_profile7_log - INFO -     76                             

2018-04-28 20:52:06,882 - memory_profile7_log - INFO -     77    198.3 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-28 20:52:06,887 - memory_profile7_log - INFO -     78                                 

2018-04-28 20:52:06,888 - memory_profile7_log - INFO -     79    198.3 MiB      0.0 MiB       def _q_iterator():

2018-04-28 20:52:06,888 - memory_profile7_log - INFO -     80                                     # data = []

2018-04-28 20:52:06,890 - memory_profile7_log - INFO -     81    469.8 MiB    220.9 MiB           for row in rows:

2018-04-28 20:52:06,890 - memory_profile7_log - INFO -     82                                         # data.append(list(row))

2018-04-28 20:52:06,890 - memory_profile7_log - INFO -     83    469.8 MiB      2.0 MiB               yield list(row)

2018-04-28 20:52:06,891 - memory_profile7_log - INFO -     84                             

2018-04-28 20:52:06,891 - memory_profile7_log - INFO -     85                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-28 20:52:06,891 - memory_profile7_log - INFO -     86    396.4 MiB    -73.4 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-28 20:52:06,891 - memory_profile7_log - INFO -     87                                 # del rows

2018-04-28 20:52:06,892 - memory_profile7_log - INFO -     88    396.4 MiB      0.0 MiB       del result

2018-04-28 20:52:06,892 - memory_profile7_log - INFO -     89                                 # del data

2018-04-28 20:52:06,894 - memory_profile7_log - INFO -     90    399.2 MiB      2.8 MiB       print df.info(memory_usage='deep')

2018-04-28 20:52:06,894 - memory_profile7_log - INFO -     91                             

2018-04-28 20:52:06,894 - memory_profile7_log - INFO -     92    399.2 MiB      0.0 MiB       return df

2018-04-28 20:52:06,894 - memory_profile7_log - INFO - 


2018-04-28 20:52:06,894 - memory_profile7_log - INFO - getting total: 403532 training data(genuine interest) for date: 2018-04-09
2018-04-28 20:52:06,900 - memory_profile7_log - INFO - Exiting: PoolWorker-3
2018-04-28 20:52:07,782 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-13
2018-04-28 20:52:30,145 - memory_profile7_log - INFO - <class 'pandas.core.frame.DataFrame'>
RangeIndex: 427635 entries, 0 to 427634
Data columns (total 5 columns):
date          427635 non-null datetime64[ns, UTC]
user_id       427635 non-null object
topic_id      427635 non-null object
is_general    427635 non-null bool
num           427635 non-null int64
dtypes: bool(1), datetime64[ns, UTC](1), int64(1), object(2)
memory usage: 108.5 MB

2018-04-28 20:52:30,148 - memory_profile7_log - INFO - None
2018-04-28 20:52:30,148 - memory_profile7_log - INFO - 

2018-04-28 20:52:30,150 - memory_profile7_log - INFO - Filename: D:\Kerjaan\python\Recommender\topic-recommender\CRON\legacy-train\legacy_bigquery.py


2018-04-28 20:52:30,150 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-28 20:52:30,151 - memory_profile7_log - INFO - ================================================

2018-04-28 20:52:30,151 - memory_profile7_log - INFO -     67    220.4 MiB    220.4 MiB   @profile

2018-04-28 20:52:30,151 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-28 20:52:30,153 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-28 20:52:30,153 - memory_profile7_log - INFO -     70                                 # https://www.sjoerdlangkemper.nl/2016/06/09/clearing-memory-in-python/

2018-04-28 20:52:30,154 - memory_profile7_log - INFO -     71    220.4 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-28 20:52:30,154 - memory_profile7_log - INFO -     72    220.4 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-28 20:52:30,154 - memory_profile7_log - INFO -     73                             

2018-04-28 20:52:30,154 - memory_profile7_log - INFO -     74    222.0 MiB      1.6 MiB       result  = client.query(query, job_config=job_config)

2018-04-28 20:52:30,154 - memory_profile7_log - INFO -     75    222.0 MiB      0.0 MiB       rows = result.result()

2018-04-28 20:52:30,160 - memory_profile7_log - INFO -     76                             

2018-04-28 20:52:30,161 - memory_profile7_log - INFO -     77    222.0 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-28 20:52:30,161 - memory_profile7_log - INFO -     78                                 

2018-04-28 20:52:30,163 - memory_profile7_log - INFO -     79    222.0 MiB      0.0 MiB       def _q_iterator():

2018-04-28 20:52:30,163 - memory_profile7_log - INFO -     80                                     # data = []

2018-04-28 20:52:30,164 - memory_profile7_log - INFO -     81    473.6 MiB    205.9 MiB           for row in rows:

2018-04-28 20:52:30,164 - memory_profile7_log - INFO -     82                                         # data.append(list(row))

2018-04-28 20:52:30,164 - memory_profile7_log - INFO -     83    473.6 MiB     14.9 MiB               yield list(row)

2018-04-28 20:52:30,165 - memory_profile7_log - INFO -     84                             

2018-04-28 20:52:30,165 - memory_profile7_log - INFO -     85                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-28 20:52:30,167 - memory_profile7_log - INFO -     86    397.8 MiB    -75.7 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-28 20:52:30,167 - memory_profile7_log - INFO -     87                                 # del rows

2018-04-28 20:52:30,167 - memory_profile7_log - INFO -     88    397.8 MiB      0.0 MiB       del result

2018-04-28 20:52:30,167 - memory_profile7_log - INFO -     89                                 # del data

2018-04-28 20:52:30,168 - memory_profile7_log - INFO -     90    400.0 MiB      2.2 MiB       print df.info(memory_usage='deep')

2018-04-28 20:52:30,173 - memory_profile7_log - INFO -     91                             

2018-04-28 20:52:30,174 - memory_profile7_log - INFO -     92    400.0 MiB      0.0 MiB       return df

2018-04-28 20:52:30,174 - memory_profile7_log - INFO - 


2018-04-28 20:52:30,176 - memory_profile7_log - INFO - getting total: 427635 training data(genuine interest) for date: 2018-04-10
2018-04-28 20:52:30,176 - memory_profile7_log - INFO - Exiting: PoolWorker-1
2018-04-28 20:52:31,301 - memory_profile7_log - INFO - Collecting training data for date: 2018-04-14
2018-04-28 20:52:43,042 - memory_profile7_log - INFO - <class 'pandas.core.frame.DataFrame'>
RangeIndex: 428418 entries, 0 to 428417
Data columns (total 5 columns):
date          428418 non-null datetime64[ns, UTC]
user_id       428418 non-null object
topic_id      428418 non-null object
is_general    428418 non-null bool
num           428418 non-null int64
dtypes: bool(1), datetime64[ns, UTC](1), int64(1), object(2)
memory usage: 108.5 MB

2018-04-28 20:52:43,045 - memory_profile7_log - INFO - None
2018-04-28 20:52:43,046 - memory_profile7_log - INFO - 

2018-04-28 20:52:43,046 - memory_profile7_log - INFO - Filename: D:\Kerjaan\python\Recommender\topic-recommender\CRON\legacy-train\legacy_bigquery.py


2018-04-28 20:52:43,046 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-28 20:52:43,048 - memory_profile7_log - INFO - ================================================

2018-04-28 20:52:43,049 - memory_profile7_log - INFO -     67    241.6 MiB    241.6 MiB   @profile

2018-04-28 20:52:43,049 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-28 20:52:43,049 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-28 20:52:43,049 - memory_profile7_log - INFO -     70                                 # https://www.sjoerdlangkemper.nl/2016/06/09/clearing-memory-in-python/

2018-04-28 20:52:43,051 - memory_profile7_log - INFO -     71    241.6 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-28 20:52:43,051 - memory_profile7_log - INFO -     72    241.6 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-28 20:52:43,052 - memory_profile7_log - INFO -     73                             

2018-04-28 20:52:43,055 - memory_profile7_log - INFO -     74    243.2 MiB      1.6 MiB       result  = client.query(query, job_config=job_config)

2018-04-28 20:52:43,058 - memory_profile7_log - INFO -     75    243.3 MiB      0.0 MiB       rows = result.result()

2018-04-28 20:52:43,058 - memory_profile7_log - INFO -     76                             

2018-04-28 20:52:43,059 - memory_profile7_log - INFO -     77    243.3 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-28 20:52:43,059 - memory_profile7_log - INFO -     78                                 

2018-04-28 20:52:43,059 - memory_profile7_log - INFO -     79    243.3 MiB      0.0 MiB       def _q_iterator():

2018-04-28 20:52:43,059 - memory_profile7_log - INFO -     80                                     # data = []

2018-04-28 20:52:43,061 - memory_profile7_log - INFO -     81    472.0 MiB   -994.4 MiB           for row in rows:

2018-04-28 20:52:43,061 - memory_profile7_log - INFO -     82                                         # data.append(list(row))

2018-04-28 20:52:43,062 - memory_profile7_log - INFO -     83    472.0 MiB  -2342.6 MiB               yield list(row)

2018-04-28 20:52:43,062 - memory_profile7_log - INFO -     84                             

2018-04-28 20:52:43,062 - memory_profile7_log - INFO -     85                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-28 20:52:43,063 - memory_profile7_log - INFO -     86    398.8 MiB    -73.2 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-28 20:52:43,063 - memory_profile7_log - INFO -     87                                 # del rows

2018-04-28 20:52:43,068 - memory_profile7_log - INFO -     88    398.8 MiB      0.0 MiB       del result

2018-04-28 20:52:43,069 - memory_profile7_log - INFO -     89                                 # del data

2018-04-28 20:52:43,069 - memory_profile7_log - INFO -     90    401.4 MiB      2.6 MiB       print df.info(memory_usage='deep')

2018-04-28 20:52:43,071 - memory_profile7_log - INFO -     91                             

2018-04-28 20:52:43,072 - memory_profile7_log - INFO -     92    401.4 MiB      0.0 MiB       return df

2018-04-28 20:52:43,072 - memory_profile7_log - INFO - 


2018-04-28 20:52:43,072 - memory_profile7_log - INFO - getting total: 428418 training data(genuine interest) for date: 2018-04-11
2018-04-28 20:52:43,072 - memory_profile7_log - INFO - Exiting: PoolWorker-4
2018-04-28 20:53:06,464 - memory_profile7_log - INFO - <class 'pandas.core.frame.DataFrame'>
RangeIndex: 462579 entries, 0 to 462578
Data columns (total 5 columns):
date          462579 non-null datetime64[ns, UTC]
user_id       462579 non-null object
topic_id      462579 non-null object
is_general    462579 non-null bool
num           462579 non-null int64
dtypes: bool(1), datetime64[ns, UTC](1), int64(1), object(2)
memory usage: 117.0 MB

2018-04-28 20:53:06,466 - memory_profile7_log - INFO - None
2018-04-28 20:53:06,467 - memory_profile7_log - INFO - 

2018-04-28 20:53:06,469 - memory_profile7_log - INFO - Filename: D:\Kerjaan\python\Recommender\topic-recommender\CRON\legacy-train\legacy_bigquery.py


2018-04-28 20:53:06,470 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-28 20:53:06,470 - memory_profile7_log - INFO - ================================================

2018-04-28 20:53:06,470 - memory_profile7_log - INFO -     67    248.7 MiB    248.7 MiB   @profile

2018-04-28 20:53:06,471 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-28 20:53:06,473 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-28 20:53:06,473 - memory_profile7_log - INFO -     70                                 # https://www.sjoerdlangkemper.nl/2016/06/09/clearing-memory-in-python/

2018-04-28 20:53:06,473 - memory_profile7_log - INFO -     71    248.7 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-28 20:53:06,474 - memory_profile7_log - INFO -     72    248.7 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-28 20:53:06,477 - memory_profile7_log - INFO -     73                             

2018-04-28 20:53:06,479 - memory_profile7_log - INFO -     74    250.4 MiB      1.7 MiB       result  = client.query(query, job_config=job_config)

2018-04-28 20:53:06,480 - memory_profile7_log - INFO -     75    250.4 MiB      0.0 MiB       rows = result.result()

2018-04-28 20:53:06,480 - memory_profile7_log - INFO -     76                             

2018-04-28 20:53:06,480 - memory_profile7_log - INFO -     77    250.4 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-28 20:53:06,480 - memory_profile7_log - INFO -     78                                 

2018-04-28 20:53:06,483 - memory_profile7_log - INFO -     79    250.4 MiB      0.0 MiB       def _q_iterator():

2018-04-28 20:53:06,483 - memory_profile7_log - INFO -     80                                     # data = []

2018-04-28 20:53:06,483 - memory_profile7_log - INFO -     81    477.5 MiB     92.1 MiB           for row in rows:

2018-04-28 20:53:06,483 - memory_profile7_log - INFO -     82                                         # data.append(list(row))

2018-04-28 20:53:06,483 - memory_profile7_log - INFO -     83    477.5 MiB   -165.1 MiB               yield list(row)

2018-04-28 20:53:06,484 - memory_profile7_log - INFO -     84                             

2018-04-28 20:53:06,484 - memory_profile7_log - INFO -     85                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-28 20:53:06,486 - memory_profile7_log - INFO -     86    402.5 MiB    -74.9 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-28 20:53:06,489 - memory_profile7_log - INFO -     87                                 # del rows

2018-04-28 20:53:06,490 - memory_profile7_log - INFO -     88    402.5 MiB      0.0 MiB       del result

2018-04-28 20:53:06,490 - memory_profile7_log - INFO -     89                                 # del data

2018-04-28 20:53:06,492 - memory_profile7_log - INFO -     90    404.5 MiB      2.0 MiB       print df.info(memory_usage='deep')

2018-04-28 20:53:06,492 - memory_profile7_log - INFO -     91                             

2018-04-28 20:53:06,493 - memory_profile7_log - INFO -     92    404.5 MiB      0.0 MiB       return df

2018-04-28 20:53:06,493 - memory_profile7_log - INFO - 


2018-04-28 20:53:06,494 - memory_profile7_log - INFO - getting total: 462579 training data(genuine interest) for date: 2018-04-12
2018-04-28 20:53:06,494 - memory_profile7_log - INFO - Exiting: PoolWorker-2
2018-04-28 20:53:42,683 - memory_profile7_log - INFO - <class 'pandas.core.frame.DataFrame'>
RangeIndex: 253995 entries, 0 to 253994
Data columns (total 5 columns):
date          253995 non-null datetime64[ns, UTC]
user_id       253995 non-null object
topic_id      253995 non-null object
is_general    253995 non-null bool
num           253995 non-null int64
dtypes: bool(1), datetime64[ns, UTC](1), int64(1), object(2)
memory usage: 64.3 MB

2018-04-28 20:53:42,684 - memory_profile7_log - INFO - None
2018-04-28 20:53:42,684 - memory_profile7_log - INFO - 

2018-04-28 20:53:42,687 - memory_profile7_log - INFO - Filename: D:\Kerjaan\python\Recommender\topic-recommender\CRON\legacy-train\legacy_bigquery.py


2018-04-28 20:53:42,687 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-28 20:53:42,687 - memory_profile7_log - INFO - ================================================

2018-04-28 20:53:42,688 - memory_profile7_log - INFO -     67    257.3 MiB    257.3 MiB   @profile

2018-04-28 20:53:42,690 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-28 20:53:42,690 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-28 20:53:42,690 - memory_profile7_log - INFO -     70                                 # https://www.sjoerdlangkemper.nl/2016/06/09/clearing-memory-in-python/

2018-04-28 20:53:42,691 - memory_profile7_log - INFO -     71    257.3 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-28 20:53:42,691 - memory_profile7_log - INFO -     72    257.3 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-28 20:53:42,697 - memory_profile7_log - INFO -     73                             

2018-04-28 20:53:42,697 - memory_profile7_log - INFO -     74    257.9 MiB      0.6 MiB       result  = client.query(query, job_config=job_config)

2018-04-28 20:53:42,698 - memory_profile7_log - INFO -     75    257.9 MiB      0.0 MiB       rows = result.result()

2018-04-28 20:53:42,700 - memory_profile7_log - INFO -     76                             

2018-04-28 20:53:42,700 - memory_profile7_log - INFO -     77    257.9 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-28 20:53:42,700 - memory_profile7_log - INFO -     78                                 

2018-04-28 20:53:42,700 - memory_profile7_log - INFO -     79    257.9 MiB      0.0 MiB       def _q_iterator():

2018-04-28 20:53:42,700 - memory_profile7_log - INFO -     80                                     # data = []

2018-04-28 20:53:42,701 - memory_profile7_log - INFO -     81    391.7 MiB     78.3 MiB           for row in rows:

2018-04-28 20:53:42,701 - memory_profile7_log - INFO -     82                                         # data.append(list(row))

2018-04-28 20:53:42,703 - memory_profile7_log - INFO -     83    391.7 MiB    -48.3 MiB               yield list(row)

2018-04-28 20:53:42,703 - memory_profile7_log - INFO -     84                             

2018-04-28 20:53:42,703 - memory_profile7_log - INFO -     85                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-28 20:53:42,703 - memory_profile7_log - INFO -     86    347.3 MiB    -44.4 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-28 20:53:42,704 - memory_profile7_log - INFO -     87                                 # del rows

2018-04-28 20:53:42,707 - memory_profile7_log - INFO -     88    347.3 MiB      0.0 MiB       del result

2018-04-28 20:53:42,710 - memory_profile7_log - INFO -     89                                 # del data

2018-04-28 20:53:42,710 - memory_profile7_log - INFO -     90    349.1 MiB      1.8 MiB       print df.info(memory_usage='deep')

2018-04-28 20:53:42,710 - memory_profile7_log - INFO -     91                             

2018-04-28 20:53:42,710 - memory_profile7_log - INFO -     92    349.1 MiB      0.0 MiB       return df

2018-04-28 20:53:42,711 - memory_profile7_log - INFO - 


2018-04-28 20:53:42,713 - memory_profile7_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-28 20:53:42,713 - memory_profile7_log - INFO - Exiting: PoolWorker-1
2018-04-28 20:53:47,171 - memory_profile7_log - INFO - <class 'pandas.core.frame.DataFrame'>
RangeIndex: 361145 entries, 0 to 361144
Data columns (total 5 columns):
date          361145 non-null datetime64[ns, UTC]
user_id       361145 non-null object
topic_id      361145 non-null object
is_general    361145 non-null bool
num           361145 non-null int64
dtypes: bool(1), datetime64[ns, UTC](1), int64(1), object(2)
memory usage: 91.4 MB

2018-04-28 20:53:47,173 - memory_profile7_log - INFO - None
2018-04-28 20:53:47,174 - memory_profile7_log - INFO - 

2018-04-28 20:53:47,174 - memory_profile7_log - INFO - Filename: D:\Kerjaan\python\Recommender\topic-recommender\CRON\legacy-train\legacy_bigquery.py


2018-04-28 20:53:47,176 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-28 20:53:47,176 - memory_profile7_log - INFO - ================================================

2018-04-28 20:53:47,177 - memory_profile7_log - INFO -     67    254.8 MiB    254.8 MiB   @profile

2018-04-28 20:53:47,177 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-28 20:53:47,177 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-28 20:53:47,177 - memory_profile7_log - INFO -     70                                 # https://www.sjoerdlangkemper.nl/2016/06/09/clearing-memory-in-python/

2018-04-28 20:53:47,178 - memory_profile7_log - INFO -     71    254.8 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-28 20:53:47,178 - memory_profile7_log - INFO -     72    254.8 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-28 20:53:47,178 - memory_profile7_log - INFO -     73                             

2018-04-28 20:53:47,178 - memory_profile7_log - INFO -     74    255.2 MiB      0.5 MiB       result  = client.query(query, job_config=job_config)

2018-04-28 20:53:47,180 - memory_profile7_log - INFO -     75    255.3 MiB      0.0 MiB       rows = result.result()

2018-04-28 20:53:47,180 - memory_profile7_log - INFO -     76                             

2018-04-28 20:53:47,180 - memory_profile7_log - INFO -     77    255.3 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-28 20:53:47,184 - memory_profile7_log - INFO -     78                                 

2018-04-28 20:53:47,184 - memory_profile7_log - INFO -     79    255.3 MiB      0.0 MiB       def _q_iterator():

2018-04-28 20:53:47,187 - memory_profile7_log - INFO -     80                                     # data = []

2018-04-28 20:53:47,187 - memory_profile7_log - INFO -     81    436.1 MiB    114.1 MiB           for row in rows:

2018-04-28 20:53:47,187 - memory_profile7_log - INFO -     82                                         # data.append(list(row))

2018-04-28 20:53:47,188 - memory_profile7_log - INFO -     83    436.1 MiB    -50.2 MiB               yield list(row)

2018-04-28 20:53:47,188 - memory_profile7_log - INFO -     84                             

2018-04-28 20:53:47,188 - memory_profile7_log - INFO -     85                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-28 20:53:47,190 - memory_profile7_log - INFO -     86    371.2 MiB    -64.9 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-28 20:53:47,190 - memory_profile7_log - INFO -     87                                 # del rows

2018-04-28 20:53:47,190 - memory_profile7_log - INFO -     88    371.2 MiB      0.0 MiB       del result

2018-04-28 20:53:47,190 - memory_profile7_log - INFO -     89                                 # del data

2018-04-28 20:53:47,190 - memory_profile7_log - INFO -     90    373.5 MiB      2.3 MiB       print df.info(memory_usage='deep')

2018-04-28 20:53:47,191 - memory_profile7_log - INFO -     91                             

2018-04-28 20:53:47,196 - memory_profile7_log - INFO -     92    373.5 MiB      0.0 MiB       return df

2018-04-28 20:53:47,197 - memory_profile7_log - INFO - 


2018-04-28 20:53:47,197 - memory_profile7_log - INFO - getting total: 361145 training data(genuine interest) for date: 2018-04-13
2018-04-28 20:53:47,198 - memory_profile7_log - INFO - Exiting: PoolWorker-3
2018-04-28 20:53:48,357 - memory_profile7_log - INFO - len datalist: 10
2018-04-28 20:53:48,358 - memory_profile7_log - INFO - All data fetch multiprocess done!!
2018-04-28 20:53:48,358 - memory_profile7_log - INFO - Filename: .\legacy_bigquery.py


2018-04-28 20:53:48,359 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-28 20:53:48,361 - memory_profile7_log - INFO - ================================================

2018-04-28 20:53:48,361 - memory_profile7_log - INFO -    265     86.8 MiB     86.8 MiB   @profile

2018-04-28 20:53:48,362 - memory_profile7_log - INFO -    266                             def BQPreprocess(loadmp, cpu, date_generated, client, query_fit):

2018-04-28 20:53:48,364 - memory_profile7_log - INFO -    267     86.8 MiB      0.0 MiB       bq_client = client

2018-04-28 20:53:48,365 - memory_profile7_log - INFO -    268     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-28 20:53:48,365 - memory_profile7_log - INFO -    269                             

2018-04-28 20:53:48,369 - memory_profile7_log - INFO -    270     86.8 MiB      0.0 MiB       datalist = []

2018-04-28 20:53:48,371 - memory_profile7_log - INFO -    271                             

2018-04-28 20:53:48,371 - memory_profile7_log - INFO -    272     86.8 MiB      0.0 MiB       if loadmp:

2018-04-28 20:53:48,371 - memory_profile7_log - INFO -    273     86.8 MiB      0.0 MiB           logger.info("Starting data fetch multiprocess..")

2018-04-28 20:53:48,372 - memory_profile7_log - INFO -    274     86.8 MiB      0.0 MiB           logger.info("number of process: %d", len(date_generated))

2018-04-28 20:53:48,374 - memory_profile7_log - INFO -    275                             

2018-04-28 20:53:48,375 - memory_profile7_log - INFO -    276     87.5 MiB      0.7 MiB           pool = mp.Pool(processes=cpu)

2018-04-28 20:53:48,375 - memory_profile7_log - INFO -    277     87.6 MiB      0.0 MiB           multprocessA = [pool.apply_async(getBig, args=(ndate.strftime("%Y-%m-%d"), loadmp, query_fit, )) for ndate in date_generated]

2018-04-28 20:53:48,378 - memory_profile7_log - INFO -    278   1146.4 MiB   1058.9 MiB           output_multprocessA = [p.get() for p in multprocessA]

2018-04-28 20:53:48,380 - memory_profile7_log - INFO -    279                             

2018-04-28 20:53:48,381 - memory_profile7_log - INFO -    280   1146.5 MiB      0.0 MiB           for m in output_multprocessA:

2018-04-28 20:53:48,391 - memory_profile7_log - INFO -    281   1146.5 MiB      0.0 MiB               if m is not None:

2018-04-28 20:53:48,392 - memory_profile7_log - INFO -    282   1146.5 MiB      0.0 MiB                   if not m.empty:

2018-04-28 20:53:48,394 - memory_profile7_log - INFO -    283   1146.5 MiB      0.0 MiB                       datalist.append(m)

2018-04-28 20:53:48,394 - memory_profile7_log - INFO -    284                             

2018-04-28 20:53:48,394 - memory_profile7_log - INFO -    285   1146.5 MiB      0.0 MiB           pool.close()

2018-04-28 20:53:48,395 - memory_profile7_log - INFO -    286   1146.4 MiB     -0.0 MiB           pool.terminate()

2018-04-28 20:53:48,397 - memory_profile7_log - INFO -    287                             

2018-04-28 20:53:48,398 - memory_profile7_log - INFO -    288   1146.4 MiB      0.0 MiB           logger.info("len datalist: %d", len(datalist))

2018-04-28 20:53:48,401 - memory_profile7_log - INFO -    289   1146.4 MiB      0.0 MiB           logger.info("All data fetch multiprocess done!!")

2018-04-28 20:53:48,403 - memory_profile7_log - INFO -    290                                 else:

2018-04-28 20:53:48,404 - memory_profile7_log - INFO -    291                                     logger.info("Starting data fetch iterative...")

2018-04-28 20:53:48,404 - memory_profile7_log - INFO -    292                                     for ndate in date_generated:

2018-04-28 20:53:48,404 - memory_profile7_log - INFO -    293                                         tframe = getBig(ndate.strftime("%Y-%m-%d"), loadmp, query_fit)

2018-04-28 20:53:48,405 - memory_profile7_log - INFO -    294                                         if tframe is not None:

2018-04-28 20:53:48,405 - memory_profile7_log - INFO -    295                                             if not tframe.empty:

2018-04-28 20:53:48,407 - memory_profile7_log - INFO -    296                                                 datalist.append(tframe)

2018-04-28 20:53:48,407 - memory_profile7_log - INFO -    297                                         else: 

2018-04-28 20:53:48,408 - memory_profile7_log - INFO -    298                                             logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-28 20:53:48,413 - memory_profile7_log - INFO -    299                                     logger.info("len datalist: %d", len(datalist))

2018-04-28 20:53:48,414 - memory_profile7_log - INFO -    300                                     logger.info("All data fetch iterative done!!")

2018-04-28 20:53:48,414 - memory_profile7_log - INFO -    301                             

2018-04-28 20:53:48,415 - memory_profile7_log - INFO -    302   1146.4 MiB      0.0 MiB       return datalist

2018-04-28 20:53:48,415 - memory_profile7_log - INFO - 


2018-04-28 20:53:48,644 - memory_profile7_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-28 20:55:08,710 - memory_profile7_log - INFO - <class 'pandas.core.frame.DataFrame'>
RangeIndex: 230425 entries, 0 to 230424
Data columns (total 5 columns):
date          230425 non-null datetime64[ns, UTC]
user_id       230425 non-null object
topic_id      230425 non-null object
is_general    230425 non-null bool
num           230425 non-null int64
dtypes: bool(1), datetime64[ns, UTC](1), int64(1), object(2)
memory usage: 58.3 MB

2018-04-28 20:55:08,713 - memory_profile7_log - INFO - None
2018-04-28 20:55:08,713 - memory_profile7_log - INFO - 

2018-04-28 20:55:08,716 - memory_profile7_log - INFO - Filename: .\legacy_bigquery.py


2018-04-28 20:55:08,717 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-28 20:55:08,717 - memory_profile7_log - INFO - ================================================

2018-04-28 20:55:08,719 - memory_profile7_log - INFO -     67   1180.5 MiB   1180.5 MiB   @profile

2018-04-28 20:55:08,720 - memory_profile7_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-28 20:55:08,720 - memory_profile7_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-28 20:55:08,723 - memory_profile7_log - INFO -     70                                 # https://www.sjoerdlangkemper.nl/2016/06/09/clearing-memory-in-python/

2018-04-28 20:55:08,724 - memory_profile7_log - INFO -     71   1180.5 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-28 20:55:08,726 - memory_profile7_log - INFO -     72   1180.5 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-28 20:55:08,726 - memory_profile7_log - INFO -     73                             

2018-04-28 20:55:08,729 - memory_profile7_log - INFO -     74   1181.7 MiB      1.2 MiB       result  = client.query(query, job_config=job_config)

2018-04-28 20:55:08,730 - memory_profile7_log - INFO -     75   1181.7 MiB      0.0 MiB       rows = result.result()

2018-04-28 20:55:08,730 - memory_profile7_log - INFO -     76                             

2018-04-28 20:55:08,730 - memory_profile7_log - INFO -     77   1181.7 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-28 20:55:08,733 - memory_profile7_log - INFO -     78                                 

2018-04-28 20:55:08,736 - memory_profile7_log - INFO -     79   1181.7 MiB      0.0 MiB       def _q_iterator():

2018-04-28 20:55:08,736 - memory_profile7_log - INFO -     80                                     # data = []

2018-04-28 20:55:08,736 - memory_profile7_log - INFO -     81   1453.2 MiB    226.5 MiB           for row in rows:

2018-04-28 20:55:08,737 - memory_profile7_log - INFO -     82                                         # data.append(list(row))

2018-04-28 20:55:08,739 - memory_profile7_log - INFO -     83   1453.2 MiB    -36.7 MiB               yield list(row)

2018-04-28 20:55:08,740 - memory_profile7_log - INFO -     84                             

2018-04-28 20:55:08,740 - memory_profile7_log - INFO -     85                                 # df = pd.DataFrame(data, columns=col_name)

2018-04-28 20:55:08,742 - memory_profile7_log - INFO -     86   1458.6 MiB      5.4 MiB       df = pd.DataFrame( _q_iterator() , columns=col_name)

2018-04-28 20:55:08,743 - memory_profile7_log - INFO -     87                                 # del rows

2018-04-28 20:55:08,746 - memory_profile7_log - INFO -     88   1458.6 MiB      0.0 MiB       del result

2018-04-28 20:55:08,746 - memory_profile7_log - INFO -     89                                 # del data

2018-04-28 20:55:08,747 - memory_profile7_log - INFO -     90   1458.9 MiB      0.3 MiB       print df.info(memory_usage='deep')

2018-04-28 20:55:08,749 - memory_profile7_log - INFO -     91                             

2018-04-28 20:55:08,750 - memory_profile7_log - INFO -     92   1458.9 MiB      0.0 MiB       return df

2018-04-28 20:55:08,750 - memory_profile7_log - INFO - 


2018-04-28 20:55:08,750 - memory_profile7_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-28 20:55:08,765 - memory_profile7_log - INFO - loading time of: 4014228 total genuine-current interest data ~ take 394.880s
2018-04-28 20:55:08,766 - memory_profile7_log - INFO - Filename: .\legacy_bigquery.py


2018-04-28 20:55:08,766 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-28 20:55:08,766 - memory_profile7_log - INFO - ================================================

2018-04-28 20:55:08,767 - memory_profile7_log - INFO -    304     86.7 MiB     86.7 MiB   @profile

2018-04-28 20:55:08,767 - memory_profile7_log - INFO -    305                             def preprocess(loadmp, cpu, cd, query_fit, date_generated):

2018-04-28 20:55:08,769 - memory_profile7_log - INFO -    306     86.8 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-28 20:55:08,769 - memory_profile7_log - INFO -    307     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-28 20:55:08,769 - memory_profile7_log - INFO -    308                             

2018-04-28 20:55:08,770 - memory_profile7_log - INFO -    309                                 # ~~~ Begin collecting data ~~~

2018-04-28 20:55:08,772 - memory_profile7_log - INFO -    310     86.8 MiB      0.0 MiB       t0 = time.time()

2018-04-28 20:55:08,772 - memory_profile7_log - INFO -    311                                 

2018-04-28 20:55:08,773 - memory_profile7_log - INFO -    312   1146.4 MiB   1059.6 MiB       datalist = BQPreprocess(loadmp, cpu, date_generated, bq_client, query_fit)

2018-04-28 20:55:08,773 - memory_profile7_log - INFO -    313                             

2018-04-28 20:55:08,773 - memory_profile7_log - INFO -    314   1146.4 MiB      0.0 MiB       if len(datalist) > 1:

2018-04-28 20:55:08,779 - memory_profile7_log - INFO -    315   1324.8 MiB    178.4 MiB           big_frame = pd.concat(datalist)

2018-04-28 20:55:08,779 - memory_profile7_log - INFO -    316   1180.4 MiB   -144.4 MiB           del datalist

2018-04-28 20:55:08,779 - memory_profile7_log - INFO -    317                                 else:

2018-04-28 20:55:08,780 - memory_profile7_log - INFO -    318                                     big_frame = datalist

2018-04-28 20:55:08,782 - memory_profile7_log - INFO -    319   1180.5 MiB      0.1 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-28 20:55:08,782 - memory_profile7_log - INFO -    320                             

2018-04-28 20:55:08,783 - memory_profile7_log - INFO -    321                                 # ~ get current news interest ~

2018-04-28 20:55:08,783 - memory_profile7_log - INFO -    322   1180.5 MiB      0.0 MiB       if not cd:

2018-04-28 20:55:08,783 - memory_profile7_log - INFO -    323                                     logger.info("Collecting training data(current date interest)..")

2018-04-28 20:55:08,785 - memory_profile7_log - INFO -    324                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-28 20:55:08,785 - memory_profile7_log - INFO -    325                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-28 20:55:08,786 - memory_profile7_log - INFO -    326                                 else:

2018-04-28 20:55:08,786 - memory_profile7_log - INFO -    327   1180.5 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-28 20:55:08,786 - memory_profile7_log - INFO -    328   1180.5 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-28 20:55:08,789 - memory_profile7_log - INFO -    329                             

2018-04-28 20:55:08,790 - memory_profile7_log - INFO -    330                                     # safe handling of query parameter

2018-04-28 20:55:08,792 - memory_profile7_log - INFO -    331                                     query_params = [

2018-04-28 20:55:08,792 - memory_profile7_log - INFO -    332   1180.5 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-28 20:55:08,792 - memory_profile7_log - INFO -    333                                     ]

2018-04-28 20:55:08,793 - memory_profile7_log - INFO -    334                             

2018-04-28 20:55:08,793 - memory_profile7_log - INFO -    335   1180.5 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-28 20:55:08,795 - memory_profile7_log - INFO -    336   1458.9 MiB    278.4 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-28 20:55:08,795 - memory_profile7_log - INFO -    337   1458.9 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-28 20:55:08,795 - memory_profile7_log - INFO -    338                             

2018-04-28 20:55:08,796 - memory_profile7_log - INFO -    339   1458.9 MiB      0.0 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-28 20:55:08,796 - memory_profile7_log - INFO -    340   1458.9 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-28 20:55:08,796 - memory_profile7_log - INFO -    341   1458.9 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-28 20:55:08,796 - memory_profile7_log - INFO -    342                             

2018-04-28 20:55:08,796 - memory_profile7_log - INFO -    343   1458.9 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-28 20:55:08,798 - memory_profile7_log - INFO -    344   1458.9 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-28 20:55:08,798 - memory_profile7_log - INFO -    345                             

2018-04-28 20:55:08,798 - memory_profile7_log - INFO -    346   1458.9 MiB      0.0 MiB       return big_frame, current_frame

2018-04-28 20:55:08,803 - memory_profile7_log - INFO - 


2018-04-28 20:55:08,808 - memory_profile7_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-28 20:55:08,950 - memory_profile7_log - INFO - train on: 3783803 total history data(D(u, t))
2018-04-28 20:55:08,951 - memory_profile7_log - INFO - transform on: 230425 total current data(D(t))
2018-04-28 20:55:22,022 - memory_profile7_log - INFO - Len of model_fit: 3783803
2018-04-28 20:55:22,023 - memory_profile7_log - INFO - Len of df_dut: 3783803
2018-04-28 20:57:41,042 - memory_profile7_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-28 20:57:41,272 - memory_profile7_log - INFO - Total train time: 152.320s
2018-04-28 20:57:41,273 - memory_profile7_log - INFO - memory left before cleaning: 83.400 percent memory...
2018-04-28 20:57:41,273 - memory_profile7_log - INFO - cleaning up some objects...
2018-04-28 20:57:41,276 - memory_profile7_log - INFO - deleting df_dut...
2018-04-28 20:57:41,276 - memory_profile7_log - INFO - deleting df_dt...
2018-04-28 20:57:41,278 - memory_profile7_log - INFO - deleting df_input...
2018-04-28 20:57:41,289 - memory_profile7_log - INFO - deleting df_input_X...
2018-04-28 20:57:41,290 - memory_profile7_log - INFO - deleting df_current...
2018-04-28 20:57:41,292 - memory_profile7_log - INFO - deleting map_topic_isgeneral...
2018-04-28 20:57:41,562 - memory_profile7_log - INFO - deleting model_fit...
2018-04-28 20:57:41,563 - memory_profile7_log - INFO - deleting result...
2018-04-28 20:57:41,641 - memory_profile7_log - INFO - Filename: .\legacy_bigquery.py


2018-04-28 20:57:41,642 - memory_profile7_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-28 20:57:41,644 - memory_profile7_log - INFO - ================================================

2018-04-28 20:57:41,644 - memory_profile7_log - INFO -    104   1458.9 MiB   1458.9 MiB   @profile

2018-04-28 20:57:41,644 - memory_profile7_log - INFO -    105                             def main(df_input, df_current, current_date, G,

2018-04-28 20:57:41,645 - memory_profile7_log - INFO -    106                                      project_id, savetrain=False, multproc=True,

2018-04-28 20:57:41,645 - memory_profile7_log - INFO -    107                                      threshold=0, start_date=None, end_date=None,

2018-04-28 20:57:41,647 - memory_profile7_log - INFO -    108                                      saveto="datastore"):

2018-04-28 20:57:41,648 - memory_profile7_log - INFO -    109                                 """

2018-04-28 20:57:41,648 - memory_profile7_log - INFO -    110                                     Main cron method

2018-04-28 20:57:41,648 - memory_profile7_log - INFO -    111                                 """

2018-04-28 20:57:41,657 - memory_profile7_log - INFO -    112                                 # ~ Data Preprocessing ~

2018-04-28 20:57:41,657 - memory_profile7_log - INFO -    113                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-28 20:57:41,660 - memory_profile7_log - INFO -    114                                 # D(u, t)

2018-04-28 20:57:41,661 - memory_profile7_log - INFO -    115   1458.9 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-28 20:57:41,663 - memory_profile7_log - INFO -    116   1578.0 MiB    119.1 MiB       df_dut = df_input.copy(deep=True)

2018-04-28 20:57:41,664 - memory_profile7_log - INFO -    117   1578.0 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-28 20:57:41,667 - memory_profile7_log - INFO -    118                             

2018-04-28 20:57:41,668 - memory_profile7_log - INFO -    119                                 # D(t)

2018-04-28 20:57:41,670 - memory_profile7_log - INFO -    120   1585.1 MiB      7.0 MiB       df_dt = df_current.copy(deep=True)

2018-04-28 20:57:41,671 - memory_profile7_log - INFO -    121   1585.1 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-28 20:57:41,671 - memory_profile7_log - INFO -    122                             

2018-04-28 20:57:41,673 - memory_profile7_log - INFO -    123                                 # ~~~~~~ Begin train ~~~~~~

2018-04-28 20:57:41,680 - memory_profile7_log - INFO -    124   1585.1 MiB      0.0 MiB       t0 = time.time()

2018-04-28 20:57:41,680 - memory_profile7_log - INFO -    125   1585.1 MiB      0.0 MiB       logger.info("train on: %d total history data(D(u, t))", len(df_dut))

2018-04-28 20:57:41,680 - memory_profile7_log - INFO -    126   1585.1 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-28 20:57:41,683 - memory_profile7_log - INFO -    127                             

2018-04-28 20:57:41,684 - memory_profile7_log - INFO -    128                                 # instantiace class

2018-04-28 20:57:41,684 - memory_profile7_log - INFO -    129   1585.1 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-28 20:57:41,684 - memory_profile7_log - INFO -    130                             

2018-04-28 20:57:41,684 - memory_profile7_log - INFO -    131                                 # ~~ Fit ~~

2018-04-28 20:57:41,686 - memory_profile7_log - INFO -    132                                 #   handling genuine news interest < current date

2018-04-28 20:57:41,686 - memory_profile7_log - INFO -    133   1596.3 MiB     11.2 MiB       NB = BR.processX(df_dut)

2018-04-28 20:57:41,687 - memory_profile7_log - INFO -    134                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-28 20:57:41,687 - memory_profile7_log - INFO -    135                                 #   nanti dipindah ke class train utama

2018-04-28 20:57:41,687 - memory_profile7_log - INFO -    136   1744.5 MiB    148.2 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-28 20:57:41,691 - memory_profile7_log - INFO -    137                                 """

2018-04-28 20:57:41,693 - memory_profile7_log - INFO -    138                                     num_y = total global click for category=ci on periode t

2018-04-28 20:57:41,694 - memory_profile7_log - INFO -    139                                     num_x = total click from user_U for category=ci on periode t

2018-04-28 20:57:41,694 - memory_profile7_log - INFO -    140                                 """

2018-04-28 20:57:41,694 - memory_profile7_log - INFO -    141   1744.5 MiB      0.0 MiB       fitby_sigmant = True

2018-04-28 20:57:41,696 - memory_profile7_log - INFO -    142   1744.5 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-28 20:57:41,697 - memory_profile7_log - INFO -    143   1744.5 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-28 20:57:41,697 - memory_profile7_log - INFO -    144   1881.6 MiB    137.1 MiB                            'is_general']]

2018-04-28 20:57:41,698 - memory_profile7_log - INFO -    145   1881.6 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-28 20:57:41,700 - memory_profile7_log - INFO -    146   1881.6 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-28 20:57:41,703 - memory_profile7_log - INFO -    147   2232.2 MiB    350.6 MiB                          verbose=False)

2018-04-28 20:57:41,703 - memory_profile7_log - INFO -    148   2232.2 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-28 20:57:41,704 - memory_profile7_log - INFO -    149   2232.2 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-28 20:57:41,704 - memory_profile7_log - INFO -    150                             

2018-04-28 20:57:41,706 - memory_profile7_log - INFO -    151                                 # ~~ and Transform ~~

2018-04-28 20:57:41,710 - memory_profile7_log - INFO -    152                                 #   handling current news interest == current date

2018-04-28 20:57:41,713 - memory_profile7_log - INFO -    153   2232.2 MiB      0.0 MiB       if df_dt.empty:

2018-04-28 20:57:41,713 - memory_profile7_log - INFO -    154                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-28 20:57:41,717 - memory_profile7_log - INFO -    155                                     return None

2018-04-28 20:57:41,717 - memory_profile7_log - INFO -    156   2232.3 MiB      0.1 MiB       NB = BR.processX(df_dt)

2018-04-28 20:57:41,719 - memory_profile7_log - INFO -    157   2242.8 MiB     10.6 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-28 20:57:41,720 - memory_profile7_log - INFO -    158                             

2018-04-28 20:57:41,720 - memory_profile7_log - INFO -    159   2242.8 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-28 20:57:41,720 - memory_profile7_log - INFO -    160   2103.7 MiB   -139.2 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-28 20:57:41,723 - memory_profile7_log - INFO -    161   2103.7 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-28 20:57:41,724 - memory_profile7_log - INFO -    162   2103.7 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-28 20:57:41,726 - memory_profile7_log - INFO -    163   2266.3 MiB    162.6 MiB                                                     verbose=False)

2018-04-28 20:57:41,726 - memory_profile7_log - INFO -    164                                 # ~~~ filter is general and specific topic ~~~

2018-04-28 20:57:41,726 - memory_profile7_log - INFO -    165                                 # the idea is just we need to rerank every topic according

2018-04-28 20:57:41,726 - memory_profile7_log - INFO -    166                                 # user_id and and is_general by p0_posterior

2018-04-28 20:57:41,727 - memory_profile7_log - INFO -    167   2266.3 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-28 20:57:41,729 - memory_profile7_log - INFO -    168   2298.8 MiB     32.5 MiB                                     'is_general']].groupby(['topic_id',

2018-04-28 20:57:41,729 - memory_profile7_log - INFO -    169   2266.3 MiB    -32.4 MiB                                                             'is_general']

2018-04-28 20:57:41,729 - memory_profile7_log - INFO -    170                                                                                      ).size().to_frame().reset_index()

2018-04-28 20:57:41,730 - memory_profile7_log - INFO -    171                             

2018-04-28 20:57:41,733 - memory_profile7_log - INFO -    172                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-28 20:57:41,736 - memory_profile7_log - INFO -    173                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-28 20:57:41,737 - memory_profile7_log - INFO -    174   2269.1 MiB      2.8 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-28 20:57:41,739 - memory_profile7_log - INFO -    175                             

2018-04-28 20:57:41,743 - memory_profile7_log - INFO -    176                                 # ~ start by provide rank for each topic type ~

2018-04-28 20:57:41,746 - memory_profile7_log - INFO -    177   2408.5 MiB    139.4 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-28 20:57:41,746 - memory_profile7_log - INFO -    178   2431.3 MiB     22.8 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-28 20:57:41,747 - memory_profile7_log - INFO -    179                             

2018-04-28 20:57:41,750 - memory_profile7_log - INFO -    180                                 # ~ set threshold to filter output

2018-04-28 20:57:41,750 - memory_profile7_log - INFO -    181   2431.3 MiB      0.0 MiB       if threshold > 0:

2018-04-28 20:57:41,752 - memory_profile7_log - INFO -    182   2431.3 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-28 20:57:41,753 - memory_profile7_log - INFO -    183   2434.1 MiB      2.8 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-28 20:57:41,753 - memory_profile7_log - INFO -    184   2339.4 MiB    -94.7 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-28 20:57:41,753 - memory_profile7_log - INFO -    185                             

2018-04-28 20:57:41,756 - memory_profile7_log - INFO -    186                                 # print "\n", model_transform[['user_id', 'topic_id', 'p0_posterior',

2018-04-28 20:57:41,759 - memory_profile7_log - INFO -    187                                 #                             'is_general', 'rank']].loc[model_transform['user_id'].str.contains('1610c8e6b6466b-05a6ac1ba8c1b8-4323461-100200-1',

2018-04-28 20:57:41,759 - memory_profile7_log - INFO -    188                                 #                                                                                                 case=False)].head(45)

2018-04-28 20:57:41,759 - memory_profile7_log - INFO -    189                                 # print "len(model_transform): %d" % len(model_transform)

2018-04-28 20:57:41,760 - memory_profile7_log - INFO -    190                             

2018-04-28 20:57:41,760 - memory_profile7_log - INFO -    191   2339.4 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-28 20:57:41,762 - memory_profile7_log - INFO -    192   2339.4 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-28 20:57:41,762 - memory_profile7_log - INFO -    193                             

2018-04-28 20:57:41,762 - memory_profile7_log - INFO -    194   2339.4 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-28 20:57:41,763 - memory_profile7_log - INFO -    195                             

2018-04-28 20:57:41,763 - memory_profile7_log - INFO -    196   2339.4 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-28 20:57:41,767 - memory_profile7_log - INFO -    197   2339.4 MiB      0.0 MiB       del df_dut

2018-04-28 20:57:41,769 - memory_profile7_log - INFO -    198   2339.4 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-28 20:57:41,769 - memory_profile7_log - INFO -    199   2339.4 MiB      0.0 MiB       del df_dt

2018-04-28 20:57:41,772 - memory_profile7_log - INFO -    200   2339.4 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-28 20:57:41,772 - memory_profile7_log - INFO -    201   2339.4 MiB      0.0 MiB       del df_input

2018-04-28 20:57:41,776 - memory_profile7_log - INFO -    202   2339.4 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-28 20:57:41,778 - memory_profile7_log - INFO -    203   2330.6 MiB     -8.8 MiB       del df_input_X

2018-04-28 20:57:41,779 - memory_profile7_log - INFO -    204   2330.6 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-28 20:57:41,779 - memory_profile7_log - INFO -    205   2330.6 MiB      0.0 MiB       del df_current

2018-04-28 20:57:41,782 - memory_profile7_log - INFO -    206   2330.6 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-28 20:57:41,782 - memory_profile7_log - INFO -    207   2330.6 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-28 20:57:41,785 - memory_profile7_log - INFO -    208   2330.6 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-28 20:57:41,785 - memory_profile7_log - INFO -    209   1980.5 MiB   -350.0 MiB       del model_fit

2018-04-28 20:57:41,786 - memory_profile7_log - INFO -    210   1980.5 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-28 20:57:41,786 - memory_profile7_log - INFO -    211   1980.5 MiB      0.0 MiB       del result

2018-04-28 20:57:41,789 - memory_profile7_log - INFO -    212   1980.5 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-28 20:57:41,792 - memory_profile7_log - INFO -    213                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-28 20:57:41,792 - memory_profile7_log - INFO -    214                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-28 20:57:41,792 - memory_profile7_log - INFO -    215   1980.5 MiB      0.0 MiB       if savetrain:

2018-04-28 20:57:41,793 - memory_profile7_log - INFO -    216                                     model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-28 20:57:41,793 - memory_profile7_log - INFO -    217                                     del model_transform

2018-04-28 20:57:41,795 - memory_profile7_log - INFO -    218                                     logger.info("deleting model_transform...")

2018-04-28 20:57:41,795 - memory_profile7_log - INFO -    219                                     logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-28 20:57:41,796 - memory_profile7_log - INFO -    220                             

2018-04-28 20:57:41,796 - memory_profile7_log - INFO -    221                                     logger.info("Begin saving trained data...")

2018-04-28 20:57:41,798 - memory_profile7_log - INFO -    222                                     # print "\n", model_transform.head(5)

2018-04-28 20:57:41,805 - memory_profile7_log - INFO -    223                                     # ~ Place your code to save the training model here ~

2018-04-28 20:57:41,805 - memory_profile7_log - INFO -    224                                     if str(saveto).lower() == "datastore":

2018-04-28 20:57:41,809 - memory_profile7_log - INFO -    225                                         logger.info("Using google datastore as storage...")

2018-04-28 20:57:41,811 - memory_profile7_log - INFO -    226                                         if multproc:

2018-04-28 20:57:41,811 - memory_profile7_log - INFO -    227                                             mh.saveDatastoreMP(model_transformsv)

2018-04-28 20:57:41,812 - memory_profile7_log - INFO -    228                                             # mh.saveDataStorePutMulti(model_transform[['user_id', 'topic_id', 'is_general', 'rank']])

2018-04-28 20:57:41,812 - memory_profile7_log - INFO -    229                                         else:

2018-04-28 20:57:41,815 - memory_profile7_log - INFO -    230                                             mh.saveDatastore(model_transformsv)

2018-04-28 20:57:41,815 - memory_profile7_log - INFO -    231                                             

2018-04-28 20:57:41,818 - memory_profile7_log - INFO -    232                                     elif str(saveto).lower() == "elastic":

2018-04-28 20:57:41,819 - memory_profile7_log - INFO -    233                                         logger.info("Using ElasticSearch as storage...")

2018-04-28 20:57:41,819 - memory_profile7_log - INFO -    234                                         mh.saveElasticS(model_transformsv)

2018-04-28 20:57:41,819 - memory_profile7_log - INFO -    235                             

2018-04-28 20:57:41,819 - memory_profile7_log - INFO -    236   1980.5 MiB      0.0 MiB       return

2018-04-28 20:57:41,821 - memory_profile7_log - INFO - 


2018-04-28 20:57:41,826 - memory_profile7_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~

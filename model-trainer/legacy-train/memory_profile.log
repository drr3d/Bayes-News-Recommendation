2018-04-27 06:35:57,176 - memory_profile_log - INFO - Generating date range with N: 3
2018-04-27 06:35:57,178 - memory_profile_log - INFO - date_generated: 
2018-04-27 06:35:57,178 - memory_profile_log - INFO -  
2018-04-27 06:35:57,178 - memory_profile_log - INFO - [datetime.datetime(2018, 4, 12, 0, 0), datetime.datetime(2018, 4, 13, 0, 0), datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-27 06:35:57,180 - memory_profile_log - INFO - 

2018-04-27 06:35:57,180 - memory_profile_log - INFO - using current date: 2018-04-15
2018-04-27 06:35:57,180 - memory_profile_log - INFO - using start date: 2018-04-12 00:00:00
2018-04-27 06:35:57,180 - memory_profile_log - INFO - using end date: 2018-04-14
2018-04-27 06:35:57,292 - memory_profile_log - INFO - Starting data fetch iterative...
2018-04-27 06:35:57,293 - memory_profile_log - INFO - Collecting training data for date: 2018-04-12
2018-04-27 06:41:40,969 - memory_profile_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 06:41:40,970 - memory_profile_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 06:41:40,971 - memory_profile_log - INFO - ================================================

2018-04-27 06:41:40,973 - memory_profile_log - INFO -     54     87.3 MiB     87.3 MiB   @profile

2018-04-27 06:41:40,974 - memory_profile_log - INFO -     55                             def load_bigquery(client, query, job_config):

2018-04-27 06:41:40,976 - memory_profile_log - INFO -     56                                 """

2018-04-27 06:41:40,976 - memory_profile_log - INFO -     57                                     Bigquery to dataframe

2018-04-27 06:41:40,977 - memory_profile_log - INFO -     58                                 """

2018-04-27 06:41:40,979 - memory_profile_log - INFO -     59     87.3 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 06:41:40,980 - memory_profile_log - INFO -     60     87.3 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 06:41:40,982 - memory_profile_log - INFO -     61                             

2018-04-27 06:41:40,983 - memory_profile_log - INFO -     62    395.2 MiB    308.0 MiB       df = client.query(query, job_config=job_config).result().to_dataframe()

2018-04-27 06:41:40,983 - memory_profile_log - INFO -     63    409.6 MiB     14.4 MiB       df_sh = df.copy(deep=True)

2018-04-27 06:41:40,984 - memory_profile_log - INFO -     64    395.5 MiB    -14.1 MiB       del df

2018-04-27 06:41:40,986 - memory_profile_log - INFO -     65                             

2018-04-27 06:41:40,986 - memory_profile_log - INFO -     66    395.5 MiB      0.0 MiB       return df_sh

2018-04-27 06:41:40,987 - memory_profile_log - INFO - 


2018-04-27 06:41:40,990 - memory_profile_log - INFO - getting total: 462579 training data(genuine interest) for date: 2018-04-12
2018-04-27 06:41:40,992 - memory_profile_log - INFO - Collecting training data for date: 2018-04-13
2018-04-27 06:46:24,096 - memory_profile_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 06:46:24,098 - memory_profile_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 06:46:24,098 - memory_profile_log - INFO - ================================================

2018-04-27 06:46:24,101 - memory_profile_log - INFO -     54    395.5 MiB    395.5 MiB   @profile

2018-04-27 06:46:24,101 - memory_profile_log - INFO -     55                             def load_bigquery(client, query, job_config):

2018-04-27 06:46:24,102 - memory_profile_log - INFO -     56                                 """

2018-04-27 06:46:24,105 - memory_profile_log - INFO -     57                                     Bigquery to dataframe

2018-04-27 06:46:24,107 - memory_profile_log - INFO -     58                                 """

2018-04-27 06:46:24,108 - memory_profile_log - INFO -     59    395.5 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 06:46:24,111 - memory_profile_log - INFO -     60    395.5 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 06:46:24,111 - memory_profile_log - INFO -     61                             

2018-04-27 06:46:24,112 - memory_profile_log - INFO -     62    492.6 MiB     97.1 MiB       df = client.query(query, job_config=job_config).result().to_dataframe()

2018-04-27 06:46:24,114 - memory_profile_log - INFO -     63    503.6 MiB     11.0 MiB       df_sh = df.copy(deep=True)

2018-04-27 06:46:24,115 - memory_profile_log - INFO -     64    492.6 MiB    -11.0 MiB       del df

2018-04-27 06:46:24,115 - memory_profile_log - INFO -     65                             

2018-04-27 06:46:24,118 - memory_profile_log - INFO -     66    492.6 MiB      0.0 MiB       return df_sh

2018-04-27 06:46:24,122 - memory_profile_log - INFO - 


2018-04-27 06:46:24,124 - memory_profile_log - INFO - getting total: 361145 training data(genuine interest) for date: 2018-04-13
2018-04-27 06:46:24,125 - memory_profile_log - INFO - Collecting training data for date: 2018-04-14
2018-04-27 06:49:40,921 - memory_profile_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 06:49:40,927 - memory_profile_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 06:49:40,927 - memory_profile_log - INFO - ================================================

2018-04-27 06:49:40,930 - memory_profile_log - INFO -     54    492.6 MiB    492.6 MiB   @profile

2018-04-27 06:49:40,930 - memory_profile_log - INFO -     55                             def load_bigquery(client, query, job_config):

2018-04-27 06:49:40,931 - memory_profile_log - INFO -     56                                 """

2018-04-27 06:49:40,933 - memory_profile_log - INFO -     57                                     Bigquery to dataframe

2018-04-27 06:49:40,934 - memory_profile_log - INFO -     58                                 """

2018-04-27 06:49:40,934 - memory_profile_log - INFO -     59    492.6 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 06:49:40,937 - memory_profile_log - INFO -     60    492.6 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 06:49:40,937 - memory_profile_log - INFO -     61                             

2018-04-27 06:49:40,938 - memory_profile_log - INFO -     62    563.1 MiB     70.5 MiB       df = client.query(query, job_config=job_config).result().to_dataframe()

2018-04-27 06:49:40,940 - memory_profile_log - INFO -     63    571.1 MiB      8.0 MiB       df_sh = df.copy(deep=True)

2018-04-27 06:49:40,940 - memory_profile_log - INFO -     64    563.4 MiB     -7.8 MiB       del df

2018-04-27 06:49:40,941 - memory_profile_log - INFO -     65                             

2018-04-27 06:49:40,943 - memory_profile_log - INFO -     66    563.4 MiB      0.0 MiB       return df_sh

2018-04-27 06:49:40,944 - memory_profile_log - INFO - 


2018-04-27 06:49:40,947 - memory_profile_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-27 06:49:40,947 - memory_profile_log - INFO - len datalist: 3
2018-04-27 06:49:40,948 - memory_profile_log - INFO - All data fetch iterative done!!
2018-04-27 06:49:41,058 - memory_profile_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-27 06:52:37,802 - memory_profile_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 06:52:37,802 - memory_profile_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 06:52:37,803 - memory_profile_log - INFO - ================================================

2018-04-27 06:52:37,805 - memory_profile_log - INFO -     54    582.6 MiB    582.6 MiB   @profile

2018-04-27 06:52:37,806 - memory_profile_log - INFO -     55                             def load_bigquery(client, query, job_config):

2018-04-27 06:52:37,808 - memory_profile_log - INFO -     56                                 """

2018-04-27 06:52:37,809 - memory_profile_log - INFO -     57                                     Bigquery to dataframe

2018-04-27 06:52:37,809 - memory_profile_log - INFO -     58                                 """

2018-04-27 06:52:37,811 - memory_profile_log - INFO -     59    582.6 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 06:52:37,811 - memory_profile_log - INFO -     60    582.6 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 06:52:37,815 - memory_profile_log - INFO -     61                             

2018-04-27 06:52:37,815 - memory_profile_log - INFO -     62    681.2 MiB     98.6 MiB       df = client.query(query, job_config=job_config).result().to_dataframe()

2018-04-27 06:52:37,818 - memory_profile_log - INFO -     63    688.2 MiB      7.0 MiB       df_sh = df.copy(deep=True)

2018-04-27 06:52:37,819 - memory_profile_log - INFO -     64    681.2 MiB     -7.0 MiB       del df

2018-04-27 06:52:37,819 - memory_profile_log - INFO -     65                             

2018-04-27 06:52:37,821 - memory_profile_log - INFO -     66    681.2 MiB      0.0 MiB       return df_sh

2018-04-27 06:52:37,822 - memory_profile_log - INFO - 


2018-04-27 06:52:37,822 - memory_profile_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-27 06:52:37,834 - memory_profile_log - INFO - loading time of: 1308144 total genuine-current interest data ~ take 1000.541s
2018-04-27 06:52:37,845 - memory_profile_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 06:52:37,845 - memory_profile_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 06:52:37,845 - memory_profile_log - INFO - ================================================

2018-04-27 06:52:37,848 - memory_profile_log - INFO -    225     87.1 MiB     87.1 MiB   @profile

2018-04-27 06:52:37,848 - memory_profile_log - INFO -    226                             def preprocess(loadmp, cpu, cd, query_fit, date_generated):

2018-04-27 06:52:37,849 - memory_profile_log - INFO -    227     87.3 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-27 06:52:37,854 - memory_profile_log - INFO -    228     87.3 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-27 06:52:37,855 - memory_profile_log - INFO -    229                             

2018-04-27 06:52:37,855 - memory_profile_log - INFO -    230                                 # ~~~ Begin collecting data ~~~

2018-04-27 06:52:37,857 - memory_profile_log - INFO -    231     87.3 MiB      0.0 MiB       t0 = time.time()

2018-04-27 06:52:37,858 - memory_profile_log - INFO -    232     87.3 MiB      0.0 MiB       datalist = []

2018-04-27 06:52:37,858 - memory_profile_log - INFO -    233                             

2018-04-27 06:52:37,861 - memory_profile_log - INFO -    234    492.6 MiB      0.0 MiB       def _getBig(procdate, client):

2018-04-27 06:52:37,861 - memory_profile_log - INFO -    235    492.6 MiB      0.0 MiB           logger.info("Collecting training data for date: %s", procdate)

2018-04-27 06:52:37,865 - memory_profile_log - INFO -    236                                     # ~ get genuine news interest ~

2018-04-27 06:52:37,865 - memory_profile_log - INFO -    237    492.6 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-27 06:52:37,868 - memory_profile_log - INFO -    238                             

2018-04-27 06:52:37,868 - memory_profile_log - INFO -    239                                     # safe handling of query parameter

2018-04-27 06:52:37,874 - memory_profile_log - INFO -    240                                     query_params = [

2018-04-27 06:52:37,878 - memory_profile_log - INFO -    241    492.6 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', procdate)

2018-04-27 06:52:37,878 - memory_profile_log - INFO -    242                                     ]

2018-04-27 06:52:37,878 - memory_profile_log - INFO -    243                             

2018-04-27 06:52:37,880 - memory_profile_log - INFO -    244    492.6 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-27 06:52:37,881 - memory_profile_log - INFO -    245    563.4 MiB    476.1 MiB           temp_df = load_bigquery(client, query_fit + query_fit_where, job_config)

2018-04-27 06:52:37,881 - memory_profile_log - INFO -    246                             

2018-04-27 06:52:37,881 - memory_profile_log - INFO -    247    563.4 MiB      0.0 MiB           if temp_df.empty:

2018-04-27 06:52:37,881 - memory_profile_log - INFO -    248                                         logger.info("%s data is empty!", procdate)

2018-04-27 06:52:37,885 - memory_profile_log - INFO -    249                                         return None

2018-04-27 06:52:37,887 - memory_profile_log - INFO -    250                                     else:

2018-04-27 06:52:37,888 - memory_profile_log - INFO -    251    563.4 MiB      0.0 MiB               logger.info("getting total: %d training data(genuine interest) for date: %s" % (len(temp_df), procdate))

2018-04-27 06:52:37,890 - memory_profile_log - INFO -    252    563.4 MiB      0.0 MiB               if loadmp:

2018-04-27 06:52:37,890 - memory_profile_log - INFO -    253                                             logger.info("Exiting: %s", mp.current_process().name)

2018-04-27 06:52:37,891 - memory_profile_log - INFO -    254    563.4 MiB      0.0 MiB               return temp_df

2018-04-27 06:52:37,891 - memory_profile_log - INFO -    255                             

2018-04-27 06:52:37,894 - memory_profile_log - INFO -    256     87.3 MiB      0.0 MiB       if loadmp:

2018-04-27 06:52:37,897 - memory_profile_log - INFO -    257                                     logger.info("Starting data fetch multiprocess..")

2018-04-27 06:52:37,898 - memory_profile_log - INFO -    258                                     logger.info("number of process: %d", len(date_generated))

2018-04-27 06:52:37,900 - memory_profile_log - INFO -    259                             

2018-04-27 06:52:37,900 - memory_profile_log - INFO -    260                                     pool = mp.Pool(processes=cpu)

2018-04-27 06:52:37,901 - memory_profile_log - INFO -    261                                     # multprocessA = [pool.apply(_getBig, args=(ndate.strftime("%Y-%m-%d"), )) for ndate in date_generated]

2018-04-27 06:52:37,903 - memory_profile_log - INFO -    262                                     multprocessA = [pool.apply_async(_getBig, args=(ndate.strftime("%Y-%m-%d"), bq_client, )) for ndate in date_generated]

2018-04-27 06:52:37,903 - memory_profile_log - INFO -    263                                     output_multprocessA = [p.get() for p in multprocessA]

2018-04-27 06:52:37,904 - memory_profile_log - INFO -    264                             

2018-04-27 06:52:37,904 - memory_profile_log - INFO -    265                                     for m in output_multprocessA:

2018-04-27 06:52:37,904 - memory_profile_log - INFO -    266                                         if m is not None:

2018-04-27 06:52:37,907 - memory_profile_log - INFO -    267                                             if not m.empty:

2018-04-27 06:52:37,911 - memory_profile_log - INFO -    268                                                 datalist.append(m)

2018-04-27 06:52:37,911 - memory_profile_log - INFO -    269                             

2018-04-27 06:52:37,911 - memory_profile_log - INFO -    270                                     logger.info("len datalist: %d", len(datalist))

2018-04-27 06:52:37,913 - memory_profile_log - INFO -    271                                     logger.info("All data fetch multiprocess done!!")

2018-04-27 06:52:37,914 - memory_profile_log - INFO -    272                                 else:

2018-04-27 06:52:37,914 - memory_profile_log - INFO -    273     87.3 MiB      0.0 MiB           logger.info("Starting data fetch iterative...")

2018-04-27 06:52:37,915 - memory_profile_log - INFO -    274    563.4 MiB      0.0 MiB           for ndate in date_generated:

2018-04-27 06:52:37,918 - memory_profile_log - INFO -    275    563.4 MiB      0.0 MiB               tframe = _getBig(ndate.strftime("%Y-%m-%d"), bq_client)

2018-04-27 06:52:37,918 - memory_profile_log - INFO -    276    563.4 MiB      0.0 MiB               if tframe is not None:

2018-04-27 06:52:37,921 - memory_profile_log - INFO -    277    563.4 MiB      0.0 MiB                   if not tframe.empty:

2018-04-27 06:52:37,921 - memory_profile_log - INFO -    278    563.4 MiB      0.0 MiB                       datalist.append(tframe)

2018-04-27 06:52:37,923 - memory_profile_log - INFO -    279                                         else: 

2018-04-27 06:52:37,923 - memory_profile_log - INFO -    280                                             logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-27 06:52:37,924 - memory_profile_log - INFO -    281    563.4 MiB      0.0 MiB           logger.info("len datalist: %d", len(datalist))

2018-04-27 06:52:37,924 - memory_profile_log - INFO -    282    563.4 MiB      0.0 MiB           logger.info("All data fetch iterative done!!")

2018-04-27 06:52:37,928 - memory_profile_log - INFO -    283                             

2018-04-27 06:52:37,928 - memory_profile_log - INFO -    284    563.4 MiB      0.0 MiB       if len(datalist) > 1:

2018-04-27 06:52:37,930 - memory_profile_log - INFO -    285    613.9 MiB     50.5 MiB           big_frame = pd.concat(datalist)

2018-04-27 06:52:37,931 - memory_profile_log - INFO -    286    582.4 MiB    -31.4 MiB           del datalist

2018-04-27 06:52:37,933 - memory_profile_log - INFO -    287                                 else:

2018-04-27 06:52:37,933 - memory_profile_log - INFO -    288                                     big_frame = datalist

2018-04-27 06:52:37,936 - memory_profile_log - INFO -    289    582.6 MiB      0.1 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-27 06:52:37,936 - memory_profile_log - INFO -    290                             

2018-04-27 06:52:37,937 - memory_profile_log - INFO -    291                                 # ~ get current news interest ~

2018-04-27 06:52:37,940 - memory_profile_log - INFO -    292    582.6 MiB      0.0 MiB       if not cd:

2018-04-27 06:52:37,940 - memory_profile_log - INFO -    293                                     logger.info("Collecting training data(current date interest)..")

2018-04-27 06:52:37,941 - memory_profile_log - INFO -    294                                     current_frame = load_bigquery(bq_client, query_transform, job_config)

2018-04-27 06:52:37,943 - memory_profile_log - INFO -    295                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-27 06:52:37,944 - memory_profile_log - INFO -    296                                 else:

2018-04-27 06:52:37,944 - memory_profile_log - INFO -    297    582.6 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-27 06:52:37,944 - memory_profile_log - INFO -    298    582.6 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-27 06:52:37,946 - memory_profile_log - INFO -    299                             

2018-04-27 06:52:37,947 - memory_profile_log - INFO -    300                                     # safe handling of query parameter

2018-04-27 06:52:37,950 - memory_profile_log - INFO -    301                                     query_params = [

2018-04-27 06:52:37,950 - memory_profile_log - INFO -    302    582.6 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-27 06:52:37,953 - memory_profile_log - INFO -    303                                     ]

2018-04-27 06:52:37,953 - memory_profile_log - INFO -    304                             

2018-04-27 06:52:37,953 - memory_profile_log - INFO -    305    582.6 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-27 06:52:37,954 - memory_profile_log - INFO -    306    681.2 MiB     98.6 MiB           current_frame = load_bigquery(bq_client, query_fit + query_fit_where, job_config)

2018-04-27 06:52:37,954 - memory_profile_log - INFO -    307    681.2 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-27 06:52:37,957 - memory_profile_log - INFO -    308                             

2018-04-27 06:52:37,963 - memory_profile_log - INFO -    309    681.3 MiB      0.1 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-27 06:52:37,964 - memory_profile_log - INFO -    310    681.3 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-27 06:52:37,970 - memory_profile_log - INFO -    311    681.3 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-27 06:52:37,970 - memory_profile_log - INFO -    312                             

2018-04-27 06:52:37,970 - memory_profile_log - INFO -    313    681.3 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-27 06:52:37,973 - memory_profile_log - INFO -    314    681.3 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-27 06:52:37,976 - memory_profile_log - INFO -    315                             

2018-04-27 06:52:37,977 - memory_profile_log - INFO -    316    681.3 MiB      0.0 MiB       del bq_client

2018-04-27 06:52:37,979 - memory_profile_log - INFO -    317    681.3 MiB      0.0 MiB       return big_frame, current_frame

2018-04-27 06:52:37,980 - memory_profile_log - INFO - 


2018-04-27 06:52:37,983 - memory_profile_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-27 06:52:38,052 - memory_profile_log - INFO - train on: 1077719 total history data(D(u, t))
2018-04-27 06:52:38,053 - memory_profile_log - INFO - transform on: 230425 total current data(D(t))
2018-04-27 06:52:42,645 - memory_profile_log - INFO - Len of model_fit: 1077719
2018-04-27 06:52:42,647 - memory_profile_log - INFO - Len of df_dut: 1077719
2018-04-27 06:54:12,332 - memory_profile_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-27 06:54:12,448 - memory_profile_log - INFO - Total train time: 94.397s
2018-04-27 06:54:12,450 - memory_profile_log - INFO - memory left before cleaning: 61.000 percent memory...
2018-04-27 06:54:12,450 - memory_profile_log - INFO - cleaning up some objects...
2018-04-27 06:54:12,451 - memory_profile_log - INFO - deleting df_dut...
2018-04-27 06:54:12,453 - memory_profile_log - INFO - deleting df_dt...
2018-04-27 06:54:12,453 - memory_profile_log - INFO - deleting df_input...
2018-04-27 06:54:12,464 - memory_profile_log - INFO - deleting df_input_X...
2018-04-27 06:54:12,466 - memory_profile_log - INFO - deleting df_current...
2018-04-27 06:54:12,467 - memory_profile_log - INFO - deleting map_topic_isgeneral...
2018-04-27 06:54:12,532 - memory_profile_log - INFO - deleting model_fit...
2018-04-27 06:54:12,532 - memory_profile_log - INFO - deleting result...
2018-04-27 06:54:12,558 - memory_profile_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 06:54:12,559 - memory_profile_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 06:54:12,559 - memory_profile_log - INFO - ================================================

2018-04-27 06:54:12,559 - memory_profile_log - INFO -     79    671.6 MiB    671.6 MiB   @profile

2018-04-27 06:54:12,561 - memory_profile_log - INFO -     80                             def main(df_input, df_current, current_date, G,

2018-04-27 06:54:12,562 - memory_profile_log - INFO -     81                                      project_id, savetrain=False, multproc=True,

2018-04-27 06:54:12,563 - memory_profile_log - INFO -     82                                      threshold=0, start_date=None, end_date=None,

2018-04-27 06:54:12,565 - memory_profile_log - INFO -     83                                      saveto="datastore"):

2018-04-27 06:54:12,565 - memory_profile_log - INFO -     84                                 """

2018-04-27 06:54:12,565 - memory_profile_log - INFO -     85                                     Main cron method

2018-04-27 06:54:12,565 - memory_profile_log - INFO -     86                                 """

2018-04-27 06:54:12,569 - memory_profile_log - INFO -     87                                 # ~ Data Preprocessing ~

2018-04-27 06:54:12,569 - memory_profile_log - INFO -     88                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-27 06:54:12,572 - memory_profile_log - INFO -     89                                 # D(u, t)

2018-04-27 06:54:12,572 - memory_profile_log - INFO -     90    671.6 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-27 06:54:12,572 - memory_profile_log - INFO -     91    705.5 MiB     33.9 MiB       df_dut = df_input.copy(deep=True)

2018-04-27 06:54:12,572 - memory_profile_log - INFO -     92    705.5 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-27 06:54:12,573 - memory_profile_log - INFO -     93                             

2018-04-27 06:54:12,575 - memory_profile_log - INFO -     94                                 # D(t)

2018-04-27 06:54:12,575 - memory_profile_log - INFO -     95    712.5 MiB      7.0 MiB       df_dt = df_current.copy(deep=True)

2018-04-27 06:54:12,575 - memory_profile_log - INFO -     96    712.5 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-27 06:54:12,575 - memory_profile_log - INFO -     97                             

2018-04-27 06:54:12,576 - memory_profile_log - INFO -     98                                 # ~~~~~~ Begin train ~~~~~~

2018-04-27 06:54:12,582 - memory_profile_log - INFO -     99    712.5 MiB      0.0 MiB       t0 = time.time()

2018-04-27 06:54:12,582 - memory_profile_log - INFO -    100    712.5 MiB      0.0 MiB       logger.info("train on: %d total history data(D(u, t))", len(df_dut))

2018-04-27 06:54:12,584 - memory_profile_log - INFO -    101    712.5 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-27 06:54:12,585 - memory_profile_log - INFO -    102                             

2018-04-27 06:54:12,585 - memory_profile_log - INFO -    103                                 # instantiace class

2018-04-27 06:54:12,586 - memory_profile_log - INFO -    104    712.5 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-27 06:54:12,588 - memory_profile_log - INFO -    105                             

2018-04-27 06:54:12,588 - memory_profile_log - INFO -    106                                 # ~~ Fit ~~

2018-04-27 06:54:12,588 - memory_profile_log - INFO -    107                                 #   handling genuine news interest < current date

2018-04-27 06:54:12,592 - memory_profile_log - INFO -    108    714.7 MiB      2.2 MiB       NB = BR.processX(df_dut)

2018-04-27 06:54:12,592 - memory_profile_log - INFO -    109                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-27 06:54:12,596 - memory_profile_log - INFO -    110                                 #   nanti dipindah ke class train utama

2018-04-27 06:54:12,598 - memory_profile_log - INFO -    111    757.2 MiB     42.5 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-27 06:54:12,598 - memory_profile_log - INFO -    112                                 """

2018-04-27 06:54:12,598 - memory_profile_log - INFO -    113                                     num_y = total global click for category=ci on periode t

2018-04-27 06:54:12,601 - memory_profile_log - INFO -    114                                     num_x = total click from user_U for category=ci on periode t

2018-04-27 06:54:12,602 - memory_profile_log - INFO -    115                                 """

2018-04-27 06:54:12,605 - memory_profile_log - INFO -    116    757.2 MiB      0.0 MiB       fitby_sigmant = True

2018-04-27 06:54:12,605 - memory_profile_log - INFO -    117    757.2 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-27 06:54:12,607 - memory_profile_log - INFO -    118    757.2 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-27 06:54:12,608 - memory_profile_log - INFO -    119    799.3 MiB     42.1 MiB                            'is_general']]

2018-04-27 06:54:12,608 - memory_profile_log - INFO -    120    799.3 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-27 06:54:12,608 - memory_profile_log - INFO -    121    799.3 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-27 06:54:12,609 - memory_profile_log - INFO -    122    897.2 MiB     97.9 MiB                          verbose=False)

2018-04-27 06:54:12,609 - memory_profile_log - INFO -    123    897.2 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-27 06:54:12,615 - memory_profile_log - INFO -    124    897.2 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-27 06:54:12,615 - memory_profile_log - INFO -    125                             

2018-04-27 06:54:12,618 - memory_profile_log - INFO -    126                                 # ~~ and Transform ~~

2018-04-27 06:54:12,618 - memory_profile_log - INFO -    127                                 #   handling current news interest == current date

2018-04-27 06:54:12,618 - memory_profile_log - INFO -    128    897.2 MiB      0.0 MiB       if df_dt.empty:

2018-04-27 06:54:12,621 - memory_profile_log - INFO -    129                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-27 06:54:12,621 - memory_profile_log - INFO -    130                                     return None

2018-04-27 06:54:12,621 - memory_profile_log - INFO -    131    897.3 MiB      0.1 MiB       NB = BR.processX(df_dt)

2018-04-27 06:54:12,625 - memory_profile_log - INFO -    132    907.9 MiB     10.6 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-27 06:54:12,625 - memory_profile_log - INFO -    133                             

2018-04-27 06:54:12,630 - memory_profile_log - INFO -    134    907.9 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-27 06:54:12,630 - memory_profile_log - INFO -    135    874.6 MiB    -33.3 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-27 06:54:12,631 - memory_profile_log - INFO -    136    874.6 MiB      0.0 MiB       model_transform = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-27 06:54:12,631 - memory_profile_log - INFO -    137    874.6 MiB      0.0 MiB                                      fitted_model=model_fit,

2018-04-27 06:54:12,632 - memory_profile_log - INFO -    138    893.1 MiB     18.5 MiB                                      verbose=False)

2018-04-27 06:54:12,638 - memory_profile_log - INFO -    139                                 # ~~~ filter is general and specific topic ~~~

2018-04-27 06:54:12,638 - memory_profile_log - INFO -    140                                 # the idea is just we need to rerank every topic according

2018-04-27 06:54:12,641 - memory_profile_log - INFO -    141                                 # user_id and and is_general by p0_posterior

2018-04-27 06:54:12,642 - memory_profile_log - INFO -    142    893.1 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-27 06:54:12,642 - memory_profile_log - INFO -    143    902.4 MiB      9.3 MiB                                     'is_general']].groupby(['topic_id',

2018-04-27 06:54:12,644 - memory_profile_log - INFO -    144    893.1 MiB     -9.2 MiB                                                             'is_general']

2018-04-27 06:54:12,645 - memory_profile_log - INFO -    145                                                                                      ).size().to_frame().reset_index()

2018-04-27 06:54:12,647 - memory_profile_log - INFO -    146                             

2018-04-27 06:54:12,648 - memory_profile_log - INFO -    147                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-27 06:54:12,650 - memory_profile_log - INFO -    148                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-27 06:54:12,651 - memory_profile_log - INFO -    149    893.1 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-27 06:54:12,651 - memory_profile_log - INFO -    150                             

2018-04-27 06:54:12,651 - memory_profile_log - INFO -    151                                 # ~ start by provide rank for each topic type ~

2018-04-27 06:54:12,654 - memory_profile_log - INFO -    152    958.4 MiB     65.2 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-27 06:54:12,654 - memory_profile_log - INFO -    153    966.1 MiB      7.7 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-27 06:54:12,654 - memory_profile_log - INFO -    154                             

2018-04-27 06:54:12,655 - memory_profile_log - INFO -    155                                 # ~ set threshold to filter output

2018-04-27 06:54:12,655 - memory_profile_log - INFO -    156    966.1 MiB      0.0 MiB       if threshold > 0:

2018-04-27 06:54:12,657 - memory_profile_log - INFO -    157    966.1 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-27 06:54:12,658 - memory_profile_log - INFO -    158    966.1 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-27 06:54:12,663 - memory_profile_log - INFO -    159    956.6 MiB     -9.5 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-27 06:54:12,664 - memory_profile_log - INFO -    160                             

2018-04-27 06:54:12,664 - memory_profile_log - INFO -    161                                 # print "\n", model_transform[['user_id', 'topic_id', 'p0_posterior',

2018-04-27 06:54:12,665 - memory_profile_log - INFO -    162                                 #                             'is_general', 'rank']].loc[model_transform['user_id'].str.contains('1610c8e6b6466b-05a6ac1ba8c1b8-4323461-100200-1',

2018-04-27 06:54:12,667 - memory_profile_log - INFO -    163                                 #                                                                                                 case=False)].head(45)

2018-04-27 06:54:12,667 - memory_profile_log - INFO -    164                                 # print "len(model_transform): %d" % len(model_transform)

2018-04-27 06:54:12,670 - memory_profile_log - INFO -    165                             

2018-04-27 06:54:12,671 - memory_profile_log - INFO -    166    956.6 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-27 06:54:12,671 - memory_profile_log - INFO -    167    956.6 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-27 06:54:12,673 - memory_profile_log - INFO -    168                             

2018-04-27 06:54:12,674 - memory_profile_log - INFO -    169    956.6 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-27 06:54:12,674 - memory_profile_log - INFO -    170                             

2018-04-27 06:54:12,674 - memory_profile_log - INFO -    171    956.6 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-27 06:54:12,674 - memory_profile_log - INFO -    172    956.6 MiB      0.0 MiB       del df_dut

2018-04-27 06:54:12,676 - memory_profile_log - INFO -    173    956.6 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-27 06:54:12,677 - memory_profile_log - INFO -    174    956.6 MiB      0.0 MiB       del df_dt

2018-04-27 06:54:12,681 - memory_profile_log - INFO -    175    956.6 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-27 06:54:12,683 - memory_profile_log - INFO -    176    956.6 MiB      0.0 MiB       del df_input

2018-04-27 06:54:12,684 - memory_profile_log - INFO -    177    956.6 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-27 06:54:12,684 - memory_profile_log - INFO -    178    947.8 MiB     -8.8 MiB       del df_input_X

2018-04-27 06:54:12,684 - memory_profile_log - INFO -    179    947.8 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-27 06:54:12,687 - memory_profile_log - INFO -    180    947.8 MiB      0.0 MiB       del df_current

2018-04-27 06:54:12,687 - memory_profile_log - INFO -    181    947.8 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-27 06:54:12,688 - memory_profile_log - INFO -    182    947.8 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-27 06:54:12,688 - memory_profile_log - INFO -    183    947.8 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-27 06:54:12,691 - memory_profile_log - INFO -    184    848.1 MiB    -99.7 MiB       del model_fit

2018-04-27 06:54:12,694 - memory_profile_log - INFO -    185    848.1 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-27 06:54:12,694 - memory_profile_log - INFO -    186    848.1 MiB      0.0 MiB       del result

2018-04-27 06:54:12,697 - memory_profile_log - INFO -    187    848.1 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-27 06:54:12,697 - memory_profile_log - INFO -    188                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-27 06:54:12,697 - memory_profile_log - INFO -    189                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-27 06:54:12,698 - memory_profile_log - INFO -    190    848.1 MiB      0.0 MiB       if savetrain:

2018-04-27 06:54:12,698 - memory_profile_log - INFO -    191                                     model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-27 06:54:12,700 - memory_profile_log - INFO -    192                                     del model_transform

2018-04-27 06:54:12,703 - memory_profile_log - INFO -    193                                     logger.info("deleting model_transform...")

2018-04-27 06:54:12,703 - memory_profile_log - INFO -    194                                     logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-27 06:54:12,706 - memory_profile_log - INFO -    195                             

2018-04-27 06:54:12,706 - memory_profile_log - INFO -    196                                     logger.info("Begin saving trained data...")

2018-04-27 06:54:12,707 - memory_profile_log - INFO -    197                                     # print "\n", model_transform.head(5)

2018-04-27 06:54:12,707 - memory_profile_log - INFO -    198                                     # ~ Place your code to save the training model here ~

2018-04-27 06:54:12,709 - memory_profile_log - INFO -    199                                     if str(saveto).lower() == "datastore":

2018-04-27 06:54:12,709 - memory_profile_log - INFO -    200                                         logger.info("Using google datastore as storage...")

2018-04-27 06:54:12,710 - memory_profile_log - INFO -    201                                         if multproc:

2018-04-27 06:54:12,710 - memory_profile_log - INFO -    202                                             mh.saveDatastoreMP(model_transformsv)

2018-04-27 06:54:12,716 - memory_profile_log - INFO -    203                                             # mh.saveDataStorePutMulti(model_transform[['user_id', 'topic_id', 'is_general', 'rank']])

2018-04-27 06:54:12,716 - memory_profile_log - INFO -    204                                         else:

2018-04-27 06:54:12,717 - memory_profile_log - INFO -    205                                             mh.saveDatastore(model_transformsv)

2018-04-27 06:54:12,719 - memory_profile_log - INFO -    206                                             

2018-04-27 06:54:12,720 - memory_profile_log - INFO -    207                                     elif str(saveto).lower() == "elastic":

2018-04-27 06:54:12,721 - memory_profile_log - INFO -    208                                         logger.info("Using ElasticSearch as storage...")

2018-04-27 06:54:12,723 - memory_profile_log - INFO -    209                                         mh.saveElasticS(model_transformsv)

2018-04-27 06:54:12,727 - memory_profile_log - INFO -    210                             

2018-04-27 06:54:12,727 - memory_profile_log - INFO -    211                                     # need save sigma_nt for daily train

2018-04-27 06:54:12,730 - memory_profile_log - INFO -    212                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-27 06:54:12,730 - memory_profile_log - INFO -    213                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-27 06:54:12,730 - memory_profile_log - INFO -    214                                     if start_date and end_date:

2018-04-27 06:54:12,732 - memory_profile_log - INFO -    215                                         if not fitby_sigmant:

2018-04-27 06:54:12,732 - memory_profile_log - INFO -    216                                             logging.info("Saving sigma Nt...")

2018-04-27 06:54:12,733 - memory_profile_log - INFO -    217                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-27 06:54:12,733 - memory_profile_log - INFO -    218                                             save_sigma_nt['start_date'] = start_date

2018-04-27 06:54:12,733 - memory_profile_log - INFO -    219                                             save_sigma_nt['end_date'] = end_date

2018-04-27 06:54:12,734 - memory_profile_log - INFO -    220                                             print save_sigma_nt.head(5)

2018-04-27 06:54:12,737 - memory_profile_log - INFO -    221                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-27 06:54:12,737 - memory_profile_log - INFO -    222                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-27 06:54:12,740 - memory_profile_log - INFO -    223    848.1 MiB      0.0 MiB       return

2018-04-27 06:54:12,740 - memory_profile_log - INFO - 


2018-04-27 06:54:12,740 - memory_profile_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~

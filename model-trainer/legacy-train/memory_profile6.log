2018-04-27 09:58:05,220 - memory_profile6_log - INFO - Generating date range with N: 3
2018-04-27 09:58:05,223 - memory_profile6_log - INFO - date_generated: 
2018-04-27 09:58:05,223 - memory_profile6_log - INFO -  
2018-04-27 09:58:05,223 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 12, 0, 0), datetime.datetime(2018, 4, 13, 0, 0), datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-27 09:58:05,224 - memory_profile6_log - INFO - 

2018-04-27 09:58:05,224 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-27 09:58:05,226 - memory_profile6_log - INFO - using start date: 2018-04-12 00:00:00
2018-04-27 09:58:05,229 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-27 09:58:05,345 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-27 09:58:05,345 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-12
2018-04-27 10:03:50,742 - memory_profile6_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 10:03:50,743 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 10:03:50,744 - memory_profile6_log - INFO - ================================================

2018-04-27 10:03:50,746 - memory_profile6_log - INFO -     54     86.8 MiB     86.8 MiB   @profile

2018-04-27 10:03:50,746 - memory_profile6_log - INFO -     55                             def load_bigquery(client, query, job_config):

2018-04-27 10:03:50,749 - memory_profile6_log - INFO -     56                                 """

2018-04-27 10:03:50,750 - memory_profile6_log - INFO -     57                                     Bigquery to dataframe

2018-04-27 10:03:50,752 - memory_profile6_log - INFO -     58                                 """

2018-04-27 10:03:50,756 - memory_profile6_log - INFO -     59     86.8 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 10:03:50,757 - memory_profile6_log - INFO -     60     86.8 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 10:03:50,759 - memory_profile6_log - INFO -     61                             

2018-04-27 10:03:50,760 - memory_profile6_log - INFO -     62     90.0 MiB      3.1 MiB       result = client.query(query, job_config=job_config)  # .result()

2018-04-27 10:03:50,762 - memory_profile6_log - INFO -     63    394.5 MiB    304.6 MiB       df = result.to_dataframe()

2018-04-27 10:03:50,763 - memory_profile6_log - INFO -     64    394.5 MiB      0.0 MiB       return df

2018-04-27 10:03:50,763 - memory_profile6_log - INFO - 


2018-04-27 10:03:50,766 - memory_profile6_log - INFO - getting total: 462579 training data(genuine interest) for date: 2018-04-12
2018-04-27 10:03:50,767 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-13
2018-04-27 10:08:26,009 - memory_profile6_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 10:08:26,012 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 10:08:26,012 - memory_profile6_log - INFO - ================================================

2018-04-27 10:08:26,013 - memory_profile6_log - INFO -     54    394.5 MiB    394.5 MiB   @profile

2018-04-27 10:08:26,013 - memory_profile6_log - INFO -     55                             def load_bigquery(client, query, job_config):

2018-04-27 10:08:26,015 - memory_profile6_log - INFO -     56                                 """

2018-04-27 10:08:26,016 - memory_profile6_log - INFO -     57                                     Bigquery to dataframe

2018-04-27 10:08:26,016 - memory_profile6_log - INFO -     58                                 """

2018-04-27 10:08:26,017 - memory_profile6_log - INFO -     59    394.5 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 10:08:26,019 - memory_profile6_log - INFO -     60    394.5 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 10:08:26,019 - memory_profile6_log - INFO -     61                             

2018-04-27 10:08:26,023 - memory_profile6_log - INFO -     62    394.5 MiB      0.0 MiB       result = client.query(query, job_config=job_config)  # .result()

2018-04-27 10:08:26,023 - memory_profile6_log - INFO -     63    492.5 MiB     97.9 MiB       df = result.to_dataframe()

2018-04-27 10:08:26,025 - memory_profile6_log - INFO -     64    492.5 MiB      0.0 MiB       return df

2018-04-27 10:08:26,026 - memory_profile6_log - INFO - 


2018-04-27 10:08:26,028 - memory_profile6_log - INFO - getting total: 361145 training data(genuine interest) for date: 2018-04-13
2018-04-27 10:08:26,029 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-27 10:11:44,040 - memory_profile6_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 10:11:44,042 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 10:11:44,043 - memory_profile6_log - INFO - ================================================

2018-04-27 10:11:44,045 - memory_profile6_log - INFO -     54    492.5 MiB    492.5 MiB   @profile

2018-04-27 10:11:44,046 - memory_profile6_log - INFO -     55                             def load_bigquery(client, query, job_config):

2018-04-27 10:11:44,046 - memory_profile6_log - INFO -     56                                 """

2018-04-27 10:11:44,048 - memory_profile6_log - INFO -     57                                     Bigquery to dataframe

2018-04-27 10:11:44,049 - memory_profile6_log - INFO -     58                                 """

2018-04-27 10:11:44,049 - memory_profile6_log - INFO -     59    492.5 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 10:11:44,052 - memory_profile6_log - INFO -     60    492.5 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 10:11:44,055 - memory_profile6_log - INFO -     61                             

2018-04-27 10:11:44,056 - memory_profile6_log - INFO -     62    492.5 MiB      0.0 MiB       result = client.query(query, job_config=job_config)  # .result()

2018-04-27 10:11:44,059 - memory_profile6_log - INFO -     63    562.6 MiB     70.1 MiB       df = result.to_dataframe()

2018-04-27 10:11:44,059 - memory_profile6_log - INFO -     64    562.6 MiB      0.0 MiB       return df

2018-04-27 10:11:44,061 - memory_profile6_log - INFO - 


2018-04-27 10:11:44,063 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-27 10:11:44,066 - memory_profile6_log - INFO - len datalist: 3
2018-04-27 10:11:44,068 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-27 10:11:44,176 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-27 10:14:44,594 - memory_profile6_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 10:14:44,595 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 10:14:44,595 - memory_profile6_log - INFO - ================================================

2018-04-27 10:14:44,598 - memory_profile6_log - INFO -     54    581.7 MiB    581.7 MiB   @profile

2018-04-27 10:14:44,598 - memory_profile6_log - INFO -     55                             def load_bigquery(client, query, job_config):

2018-04-27 10:14:44,599 - memory_profile6_log - INFO -     56                                 """

2018-04-27 10:14:44,601 - memory_profile6_log - INFO -     57                                     Bigquery to dataframe

2018-04-27 10:14:44,601 - memory_profile6_log - INFO -     58                                 """

2018-04-27 10:14:44,602 - memory_profile6_log - INFO -     59    581.7 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 10:14:44,605 - memory_profile6_log - INFO -     60    581.7 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 10:14:44,607 - memory_profile6_log - INFO -     61                             

2018-04-27 10:14:44,608 - memory_profile6_log - INFO -     62    581.7 MiB      0.0 MiB       result = client.query(query, job_config=job_config)  # .result()

2018-04-27 10:14:44,609 - memory_profile6_log - INFO -     63    680.3 MiB     98.6 MiB       df = result.to_dataframe()

2018-04-27 10:14:44,611 - memory_profile6_log - INFO -     64    680.3 MiB      0.0 MiB       return df

2018-04-27 10:14:44,611 - memory_profile6_log - INFO - 


2018-04-27 10:14:44,614 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-27 10:14:44,625 - memory_profile6_log - INFO - loading time of: 1308144 total genuine-current interest data ~ take 999.280s
2018-04-27 10:14:44,635 - memory_profile6_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 10:14:44,635 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 10:14:44,637 - memory_profile6_log - INFO - ================================================

2018-04-27 10:14:44,638 - memory_profile6_log - INFO -    243     86.7 MiB     86.7 MiB   @profile

2018-04-27 10:14:44,638 - memory_profile6_log - INFO -    244                             def preprocess(loadmp, cpu, cd, query_fit, date_generated):

2018-04-27 10:14:44,640 - memory_profile6_log - INFO -    245     86.8 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-27 10:14:44,640 - memory_profile6_log - INFO -    246     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-27 10:14:44,641 - memory_profile6_log - INFO -    247                             

2018-04-27 10:14:44,641 - memory_profile6_log - INFO -    248                                 # ~~~ Begin collecting data ~~~

2018-04-27 10:14:44,641 - memory_profile6_log - INFO -    249     86.8 MiB      0.0 MiB       t0 = time.time()

2018-04-27 10:14:44,642 - memory_profile6_log - INFO -    250     86.8 MiB      0.0 MiB       datalist = []

2018-04-27 10:14:44,642 - memory_profile6_log - INFO -    251                             

2018-04-27 10:14:44,645 - memory_profile6_log - INFO -    252    492.5 MiB      0.0 MiB       def _getBig(procdate, client):

2018-04-27 10:14:44,647 - memory_profile6_log - INFO -    253    492.5 MiB      0.0 MiB           logger.info("Collecting training data for date: %s", procdate)

2018-04-27 10:14:44,648 - memory_profile6_log - INFO -    254                                     # ~ get genuine news interest ~

2018-04-27 10:14:44,648 - memory_profile6_log - INFO -    255    492.5 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-27 10:14:44,651 - memory_profile6_log - INFO -    256                             

2018-04-27 10:14:44,653 - memory_profile6_log - INFO -    257                                     # safe handling of query parameter

2018-04-27 10:14:44,653 - memory_profile6_log - INFO -    258                                     query_params = [

2018-04-27 10:14:44,654 - memory_profile6_log - INFO -    259    492.5 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', procdate)

2018-04-27 10:14:44,654 - memory_profile6_log - INFO -    260                                     ]

2018-04-27 10:14:44,657 - memory_profile6_log - INFO -    261                             

2018-04-27 10:14:44,657 - memory_profile6_log - INFO -    262    492.5 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-27 10:14:44,657 - memory_profile6_log - INFO -    263    562.6 MiB    475.7 MiB           temp_df = load_bigquery(client, query_fit + query_fit_where, job_config)

2018-04-27 10:14:44,657 - memory_profile6_log - INFO -    264                                     # temp_df = loadBQ(client, query_fit + query_fit_where, job_config)

2018-04-27 10:14:44,657 - memory_profile6_log - INFO -    265                             

2018-04-27 10:14:44,658 - memory_profile6_log - INFO -    266    562.6 MiB      0.0 MiB           if temp_df.empty:

2018-04-27 10:14:44,658 - memory_profile6_log - INFO -    267                                         logger.info("%s data is empty!", procdate)

2018-04-27 10:14:44,658 - memory_profile6_log - INFO -    268                                         return None

2018-04-27 10:14:44,661 - memory_profile6_log - INFO -    269                                     else:

2018-04-27 10:14:44,661 - memory_profile6_log - INFO -    270    562.6 MiB      0.0 MiB               logger.info("getting total: %d training data(genuine interest) for date: %s" % (len(temp_df), procdate))

2018-04-27 10:14:44,661 - memory_profile6_log - INFO -    271    562.6 MiB      0.0 MiB               if loadmp:

2018-04-27 10:14:44,661 - memory_profile6_log - INFO -    272                                             logger.info("Exiting: %s", mp.current_process().name)

2018-04-27 10:14:44,661 - memory_profile6_log - INFO -    273    562.6 MiB      0.0 MiB               return temp_df

2018-04-27 10:14:44,663 - memory_profile6_log - INFO -    274                             

2018-04-27 10:14:44,663 - memory_profile6_log - INFO -    275     86.8 MiB      0.0 MiB       if loadmp:

2018-04-27 10:14:44,663 - memory_profile6_log - INFO -    276                                     logger.info("Starting data fetch multiprocess..")

2018-04-27 10:14:44,664 - memory_profile6_log - INFO -    277                                     logger.info("number of process: %d", len(date_generated))

2018-04-27 10:14:44,664 - memory_profile6_log - INFO -    278                             

2018-04-27 10:14:44,664 - memory_profile6_log - INFO -    279                                     pool = mp.Pool(processes=cpu)

2018-04-27 10:14:44,667 - memory_profile6_log - INFO -    280                                     # multprocessA = [pool.apply(_getBig, args=(ndate.strftime("%Y-%m-%d"), )) for ndate in date_generated]

2018-04-27 10:14:44,670 - memory_profile6_log - INFO -    281                                     multprocessA = [pool.apply_async(_getBig, args=(ndate.strftime("%Y-%m-%d"), bq_client, )) for ndate in date_generated]

2018-04-27 10:14:44,670 - memory_profile6_log - INFO -    282                                     output_multprocessA = [p.get() for p in multprocessA]

2018-04-27 10:14:44,671 - memory_profile6_log - INFO -    283                             

2018-04-27 10:14:44,674 - memory_profile6_log - INFO -    284                                     for m in output_multprocessA:

2018-04-27 10:14:44,674 - memory_profile6_log - INFO -    285                                         if m is not None:

2018-04-27 10:14:44,674 - memory_profile6_log - INFO -    286                                             if not m.empty:

2018-04-27 10:14:44,676 - memory_profile6_log - INFO -    287                                                 datalist.append(m)

2018-04-27 10:14:44,676 - memory_profile6_log - INFO -    288                             

2018-04-27 10:14:44,678 - memory_profile6_log - INFO -    289                                     logger.info("len datalist: %d", len(datalist))

2018-04-27 10:14:44,680 - memory_profile6_log - INFO -    290                                     logger.info("All data fetch multiprocess done!!")

2018-04-27 10:14:44,680 - memory_profile6_log - INFO -    291                                 else:

2018-04-27 10:14:44,683 - memory_profile6_log - INFO -    292     86.8 MiB      0.0 MiB           logger.info("Starting data fetch iterative...")

2018-04-27 10:14:44,684 - memory_profile6_log - INFO -    293    562.6 MiB      0.0 MiB           for ndate in date_generated:

2018-04-27 10:14:44,684 - memory_profile6_log - INFO -    294    562.6 MiB      0.0 MiB               tframe = _getBig(ndate.strftime("%Y-%m-%d"), bq_client)

2018-04-27 10:14:44,684 - memory_profile6_log - INFO -    295    562.6 MiB      0.0 MiB               if tframe is not None:

2018-04-27 10:14:44,686 - memory_profile6_log - INFO -    296    562.6 MiB      0.0 MiB                   if not tframe.empty:

2018-04-27 10:14:44,686 - memory_profile6_log - INFO -    297    562.6 MiB      0.0 MiB                       datalist.append(tframe)

2018-04-27 10:14:44,687 - memory_profile6_log - INFO -    298                                         else: 

2018-04-27 10:14:44,690 - memory_profile6_log - INFO -    299                                             logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-27 10:14:44,690 - memory_profile6_log - INFO -    300    562.6 MiB      0.0 MiB           logger.info("len datalist: %d", len(datalist))

2018-04-27 10:14:44,690 - memory_profile6_log - INFO -    301    562.6 MiB      0.0 MiB           logger.info("All data fetch iterative done!!")

2018-04-27 10:14:44,690 - memory_profile6_log - INFO -    302                             

2018-04-27 10:14:44,691 - memory_profile6_log - INFO -    303    562.6 MiB      0.0 MiB       if len(datalist) > 1:

2018-04-27 10:14:44,693 - memory_profile6_log - INFO -    304    613.0 MiB     50.5 MiB           big_frame = pd.concat(datalist)

2018-04-27 10:14:44,694 - memory_profile6_log - INFO -    305    581.6 MiB    -31.4 MiB           del datalist

2018-04-27 10:14:44,694 - memory_profile6_log - INFO -    306                                 else:

2018-04-27 10:14:44,697 - memory_profile6_log - INFO -    307                                     big_frame = datalist

2018-04-27 10:14:44,697 - memory_profile6_log - INFO -    308    581.7 MiB      0.1 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-27 10:14:44,700 - memory_profile6_log - INFO -    309                             

2018-04-27 10:14:44,701 - memory_profile6_log - INFO -    310                                 # ~ get current news interest ~

2018-04-27 10:14:44,701 - memory_profile6_log - INFO -    311    581.7 MiB      0.0 MiB       if not cd:

2018-04-27 10:14:44,703 - memory_profile6_log - INFO -    312                                     logger.info("Collecting training data(current date interest)..")

2018-04-27 10:14:44,703 - memory_profile6_log - INFO -    313                                     current_frame = load_bigquery(bq_client, query_transform, job_config)

2018-04-27 10:14:44,703 - memory_profile6_log - INFO -    314                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-27 10:14:44,703 - memory_profile6_log - INFO -    315                                 else:

2018-04-27 10:14:44,703 - memory_profile6_log - INFO -    316    581.7 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-27 10:14:44,703 - memory_profile6_log - INFO -    317    581.7 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-27 10:14:44,704 - memory_profile6_log - INFO -    318                             

2018-04-27 10:14:44,704 - memory_profile6_log - INFO -    319                                     # safe handling of query parameter

2018-04-27 10:14:44,704 - memory_profile6_log - INFO -    320                                     query_params = [

2018-04-27 10:14:44,706 - memory_profile6_log - INFO -    321    581.7 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-27 10:14:44,706 - memory_profile6_log - INFO -    322                                     ]

2018-04-27 10:14:44,707 - memory_profile6_log - INFO -    323                             

2018-04-27 10:14:44,707 - memory_profile6_log - INFO -    324    581.7 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-27 10:14:44,707 - memory_profile6_log - INFO -    325    680.3 MiB     98.6 MiB           current_frame = load_bigquery(bq_client, query_fit + query_fit_where, job_config)

2018-04-27 10:14:44,707 - memory_profile6_log - INFO -    326    680.3 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-27 10:14:44,707 - memory_profile6_log - INFO -    327                             

2018-04-27 10:14:44,707 - memory_profile6_log - INFO -    328    680.4 MiB      0.1 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-27 10:14:44,711 - memory_profile6_log - INFO -    329    680.4 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-27 10:14:44,713 - memory_profile6_log - INFO -    330    680.4 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-27 10:14:44,713 - memory_profile6_log - INFO -    331                             

2018-04-27 10:14:44,713 - memory_profile6_log - INFO -    332    680.4 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-27 10:14:44,714 - memory_profile6_log - INFO -    333    680.4 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-27 10:14:44,716 - memory_profile6_log - INFO -    334                             

2018-04-27 10:14:44,716 - memory_profile6_log - INFO -    335    680.4 MiB      0.0 MiB       return big_frame, current_frame

2018-04-27 10:14:44,717 - memory_profile6_log - INFO - 


2018-04-27 10:14:44,720 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-27 10:14:44,799 - memory_profile6_log - INFO - train on: 1077719 total history data(D(u, t))
2018-04-27 10:14:44,802 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-27 10:14:49,756 - memory_profile6_log - INFO - Len of model_fit: 1077719
2018-04-27 10:14:49,759 - memory_profile6_log - INFO - Len of df_dut: 1077719
2018-04-27 10:16:20,329 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-27 10:16:20,430 - memory_profile6_log - INFO - Total train time: 95.630s
2018-04-27 10:16:20,431 - memory_profile6_log - INFO - memory left before cleaning: 77.600 percent memory...
2018-04-27 10:16:20,433 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-27 10:16:20,434 - memory_profile6_log - INFO - deleting df_dut...
2018-04-27 10:16:20,434 - memory_profile6_log - INFO - deleting df_dt...
2018-04-27 10:16:20,436 - memory_profile6_log - INFO - deleting df_input...
2018-04-27 10:16:20,446 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-27 10:16:20,447 - memory_profile6_log - INFO - deleting df_current...
2018-04-27 10:16:20,448 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-27 10:16:20,523 - memory_profile6_log - INFO - deleting model_fit...
2018-04-27 10:16:20,525 - memory_profile6_log - INFO - deleting result...
2018-04-27 10:16:20,558 - memory_profile6_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 10:16:20,559 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 10:16:20,559 - memory_profile6_log - INFO - ================================================

2018-04-27 10:16:20,561 - memory_profile6_log - INFO -     97    670.7 MiB    670.7 MiB   @profile

2018-04-27 10:16:20,561 - memory_profile6_log - INFO -     98                             def main(df_input, df_current, current_date, G,

2018-04-27 10:16:20,562 - memory_profile6_log - INFO -     99                                      project_id, savetrain=False, multproc=True,

2018-04-27 10:16:20,562 - memory_profile6_log - INFO -    100                                      threshold=0, start_date=None, end_date=None,

2018-04-27 10:16:20,563 - memory_profile6_log - INFO -    101                                      saveto="datastore"):

2018-04-27 10:16:20,563 - memory_profile6_log - INFO -    102                                 """

2018-04-27 10:16:20,565 - memory_profile6_log - INFO -    103                                     Main cron method

2018-04-27 10:16:20,565 - memory_profile6_log - INFO -    104                                 """

2018-04-27 10:16:20,566 - memory_profile6_log - INFO -    105                                 # ~ Data Preprocessing ~

2018-04-27 10:16:20,566 - memory_profile6_log - INFO -    106                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-27 10:16:20,572 - memory_profile6_log - INFO -    107                                 # D(u, t)

2018-04-27 10:16:20,572 - memory_profile6_log - INFO -    108    670.7 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-27 10:16:20,573 - memory_profile6_log - INFO -    109    704.6 MiB     33.9 MiB       df_dut = df_input.copy(deep=True)

2018-04-27 10:16:20,575 - memory_profile6_log - INFO -    110    704.6 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-27 10:16:20,575 - memory_profile6_log - INFO -    111                             

2018-04-27 10:16:20,575 - memory_profile6_log - INFO -    112                                 # D(t)

2018-04-27 10:16:20,576 - memory_profile6_log - INFO -    113    711.7 MiB      7.0 MiB       df_dt = df_current.copy(deep=True)

2018-04-27 10:16:20,578 - memory_profile6_log - INFO -    114    711.7 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-27 10:16:20,578 - memory_profile6_log - INFO -    115                             

2018-04-27 10:16:20,582 - memory_profile6_log - INFO -    116                                 # ~~~~~~ Begin train ~~~~~~

2018-04-27 10:16:20,582 - memory_profile6_log - INFO -    117    711.7 MiB      0.0 MiB       t0 = time.time()

2018-04-27 10:16:20,585 - memory_profile6_log - INFO -    118    711.7 MiB      0.0 MiB       logger.info("train on: %d total history data(D(u, t))", len(df_dut))

2018-04-27 10:16:20,585 - memory_profile6_log - INFO -    119    711.7 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-27 10:16:20,585 - memory_profile6_log - INFO -    120                             

2018-04-27 10:16:20,588 - memory_profile6_log - INFO -    121                                 # instantiace class

2018-04-27 10:16:20,588 - memory_profile6_log - INFO -    122    711.7 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-27 10:16:20,588 - memory_profile6_log - INFO -    123                             

2018-04-27 10:16:20,589 - memory_profile6_log - INFO -    124                                 # ~~ Fit ~~

2018-04-27 10:16:20,589 - memory_profile6_log - INFO -    125                                 #   handling genuine news interest < current date

2018-04-27 10:16:20,594 - memory_profile6_log - INFO -    126    715.4 MiB      3.7 MiB       NB = BR.processX(df_dut)

2018-04-27 10:16:20,595 - memory_profile6_log - INFO -    127                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-27 10:16:20,595 - memory_profile6_log - INFO -    128                                 #   nanti dipindah ke class train utama

2018-04-27 10:16:20,596 - memory_profile6_log - INFO -    129    757.7 MiB     42.3 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-27 10:16:20,596 - memory_profile6_log - INFO -    130                                 """

2018-04-27 10:16:20,598 - memory_profile6_log - INFO -    131                                     num_y = total global click for category=ci on periode t

2018-04-27 10:16:20,598 - memory_profile6_log - INFO -    132                                     num_x = total click from user_U for category=ci on periode t

2018-04-27 10:16:20,598 - memory_profile6_log - INFO -    133                                 """

2018-04-27 10:16:20,599 - memory_profile6_log - INFO -    134    757.7 MiB      0.0 MiB       fitby_sigmant = True

2018-04-27 10:16:20,601 - memory_profile6_log - INFO -    135    757.7 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-27 10:16:20,601 - memory_profile6_log - INFO -    136    757.7 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-27 10:16:20,605 - memory_profile6_log - INFO -    137    799.9 MiB     42.1 MiB                            'is_general']]

2018-04-27 10:16:20,605 - memory_profile6_log - INFO -    138    799.9 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-27 10:16:20,608 - memory_profile6_log - INFO -    139    799.9 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-27 10:16:20,608 - memory_profile6_log - INFO -    140    897.8 MiB     97.9 MiB                          verbose=False)

2018-04-27 10:16:20,608 - memory_profile6_log - INFO -    141    897.8 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-27 10:16:20,609 - memory_profile6_log - INFO -    142    897.8 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-27 10:16:20,611 - memory_profile6_log - INFO -    143                             

2018-04-27 10:16:20,611 - memory_profile6_log - INFO -    144                                 # ~~ and Transform ~~

2018-04-27 10:16:20,611 - memory_profile6_log - INFO -    145                                 #   handling current news interest == current date

2018-04-27 10:16:20,612 - memory_profile6_log - INFO -    146    897.8 MiB      0.0 MiB       if df_dt.empty:

2018-04-27 10:16:20,615 - memory_profile6_log - INFO -    147                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-27 10:16:20,617 - memory_profile6_log - INFO -    148                                     return None

2018-04-27 10:16:20,618 - memory_profile6_log - INFO -    149    897.8 MiB      0.0 MiB       NB = BR.processX(df_dt)

2018-04-27 10:16:20,618 - memory_profile6_log - INFO -    150    908.3 MiB     10.6 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-27 10:16:20,621 - memory_profile6_log - INFO -    151                             

2018-04-27 10:16:20,621 - memory_profile6_log - INFO -    152    908.3 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-27 10:16:20,621 - memory_profile6_log - INFO -    153    875.0 MiB    -33.3 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-27 10:16:20,622 - memory_profile6_log - INFO -    154    875.0 MiB      0.0 MiB       model_transform = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-27 10:16:20,624 - memory_profile6_log - INFO -    155    875.0 MiB      0.0 MiB                                      fitted_model=model_fit,

2018-04-27 10:16:20,624 - memory_profile6_log - INFO -    156    889.1 MiB     14.1 MiB                                      verbose=False)

2018-04-27 10:16:20,628 - memory_profile6_log - INFO -    157                                 # ~~~ filter is general and specific topic ~~~

2018-04-27 10:16:20,628 - memory_profile6_log - INFO -    158                                 # the idea is just we need to rerank every topic according

2018-04-27 10:16:20,628 - memory_profile6_log - INFO -    159                                 # user_id and and is_general by p0_posterior

2018-04-27 10:16:20,630 - memory_profile6_log - INFO -    160    889.1 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-27 10:16:20,631 - memory_profile6_log - INFO -    161    898.4 MiB      9.3 MiB                                     'is_general']].groupby(['topic_id',

2018-04-27 10:16:20,631 - memory_profile6_log - INFO -    162    889.1 MiB     -9.2 MiB                                                             'is_general']

2018-04-27 10:16:20,631 - memory_profile6_log - INFO -    163                                                                                      ).size().to_frame().reset_index()

2018-04-27 10:16:20,632 - memory_profile6_log - INFO -    164                             

2018-04-27 10:16:20,632 - memory_profile6_log - INFO -    165                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-27 10:16:20,634 - memory_profile6_log - INFO -    166                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-27 10:16:20,634 - memory_profile6_log - INFO -    167    889.1 MiB      0.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-27 10:16:20,634 - memory_profile6_log - INFO -    168                             

2018-04-27 10:16:20,638 - memory_profile6_log - INFO -    169                                 # ~ start by provide rank for each topic type ~

2018-04-27 10:16:20,640 - memory_profile6_log - INFO -    170    961.7 MiB     72.6 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-27 10:16:20,641 - memory_profile6_log - INFO -    171    969.5 MiB      7.7 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-27 10:16:20,641 - memory_profile6_log - INFO -    172                             

2018-04-27 10:16:20,641 - memory_profile6_log - INFO -    173                                 # ~ set threshold to filter output

2018-04-27 10:16:20,641 - memory_profile6_log - INFO -    174    969.5 MiB      0.0 MiB       if threshold > 0:

2018-04-27 10:16:20,642 - memory_profile6_log - INFO -    175    969.5 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-27 10:16:20,644 - memory_profile6_log - INFO -    176    969.5 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-27 10:16:20,644 - memory_profile6_log - INFO -    177    959.4 MiB    -10.0 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-27 10:16:20,644 - memory_profile6_log - INFO -    178                             

2018-04-27 10:16:20,645 - memory_profile6_log - INFO -    179                                 # print "\n", model_transform[['user_id', 'topic_id', 'p0_posterior',

2018-04-27 10:16:20,647 - memory_profile6_log - INFO -    180                                 #                             'is_general', 'rank']].loc[model_transform['user_id'].str.contains('1610c8e6b6466b-05a6ac1ba8c1b8-4323461-100200-1',

2018-04-27 10:16:20,647 - memory_profile6_log - INFO -    181                                 #                                                                                                 case=False)].head(45)

2018-04-27 10:16:20,651 - memory_profile6_log - INFO -    182                                 # print "len(model_transform): %d" % len(model_transform)

2018-04-27 10:16:20,651 - memory_profile6_log - INFO -    183                             

2018-04-27 10:16:20,654 - memory_profile6_log - INFO -    184    959.4 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-27 10:16:20,654 - memory_profile6_log - INFO -    185    959.4 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-27 10:16:20,654 - memory_profile6_log - INFO -    186                             

2018-04-27 10:16:20,655 - memory_profile6_log - INFO -    187    959.4 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-27 10:16:20,657 - memory_profile6_log - INFO -    188                             

2018-04-27 10:16:20,657 - memory_profile6_log - INFO -    189    959.4 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-27 10:16:20,657 - memory_profile6_log - INFO -    190    959.4 MiB      0.0 MiB       del df_dut

2018-04-27 10:16:20,657 - memory_profile6_log - INFO -    191    959.4 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-27 10:16:20,658 - memory_profile6_log - INFO -    192    959.4 MiB      0.0 MiB       del df_dt

2018-04-27 10:16:20,658 - memory_profile6_log - INFO -    193    959.4 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-27 10:16:20,663 - memory_profile6_log - INFO -    194    959.4 MiB      0.0 MiB       del df_input

2018-04-27 10:16:20,664 - memory_profile6_log - INFO -    195    959.4 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-27 10:16:20,665 - memory_profile6_log - INFO -    196    950.6 MiB     -8.8 MiB       del df_input_X

2018-04-27 10:16:20,665 - memory_profile6_log - INFO -    197    950.6 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-27 10:16:20,667 - memory_profile6_log - INFO -    198    950.6 MiB      0.0 MiB       del df_current

2018-04-27 10:16:20,667 - memory_profile6_log - INFO -    199    950.6 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-27 10:16:20,667 - memory_profile6_log - INFO -    200    950.6 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-27 10:16:20,667 - memory_profile6_log - INFO -    201    950.6 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-27 10:16:20,668 - memory_profile6_log - INFO -    202    850.9 MiB    -99.7 MiB       del model_fit

2018-04-27 10:16:20,668 - memory_profile6_log - INFO -    203    850.9 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-27 10:16:20,670 - memory_profile6_log - INFO -    204    850.9 MiB      0.0 MiB       del result

2018-04-27 10:16:20,671 - memory_profile6_log - INFO -    205    850.9 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-27 10:16:20,671 - memory_profile6_log - INFO -    206                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-27 10:16:20,674 - memory_profile6_log - INFO -    207                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-27 10:16:20,676 - memory_profile6_log - INFO -    208    850.9 MiB      0.0 MiB       if savetrain:

2018-04-27 10:16:20,677 - memory_profile6_log - INFO -    209                                     model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-27 10:16:20,677 - memory_profile6_log - INFO -    210                                     del model_transform

2018-04-27 10:16:20,677 - memory_profile6_log - INFO -    211                                     logger.info("deleting model_transform...")

2018-04-27 10:16:20,678 - memory_profile6_log - INFO -    212                                     logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-27 10:16:20,678 - memory_profile6_log - INFO -    213                             

2018-04-27 10:16:20,680 - memory_profile6_log - INFO -    214                                     logger.info("Begin saving trained data...")

2018-04-27 10:16:20,680 - memory_profile6_log - INFO -    215                                     # print "\n", model_transform.head(5)

2018-04-27 10:16:20,680 - memory_profile6_log - INFO -    216                                     # ~ Place your code to save the training model here ~

2018-04-27 10:16:20,680 - memory_profile6_log - INFO -    217                                     if str(saveto).lower() == "datastore":

2018-04-27 10:16:20,681 - memory_profile6_log - INFO -    218                                         logger.info("Using google datastore as storage...")

2018-04-27 10:16:20,681 - memory_profile6_log - INFO -    219                                         if multproc:

2018-04-27 10:16:20,684 - memory_profile6_log - INFO -    220                                             mh.saveDatastoreMP(model_transformsv)

2018-04-27 10:16:20,687 - memory_profile6_log - INFO -    221                                             # mh.saveDataStorePutMulti(model_transform[['user_id', 'topic_id', 'is_general', 'rank']])

2018-04-27 10:16:20,687 - memory_profile6_log - INFO -    222                                         else:

2018-04-27 10:16:20,687 - memory_profile6_log - INFO -    223                                             mh.saveDatastore(model_transformsv)

2018-04-27 10:16:20,687 - memory_profile6_log - INFO -    224                                             

2018-04-27 10:16:20,688 - memory_profile6_log - INFO -    225                                     elif str(saveto).lower() == "elastic":

2018-04-27 10:16:20,688 - memory_profile6_log - INFO -    226                                         logger.info("Using ElasticSearch as storage...")

2018-04-27 10:16:20,688 - memory_profile6_log - INFO -    227                                         mh.saveElasticS(model_transformsv)

2018-04-27 10:16:20,690 - memory_profile6_log - INFO -    228                             

2018-04-27 10:16:20,690 - memory_profile6_log - INFO -    229                                     # need save sigma_nt for daily train

2018-04-27 10:16:20,690 - memory_profile6_log - INFO -    230                                     # secara defaul perhitungan fit menggunakan sigma_Nt

2018-04-27 10:16:20,691 - memory_profile6_log - INFO -    231                                     #   jadi prosedur ini hanya berlaku jika fitby_sigmant = False

2018-04-27 10:16:20,697 - memory_profile6_log - INFO -    232                                     if start_date and end_date:

2018-04-27 10:16:20,700 - memory_profile6_log - INFO -    233                                         if not fitby_sigmant:

2018-04-27 10:16:20,700 - memory_profile6_log - INFO -    234                                             logging.info("Saving sigma Nt...")

2018-04-27 10:16:20,701 - memory_profile6_log - INFO -    235                                             save_sigma_nt = BR.sum_all_nt.copy(deep=True)

2018-04-27 10:16:20,703 - memory_profile6_log - INFO -    236                                             save_sigma_nt['start_date'] = start_date

2018-04-27 10:16:20,703 - memory_profile6_log - INFO -    237                                             save_sigma_nt['end_date'] = end_date

2018-04-27 10:16:20,709 - memory_profile6_log - INFO -    238                                             print save_sigma_nt.head(5)

2018-04-27 10:16:20,711 - memory_profile6_log - INFO -    239                                             print "len(save_sigma_nt): %d" % len(save_sigma_nt)

2018-04-27 10:16:20,711 - memory_profile6_log - INFO -    240                                             # mh.saveDatastoreMP(save_sigma_nt)

2018-04-27 10:16:20,713 - memory_profile6_log - INFO -    241    850.9 MiB      0.0 MiB       return

2018-04-27 10:16:20,713 - memory_profile6_log - INFO - 


2018-04-27 10:16:20,713 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-27 11:41:24,834 - memory_profile6_log - INFO - Generating date range with N: 3
2018-04-27 11:41:24,838 - memory_profile6_log - INFO - date_generated: 
2018-04-27 11:41:24,838 - memory_profile6_log - INFO -  
2018-04-27 11:41:24,838 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 12, 0, 0), datetime.datetime(2018, 4, 13, 0, 0), datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-27 11:41:24,839 - memory_profile6_log - INFO - 

2018-04-27 11:41:24,839 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-27 11:41:24,841 - memory_profile6_log - INFO - using start date: 2018-04-12 00:00:00
2018-04-27 11:41:24,842 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-27 11:41:24,947 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-27 11:41:24,950 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-12
2018-04-27 11:47:24,115 - memory_profile6_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 11:47:24,115 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 11:47:24,117 - memory_profile6_log - INFO - ================================================

2018-04-27 11:47:24,118 - memory_profile6_log - INFO -     55     86.8 MiB     86.8 MiB   @profile

2018-04-27 11:47:24,118 - memory_profile6_log - INFO -     56                             def load_bigquery(client, query, job_config):

2018-04-27 11:47:24,119 - memory_profile6_log - INFO -     57                                 """

2018-04-27 11:47:24,121 - memory_profile6_log - INFO -     58                                     Bigquery to dataframe

2018-04-27 11:47:24,121 - memory_profile6_log - INFO -     59                                 """

2018-04-27 11:47:24,122 - memory_profile6_log - INFO -     60     86.8 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 11:47:24,124 - memory_profile6_log - INFO -     61     86.8 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 11:47:24,127 - memory_profile6_log - INFO -     62                             

2018-04-27 11:47:24,128 - memory_profile6_log - INFO -     63     89.9 MiB      3.1 MiB       result = client.query(query, job_config=job_config)  # .result()

2018-04-27 11:47:24,128 - memory_profile6_log - INFO -     64    394.4 MiB    304.5 MiB       df = result.to_dataframe()

2018-04-27 11:47:24,130 - memory_profile6_log - INFO -     65    394.4 MiB      0.0 MiB       return df

2018-04-27 11:47:24,131 - memory_profile6_log - INFO - 


2018-04-27 11:47:24,132 - memory_profile6_log - INFO - getting total: 462579 training data(genuine interest) for date: 2018-04-12
2018-04-27 11:47:24,134 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-13
2018-04-27 11:51:55,391 - memory_profile6_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 11:51:55,392 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 11:51:55,394 - memory_profile6_log - INFO - ================================================

2018-04-27 11:51:55,395 - memory_profile6_log - INFO -     55    394.4 MiB    394.4 MiB   @profile

2018-04-27 11:51:55,397 - memory_profile6_log - INFO -     56                             def load_bigquery(client, query, job_config):

2018-04-27 11:51:55,398 - memory_profile6_log - INFO -     57                                 """

2018-04-27 11:51:55,398 - memory_profile6_log - INFO -     58                                     Bigquery to dataframe

2018-04-27 11:51:55,400 - memory_profile6_log - INFO -     59                                 """

2018-04-27 11:51:55,401 - memory_profile6_log - INFO -     60    394.4 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 11:51:55,404 - memory_profile6_log - INFO -     61    394.4 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 11:51:55,404 - memory_profile6_log - INFO -     62                             

2018-04-27 11:51:55,405 - memory_profile6_log - INFO -     63    394.4 MiB      0.0 MiB       result = client.query(query, job_config=job_config)  # .result()

2018-04-27 11:51:55,405 - memory_profile6_log - INFO -     64    491.8 MiB     97.3 MiB       df = result.to_dataframe()

2018-04-27 11:51:55,407 - memory_profile6_log - INFO -     65    491.8 MiB      0.0 MiB       return df

2018-04-27 11:51:55,407 - memory_profile6_log - INFO - 


2018-04-27 11:51:55,408 - memory_profile6_log - INFO - getting total: 361145 training data(genuine interest) for date: 2018-04-13
2018-04-27 11:51:55,411 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-27 11:55:06,762 - memory_profile6_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 11:55:06,763 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 11:55:06,765 - memory_profile6_log - INFO - ================================================

2018-04-27 11:55:06,766 - memory_profile6_log - INFO -     55    491.8 MiB    491.8 MiB   @profile

2018-04-27 11:55:06,767 - memory_profile6_log - INFO -     56                             def load_bigquery(client, query, job_config):

2018-04-27 11:55:06,769 - memory_profile6_log - INFO -     57                                 """

2018-04-27 11:55:06,770 - memory_profile6_log - INFO -     58                                     Bigquery to dataframe

2018-04-27 11:55:06,775 - memory_profile6_log - INFO -     59                                 """

2018-04-27 11:55:06,779 - memory_profile6_log - INFO -     60    491.8 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 11:55:06,779 - memory_profile6_log - INFO -     61    491.8 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 11:55:06,782 - memory_profile6_log - INFO -     62                             

2018-04-27 11:55:06,783 - memory_profile6_log - INFO -     63    491.8 MiB      0.0 MiB       result = client.query(query, job_config=job_config)  # .result()

2018-04-27 11:55:06,792 - memory_profile6_log - INFO -     64    561.8 MiB     70.1 MiB       df = result.to_dataframe()

2018-04-27 11:55:06,796 - memory_profile6_log - INFO -     65    561.8 MiB      0.0 MiB       return df

2018-04-27 11:55:06,796 - memory_profile6_log - INFO - 


2018-04-27 11:55:06,799 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-27 11:55:06,801 - memory_profile6_log - INFO - len datalist: 3
2018-04-27 11:55:06,802 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-27 11:55:06,907 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-27 11:58:03,308 - memory_profile6_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 11:58:03,309 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 11:58:03,311 - memory_profile6_log - INFO - ================================================

2018-04-27 11:58:03,312 - memory_profile6_log - INFO -     55    581.2 MiB    581.2 MiB   @profile

2018-04-27 11:58:03,312 - memory_profile6_log - INFO -     56                             def load_bigquery(client, query, job_config):

2018-04-27 11:58:03,313 - memory_profile6_log - INFO -     57                                 """

2018-04-27 11:58:03,315 - memory_profile6_log - INFO -     58                                     Bigquery to dataframe

2018-04-27 11:58:03,315 - memory_profile6_log - INFO -     59                                 """

2018-04-27 11:58:03,316 - memory_profile6_log - INFO -     60    581.2 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 11:58:03,318 - memory_profile6_log - INFO -     61    581.2 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 11:58:03,321 - memory_profile6_log - INFO -     62                             

2018-04-27 11:58:03,322 - memory_profile6_log - INFO -     63    581.3 MiB      0.0 MiB       result = client.query(query, job_config=job_config)  # .result()

2018-04-27 11:58:03,323 - memory_profile6_log - INFO -     64    680.6 MiB     99.4 MiB       df = result.to_dataframe()

2018-04-27 11:58:03,325 - memory_profile6_log - INFO -     65    680.6 MiB      0.0 MiB       return df

2018-04-27 11:58:03,325 - memory_profile6_log - INFO - 


2018-04-27 11:58:03,326 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-27 11:58:03,335 - memory_profile6_log - INFO - loading time of: 1308144 total genuine-current interest data ~ take 998.388s
2018-04-27 11:58:03,346 - memory_profile6_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 11:58:03,346 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 11:58:03,348 - memory_profile6_log - INFO - ================================================

2018-04-27 11:58:03,349 - memory_profile6_log - INFO -    233     86.7 MiB     86.7 MiB   @profile

2018-04-27 11:58:03,351 - memory_profile6_log - INFO -    234                             def preprocess(loadmp, cpu, cd, query_fit, date_generated):

2018-04-27 11:58:03,352 - memory_profile6_log - INFO -    235     86.8 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-27 11:58:03,354 - memory_profile6_log - INFO -    236     86.8 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-27 11:58:03,355 - memory_profile6_log - INFO -    237                             

2018-04-27 11:58:03,355 - memory_profile6_log - INFO -    238                                 # ~~~ Begin collecting data ~~~

2018-04-27 11:58:03,357 - memory_profile6_log - INFO -    239     86.8 MiB      0.0 MiB       t0 = time.time()

2018-04-27 11:58:03,357 - memory_profile6_log - INFO -    240     86.8 MiB      0.0 MiB       datalist = []

2018-04-27 11:58:03,358 - memory_profile6_log - INFO -    241                             

2018-04-27 11:58:03,358 - memory_profile6_log - INFO -    242    491.8 MiB      0.0 MiB       def _getBig(procdate, client):

2018-04-27 11:58:03,358 - memory_profile6_log - INFO -    243    491.8 MiB      0.0 MiB           logger.info("Collecting training data for date: %s", procdate)

2018-04-27 11:58:03,358 - memory_profile6_log - INFO -    244                                     # ~ get genuine news interest ~

2018-04-27 11:58:03,358 - memory_profile6_log - INFO -    245    491.8 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-27 11:58:03,358 - memory_profile6_log - INFO -    246                             

2018-04-27 11:58:03,359 - memory_profile6_log - INFO -    247                                     # safe handling of query parameter

2018-04-27 11:58:03,359 - memory_profile6_log - INFO -    248                                     query_params = [

2018-04-27 11:58:03,364 - memory_profile6_log - INFO -    249    491.8 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', procdate)

2018-04-27 11:58:03,365 - memory_profile6_log - INFO -    250                                     ]

2018-04-27 11:58:03,365 - memory_profile6_log - INFO -    251                             

2018-04-27 11:58:03,365 - memory_profile6_log - INFO -    252    491.8 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-27 11:58:03,367 - memory_profile6_log - INFO -    253    561.8 MiB    475.0 MiB           temp_df = load_bigquery(client, query_fit + query_fit_where, job_config)

2018-04-27 11:58:03,368 - memory_profile6_log - INFO -    254                                     # temp_df = loadBQ(client, query_fit + query_fit_where, job_config)

2018-04-27 11:58:03,368 - memory_profile6_log - INFO -    255                             

2018-04-27 11:58:03,368 - memory_profile6_log - INFO -    256    561.8 MiB      0.0 MiB           if temp_df.empty:

2018-04-27 11:58:03,369 - memory_profile6_log - INFO -    257                                         logger.info("%s data is empty!", procdate)

2018-04-27 11:58:03,369 - memory_profile6_log - INFO -    258                                         return None

2018-04-27 11:58:03,369 - memory_profile6_log - INFO -    259                                     else:

2018-04-27 11:58:03,371 - memory_profile6_log - INFO -    260    561.8 MiB      0.0 MiB               logger.info("getting total: %d training data(genuine interest) for date: %s" % (len(temp_df), procdate))

2018-04-27 11:58:03,371 - memory_profile6_log - INFO -    261    561.8 MiB      0.0 MiB               if loadmp:

2018-04-27 11:58:03,371 - memory_profile6_log - INFO -    262                                             logger.info("Exiting: %s", mp.current_process().name)

2018-04-27 11:58:03,371 - memory_profile6_log - INFO -    263    561.8 MiB      0.0 MiB               return temp_df

2018-04-27 11:58:03,371 - memory_profile6_log - INFO -    264                             

2018-04-27 11:58:03,375 - memory_profile6_log - INFO -    265     86.8 MiB      0.0 MiB       if loadmp:

2018-04-27 11:58:03,375 - memory_profile6_log - INFO -    266                                     logger.info("Starting data fetch multiprocess..")

2018-04-27 11:58:03,378 - memory_profile6_log - INFO -    267                                     logger.info("number of process: %d", len(date_generated))

2018-04-27 11:58:03,378 - memory_profile6_log - INFO -    268                             

2018-04-27 11:58:03,378 - memory_profile6_log - INFO -    269                                     pool = mp.Pool(processes=cpu)

2018-04-27 11:58:03,380 - memory_profile6_log - INFO -    270                                     # multprocessA = [pool.apply(_getBig, args=(ndate.strftime("%Y-%m-%d"), )) for ndate in date_generated]

2018-04-27 11:58:03,380 - memory_profile6_log - INFO -    271                                     multprocessA = [pool.apply_async(_getBig, args=(ndate.strftime("%Y-%m-%d"), bq_client, )) for ndate in date_generated]

2018-04-27 11:58:03,380 - memory_profile6_log - INFO -    272                                     output_multprocessA = [p.get() for p in multprocessA]

2018-04-27 11:58:03,381 - memory_profile6_log - INFO -    273                             

2018-04-27 11:58:03,381 - memory_profile6_log - INFO -    274                                     for m in output_multprocessA:

2018-04-27 11:58:03,381 - memory_profile6_log - INFO -    275                                         if m is not None:

2018-04-27 11:58:03,381 - memory_profile6_log - INFO -    276                                             if not m.empty:

2018-04-27 11:58:03,381 - memory_profile6_log - INFO -    277                                                 datalist.append(m)

2018-04-27 11:58:03,382 - memory_profile6_log - INFO -    278                             

2018-04-27 11:58:03,382 - memory_profile6_log - INFO -    279                                     logger.info("len datalist: %d", len(datalist))

2018-04-27 11:58:03,382 - memory_profile6_log - INFO -    280                                     logger.info("All data fetch multiprocess done!!")

2018-04-27 11:58:03,387 - memory_profile6_log - INFO -    281                                 else:

2018-04-27 11:58:03,387 - memory_profile6_log - INFO -    282     86.8 MiB      0.0 MiB           logger.info("Starting data fetch iterative...")

2018-04-27 11:58:03,388 - memory_profile6_log - INFO -    283    561.8 MiB      0.0 MiB           for ndate in date_generated:

2018-04-27 11:58:03,390 - memory_profile6_log - INFO -    284    561.8 MiB      0.0 MiB               tframe = _getBig(ndate.strftime("%Y-%m-%d"), bq_client)

2018-04-27 11:58:03,390 - memory_profile6_log - INFO -    285    561.8 MiB      0.0 MiB               if tframe is not None:

2018-04-27 11:58:03,391 - memory_profile6_log - INFO -    286    561.8 MiB      0.0 MiB                   if not tframe.empty:

2018-04-27 11:58:03,391 - memory_profile6_log - INFO -    287    561.8 MiB      0.0 MiB                       datalist.append(tframe)

2018-04-27 11:58:03,391 - memory_profile6_log - INFO -    288                                         else: 

2018-04-27 11:58:03,392 - memory_profile6_log - INFO -    289                                             logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-27 11:58:03,394 - memory_profile6_log - INFO -    290    561.8 MiB      0.0 MiB           logger.info("len datalist: %d", len(datalist))

2018-04-27 11:58:03,394 - memory_profile6_log - INFO -    291    561.8 MiB      0.0 MiB           logger.info("All data fetch iterative done!!")

2018-04-27 11:58:03,398 - memory_profile6_log - INFO -    292                             

2018-04-27 11:58:03,398 - memory_profile6_log - INFO -    293    561.8 MiB      0.0 MiB       if len(datalist) > 1:

2018-04-27 11:58:03,400 - memory_profile6_log - INFO -    294    612.6 MiB     50.8 MiB           big_frame = pd.concat(datalist)

2018-04-27 11:58:03,400 - memory_profile6_log - INFO -    295    581.1 MiB    -31.4 MiB           del datalist

2018-04-27 11:58:03,401 - memory_profile6_log - INFO -    296                                 else:

2018-04-27 11:58:03,401 - memory_profile6_log - INFO -    297                                     big_frame = datalist

2018-04-27 11:58:03,401 - memory_profile6_log - INFO -    298    581.2 MiB      0.1 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-27 11:58:03,404 - memory_profile6_log - INFO -    299                             

2018-04-27 11:58:03,404 - memory_profile6_log - INFO -    300                                 # ~ get current news interest ~

2018-04-27 11:58:03,404 - memory_profile6_log - INFO -    301    581.2 MiB      0.0 MiB       if not cd:

2018-04-27 11:58:03,404 - memory_profile6_log - INFO -    302                                     logger.info("Collecting training data(current date interest)..")

2018-04-27 11:58:03,404 - memory_profile6_log - INFO -    303                                     current_frame = load_bigquery(bq_client, query_transform, job_config)

2018-04-27 11:58:03,408 - memory_profile6_log - INFO -    304                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-27 11:58:03,408 - memory_profile6_log - INFO -    305                                 else:

2018-04-27 11:58:03,410 - memory_profile6_log - INFO -    306    581.2 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-27 11:58:03,411 - memory_profile6_log - INFO -    307    581.2 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-27 11:58:03,411 - memory_profile6_log - INFO -    308                             

2018-04-27 11:58:03,411 - memory_profile6_log - INFO -    309                                     # safe handling of query parameter

2018-04-27 11:58:03,413 - memory_profile6_log - INFO -    310                                     query_params = [

2018-04-27 11:58:03,414 - memory_profile6_log - INFO -    311    581.2 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-27 11:58:03,420 - memory_profile6_log - INFO -    312                                     ]

2018-04-27 11:58:03,420 - memory_profile6_log - INFO -    313                             

2018-04-27 11:58:03,421 - memory_profile6_log - INFO -    314    581.2 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-27 11:58:03,421 - memory_profile6_log - INFO -    315    680.6 MiB     99.4 MiB           current_frame = load_bigquery(bq_client, query_fit + query_fit_where, job_config)

2018-04-27 11:58:03,423 - memory_profile6_log - INFO -    316    680.6 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-27 11:58:03,424 - memory_profile6_log - INFO -    317                             

2018-04-27 11:58:03,424 - memory_profile6_log - INFO -    318    680.7 MiB      0.1 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-27 11:58:03,424 - memory_profile6_log - INFO -    319    680.7 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-27 11:58:03,426 - memory_profile6_log - INFO -    320    680.7 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-27 11:58:03,426 - memory_profile6_log - INFO -    321                             

2018-04-27 11:58:03,427 - memory_profile6_log - INFO -    322    680.7 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-27 11:58:03,427 - memory_profile6_log - INFO -    323    680.7 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-27 11:58:03,430 - memory_profile6_log - INFO -    324                             

2018-04-27 11:58:03,431 - memory_profile6_log - INFO -    325    680.7 MiB      0.0 MiB       return big_frame, current_frame

2018-04-27 11:58:03,433 - memory_profile6_log - INFO - 


2018-04-27 11:58:03,436 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-27 11:58:03,499 - memory_profile6_log - INFO - train on: 1077719 total history data(D(u, t))
2018-04-27 11:58:03,500 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-27 11:58:07,934 - memory_profile6_log - INFO - Len of model_fit: 1077719
2018-04-27 11:58:07,936 - memory_profile6_log - INFO - Len of df_dut: 1077719
2018-04-27 12:00:47,486 - memory_profile6_log - INFO - Generating date range with N: 3
2018-04-27 12:00:47,487 - memory_profile6_log - INFO - date_generated: 
2018-04-27 12:00:47,489 - memory_profile6_log - INFO -  
2018-04-27 12:00:47,490 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 12, 0, 0), datetime.datetime(2018, 4, 13, 0, 0), datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-27 12:00:47,490 - memory_profile6_log - INFO - 

2018-04-27 12:00:47,490 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-27 12:00:47,492 - memory_profile6_log - INFO - using start date: 2018-04-12 00:00:00
2018-04-27 12:00:47,493 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-27 12:00:47,601 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-27 12:00:47,602 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-12
2018-04-27 12:03:10,165 - memory_profile6_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 12:03:10,167 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 12:03:10,168 - memory_profile6_log - INFO - ================================================

2018-04-27 12:03:10,170 - memory_profile6_log - INFO -     67     86.7 MiB     86.7 MiB   @profile

2018-04-27 12:03:10,171 - memory_profile6_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-27 12:03:10,171 - memory_profile6_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-27 12:03:10,173 - memory_profile6_log - INFO -     70     86.7 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 12:03:10,174 - memory_profile6_log - INFO -     71     86.7 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 12:03:10,174 - memory_profile6_log - INFO -     72                             

2018-04-27 12:03:10,174 - memory_profile6_log - INFO -     73     89.8 MiB      3.1 MiB       result  = client.query(query, job_config=job_config)

2018-04-27 12:03:10,177 - memory_profile6_log - INFO -     74     89.8 MiB      0.1 MiB       rows = result.result()

2018-04-27 12:03:10,177 - memory_profile6_log - INFO -     75                             

2018-04-27 12:03:10,180 - memory_profile6_log - INFO -     76     89.8 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-27 12:03:10,180 - memory_profile6_log - INFO -     77                                 

2018-04-27 12:03:10,180 - memory_profile6_log - INFO -     78     89.8 MiB      0.0 MiB       data = []

2018-04-27 12:03:10,181 - memory_profile6_log - INFO -     79    498.8 MiB  -1599.1 MiB       for row in list(rows):

2018-04-27 12:03:10,183 - memory_profile6_log - INFO -     80    498.8 MiB  -1952.8 MiB           data.append(list(row))

2018-04-27 12:03:10,184 - memory_profile6_log - INFO -     81                             

2018-04-27 12:03:10,184 - memory_profile6_log - INFO -     82    510.5 MiB     11.7 MiB       df = pd.DataFrame(data, columns=col_name)

2018-04-27 12:03:10,184 - memory_profile6_log - INFO -     83    510.5 MiB      0.0 MiB       del rows

2018-04-27 12:03:10,187 - memory_profile6_log - INFO -     84    510.5 MiB      0.0 MiB       del result

2018-04-27 12:03:10,188 - memory_profile6_log - INFO -     85    393.6 MiB   -116.9 MiB       del data

2018-04-27 12:03:10,190 - memory_profile6_log - INFO -     86    393.6 MiB      0.0 MiB       gc.collect()

2018-04-27 12:03:10,193 - memory_profile6_log - INFO -     87    393.6 MiB      0.0 MiB       return df

2018-04-27 12:03:10,194 - memory_profile6_log - INFO - 


2018-04-27 12:03:10,196 - memory_profile6_log - INFO - getting total: 462579 training data(genuine interest) for date: 2018-04-12
2018-04-27 12:03:10,197 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-13
2018-04-27 12:04:59,582 - memory_profile6_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 12:04:59,584 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 12:04:59,584 - memory_profile6_log - INFO - ================================================

2018-04-27 12:04:59,585 - memory_profile6_log - INFO -     67    393.6 MiB    393.6 MiB   @profile

2018-04-27 12:04:59,585 - memory_profile6_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-27 12:04:59,586 - memory_profile6_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-27 12:04:59,586 - memory_profile6_log - INFO -     70    393.6 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 12:04:59,588 - memory_profile6_log - INFO -     71    393.6 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 12:04:59,588 - memory_profile6_log - INFO -     72                             

2018-04-27 12:04:59,588 - memory_profile6_log - INFO -     73    393.6 MiB      0.0 MiB       result  = client.query(query, job_config=job_config)

2018-04-27 12:04:59,589 - memory_profile6_log - INFO -     74    393.6 MiB      0.0 MiB       rows = result.result()

2018-04-27 12:04:59,591 - memory_profile6_log - INFO -     75                             

2018-04-27 12:04:59,591 - memory_profile6_log - INFO -     76    393.6 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-27 12:04:59,594 - memory_profile6_log - INFO -     77                                 

2018-04-27 12:04:59,595 - memory_profile6_log - INFO -     78    393.6 MiB      0.0 MiB       data = []

2018-04-27 12:04:59,596 - memory_profile6_log - INFO -     79    575.8 MiB  -3054.1 MiB       for row in list(rows):

2018-04-27 12:04:59,598 - memory_profile6_log - INFO -     80    575.8 MiB  -3195.9 MiB           data.append(list(row))

2018-04-27 12:04:59,598 - memory_profile6_log - INFO -     81                             

2018-04-27 12:04:59,598 - memory_profile6_log - INFO -     82    584.8 MiB      9.0 MiB       df = pd.DataFrame(data, columns=col_name)

2018-04-27 12:04:59,599 - memory_profile6_log - INFO -     83    584.8 MiB      0.0 MiB       del rows

2018-04-27 12:04:59,601 - memory_profile6_log - INFO -     84    584.8 MiB      0.0 MiB       del result

2018-04-27 12:04:59,601 - memory_profile6_log - INFO -     85    498.0 MiB    -86.7 MiB       del data

2018-04-27 12:04:59,605 - memory_profile6_log - INFO -     86    498.0 MiB      0.0 MiB       gc.collect()

2018-04-27 12:04:59,605 - memory_profile6_log - INFO -     87    498.0 MiB      0.0 MiB       return df

2018-04-27 12:04:59,607 - memory_profile6_log - INFO - 


2018-04-27 12:04:59,608 - memory_profile6_log - INFO - getting total: 361145 training data(genuine interest) for date: 2018-04-13
2018-04-27 12:04:59,608 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-27 12:06:16,907 - memory_profile6_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 12:06:16,907 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 12:06:16,908 - memory_profile6_log - INFO - ================================================

2018-04-27 12:06:16,911 - memory_profile6_log - INFO -     67    498.0 MiB    498.0 MiB   @profile

2018-04-27 12:06:16,911 - memory_profile6_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-27 12:06:16,913 - memory_profile6_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-27 12:06:16,914 - memory_profile6_log - INFO -     70    498.0 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 12:06:16,914 - memory_profile6_log - INFO -     71    498.0 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 12:06:16,915 - memory_profile6_log - INFO -     72                             

2018-04-27 12:06:16,918 - memory_profile6_log - INFO -     73    498.0 MiB      0.0 MiB       result  = client.query(query, job_config=job_config)

2018-04-27 12:06:16,921 - memory_profile6_log - INFO -     74    498.0 MiB     -0.0 MiB       rows = result.result()

2018-04-27 12:06:16,921 - memory_profile6_log - INFO -     75                             

2018-04-27 12:06:16,923 - memory_profile6_log - INFO -     76    498.0 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-27 12:06:16,924 - memory_profile6_log - INFO -     77                                 

2018-04-27 12:06:16,924 - memory_profile6_log - INFO -     78    498.0 MiB      0.0 MiB       data = []

2018-04-27 12:06:16,926 - memory_profile6_log - INFO -     79    625.4 MiB  -3742.0 MiB       for row in list(rows):

2018-04-27 12:06:16,927 - memory_profile6_log - INFO -     80    625.4 MiB  -3854.2 MiB           data.append(list(row))

2018-04-27 12:06:16,930 - memory_profile6_log - INFO -     81                             

2018-04-27 12:06:16,930 - memory_profile6_log - INFO -     82    631.8 MiB      6.3 MiB       df = pd.DataFrame(data, columns=col_name)

2018-04-27 12:06:16,933 - memory_profile6_log - INFO -     83    631.8 MiB      0.0 MiB       del rows

2018-04-27 12:06:16,934 - memory_profile6_log - INFO -     84    631.8 MiB      0.0 MiB       del result

2018-04-27 12:06:16,934 - memory_profile6_log - INFO -     85    569.8 MiB    -62.0 MiB       del data

2018-04-27 12:06:16,936 - memory_profile6_log - INFO -     86    569.8 MiB      0.0 MiB       gc.collect()

2018-04-27 12:06:16,937 - memory_profile6_log - INFO -     87    569.8 MiB      0.0 MiB       return df

2018-04-27 12:06:16,940 - memory_profile6_log - INFO - 


2018-04-27 12:06:16,940 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-27 12:06:16,943 - memory_profile6_log - INFO - len datalist: 3
2018-04-27 12:06:16,944 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-27 12:06:17,040 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-27 12:07:26,720 - memory_profile6_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 12:07:26,720 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 12:07:26,723 - memory_profile6_log - INFO - ================================================

2018-04-27 12:07:26,723 - memory_profile6_log - INFO -     67    589.5 MiB    589.5 MiB   @profile

2018-04-27 12:07:26,724 - memory_profile6_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-27 12:07:26,726 - memory_profile6_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-27 12:07:26,727 - memory_profile6_log - INFO -     70    589.5 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 12:07:26,729 - memory_profile6_log - INFO -     71    589.5 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 12:07:26,732 - memory_profile6_log - INFO -     72                             

2018-04-27 12:07:26,732 - memory_profile6_log - INFO -     73    589.5 MiB      0.0 MiB       result  = client.query(query, job_config=job_config)

2018-04-27 12:07:26,733 - memory_profile6_log - INFO -     74    589.5 MiB      0.0 MiB       rows = result.result()

2018-04-27 12:07:26,733 - memory_profile6_log - INFO -     75                             

2018-04-27 12:07:26,734 - memory_profile6_log - INFO -     76    589.5 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-27 12:07:26,734 - memory_profile6_log - INFO -     77                                 

2018-04-27 12:07:26,736 - memory_profile6_log - INFO -     78    589.5 MiB      0.0 MiB       data = []

2018-04-27 12:07:26,736 - memory_profile6_log - INFO -     79    714.8 MiB  -1147.8 MiB       for row in list(rows):

2018-04-27 12:07:26,737 - memory_profile6_log - INFO -     80    714.8 MiB  -1262.2 MiB           data.append(list(row))

2018-04-27 12:07:26,744 - memory_profile6_log - INFO -     81                             

2018-04-27 12:07:26,747 - memory_profile6_log - INFO -     82    720.5 MiB      5.7 MiB       df = pd.DataFrame(data, columns=col_name)

2018-04-27 12:07:26,749 - memory_profile6_log - INFO -     83    720.5 MiB      0.0 MiB       del rows

2018-04-27 12:07:26,750 - memory_profile6_log - INFO -     84    720.5 MiB      0.0 MiB       del result

2018-04-27 12:07:26,752 - memory_profile6_log - INFO -     85    652.2 MiB    -68.4 MiB       del data

2018-04-27 12:07:26,753 - memory_profile6_log - INFO -     86    652.2 MiB      0.0 MiB       gc.collect()

2018-04-27 12:07:26,755 - memory_profile6_log - INFO -     87    652.2 MiB      0.0 MiB       return df

2018-04-27 12:07:26,756 - memory_profile6_log - INFO - 


2018-04-27 12:07:26,757 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-27 12:07:26,766 - memory_profile6_log - INFO - loading time of: 1308144 total genuine-current interest data ~ take 399.164s
2018-04-27 12:07:26,776 - memory_profile6_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 12:07:26,778 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 12:07:26,778 - memory_profile6_log - INFO - ================================================

2018-04-27 12:07:26,779 - memory_profile6_log - INFO -    233     86.5 MiB     86.5 MiB   @profile

2018-04-27 12:07:26,779 - memory_profile6_log - INFO -    234                             def preprocess(loadmp, cpu, cd, query_fit, date_generated):

2018-04-27 12:07:26,779 - memory_profile6_log - INFO -    235     86.7 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-27 12:07:26,780 - memory_profile6_log - INFO -    236     86.7 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-27 12:07:26,780 - memory_profile6_log - INFO -    237                             

2018-04-27 12:07:26,782 - memory_profile6_log - INFO -    238                                 # ~~~ Begin collecting data ~~~

2018-04-27 12:07:26,782 - memory_profile6_log - INFO -    239     86.7 MiB      0.0 MiB       t0 = time.time()

2018-04-27 12:07:26,782 - memory_profile6_log - INFO -    240     86.7 MiB      0.0 MiB       datalist = []

2018-04-27 12:07:26,782 - memory_profile6_log - INFO -    241                             

2018-04-27 12:07:26,782 - memory_profile6_log - INFO -    242    498.0 MiB      0.0 MiB       def _getBig(procdate, client):

2018-04-27 12:07:26,782 - memory_profile6_log - INFO -    243    498.0 MiB      0.0 MiB           logger.info("Collecting training data for date: %s", procdate)

2018-04-27 12:07:26,782 - memory_profile6_log - INFO -    244                                     # ~ get genuine news interest ~

2018-04-27 12:07:26,782 - memory_profile6_log - INFO -    245    498.0 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-27 12:07:26,782 - memory_profile6_log - INFO -    246                             

2018-04-27 12:07:26,782 - memory_profile6_log - INFO -    247                                     # safe handling of query parameter

2018-04-27 12:07:26,786 - memory_profile6_log - INFO -    248                                     query_params = [

2018-04-27 12:07:26,788 - memory_profile6_log - INFO -    249    498.0 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', procdate)

2018-04-27 12:07:26,788 - memory_profile6_log - INFO -    250                                     ]

2018-04-27 12:07:26,788 - memory_profile6_log - INFO -    251                             

2018-04-27 12:07:26,789 - memory_profile6_log - INFO -    252    498.0 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-27 12:07:26,789 - memory_profile6_log - INFO -    253                                     # temp_df = load_bigquery(client, query_fit + query_fit_where, job_config)

2018-04-27 12:07:26,789 - memory_profile6_log - INFO -    254    569.8 MiB    483.1 MiB           temp_df = loadBQ(client, query_fit + query_fit_where, job_config)

2018-04-27 12:07:26,789 - memory_profile6_log - INFO -    255                             

2018-04-27 12:07:26,789 - memory_profile6_log - INFO -    256    569.8 MiB      0.0 MiB           if temp_df.empty:

2018-04-27 12:07:26,789 - memory_profile6_log - INFO -    257                                         logger.info("%s data is empty!", procdate)

2018-04-27 12:07:26,789 - memory_profile6_log - INFO -    258                                         return None

2018-04-27 12:07:26,790 - memory_profile6_log - INFO -    259                                     else:

2018-04-27 12:07:26,790 - memory_profile6_log - INFO -    260    569.8 MiB      0.0 MiB               logger.info("getting total: %d training data(genuine interest) for date: %s" % (len(temp_df), procdate))

2018-04-27 12:07:26,790 - memory_profile6_log - INFO -    261    569.8 MiB      0.0 MiB               if loadmp:

2018-04-27 12:07:26,790 - memory_profile6_log - INFO -    262                                             logger.info("Exiting: %s", mp.current_process().name)

2018-04-27 12:07:26,790 - memory_profile6_log - INFO -    263    569.8 MiB      0.0 MiB               return temp_df

2018-04-27 12:07:26,792 - memory_profile6_log - INFO -    264                             

2018-04-27 12:07:26,792 - memory_profile6_log - INFO -    265     86.7 MiB      0.0 MiB       if loadmp:

2018-04-27 12:07:26,792 - memory_profile6_log - INFO -    266                                     logger.info("Starting data fetch multiprocess..")

2018-04-27 12:07:26,792 - memory_profile6_log - INFO -    267                                     logger.info("number of process: %d", len(date_generated))

2018-04-27 12:07:26,792 - memory_profile6_log - INFO -    268                             

2018-04-27 12:07:26,792 - memory_profile6_log - INFO -    269                                     pool = mp.Pool(processes=cpu)

2018-04-27 12:07:26,792 - memory_profile6_log - INFO -    270                                     # multprocessA = [pool.apply(_getBig, args=(ndate.strftime("%Y-%m-%d"), )) for ndate in date_generated]

2018-04-27 12:07:26,792 - memory_profile6_log - INFO -    271                                     multprocessA = [pool.apply_async(_getBig, args=(ndate.strftime("%Y-%m-%d"), bq_client, )) for ndate in date_generated]

2018-04-27 12:07:26,792 - memory_profile6_log - INFO -    272                                     output_multprocessA = [p.get() for p in multprocessA]

2018-04-27 12:07:26,793 - memory_profile6_log - INFO -    273                             

2018-04-27 12:07:26,793 - memory_profile6_log - INFO -    274                                     for m in output_multprocessA:

2018-04-27 12:07:26,795 - memory_profile6_log - INFO -    275                                         if m is not None:

2018-04-27 12:07:26,795 - memory_profile6_log - INFO -    276                                             if not m.empty:

2018-04-27 12:07:26,795 - memory_profile6_log - INFO -    277                                                 datalist.append(m)

2018-04-27 12:07:26,795 - memory_profile6_log - INFO -    278                             

2018-04-27 12:07:26,801 - memory_profile6_log - INFO -    279                                     logger.info("len datalist: %d", len(datalist))

2018-04-27 12:07:26,802 - memory_profile6_log - INFO -    280                                     logger.info("All data fetch multiprocess done!!")

2018-04-27 12:07:26,802 - memory_profile6_log - INFO -    281                                 else:

2018-04-27 12:07:26,802 - memory_profile6_log - INFO -    282     86.7 MiB      0.0 MiB           logger.info("Starting data fetch iterative...")

2018-04-27 12:07:26,802 - memory_profile6_log - INFO -    283    569.8 MiB      0.0 MiB           for ndate in date_generated:

2018-04-27 12:07:26,802 - memory_profile6_log - INFO -    284    569.8 MiB      0.0 MiB               tframe = _getBig(ndate.strftime("%Y-%m-%d"), bq_client)

2018-04-27 12:07:26,803 - memory_profile6_log - INFO -    285    569.8 MiB      0.0 MiB               if tframe is not None:

2018-04-27 12:07:26,803 - memory_profile6_log - INFO -    286    569.8 MiB      0.0 MiB                   if not tframe.empty:

2018-04-27 12:07:26,803 - memory_profile6_log - INFO -    287    569.8 MiB      0.0 MiB                       datalist.append(tframe)

2018-04-27 12:07:26,803 - memory_profile6_log - INFO -    288                                         else: 

2018-04-27 12:07:26,803 - memory_profile6_log - INFO -    289                                             logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-27 12:07:26,805 - memory_profile6_log - INFO -    290    569.8 MiB      0.0 MiB           logger.info("len datalist: %d", len(datalist))

2018-04-27 12:07:26,805 - memory_profile6_log - INFO -    291    569.8 MiB      0.0 MiB           logger.info("All data fetch iterative done!!")

2018-04-27 12:07:26,805 - memory_profile6_log - INFO -    292                             

2018-04-27 12:07:26,805 - memory_profile6_log - INFO -    293    569.8 MiB      0.0 MiB       if len(datalist) > 1:

2018-04-27 12:07:26,805 - memory_profile6_log - INFO -    294    620.8 MiB     51.0 MiB           big_frame = pd.concat(datalist)

2018-04-27 12:07:26,805 - memory_profile6_log - INFO -    295    589.4 MiB    -31.4 MiB           del datalist

2018-04-27 12:07:26,805 - memory_profile6_log - INFO -    296                                 else:

2018-04-27 12:07:26,805 - memory_profile6_log - INFO -    297                                     big_frame = datalist

2018-04-27 12:07:26,805 - memory_profile6_log - INFO -    298    589.5 MiB      0.1 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-27 12:07:26,806 - memory_profile6_log - INFO -    299                             

2018-04-27 12:07:26,806 - memory_profile6_log - INFO -    300                                 # ~ get current news interest ~

2018-04-27 12:07:26,806 - memory_profile6_log - INFO -    301    589.5 MiB      0.0 MiB       if not cd:

2018-04-27 12:07:26,808 - memory_profile6_log - INFO -    302                                     logger.info("Collecting training data(current date interest)..")

2018-04-27 12:07:26,808 - memory_profile6_log - INFO -    303                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-27 12:07:26,808 - memory_profile6_log - INFO -    304                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-27 12:07:26,813 - memory_profile6_log - INFO -    305                                 else:

2018-04-27 12:07:26,813 - memory_profile6_log - INFO -    306    589.5 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-27 12:07:26,815 - memory_profile6_log - INFO -    307    589.5 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-27 12:07:26,815 - memory_profile6_log - INFO -    308                             

2018-04-27 12:07:26,815 - memory_profile6_log - INFO -    309                                     # safe handling of query parameter

2018-04-27 12:07:26,815 - memory_profile6_log - INFO -    310                                     query_params = [

2018-04-27 12:07:26,815 - memory_profile6_log - INFO -    311    589.5 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-27 12:07:26,815 - memory_profile6_log - INFO -    312                                     ]

2018-04-27 12:07:26,815 - memory_profile6_log - INFO -    313                             

2018-04-27 12:07:26,816 - memory_profile6_log - INFO -    314    589.5 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-27 12:07:26,816 - memory_profile6_log - INFO -    315    652.2 MiB     62.7 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-27 12:07:26,816 - memory_profile6_log - INFO -    316    652.2 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-27 12:07:26,818 - memory_profile6_log - INFO -    317                             

2018-04-27 12:07:26,818 - memory_profile6_log - INFO -    318    652.2 MiB      0.1 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-27 12:07:26,818 - memory_profile6_log - INFO -    319    652.2 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-27 12:07:26,818 - memory_profile6_log - INFO -    320    652.2 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-27 12:07:26,819 - memory_profile6_log - INFO -    321                             

2018-04-27 12:07:26,819 - memory_profile6_log - INFO -    322    652.2 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-27 12:07:26,819 - memory_profile6_log - INFO -    323    652.2 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-27 12:07:26,819 - memory_profile6_log - INFO -    324                             

2018-04-27 12:07:26,819 - memory_profile6_log - INFO -    325    652.2 MiB      0.0 MiB       return big_frame, current_frame

2018-04-27 12:07:26,819 - memory_profile6_log - INFO - 


2018-04-27 12:07:26,823 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-27 12:07:26,898 - memory_profile6_log - INFO - train on: 1077719 total history data(D(u, t))
2018-04-27 12:07:26,898 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-27 12:07:31,411 - memory_profile6_log - INFO - Len of model_fit: 1077719
2018-04-27 12:07:31,411 - memory_profile6_log - INFO - Len of df_dut: 1077719
2018-04-27 12:09:02,730 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-27 12:09:02,838 - memory_profile6_log - INFO - Total train time: 95.940s
2018-04-27 12:09:02,838 - memory_profile6_log - INFO - memory left before cleaning: 84.100 percent memory...
2018-04-27 12:09:02,839 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-27 12:09:02,841 - memory_profile6_log - INFO - deleting df_dut...
2018-04-27 12:09:02,841 - memory_profile6_log - INFO - deleting df_dt...
2018-04-27 12:09:02,842 - memory_profile6_log - INFO - deleting df_input...
2018-04-27 12:09:02,851 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-27 12:09:02,851 - memory_profile6_log - INFO - deleting df_current...
2018-04-27 12:09:02,851 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-27 12:09:02,911 - memory_profile6_log - INFO - deleting model_fit...
2018-04-27 12:09:02,913 - memory_profile6_log - INFO - deleting result...
2018-04-27 12:09:02,947 - memory_profile6_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 12:09:02,948 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 12:09:02,948 - memory_profile6_log - INFO - ================================================

2018-04-27 12:09:02,950 - memory_profile6_log - INFO -     99    642.5 MiB    642.5 MiB   @profile

2018-04-27 12:09:02,950 - memory_profile6_log - INFO -    100                             def main(df_input, df_current, current_date, G,

2018-04-27 12:09:02,950 - memory_profile6_log - INFO -    101                                      project_id, savetrain=False, multproc=True,

2018-04-27 12:09:02,950 - memory_profile6_log - INFO -    102                                      threshold=0, start_date=None, end_date=None,

2018-04-27 12:09:02,951 - memory_profile6_log - INFO -    103                                      saveto="datastore"):

2018-04-27 12:09:02,951 - memory_profile6_log - INFO -    104                                 """

2018-04-27 12:09:02,951 - memory_profile6_log - INFO -    105                                     Main cron method

2018-04-27 12:09:02,953 - memory_profile6_log - INFO -    106                                 """

2018-04-27 12:09:02,953 - memory_profile6_log - INFO -    107                                 # ~ Data Preprocessing ~

2018-04-27 12:09:02,953 - memory_profile6_log - INFO -    108                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-27 12:09:02,953 - memory_profile6_log - INFO -    109                                 # D(u, t)

2018-04-27 12:09:02,953 - memory_profile6_log - INFO -    110    642.5 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-27 12:09:02,953 - memory_profile6_log - INFO -    111    676.5 MiB     33.9 MiB       df_dut = df_input.copy(deep=True)

2018-04-27 12:09:02,953 - memory_profile6_log - INFO -    112    676.5 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-27 12:09:02,954 - memory_profile6_log - INFO -    113                             

2018-04-27 12:09:02,954 - memory_profile6_log - INFO -    114                                 # D(t)

2018-04-27 12:09:02,954 - memory_profile6_log - INFO -    115    683.7 MiB      7.3 MiB       df_dt = df_current.copy(deep=True)

2018-04-27 12:09:02,954 - memory_profile6_log - INFO -    116    683.7 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-27 12:09:02,954 - memory_profile6_log - INFO -    117                             

2018-04-27 12:09:02,956 - memory_profile6_log - INFO -    118                                 # ~~~~~~ Begin train ~~~~~~

2018-04-27 12:09:02,956 - memory_profile6_log - INFO -    119    683.7 MiB      0.0 MiB       t0 = time.time()

2018-04-27 12:09:02,956 - memory_profile6_log - INFO -    120    683.7 MiB      0.0 MiB       logger.info("train on: %d total history data(D(u, t))", len(df_dut))

2018-04-27 12:09:02,960 - memory_profile6_log - INFO -    121    683.7 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-27 12:09:02,961 - memory_profile6_log - INFO -    122                             

2018-04-27 12:09:02,961 - memory_profile6_log - INFO -    123                                 # instantiace class

2018-04-27 12:09:02,963 - memory_profile6_log - INFO -    124    683.7 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-27 12:09:02,963 - memory_profile6_log - INFO -    125                             

2018-04-27 12:09:02,963 - memory_profile6_log - INFO -    126                                 # ~~ Fit ~~

2018-04-27 12:09:02,963 - memory_profile6_log - INFO -    127                                 #   handling genuine news interest < current date

2018-04-27 12:09:02,964 - memory_profile6_log - INFO -    128    688.1 MiB      4.4 MiB       NB = BR.processX(df_dut)

2018-04-27 12:09:02,964 - memory_profile6_log - INFO -    129                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-27 12:09:02,966 - memory_profile6_log - INFO -    130                                 #   nanti dipindah ke class train utama

2018-04-27 12:09:02,966 - memory_profile6_log - INFO -    131    730.3 MiB     42.1 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-27 12:09:02,966 - memory_profile6_log - INFO -    132                                 """

2018-04-27 12:09:02,966 - memory_profile6_log - INFO -    133                                     num_y = total global click for category=ci on periode t

2018-04-27 12:09:02,966 - memory_profile6_log - INFO -    134                                     num_x = total click from user_U for category=ci on periode t

2018-04-27 12:09:02,967 - memory_profile6_log - INFO -    135                                 """

2018-04-27 12:09:02,967 - memory_profile6_log - INFO -    136    730.3 MiB      0.0 MiB       fitby_sigmant = True

2018-04-27 12:09:02,967 - memory_profile6_log - INFO -    137    730.3 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-27 12:09:02,967 - memory_profile6_log - INFO -    138    730.3 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-27 12:09:02,967 - memory_profile6_log - INFO -    139    772.4 MiB     42.1 MiB                            'is_general']]

2018-04-27 12:09:02,967 - memory_profile6_log - INFO -    140    772.4 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-27 12:09:02,967 - memory_profile6_log - INFO -    141    772.4 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-27 12:09:02,967 - memory_profile6_log - INFO -    142    873.6 MiB    101.2 MiB                          verbose=False)

2018-04-27 12:09:02,967 - memory_profile6_log - INFO -    143    873.6 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-27 12:09:02,967 - memory_profile6_log - INFO -    144    873.6 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-27 12:09:02,969 - memory_profile6_log - INFO -    145                             

2018-04-27 12:09:02,969 - memory_profile6_log - INFO -    146                                 # ~~ and Transform ~~

2018-04-27 12:09:02,969 - memory_profile6_log - INFO -    147                                 #   handling current news interest == current date

2018-04-27 12:09:02,969 - memory_profile6_log - INFO -    148    873.6 MiB      0.0 MiB       if df_dt.empty:

2018-04-27 12:09:02,974 - memory_profile6_log - INFO -    149                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-27 12:09:02,974 - memory_profile6_log - INFO -    150                                     return None

2018-04-27 12:09:02,976 - memory_profile6_log - INFO -    151    874.1 MiB      0.5 MiB       NB = BR.processX(df_dt)

2018-04-27 12:09:02,976 - memory_profile6_log - INFO -    152    884.7 MiB     10.6 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-27 12:09:02,976 - memory_profile6_log - INFO -    153                             

2018-04-27 12:09:02,977 - memory_profile6_log - INFO -    154    884.7 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-27 12:09:02,977 - memory_profile6_log - INFO -    155    851.3 MiB    -33.3 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-27 12:09:02,977 - memory_profile6_log - INFO -    156    851.3 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-27 12:09:02,979 - memory_profile6_log - INFO -    157    851.3 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-27 12:09:02,979 - memory_profile6_log - INFO -    158    914.2 MiB     62.9 MiB                                                     verbose=False)

2018-04-27 12:09:02,979 - memory_profile6_log - INFO -    159                                 # ~~~ filter is general and specific topic ~~~

2018-04-27 12:09:02,979 - memory_profile6_log - INFO -    160                                 # the idea is just we need to rerank every topic according

2018-04-27 12:09:02,979 - memory_profile6_log - INFO -    161                                 # user_id and and is_general by p0_posterior

2018-04-27 12:09:02,980 - memory_profile6_log - INFO -    162    914.2 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-27 12:09:02,980 - memory_profile6_log - INFO -    163    923.4 MiB      9.3 MiB                                     'is_general']].groupby(['topic_id',

2018-04-27 12:09:02,980 - memory_profile6_log - INFO -    164    914.3 MiB     -9.2 MiB                                                             'is_general']

2018-04-27 12:09:02,980 - memory_profile6_log - INFO -    165                                                                                      ).size().to_frame().reset_index()

2018-04-27 12:09:02,980 - memory_profile6_log - INFO -    166                             

2018-04-27 12:09:02,980 - memory_profile6_log - INFO -    167                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-27 12:09:02,980 - memory_profile6_log - INFO -    168                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-27 12:09:02,980 - memory_profile6_log - INFO -    169    914.4 MiB      0.1 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-27 12:09:02,980 - memory_profile6_log - INFO -    170                             

2018-04-27 12:09:02,982 - memory_profile6_log - INFO -    171                                 # ~ start by provide rank for each topic type ~

2018-04-27 12:09:02,982 - memory_profile6_log - INFO -    172   1004.7 MiB     90.4 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-27 12:09:02,982 - memory_profile6_log - INFO -    173   1013.9 MiB      9.2 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-27 12:09:02,982 - memory_profile6_log - INFO -    174                             

2018-04-27 12:09:02,982 - memory_profile6_log - INFO -    175                                 # ~ set threshold to filter output

2018-04-27 12:09:02,987 - memory_profile6_log - INFO -    176   1013.9 MiB      0.0 MiB       if threshold > 0:

2018-04-27 12:09:02,987 - memory_profile6_log - INFO -    177   1013.9 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-27 12:09:02,989 - memory_profile6_log - INFO -    178   1013.9 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-27 12:09:02,989 - memory_profile6_log - INFO -    179   1004.2 MiB     -9.7 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-27 12:09:02,990 - memory_profile6_log - INFO -    180                             

2018-04-27 12:09:02,990 - memory_profile6_log - INFO -    181                                 # print "\n", model_transform[['user_id', 'topic_id', 'p0_posterior',

2018-04-27 12:09:02,990 - memory_profile6_log - INFO -    182                                 #                             'is_general', 'rank']].loc[model_transform['user_id'].str.contains('1610c8e6b6466b-05a6ac1ba8c1b8-4323461-100200-1',

2018-04-27 12:09:02,990 - memory_profile6_log - INFO -    183                                 #                                                                                                 case=False)].head(45)

2018-04-27 12:09:02,990 - memory_profile6_log - INFO -    184                                 # print "len(model_transform): %d" % len(model_transform)

2018-04-27 12:09:02,990 - memory_profile6_log - INFO -    185                             

2018-04-27 12:09:02,990 - memory_profile6_log - INFO -    186   1004.2 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-27 12:09:02,990 - memory_profile6_log - INFO -    187   1004.2 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-27 12:09:02,992 - memory_profile6_log - INFO -    188                             

2018-04-27 12:09:02,992 - memory_profile6_log - INFO -    189   1004.2 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-27 12:09:02,992 - memory_profile6_log - INFO -    190                             

2018-04-27 12:09:02,992 - memory_profile6_log - INFO -    191   1004.2 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-27 12:09:02,993 - memory_profile6_log - INFO -    192   1004.2 MiB      0.0 MiB       del df_dut

2018-04-27 12:09:02,993 - memory_profile6_log - INFO -    193   1004.2 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-27 12:09:02,993 - memory_profile6_log - INFO -    194   1004.2 MiB      0.0 MiB       del df_dt

2018-04-27 12:09:02,993 - memory_profile6_log - INFO -    195   1004.2 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-27 12:09:02,993 - memory_profile6_log - INFO -    196   1004.2 MiB      0.0 MiB       del df_input

2018-04-27 12:09:02,993 - memory_profile6_log - INFO -    197   1004.2 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-27 12:09:02,993 - memory_profile6_log - INFO -    198    995.4 MiB     -8.8 MiB       del df_input_X

2018-04-27 12:09:02,993 - memory_profile6_log - INFO -    199    995.4 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-27 12:09:02,993 - memory_profile6_log - INFO -    200    995.4 MiB      0.0 MiB       del df_current

2018-04-27 12:09:02,994 - memory_profile6_log - INFO -    201    995.4 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-27 12:09:02,994 - memory_profile6_log - INFO -    202    995.4 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-27 12:09:02,994 - memory_profile6_log - INFO -    203    995.4 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-27 12:09:02,994 - memory_profile6_log - INFO -    204    895.7 MiB    -99.7 MiB       del model_fit

2018-04-27 12:09:02,994 - memory_profile6_log - INFO -    205    895.7 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-27 12:09:03,000 - memory_profile6_log - INFO -    206    895.7 MiB      0.0 MiB       del result

2018-04-27 12:09:03,000 - memory_profile6_log - INFO -    207    895.7 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-27 12:09:03,002 - memory_profile6_log - INFO -    208                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-27 12:09:03,002 - memory_profile6_log - INFO -    209                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-27 12:09:03,003 - memory_profile6_log - INFO -    210    895.7 MiB      0.0 MiB       if savetrain:

2018-04-27 12:09:03,003 - memory_profile6_log - INFO -    211                                     model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-27 12:09:03,003 - memory_profile6_log - INFO -    212                                     del model_transform

2018-04-27 12:09:03,003 - memory_profile6_log - INFO -    213                                     logger.info("deleting model_transform...")

2018-04-27 12:09:03,003 - memory_profile6_log - INFO -    214                                     logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-27 12:09:03,005 - memory_profile6_log - INFO -    215                             

2018-04-27 12:09:03,005 - memory_profile6_log - INFO -    216                                     logger.info("Begin saving trained data...")

2018-04-27 12:09:03,005 - memory_profile6_log - INFO -    217                                     # print "\n", model_transform.head(5)

2018-04-27 12:09:03,005 - memory_profile6_log - INFO -    218                                     # ~ Place your code to save the training model here ~

2018-04-27 12:09:03,005 - memory_profile6_log - INFO -    219                                     if str(saveto).lower() == "datastore":

2018-04-27 12:09:03,006 - memory_profile6_log - INFO -    220                                         logger.info("Using google datastore as storage...")

2018-04-27 12:09:03,006 - memory_profile6_log - INFO -    221                                         if multproc:

2018-04-27 12:09:03,006 - memory_profile6_log - INFO -    222                                             mh.saveDatastoreMP(model_transformsv)

2018-04-27 12:09:03,006 - memory_profile6_log - INFO -    223                                             # mh.saveDataStorePutMulti(model_transform[['user_id', 'topic_id', 'is_general', 'rank']])

2018-04-27 12:09:03,006 - memory_profile6_log - INFO -    224                                         else:

2018-04-27 12:09:03,006 - memory_profile6_log - INFO -    225                                             mh.saveDatastore(model_transformsv)

2018-04-27 12:09:03,006 - memory_profile6_log - INFO -    226                                             

2018-04-27 12:09:03,006 - memory_profile6_log - INFO -    227                                     elif str(saveto).lower() == "elastic":

2018-04-27 12:09:03,006 - memory_profile6_log - INFO -    228                                         logger.info("Using ElasticSearch as storage...")

2018-04-27 12:09:03,007 - memory_profile6_log - INFO -    229                                         mh.saveElasticS(model_transformsv)

2018-04-27 12:09:03,007 - memory_profile6_log - INFO -    230                             

2018-04-27 12:09:03,007 - memory_profile6_log - INFO -    231    895.7 MiB      0.0 MiB       return

2018-04-27 12:09:03,013 - memory_profile6_log - INFO - 


2018-04-27 12:09:03,013 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
2018-04-27 14:05:25,558 - memory_profile6_log - INFO - Generating date range with N: 3
2018-04-27 14:05:25,561 - memory_profile6_log - INFO - date_generated: 
2018-04-27 14:05:25,562 - memory_profile6_log - INFO -  
2018-04-27 14:05:25,562 - memory_profile6_log - INFO - [datetime.datetime(2018, 4, 12, 0, 0), datetime.datetime(2018, 4, 13, 0, 0), datetime.datetime(2018, 4, 14, 0, 0)]
2018-04-27 14:05:25,562 - memory_profile6_log - INFO - 

2018-04-27 14:05:25,563 - memory_profile6_log - INFO - using current date: 2018-04-15
2018-04-27 14:05:25,565 - memory_profile6_log - INFO - using start date: 2018-04-12 00:00:00
2018-04-27 14:05:25,565 - memory_profile6_log - INFO - using end date: 2018-04-14
2018-04-27 14:05:25,677 - memory_profile6_log - INFO - Starting data fetch iterative...
2018-04-27 14:05:25,678 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-12
2018-04-27 14:07:59,473 - memory_profile6_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 14:07:59,474 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 14:07:59,476 - memory_profile6_log - INFO - ================================================

2018-04-27 14:07:59,476 - memory_profile6_log - INFO -     67     86.4 MiB     86.4 MiB   @profile

2018-04-27 14:07:59,477 - memory_profile6_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-27 14:07:59,480 - memory_profile6_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-27 14:07:59,480 - memory_profile6_log - INFO -     70     86.4 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 14:07:59,482 - memory_profile6_log - INFO -     71     86.4 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 14:07:59,482 - memory_profile6_log - INFO -     72                             

2018-04-27 14:07:59,483 - memory_profile6_log - INFO -     73     89.5 MiB      3.0 MiB       result  = client.query(query, job_config=job_config)

2018-04-27 14:07:59,486 - memory_profile6_log - INFO -     74     89.6 MiB      0.1 MiB       rows = result.result()

2018-04-27 14:07:59,486 - memory_profile6_log - INFO -     75                             

2018-04-27 14:07:59,487 - memory_profile6_log - INFO -     76     89.6 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-27 14:07:59,489 - memory_profile6_log - INFO -     77                                 

2018-04-27 14:07:59,490 - memory_profile6_log - INFO -     78     89.6 MiB      0.0 MiB       data = []

2018-04-27 14:07:59,490 - memory_profile6_log - INFO -     79    473.0 MiB    295.0 MiB       for row in rows:

2018-04-27 14:07:59,492 - memory_profile6_log - INFO -     80    473.0 MiB    -20.0 MiB           data.append(list(row))

2018-04-27 14:07:59,493 - memory_profile6_log - INFO -     81                             

2018-04-27 14:07:59,493 - memory_profile6_log - INFO -     82    487.4 MiB     14.4 MiB       df = pd.DataFrame(data, columns=col_name)

2018-04-27 14:07:59,494 - memory_profile6_log - INFO -     83    487.4 MiB      0.0 MiB       del rows

2018-04-27 14:07:59,497 - memory_profile6_log - INFO -     84    487.4 MiB      0.0 MiB       del result

2018-04-27 14:07:59,500 - memory_profile6_log - INFO -     85    394.7 MiB    -92.7 MiB       del data

2018-04-27 14:07:59,502 - memory_profile6_log - INFO -     86                              

2018-04-27 14:07:59,503 - memory_profile6_log - INFO -     87    394.7 MiB      0.0 MiB       return df

2018-04-27 14:07:59,505 - memory_profile6_log - INFO - 


2018-04-27 14:07:59,506 - memory_profile6_log - INFO - getting total: 462579 training data(genuine interest) for date: 2018-04-12
2018-04-27 14:07:59,507 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-13
2018-04-27 14:10:02,680 - memory_profile6_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 14:10:02,681 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 14:10:02,683 - memory_profile6_log - INFO - ================================================

2018-04-27 14:10:02,684 - memory_profile6_log - INFO -     67    394.7 MiB    394.7 MiB   @profile

2018-04-27 14:10:02,684 - memory_profile6_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-27 14:10:02,686 - memory_profile6_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-27 14:10:02,687 - memory_profile6_log - INFO -     70    394.7 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 14:10:02,687 - memory_profile6_log - INFO -     71    394.7 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 14:10:02,690 - memory_profile6_log - INFO -     72                             

2018-04-27 14:10:02,693 - memory_profile6_log - INFO -     73    394.7 MiB      0.0 MiB       result  = client.query(query, job_config=job_config)

2018-04-27 14:10:02,694 - memory_profile6_log - INFO -     74    394.7 MiB      0.0 MiB       rows = result.result()

2018-04-27 14:10:02,697 - memory_profile6_log - INFO -     75                             

2018-04-27 14:10:02,697 - memory_profile6_log - INFO -     76    394.7 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-27 14:10:02,698 - memory_profile6_log - INFO -     77                                 

2018-04-27 14:10:02,700 - memory_profile6_log - INFO -     78    394.7 MiB      0.0 MiB       data = []

2018-04-27 14:10:02,703 - memory_profile6_log - INFO -     79    556.9 MiB     86.6 MiB       for row in rows:

2018-04-27 14:10:02,703 - memory_profile6_log - INFO -     80    556.9 MiB    -20.1 MiB           data.append(list(row))

2018-04-27 14:10:02,706 - memory_profile6_log - INFO -     81                             

2018-04-27 14:10:02,707 - memory_profile6_log - INFO -     82    568.0 MiB     11.1 MiB       df = pd.DataFrame(data, columns=col_name)

2018-04-27 14:10:02,709 - memory_profile6_log - INFO -     83    568.0 MiB      0.0 MiB       del rows

2018-04-27 14:10:02,710 - memory_profile6_log - INFO -     84    568.0 MiB      0.0 MiB       del result

2018-04-27 14:10:02,713 - memory_profile6_log - INFO -     85    492.9 MiB    -75.1 MiB       del data

2018-04-27 14:10:02,713 - memory_profile6_log - INFO -     86                              

2018-04-27 14:10:02,714 - memory_profile6_log - INFO -     87    492.9 MiB      0.0 MiB       return df

2018-04-27 14:10:02,717 - memory_profile6_log - INFO - 


2018-04-27 14:10:02,717 - memory_profile6_log - INFO - getting total: 361145 training data(genuine interest) for date: 2018-04-13
2018-04-27 14:10:02,719 - memory_profile6_log - INFO - Collecting training data for date: 2018-04-14
2018-04-27 14:11:26,532 - memory_profile6_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 14:11:26,532 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 14:11:26,535 - memory_profile6_log - INFO - ================================================

2018-04-27 14:11:26,535 - memory_profile6_log - INFO -     67    492.9 MiB    492.9 MiB   @profile

2018-04-27 14:11:26,536 - memory_profile6_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-27 14:11:26,539 - memory_profile6_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-27 14:11:26,542 - memory_profile6_log - INFO -     70    492.9 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 14:11:26,543 - memory_profile6_log - INFO -     71    492.9 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 14:11:26,545 - memory_profile6_log - INFO -     72                             

2018-04-27 14:11:26,546 - memory_profile6_log - INFO -     73    492.9 MiB      0.0 MiB       result  = client.query(query, job_config=job_config)

2018-04-27 14:11:26,546 - memory_profile6_log - INFO -     74    492.9 MiB      0.0 MiB       rows = result.result()

2018-04-27 14:11:26,549 - memory_profile6_log - INFO -     75                             

2018-04-27 14:11:26,549 - memory_profile6_log - INFO -     76    492.9 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-27 14:11:26,549 - memory_profile6_log - INFO -     77                                 

2018-04-27 14:11:26,551 - memory_profile6_log - INFO -     78    492.9 MiB      0.0 MiB       data = []

2018-04-27 14:11:26,555 - memory_profile6_log - INFO -     79    619.2 MiB     72.0 MiB       for row in rows:

2018-04-27 14:11:26,555 - memory_profile6_log - INFO -     80    619.2 MiB    -14.4 MiB           data.append(list(row))

2018-04-27 14:11:26,556 - memory_profile6_log - INFO -     81                             

2018-04-27 14:11:26,558 - memory_profile6_log - INFO -     82    627.0 MiB      7.8 MiB       df = pd.DataFrame(data, columns=col_name)

2018-04-27 14:11:26,559 - memory_profile6_log - INFO -     83    627.0 MiB      0.0 MiB       del rows

2018-04-27 14:11:26,559 - memory_profile6_log - INFO -     84    627.0 MiB      0.0 MiB       del result

2018-04-27 14:11:26,559 - memory_profile6_log - INFO -     85    567.7 MiB    -59.3 MiB       del data

2018-04-27 14:11:26,561 - memory_profile6_log - INFO -     86                              

2018-04-27 14:11:26,562 - memory_profile6_log - INFO -     87    567.7 MiB      0.0 MiB       return df

2018-04-27 14:11:26,565 - memory_profile6_log - INFO - 


2018-04-27 14:11:26,565 - memory_profile6_log - INFO - getting total: 253995 training data(genuine interest) for date: 2018-04-14
2018-04-27 14:11:26,569 - memory_profile6_log - INFO - len datalist: 3
2018-04-27 14:11:26,571 - memory_profile6_log - INFO - All data fetch iterative done!!
2018-04-27 14:11:26,674 - memory_profile6_log - INFO - Collecting training data(current date interest) using argument: 2018-04-15
2018-04-27 14:12:42,563 - memory_profile6_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 14:12:42,565 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 14:12:42,566 - memory_profile6_log - INFO - ================================================

2018-04-27 14:12:42,568 - memory_profile6_log - INFO -     67    586.8 MiB    586.8 MiB   @profile

2018-04-27 14:12:42,569 - memory_profile6_log - INFO -     68                             def loadBQ(client, query, job_config, tabletype="history"):

2018-04-27 14:12:42,571 - memory_profile6_log - INFO -     69                                 # https://github.com/ofek/pypinfo/issues/27

2018-04-27 14:12:42,572 - memory_profile6_log - INFO -     70    586.8 MiB      0.0 MiB       job_config.use_legacy_sql = False

2018-04-27 14:12:42,572 - memory_profile6_log - INFO -     71    586.8 MiB      0.0 MiB       job_config.allowLargeResults = True

2018-04-27 14:12:42,575 - memory_profile6_log - INFO -     72                             

2018-04-27 14:12:42,576 - memory_profile6_log - INFO -     73    586.8 MiB      0.0 MiB       result  = client.query(query, job_config=job_config)

2018-04-27 14:12:42,578 - memory_profile6_log - INFO -     74    586.8 MiB      0.0 MiB       rows = result.result()

2018-04-27 14:12:42,579 - memory_profile6_log - INFO -     75                             

2018-04-27 14:12:42,582 - memory_profile6_log - INFO -     76    586.8 MiB      0.0 MiB       col_name = [field.name for field in rows.schema]

2018-04-27 14:12:42,582 - memory_profile6_log - INFO -     77                                 

2018-04-27 14:12:42,584 - memory_profile6_log - INFO -     78    586.8 MiB      0.0 MiB       data = []

2018-04-27 14:12:42,585 - memory_profile6_log - INFO -     79    707.6 MiB     88.7 MiB       for row in rows:

2018-04-27 14:12:42,588 - memory_profile6_log - INFO -     80    707.6 MiB      8.1 MiB           data.append(list(row))

2018-04-27 14:12:42,589 - memory_profile6_log - INFO -     81                             

2018-04-27 14:12:42,591 - memory_profile6_log - INFO -     82    714.7 MiB      7.1 MiB       df = pd.DataFrame(data, columns=col_name)

2018-04-27 14:12:42,592 - memory_profile6_log - INFO -     83    714.7 MiB      0.0 MiB       del rows

2018-04-27 14:12:42,594 - memory_profile6_log - INFO -     84    714.7 MiB      0.0 MiB       del result

2018-04-27 14:12:42,595 - memory_profile6_log - INFO -     85    653.2 MiB    -61.6 MiB       del data

2018-04-27 14:12:42,598 - memory_profile6_log - INFO -     86                              

2018-04-27 14:12:42,599 - memory_profile6_log - INFO -     87    653.2 MiB      0.0 MiB       return df

2018-04-27 14:12:42,601 - memory_profile6_log - INFO - 


2018-04-27 14:12:42,602 - memory_profile6_log - INFO - getting total: 230425 training data(current date interest) for date: 2018-04-15
2018-04-27 14:12:42,618 - memory_profile6_log - INFO - loading time of: 1308144 total genuine-current interest data ~ take 436.941s
2018-04-27 14:12:42,632 - memory_profile6_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 14:12:42,632 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 14:12:42,634 - memory_profile6_log - INFO - ================================================

2018-04-27 14:12:42,634 - memory_profile6_log - INFO -    233     86.3 MiB     86.3 MiB   @profile

2018-04-27 14:12:42,635 - memory_profile6_log - INFO -    234                             def preprocess(loadmp, cpu, cd, query_fit, date_generated):

2018-04-27 14:12:42,635 - memory_profile6_log - INFO -    235     86.4 MiB      0.1 MiB       bq_client = bigquery.Client()

2018-04-27 14:12:42,637 - memory_profile6_log - INFO -    236     86.4 MiB      0.0 MiB       job_config = bigquery.QueryJobConfig()

2018-04-27 14:12:42,638 - memory_profile6_log - INFO -    237                             

2018-04-27 14:12:42,641 - memory_profile6_log - INFO -    238                                 # ~~~ Begin collecting data ~~~

2018-04-27 14:12:42,642 - memory_profile6_log - INFO -    239     86.4 MiB      0.0 MiB       t0 = time.time()

2018-04-27 14:12:42,644 - memory_profile6_log - INFO -    240     86.4 MiB      0.0 MiB       datalist = []

2018-04-27 14:12:42,644 - memory_profile6_log - INFO -    241                             

2018-04-27 14:12:42,645 - memory_profile6_log - INFO -    242    492.9 MiB      0.0 MiB       def _getBig(procdate, client):

2018-04-27 14:12:42,647 - memory_profile6_log - INFO -    243    492.9 MiB      0.0 MiB           logger.info("Collecting training data for date: %s", procdate)

2018-04-27 14:12:42,648 - memory_profile6_log - INFO -    244                                     # ~ get genuine news interest ~

2018-04-27 14:12:42,648 - memory_profile6_log - INFO -    245    492.9 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-27 14:12:42,648 - memory_profile6_log - INFO -    246                             

2018-04-27 14:12:42,650 - memory_profile6_log - INFO -    247                                     # safe handling of query parameter

2018-04-27 14:12:42,650 - memory_profile6_log - INFO -    248                                     query_params = [

2018-04-27 14:12:42,654 - memory_profile6_log - INFO -    249    492.9 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', procdate)

2018-04-27 14:12:42,654 - memory_profile6_log - INFO -    250                                     ]

2018-04-27 14:12:42,655 - memory_profile6_log - INFO -    251                             

2018-04-27 14:12:42,657 - memory_profile6_log - INFO -    252    492.9 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-27 14:12:42,657 - memory_profile6_log - INFO -    253                                     # temp_df = load_bigquery(client, query_fit + query_fit_where, job_config)

2018-04-27 14:12:42,657 - memory_profile6_log - INFO -    254    567.7 MiB    481.3 MiB           temp_df = loadBQ(client, query_fit + query_fit_where, job_config)

2018-04-27 14:12:42,657 - memory_profile6_log - INFO -    255                             

2018-04-27 14:12:42,658 - memory_profile6_log - INFO -    256    567.7 MiB      0.0 MiB           if temp_df.empty:

2018-04-27 14:12:42,658 - memory_profile6_log - INFO -    257                                         logger.info("%s data is empty!", procdate)

2018-04-27 14:12:42,660 - memory_profile6_log - INFO -    258                                         return None

2018-04-27 14:12:42,661 - memory_profile6_log - INFO -    259                                     else:

2018-04-27 14:12:42,661 - memory_profile6_log - INFO -    260    567.7 MiB      0.0 MiB               logger.info("getting total: %d training data(genuine interest) for date: %s" % (len(temp_df), procdate))

2018-04-27 14:12:42,664 - memory_profile6_log - INFO -    261    567.7 MiB      0.0 MiB               if loadmp:

2018-04-27 14:12:42,665 - memory_profile6_log - INFO -    262                                             logger.info("Exiting: %s", mp.current_process().name)

2018-04-27 14:12:42,667 - memory_profile6_log - INFO -    263    567.7 MiB      0.0 MiB               return temp_df

2018-04-27 14:12:42,667 - memory_profile6_log - INFO -    264                             

2018-04-27 14:12:42,667 - memory_profile6_log - INFO -    265     86.4 MiB      0.0 MiB       if loadmp:

2018-04-27 14:12:42,668 - memory_profile6_log - INFO -    266                                     logger.info("Starting data fetch multiprocess..")

2018-04-27 14:12:42,670 - memory_profile6_log - INFO -    267                                     logger.info("number of process: %d", len(date_generated))

2018-04-27 14:12:42,671 - memory_profile6_log - INFO -    268                             

2018-04-27 14:12:42,671 - memory_profile6_log - INFO -    269                                     pool = mp.Pool(processes=cpu)

2018-04-27 14:12:42,673 - memory_profile6_log - INFO -    270                                     # multprocessA = [pool.apply(_getBig, args=(ndate.strftime("%Y-%m-%d"), )) for ndate in date_generated]

2018-04-27 14:12:42,673 - memory_profile6_log - INFO -    271                                     multprocessA = [pool.apply_async(_getBig, args=(ndate.strftime("%Y-%m-%d"), bq_client, )) for ndate in date_generated]

2018-04-27 14:12:42,677 - memory_profile6_log - INFO -    272                                     output_multprocessA = [p.get() for p in multprocessA]

2018-04-27 14:12:42,678 - memory_profile6_log - INFO -    273                             

2018-04-27 14:12:42,680 - memory_profile6_log - INFO -    274                                     for m in output_multprocessA:

2018-04-27 14:12:42,680 - memory_profile6_log - INFO -    275                                         if m is not None:

2018-04-27 14:12:42,681 - memory_profile6_log - INFO -    276                                             if not m.empty:

2018-04-27 14:12:42,683 - memory_profile6_log - INFO -    277                                                 datalist.append(m)

2018-04-27 14:12:42,683 - memory_profile6_log - INFO -    278                             

2018-04-27 14:12:42,684 - memory_profile6_log - INFO -    279                                     logger.info("len datalist: %d", len(datalist))

2018-04-27 14:12:42,684 - memory_profile6_log - INFO -    280                                     logger.info("All data fetch multiprocess done!!")

2018-04-27 14:12:42,686 - memory_profile6_log - INFO -    281                                 else:

2018-04-27 14:12:42,686 - memory_profile6_log - INFO -    282     86.4 MiB      0.0 MiB           logger.info("Starting data fetch iterative...")

2018-04-27 14:12:42,690 - memory_profile6_log - INFO -    283    567.7 MiB      0.0 MiB           for ndate in date_generated:

2018-04-27 14:12:42,690 - memory_profile6_log - INFO -    284    567.7 MiB      0.0 MiB               tframe = _getBig(ndate.strftime("%Y-%m-%d"), bq_client)

2018-04-27 14:12:42,691 - memory_profile6_log - INFO -    285    567.7 MiB      0.0 MiB               if tframe is not None:

2018-04-27 14:12:42,693 - memory_profile6_log - INFO -    286    567.7 MiB      0.0 MiB                   if not tframe.empty:

2018-04-27 14:12:42,694 - memory_profile6_log - INFO -    287    567.7 MiB      0.0 MiB                       datalist.append(tframe)

2018-04-27 14:12:42,694 - memory_profile6_log - INFO -    288                                         else: 

2018-04-27 14:12:42,696 - memory_profile6_log - INFO -    289                                             logger.info("tframe for date: %s is empty", ndate.strftime("%Y-%m-%d"))

2018-04-27 14:12:42,696 - memory_profile6_log - INFO -    290    567.7 MiB      0.0 MiB           logger.info("len datalist: %d", len(datalist))

2018-04-27 14:12:42,697 - memory_profile6_log - INFO -    291    567.7 MiB      0.0 MiB           logger.info("All data fetch iterative done!!")

2018-04-27 14:12:42,697 - memory_profile6_log - INFO -    292                             

2018-04-27 14:12:42,697 - memory_profile6_log - INFO -    293    567.7 MiB      0.0 MiB       if len(datalist) > 1:

2018-04-27 14:12:42,701 - memory_profile6_log - INFO -    294    618.2 MiB     50.5 MiB           big_frame = pd.concat(datalist)

2018-04-27 14:12:42,701 - memory_profile6_log - INFO -    295    586.7 MiB    -31.4 MiB           del datalist

2018-04-27 14:12:42,703 - memory_profile6_log - INFO -    296                                 else:

2018-04-27 14:12:42,703 - memory_profile6_log - INFO -    297                                     big_frame = datalist

2018-04-27 14:12:42,704 - memory_profile6_log - INFO -    298    586.8 MiB      0.1 MiB       big_frame['date'] = pd.to_datetime(big_frame['date'], format='%Y-%m-%d', errors='coerce')

2018-04-27 14:12:42,706 - memory_profile6_log - INFO -    299                             

2018-04-27 14:12:42,707 - memory_profile6_log - INFO -    300                                 # ~ get current news interest ~

2018-04-27 14:12:42,707 - memory_profile6_log - INFO -    301    586.8 MiB      0.0 MiB       if not cd:

2018-04-27 14:12:42,707 - memory_profile6_log - INFO -    302                                     logger.info("Collecting training data(current date interest)..")

2018-04-27 14:12:42,707 - memory_profile6_log - INFO -    303                                     current_frame = loadBQ(bq_client, query_transform, job_config)

2018-04-27 14:12:42,707 - memory_profile6_log - INFO -    304                                     logger.info("getting total: %d training data(current date interest)" % (len(current_frame)))

2018-04-27 14:12:42,709 - memory_profile6_log - INFO -    305                                 else:

2018-04-27 14:12:42,709 - memory_profile6_log - INFO -    306    586.8 MiB      0.0 MiB           logger.info("Collecting training data(current date interest) using argument: %s", cd)

2018-04-27 14:12:42,713 - memory_profile6_log - INFO -    307    586.8 MiB      0.0 MiB           query_fit_where = "WHERE _PARTITIONTIME = TIMESTAMP(@start_date)"

2018-04-27 14:12:42,713 - memory_profile6_log - INFO -    308                             

2018-04-27 14:12:42,714 - memory_profile6_log - INFO -    309                                     # safe handling of query parameter

2018-04-27 14:12:42,716 - memory_profile6_log - INFO -    310                                     query_params = [

2018-04-27 14:12:42,717 - memory_profile6_log - INFO -    311    586.8 MiB      0.0 MiB               bigquery.ScalarQueryParameter('start_date', 'STRING', str_datecurrent)

2018-04-27 14:12:42,717 - memory_profile6_log - INFO -    312                                     ]

2018-04-27 14:12:42,717 - memory_profile6_log - INFO -    313                             

2018-04-27 14:12:42,717 - memory_profile6_log - INFO -    314    586.8 MiB      0.0 MiB           job_config.query_parameters = query_params

2018-04-27 14:12:42,719 - memory_profile6_log - INFO -    315    653.2 MiB     66.3 MiB           current_frame = loadBQ(bq_client, query_fit + query_fit_where, job_config)

2018-04-27 14:12:42,720 - memory_profile6_log - INFO -    316    653.2 MiB      0.0 MiB           logger.info("getting total: %d training data(current date interest) for date: %s" % (len(current_frame), str_datecurrent))

2018-04-27 14:12:42,720 - memory_profile6_log - INFO -    317                             

2018-04-27 14:12:42,720 - memory_profile6_log - INFO -    318    653.3 MiB      0.1 MiB       current_frame['date'] = date_current  # we need manually adding date, because table not support

2018-04-27 14:12:42,721 - memory_profile6_log - INFO -    319    653.3 MiB      0.0 MiB       current_frame['date'] = pd.to_datetime(current_frame['date'],

2018-04-27 14:12:42,726 - memory_profile6_log - INFO -    320    653.3 MiB      0.0 MiB                                              format='%Y-%m-%d', errors='coerce')

2018-04-27 14:12:42,727 - memory_profile6_log - INFO -    321                             

2018-04-27 14:12:42,729 - memory_profile6_log - INFO -    322    653.3 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-27 14:12:42,730 - memory_profile6_log - INFO -    323    653.3 MiB      0.0 MiB       logger.info("loading time of: %d total genuine-current interest data ~ take %0.3fs" % (len(current_frame) + len(big_frame), train_time))

2018-04-27 14:12:42,730 - memory_profile6_log - INFO -    324                             

2018-04-27 14:12:42,732 - memory_profile6_log - INFO -    325    653.3 MiB      0.0 MiB       return big_frame, current_frame

2018-04-27 14:12:42,733 - memory_profile6_log - INFO - 


2018-04-27 14:12:42,736 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~
2018-04-27 14:12:42,822 - memory_profile6_log - INFO - train on: 1077719 total history data(D(u, t))
2018-04-27 14:12:42,822 - memory_profile6_log - INFO - transform on: 230425 total current data(D(t))
2018-04-27 14:12:47,736 - memory_profile6_log - INFO - Len of model_fit: 1077719
2018-04-27 14:12:47,736 - memory_profile6_log - INFO - Len of df_dut: 1077719
2018-04-27 14:14:33,868 - memory_profile6_log - INFO - Filtering topic by top: 15 for each(General-Specific) !!
2018-04-27 14:14:33,993 - memory_profile6_log - INFO - Total train time: 111.172s
2018-04-27 14:14:33,994 - memory_profile6_log - INFO - memory left before cleaning: 86.400 percent memory...
2018-04-27 14:14:33,996 - memory_profile6_log - INFO - cleaning up some objects...
2018-04-27 14:14:33,996 - memory_profile6_log - INFO - deleting df_dut...
2018-04-27 14:14:33,999 - memory_profile6_log - INFO - deleting df_dt...
2018-04-27 14:14:34,000 - memory_profile6_log - INFO - deleting df_input...
2018-04-27 14:14:34,009 - memory_profile6_log - INFO - deleting df_input_X...
2018-04-27 14:14:34,012 - memory_profile6_log - INFO - deleting df_current...
2018-04-27 14:14:34,013 - memory_profile6_log - INFO - deleting map_topic_isgeneral...
2018-04-27 14:14:34,085 - memory_profile6_log - INFO - deleting model_fit...
2018-04-27 14:14:34,085 - memory_profile6_log - INFO - deleting result...
2018-04-27 14:14:34,130 - memory_profile6_log - INFO - Filename: .\legacy_bigquery.py


2018-04-27 14:14:34,131 - memory_profile6_log - INFO - Line #    Mem usage    Increment   Line Contents

2018-04-27 14:14:34,131 - memory_profile6_log - INFO - ================================================

2018-04-27 14:14:34,132 - memory_profile6_log - INFO -     99    643.6 MiB    643.6 MiB   @profile

2018-04-27 14:14:34,134 - memory_profile6_log - INFO -    100                             def main(df_input, df_current, current_date, G,

2018-04-27 14:14:34,135 - memory_profile6_log - INFO -    101                                      project_id, savetrain=False, multproc=True,

2018-04-27 14:14:34,137 - memory_profile6_log - INFO -    102                                      threshold=0, start_date=None, end_date=None,

2018-04-27 14:14:34,137 - memory_profile6_log - INFO -    103                                      saveto="datastore"):

2018-04-27 14:14:34,138 - memory_profile6_log - INFO -    104                                 """

2018-04-27 14:14:34,138 - memory_profile6_log - INFO -    105                                     Main cron method

2018-04-27 14:14:34,138 - memory_profile6_log - INFO -    106                                 """

2018-04-27 14:14:34,138 - memory_profile6_log - INFO -    107                                 # ~ Data Preprocessing ~

2018-04-27 14:14:34,142 - memory_profile6_log - INFO -    108                                 # split data train, untuk menggambarkan data berasal dari 2 table

2018-04-27 14:14:34,142 - memory_profile6_log - INFO -    109                                 # D(u, t)

2018-04-27 14:14:34,144 - memory_profile6_log - INFO -    110    643.6 MiB      0.0 MiB       logger.info("~~~~~~~~~~~~~~~~~~ Begin Main Process ~~~~~~~~~~~~~~~~~~~~~")

2018-04-27 14:14:34,145 - memory_profile6_log - INFO -    111    677.5 MiB     33.9 MiB       df_dut = df_input.copy(deep=True)

2018-04-27 14:14:34,145 - memory_profile6_log - INFO -    112    677.5 MiB      0.0 MiB       df_dut['date'] = pd.to_datetime(df_dut['date'], format='%Y-%m-%d', errors='coerce')

2018-04-27 14:14:34,147 - memory_profile6_log - INFO -    113                             

2018-04-27 14:14:34,147 - memory_profile6_log - INFO -    114                                 # D(t)

2018-04-27 14:14:34,148 - memory_profile6_log - INFO -    115    684.8 MiB      7.3 MiB       df_dt = df_current.copy(deep=True)

2018-04-27 14:14:34,148 - memory_profile6_log - INFO -    116    684.8 MiB      0.0 MiB       df_dt['date'] = pd.to_datetime(df_dt['date'], format='%Y-%m-%d', errors='coerce')

2018-04-27 14:14:34,148 - memory_profile6_log - INFO -    117                             

2018-04-27 14:14:34,148 - memory_profile6_log - INFO -    118                                 # ~~~~~~ Begin train ~~~~~~

2018-04-27 14:14:34,150 - memory_profile6_log - INFO -    119    684.8 MiB      0.0 MiB       t0 = time.time()

2018-04-27 14:14:34,150 - memory_profile6_log - INFO -    120    684.8 MiB      0.0 MiB       logger.info("train on: %d total history data(D(u, t))", len(df_dut))

2018-04-27 14:14:34,155 - memory_profile6_log - INFO -    121    684.8 MiB      0.0 MiB       logger.info("transform on: %d total current data(D(t))", len(df_dt))

2018-04-27 14:14:34,157 - memory_profile6_log - INFO -    122                             

2018-04-27 14:14:34,157 - memory_profile6_log - INFO -    123                                 # instantiace class

2018-04-27 14:14:34,157 - memory_profile6_log - INFO -    124    684.8 MiB      0.0 MiB       BR = GBayesTopicRecommender(current_date, G=G)

2018-04-27 14:14:34,161 - memory_profile6_log - INFO -    125                             

2018-04-27 14:14:34,161 - memory_profile6_log - INFO -    126                                 # ~~ Fit ~~

2018-04-27 14:14:34,163 - memory_profile6_log - INFO -    127                                 #   handling genuine news interest < current date

2018-04-27 14:14:34,163 - memory_profile6_log - INFO -    128    688.6 MiB      3.8 MiB       NB = BR.processX(df_dut)

2018-04-27 14:14:34,165 - memory_profile6_log - INFO -    129                                 # mulai dari sini sampai df_input_x setiap fit dan transform

2018-04-27 14:14:34,167 - memory_profile6_log - INFO -    130                                 #   nanti dipindah ke class train utama

2018-04-27 14:14:34,167 - memory_profile6_log - INFO -    131    731.6 MiB     43.0 MiB       result = pd.merge(df_dut, NB, on=['date', 'topic_id'])

2018-04-27 14:14:34,171 - memory_profile6_log - INFO -    132                                 """

2018-04-27 14:14:34,171 - memory_profile6_log - INFO -    133                                     num_y = total global click for category=ci on periode t

2018-04-27 14:14:34,171 - memory_profile6_log - INFO -    134                                     num_x = total click from user_U for category=ci on periode t

2018-04-27 14:14:34,171 - memory_profile6_log - INFO -    135                                 """

2018-04-27 14:14:34,173 - memory_profile6_log - INFO -    136    731.6 MiB      0.0 MiB       fitby_sigmant = True

2018-04-27 14:14:34,173 - memory_profile6_log - INFO -    137    731.6 MiB      0.0 MiB       df_input_X = result[['date', 'user_id',

2018-04-27 14:14:34,174 - memory_profile6_log - INFO -    138    731.6 MiB      0.0 MiB                            'topic_id', 'num_x', 'num_y',

2018-04-27 14:14:34,177 - memory_profile6_log - INFO -    139    773.7 MiB     42.1 MiB                            'is_general']]

2018-04-27 14:14:34,177 - memory_profile6_log - INFO -    140    773.7 MiB      0.0 MiB       model_fit = BR.fit(df_dut, df_input_X,

2018-04-27 14:14:34,180 - memory_profile6_log - INFO -    141    773.7 MiB      0.0 MiB                          full_bayes=False, use_sigmant=fitby_sigmant,

2018-04-27 14:14:34,181 - memory_profile6_log - INFO -    142    877.7 MiB    104.0 MiB                          verbose=False)

2018-04-27 14:14:34,181 - memory_profile6_log - INFO -    143    877.7 MiB      0.0 MiB       logger.info("Len of model_fit: %d", len(model_fit))

2018-04-27 14:14:34,183 - memory_profile6_log - INFO -    144    877.7 MiB      0.0 MiB       logger.info("Len of df_dut: %d", len(df_dut))

2018-04-27 14:14:34,184 - memory_profile6_log - INFO -    145                             

2018-04-27 14:14:34,184 - memory_profile6_log - INFO -    146                                 # ~~ and Transform ~~

2018-04-27 14:14:34,187 - memory_profile6_log - INFO -    147                                 #   handling current news interest == current date

2018-04-27 14:14:34,188 - memory_profile6_log - INFO -    148    877.7 MiB      0.0 MiB       if df_dt.empty:

2018-04-27 14:14:34,190 - memory_profile6_log - INFO -    149                                     print "Cek your df_dt, cannot be emtpy!!"

2018-04-27 14:14:34,190 - memory_profile6_log - INFO -    150                                     return None

2018-04-27 14:14:34,193 - memory_profile6_log - INFO -    151    878.1 MiB      0.4 MiB       NB = BR.processX(df_dt)

2018-04-27 14:14:34,194 - memory_profile6_log - INFO -    152    888.7 MiB     10.6 MiB       result = pd.merge(df_dt, NB, on=['date', 'topic_id'])

2018-04-27 14:14:34,198 - memory_profile6_log - INFO -    153                             

2018-04-27 14:14:34,200 - memory_profile6_log - INFO -    154    888.7 MiB      0.0 MiB       df_input_X = result[['date', 'user_id', 'topic_id',

2018-04-27 14:14:34,201 - memory_profile6_log - INFO -    155    855.4 MiB    -33.3 MiB                            'num_x', 'num_y', 'is_general']]

2018-04-27 14:14:34,203 - memory_profile6_log - INFO -    156    855.4 MiB      0.0 MiB       model_transform, fitted_models = BR.transform(df1=df_dt, df2=df_input_X,

2018-04-27 14:14:34,203 - memory_profile6_log - INFO -    157    855.4 MiB      0.0 MiB                                                     fitted_model=model_fit,

2018-04-27 14:14:34,204 - memory_profile6_log - INFO -    158    920.0 MiB     64.6 MiB                                                     verbose=False)

2018-04-27 14:14:34,206 - memory_profile6_log - INFO -    159                                 # ~~~ filter is general and specific topic ~~~

2018-04-27 14:14:34,209 - memory_profile6_log - INFO -    160                                 # the idea is just we need to rerank every topic according

2018-04-27 14:14:34,210 - memory_profile6_log - INFO -    161                                 # user_id and and is_general by p0_posterior

2018-04-27 14:14:34,211 - memory_profile6_log - INFO -    162    920.0 MiB      0.0 MiB       map_topic_isgeneral = df_dut[['topic_id',

2018-04-27 14:14:34,213 - memory_profile6_log - INFO -    163    929.2 MiB      9.3 MiB                                     'is_general']].groupby(['topic_id',

2018-04-27 14:14:34,213 - memory_profile6_log - INFO -    164    920.0 MiB     -9.2 MiB                                                             'is_general']

2018-04-27 14:14:34,213 - memory_profile6_log - INFO -    165                                                                                      ).size().to_frame().reset_index()

2018-04-27 14:14:34,214 - memory_profile6_log - INFO -    166                             

2018-04-27 14:14:34,216 - memory_profile6_log - INFO -    167                                 # map_topic_isgeneral = map_topic_isgeneral.loc[~map_topic_isgeneral.index.duplicated(keep='first')]

2018-04-27 14:14:34,216 - memory_profile6_log - INFO -    168                                 # model_transform = model_transform.loc[~model_transform.index.duplicated(keep='first')]

2018-04-27 14:14:34,217 - memory_profile6_log - INFO -    169    921.0 MiB      1.0 MiB       model_transform['is_general'] = model_transform['topic_id'].map(map_topic_isgeneral.drop_duplicates('topic_id').set_index('topic_id')['is_general'])

2018-04-27 14:14:34,221 - memory_profile6_log - INFO -    170                             

2018-04-27 14:14:34,223 - memory_profile6_log - INFO -    171                                 # ~ start by provide rank for each topic type ~

2018-04-27 14:14:34,224 - memory_profile6_log - INFO -    172   1008.2 MiB     87.3 MiB       model_transform['rank'] = model_transform.groupby(['user_id', 'is_general'])['p0_posterior'].rank(ascending=False)

2018-04-27 14:14:34,226 - memory_profile6_log - INFO -    173   1019.1 MiB     10.9 MiB       model_transform = model_transform.sort_values(['is_general', 'rank'], ascending=[False, True])

2018-04-27 14:14:34,227 - memory_profile6_log - INFO -    174                             

2018-04-27 14:14:34,229 - memory_profile6_log - INFO -    175                                 # ~ set threshold to filter output

2018-04-27 14:14:34,230 - memory_profile6_log - INFO -    176   1019.1 MiB      0.0 MiB       if threshold > 0:

2018-04-27 14:14:34,233 - memory_profile6_log - INFO -    177   1019.1 MiB      0.0 MiB           logger.info("Filtering topic by top: %d for each(General-Specific) !!", threshold)

2018-04-27 14:14:34,236 - memory_profile6_log - INFO -    178   1019.1 MiB      0.0 MiB           model_transform = model_transform[(model_transform['rank'] <= threshold) &

2018-04-27 14:14:34,239 - memory_profile6_log - INFO -    179   1007.7 MiB    -11.4 MiB                                             (model_transform['p0_posterior'] > 0.)]

2018-04-27 14:14:34,240 - memory_profile6_log - INFO -    180                             

2018-04-27 14:14:34,244 - memory_profile6_log - INFO -    181                                 # print "\n", model_transform[['user_id', 'topic_id', 'p0_posterior',

2018-04-27 14:14:34,246 - memory_profile6_log - INFO -    182                                 #                             'is_general', 'rank']].loc[model_transform['user_id'].str.contains('1610c8e6b6466b-05a6ac1ba8c1b8-4323461-100200-1',

2018-04-27 14:14:34,246 - memory_profile6_log - INFO -    183                                 #                                                                                                 case=False)].head(45)

2018-04-27 14:14:34,247 - memory_profile6_log - INFO -    184                                 # print "len(model_transform): %d" % len(model_transform)

2018-04-27 14:14:34,249 - memory_profile6_log - INFO -    185                             

2018-04-27 14:14:34,249 - memory_profile6_log - INFO -    186   1007.7 MiB      0.0 MiB       train_time = time.time() - t0

2018-04-27 14:14:34,250 - memory_profile6_log - INFO -    187   1007.7 MiB      0.0 MiB       logger.info("Total train time: %0.3fs", train_time)

2018-04-27 14:14:34,250 - memory_profile6_log - INFO -    188                             

2018-04-27 14:14:34,255 - memory_profile6_log - INFO -    189   1007.7 MiB      0.0 MiB       logger.info("memory left before cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-27 14:14:34,256 - memory_profile6_log - INFO -    190                             

2018-04-27 14:14:34,256 - memory_profile6_log - INFO -    191   1007.7 MiB      0.0 MiB       logger.info("cleaning up some objects...")

2018-04-27 14:14:34,259 - memory_profile6_log - INFO -    192   1007.7 MiB      0.0 MiB       del df_dut

2018-04-27 14:14:34,259 - memory_profile6_log - INFO -    193   1007.7 MiB      0.0 MiB       logger.info("deleting df_dut...")

2018-04-27 14:14:34,259 - memory_profile6_log - INFO -    194   1007.7 MiB      0.0 MiB       del df_dt

2018-04-27 14:14:34,260 - memory_profile6_log - INFO -    195   1007.7 MiB      0.0 MiB       logger.info("deleting df_dt...")

2018-04-27 14:14:34,260 - memory_profile6_log - INFO -    196   1007.7 MiB      0.0 MiB       del df_input

2018-04-27 14:14:34,262 - memory_profile6_log - INFO -    197   1007.7 MiB      0.0 MiB       logger.info("deleting df_input...")

2018-04-27 14:14:34,262 - memory_profile6_log - INFO -    198    998.9 MiB     -8.8 MiB       del df_input_X

2018-04-27 14:14:34,263 - memory_profile6_log - INFO -    199    998.9 MiB      0.0 MiB       logger.info("deleting df_input_X...")

2018-04-27 14:14:34,266 - memory_profile6_log - INFO -    200    998.9 MiB      0.0 MiB       del df_current

2018-04-27 14:14:34,269 - memory_profile6_log - INFO -    201    998.9 MiB      0.0 MiB       logger.info("deleting df_current...")

2018-04-27 14:14:34,270 - memory_profile6_log - INFO -    202    998.9 MiB      0.0 MiB       del map_topic_isgeneral

2018-04-27 14:14:34,270 - memory_profile6_log - INFO -    203    998.9 MiB      0.0 MiB       logger.info("deleting map_topic_isgeneral...")

2018-04-27 14:14:34,270 - memory_profile6_log - INFO -    204    899.2 MiB    -99.7 MiB       del model_fit

2018-04-27 14:14:34,272 - memory_profile6_log - INFO -    205    899.2 MiB      0.0 MiB       logger.info("deleting model_fit...")

2018-04-27 14:14:34,272 - memory_profile6_log - INFO -    206    899.2 MiB      0.0 MiB       del result

2018-04-27 14:14:34,272 - memory_profile6_log - INFO -    207    899.2 MiB      0.0 MiB       logger.info("deleting result...")

2018-04-27 14:14:34,273 - memory_profile6_log - INFO -    208                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-27 14:14:34,273 - memory_profile6_log - INFO -    209                                 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Save model Here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2018-04-27 14:14:34,273 - memory_profile6_log - INFO -    210    899.2 MiB      0.0 MiB       if savetrain:

2018-04-27 14:14:34,273 - memory_profile6_log - INFO -    211                                     model_transformsv = model_transform[['user_id', 'topic_id', 'is_general', 'rank']].copy(deep=True)

2018-04-27 14:14:34,273 - memory_profile6_log - INFO -    212                                     del model_transform

2018-04-27 14:14:34,278 - memory_profile6_log - INFO -    213                                     logger.info("deleting model_transform...")

2018-04-27 14:14:34,278 - memory_profile6_log - INFO -    214                                     logger.info("memory left after cleaning: %.3f percent memory...", psutil.virtual_memory().percent)

2018-04-27 14:14:34,279 - memory_profile6_log - INFO -    215                             

2018-04-27 14:14:34,279 - memory_profile6_log - INFO -    216                                     logger.info("Begin saving trained data...")

2018-04-27 14:14:34,279 - memory_profile6_log - INFO -    217                                     # print "\n", model_transform.head(5)

2018-04-27 14:14:34,279 - memory_profile6_log - INFO -    218                                     # ~ Place your code to save the training model here ~

2018-04-27 14:14:34,279 - memory_profile6_log - INFO -    219                                     if str(saveto).lower() == "datastore":

2018-04-27 14:14:34,280 - memory_profile6_log - INFO -    220                                         logger.info("Using google datastore as storage...")

2018-04-27 14:14:34,280 - memory_profile6_log - INFO -    221                                         if multproc:

2018-04-27 14:14:34,280 - memory_profile6_log - INFO -    222                                             mh.saveDatastoreMP(model_transformsv)

2018-04-27 14:14:34,280 - memory_profile6_log - INFO -    223                                             # mh.saveDataStorePutMulti(model_transform[['user_id', 'topic_id', 'is_general', 'rank']])

2018-04-27 14:14:34,280 - memory_profile6_log - INFO -    224                                         else:

2018-04-27 14:14:34,282 - memory_profile6_log - INFO -    225                                             mh.saveDatastore(model_transformsv)

2018-04-27 14:14:34,282 - memory_profile6_log - INFO -    226                                             

2018-04-27 14:14:34,282 - memory_profile6_log - INFO -    227                                     elif str(saveto).lower() == "elastic":

2018-04-27 14:14:34,282 - memory_profile6_log - INFO -    228                                         logger.info("Using ElasticSearch as storage...")

2018-04-27 14:14:34,282 - memory_profile6_log - INFO -    229                                         mh.saveElasticS(model_transformsv)

2018-04-27 14:14:34,282 - memory_profile6_log - INFO -    230                             

2018-04-27 14:14:34,283 - memory_profile6_log - INFO -    231    899.2 MiB      0.0 MiB       return

2018-04-27 14:14:34,283 - memory_profile6_log - INFO - 


2018-04-27 14:14:34,283 - memory_profile6_log - INFO - ~~~~~~~~~~~~~~ All Legacy Train operation is complete ~~~~~~~~~~~~~~~~~
